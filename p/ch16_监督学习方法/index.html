<!DOCTYPE html>
<html lang="en-us">
    <head><meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'><meta name='description' content='biopython官方教程学习笔记'><title>ch16_监督学习方法</title>

<link rel='canonical' href='https://example.com/p/ch16_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/'>

<link rel="stylesheet" href="/scss/style.min.css"><meta property='og:title' content='ch16_监督学习方法'>
<meta property='og:description' content='biopython官方教程学习笔记'>
<meta property='og:url' content='https://example.com/p/ch16_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/'>
<meta property='og:site_name' content='叶宇浩随记博客'>
<meta property='og:type' content='article'><meta property='article:section' content='Post' /><meta property='article:tag' content='tutorials' /><meta property='article:published_time' content='2021-08-16T00:00:00&#43;00:00'/><meta property='article:modified_time' content='2021-08-16T00:00:00&#43;00:00'/>
<meta name="twitter:title" content="ch16_监督学习方法">
<meta name="twitter:description" content="biopython官方教程学习笔记">
    </head>
    <body class="
    article-page has-toc
">
    <script>
        (function() {
            const colorSchemeKey = 'StackColorScheme';
            if(!localStorage.getItem(colorSchemeKey)){
                localStorage.setItem(colorSchemeKey, "auto");
            }
        })();
    </script><script>
    (function() {
        const colorSchemeKey = 'StackColorScheme';
        const colorSchemeItem = localStorage.getItem(colorSchemeKey);
        const supportDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches === true;

        if (colorSchemeItem == 'dark' || colorSchemeItem === 'auto' && supportDarkMode) {
            

            document.documentElement.dataset.scheme = 'dark';
        } else {
            document.documentElement.dataset.scheme = 'light';
        }
    })();
</script>
<div class="container main-container flex 
    
        extended
    
">
    
        <div id="article-toolbar">
            <a href="/" class="back-home">
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-chevron-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <polyline points="15 6 9 12 15 18" />
</svg>



                <span>Back</span>
            </a>
        </div>
    
<main class="main full-width">
    <article class="main-article">
    <header class="article-header">

    <div class="article-details">
    
    <header class="article-category">
        
            <a href="/categories/biopython/" style="background-color: #002be1; color: #fff;">
                生物编程
            </a>
        
            <a href="/categories/bioinformatic/" style="background-color: #7816ff; color: #fff;">
                生物信息学&amp;计算生物学
            </a>
        
            <a href="/categories/deeplearning/" style="background-color: #0f1503; color: #fff;">
                深度学习
            </a>
        
    </header>
    

    <h2 class="article-title">
        <a href="/p/ch16_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/">ch16_监督学习方法</a>
    </h2>

    
    <h3 class="article-subtitle">
        biopython官方教程学习笔记
    </h3>
    

    
    <footer class="article-time">
        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <path d="M11.795 21h-6.795a2 2 0 0 1 -2 -2v-12a2 2 0 0 1 2 -2h12a2 2 0 0 1 2 2v4" />
  <circle cx="18" cy="18" r="4" />
  <path d="M15 3v4" />
  <path d="M7 3v4" />
  <path d="M3 11h16" />
  <path d="M18 16.496v1.504l1 1" />
</svg>
                <time class="article-time--published">Aug 16, 2021</time>
            </div>
        

        
            <div>
                <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <polyline points="12 7 12 12 15 15" />
</svg>



                <time class="article-time--reading">
                    7 minute read
                </time>
            </div>
        
    </footer>
    
</div>
</header>

    <section class="article-content">
    <p><strong>biopython官方地址：</strong><a class="link" href="https://biopython.org/"  target="_blank" rel="noopener"
    ><strong>https://biopython.org/</strong></a></p>
<p><strong>github地址：</strong><a class="link" href="https://github.com/biopython/biopython/blob/master/README.rst"  target="_blank" rel="noopener"
    ><strong>https://github.com/biopython/biopython/blob/master/</strong></a></p>
<p><strong>中文版教程：</strong><a class="link" href="https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html"  target="_blank" rel="noopener"
    ><strong>https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html</strong></a></p>
<p><strong>biopython包的说明（具体到每个模块了）：</strong><a class="link" href="https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html"  target="_blank" rel="noopener"
    ><strong>https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html</strong></a></p>
<p>注意本章介绍的所有监督学习方法都需要先安装Numerical Python （numpy）。</p>
<h2 id="logistic回归模型">Logistic回归模型</h2>
<h3 id="背景和目的">背景和目的</h3>
<p>Logistic回归是一种监督学习方法，通过若干预测变量 <em>x__i</em> 的加权和来尝试将样本划分为 <em>K</em> 个不同类别。Logistic回归模型可用来计算预测变量的权重 β_i_ 。在Biopython中，logistic回归模型目前只实现了二类别（ <em>K</em> = 2 ）分类，而预测变量的数量没有限制。<br>
作为一个例子，我们试着预测细菌中的操纵子结构。一个操纵子是在一条DNA链上许多相邻基因组成的一个集合，可以被共同转录为一条mRNA分子。这条mRNA分子经翻译后产生多个不同的蛋白质。我们将以枯草芽孢杆菌的操纵子数据进行说明，它的一个操纵子平均包含2.4个基因。<br>
作为理解细菌的基因调节的第一步，我们需要知道其操纵子的结构。枯草芽孢杆菌大约10%的基因操纵子结构已经通过实验获知。剩下的90%的基因操纵子结构可以通过一种监督学习方法来预测。<br>
在这种监督学习方法中，我们需要选择某些与操纵子结构有关的容易度量的预测变量 <em>x__i</em> 。例如可以选择基因间碱基对距离来来作为其中一个预测变量。同一个操纵子中的相邻基因往往距离相对较近，而位于不同操纵子的相邻基因间通常具有更大的空间来容纳启动子和终止子序列。另一个预测变量可以基于基因表达量度。根据操纵子的定义，属于同一个操纵子的基因有相同的基因表达谱，而不同操纵子的两个基因的表达谱也不相同。在实际操作中，由于存在测量误差，对相同操纵子的基因表达轮廓的测量不会完全一致。为了测量基因表达轮廓的相似性，我们假设测量误差服从正态分布，然后计算对应的对数似然分值。<br>
现在我们有了两个预测变量，可以据此预测在同一条DNA链上两个相邻基因是否属于相同的操纵子： - _x_1 ：两基因间的碱基对数； - _x_2 ：两基因表达谱的相似度。<br>
在logistic回归模型中，我们使用这两个预测变量的加权和来计算一个联合得分 <em>S</em>：<br>
S=β0+β1x1+β2x2.S=β0+β1x1+β2x2.<br>
根据下面两组示例基因，logistic回归模型对参数 β0 ， β1, β2 给出合适的值： - OP: 相邻基因，相同DNA链，属于相同操纵子； - NOP: 相邻基因，相同DNA链，属于不同操纵子。<br>
在logistic回归模型中，属于某个类别的概率依赖于通过logistic函数得出的分数。对于这两类OP和NOP，相应概率可如下表述：<br>
<figure 
	
		class="gallery-image" 
		style="
			flex-grow: 486; 
			flex-basis: 1167px"
	>
	<a href="/p/ch16_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/picture/ch16_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.png" data-size="798x164">
		<img src="/p/ch16_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/picture/ch16_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95.png"
			width="798"
			height="164"
			srcset="/p/ch16_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/picture/ch16_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95_hu97bf3933ea80354749d642e9b9d55d3b_42016_480x0_resize_box_3.png 480w, /p/ch16_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/picture/ch16_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95_hu97bf3933ea80354749d642e9b9d55d3b_42016_1024x0_resize_box_3.png 1024w"
			loading="lazy"
			>
	</a>
	
</figure>
使用一组已知是否属于相同操纵子（OP类别）或不同操纵子（NOP类别）的基因对，通过最大化相应概率函数的对数似然值，我们可以计算权重 β0, β1, β2 。</p>
<h3 id="训练logistic回归模型">训练logistic回归模型</h3>
<p>已知类别(OP or NOP)的相邻基因对.如果两个基因相重叠，其基因间距离为负值</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">基因对</span>	<span class="n">基因间距离</span> <span class="p">(</span><span class="n">x1</span><span class="p">)</span>	<span class="n">基因表达得分</span> <span class="p">(</span><span class="n">x2</span><span class="p">)</span>	<span class="n">类别</span>
</span></span><span class="line"><span class="cl"><span class="n">cotJA</span> <span class="err">—</span> <span class="n">cotJB</span>	<span class="o">-</span><span class="mi">53</span>	<span class="o">-</span><span class="mf">200.78</span>	<span class="n">OP</span>
</span></span><span class="line"><span class="cl"><span class="n">yesK</span> <span class="err">—</span> <span class="n">yesL</span>	<span class="mi">117</span>	<span class="o">-</span><span class="mf">267.14</span>	<span class="n">OP</span>
</span></span><span class="line"><span class="cl"><span class="n">lplA</span> <span class="err">—</span> <span class="n">lplB</span>	<span class="mi">57</span>	<span class="o">-</span><span class="mf">163.47</span>	<span class="n">OP</span>
</span></span><span class="line"><span class="cl"><span class="n">lplB</span> <span class="err">—</span> <span class="n">lplC</span>	<span class="mi">16</span>	<span class="o">-</span><span class="mf">190.30</span>	<span class="n">OP</span>
</span></span><span class="line"><span class="cl"><span class="n">lplC</span> <span class="err">—</span> <span class="n">lplD</span>	<span class="mi">11</span>	<span class="o">-</span><span class="mf">220.94</span>	<span class="n">OP</span>
</span></span><span class="line"><span class="cl"><span class="n">lplD</span> <span class="err">—</span> <span class="n">yetF</span>	<span class="mi">85</span>	<span class="o">-</span><span class="mf">193.94</span>	<span class="n">OP</span>
</span></span><span class="line"><span class="cl"><span class="n">yfmT</span> <span class="err">—</span> <span class="n">yfmS</span>	<span class="mi">16</span>	<span class="o">-</span><span class="mf">182.71</span>	<span class="n">OP</span>
</span></span><span class="line"><span class="cl"><span class="n">yfmF</span> <span class="err">—</span> <span class="n">yfmE</span>	<span class="mi">15</span>	<span class="o">-</span><span class="mf">180.41</span>	<span class="n">OP</span>
</span></span><span class="line"><span class="cl"><span class="n">citS</span> <span class="err">—</span> <span class="n">citT</span>	<span class="o">-</span><span class="mi">26</span>	<span class="o">-</span><span class="mf">181.73</span>	<span class="n">OP</span>
</span></span><span class="line"><span class="cl"><span class="n">citM</span> <span class="err">—</span> <span class="n">yflN</span>	<span class="mi">58</span>	<span class="o">-</span><span class="mf">259.87</span>	<span class="n">OP</span>
</span></span><span class="line"><span class="cl"><span class="n">yfiI</span> <span class="err">—</span> <span class="n">yfiJ</span>	<span class="mi">126</span>	<span class="o">-</span><span class="mf">414.53</span>	<span class="n">NOP</span>
</span></span><span class="line"><span class="cl"><span class="n">lipB</span> <span class="err">—</span> <span class="n">yfiQ</span>	<span class="mi">191</span>	<span class="o">-</span><span class="mf">249.57</span>	<span class="n">NOP</span>
</span></span><span class="line"><span class="cl"><span class="n">yfiU</span> <span class="err">—</span> <span class="n">yfiV</span>	<span class="mi">113</span>	<span class="o">-</span><span class="mf">265.28</span>	<span class="n">NOP</span>
</span></span><span class="line"><span class="cl"><span class="n">yfhH</span> <span class="err">—</span> <span class="n">yfhI</span>	<span class="mi">145</span>	<span class="o">-</span><span class="mf">312.99</span>	<span class="n">NOP</span>
</span></span><span class="line"><span class="cl"><span class="n">cotY</span> <span class="err">—</span> <span class="n">cotX</span>	<span class="mi">154</span>	<span class="o">-</span><span class="mf">213.83</span>	<span class="n">NOP</span>
</span></span><span class="line"><span class="cl"><span class="n">yjoB</span> <span class="err">—</span> <span class="n">rapA</span>	<span class="mi">147</span>	<span class="o">-</span><span class="mf">380.85</span>	<span class="n">NOP</span>
</span></span><span class="line"><span class="cl"><span class="n">ptsI</span> <span class="err">—</span> <span class="n">splA</span>	<span class="mi">93</span>	<span class="o">-</span><span class="mf">291.13</span>	<span class="n">NO</span>
</span></span></code></pre></div><p>列出了枯草芽孢杆菌的一些基因对，这些基因的操纵子结构已知。让我们根据表中的这些数据来计算其logistic回归模型：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">Bio</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">xs</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mi">53</span><span class="p">,</span> <span class="o">-</span><span class="mf">200.78</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mi">117</span><span class="p">,</span> <span class="o">-</span><span class="mf">267.14</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mi">57</span><span class="p">,</span> <span class="o">-</span><span class="mf">163.47</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="o">-</span><span class="mf">190.30</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mi">11</span><span class="p">,</span> <span class="o">-</span><span class="mf">220.94</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mi">85</span><span class="p">,</span> <span class="o">-</span><span class="mf">193.94</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="o">-</span><span class="mf">182.71</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mi">15</span><span class="p">,</span> <span class="o">-</span><span class="mf">180.41</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="o">-</span><span class="mi">26</span><span class="p">,</span> <span class="o">-</span><span class="mf">181.73</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mi">58</span><span class="p">,</span> <span class="o">-</span><span class="mf">259.87</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mi">126</span><span class="p">,</span> <span class="o">-</span><span class="mf">414.53</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mi">191</span><span class="p">,</span> <span class="o">-</span><span class="mf">249.57</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mi">113</span><span class="p">,</span> <span class="o">-</span><span class="mf">265.28</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mi">145</span><span class="p">,</span> <span class="o">-</span><span class="mf">312.99</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mi">154</span><span class="p">,</span> <span class="o">-</span><span class="mf">213.83</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mi">147</span><span class="p">,</span> <span class="o">-</span><span class="mf">380.85</span><span class="p">],</span>
</span></span><span class="line"><span class="cl">          <span class="p">[</span><span class="mi">93</span><span class="p">,</span> <span class="o">-</span><span class="mf">291.13</span><span class="p">]]</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">ys</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="mi">1</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="mi">0</span><span class="p">,</span>
</span></span><span class="line"><span class="cl">          <span class="mi">0</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">)</span>
</span></span></code></pre></div><p>这里， xs 和 ys 是训练数据： xs 包含每个基因对的预测变量， ys 指定是否这个基因对属于相同操纵子（ 1 ，类别OP）或不同操纵子（0，类别NOP）。Logistic回归模型结果存储在 model 中，包含权重 β0, β1, and β2:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">model</span><span class="o">.</span><span class="n">beta</span>
</span></span><span class="line"><span class="cl"><span class="p">[</span><span class="mf">8.9830290157144681</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.035968960444850887</span><span class="p">,</span> <span class="mf">0.02181395662983519</span><span class="p">]</span>
</span></span></code></pre></div><p>注意 β1 是负的，这是因为具有更短基因间距离的基因对有更高的概率属于相同操纵子（类别OP）。另一方面， β2 为正，因为属于相同操纵子的基因对通常有更高的基因表达谱相似性得分。参数 β0 是正值是因为在这个训练数据中操纵子基因对占据大多数。<br>
函数 train 有两个可选参数： update_fn 和 typecode 。 update_fn 可用来指定一个回调函数，以迭代数和对数似然值做参数。在这个例子中，我们可以使用这个回调函数追踪模型计算（使用Newton-Raphson迭代来最大化logistic回归模型的对数似然函数）进度：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="k">def</span> <span class="nf">show_progress</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span> <span class="s2">&#34;Iteration:&#34;</span><span class="p">,</span> <span class="n">iteration</span><span class="p">,</span> <span class="s2">&#34;Log-likelihood function:&#34;</span><span class="p">,</span> <span class="n">loglikelihood</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">update_fn</span><span class="o">=</span><span class="n">show_progress</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">11.7835020695</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">7.15886767672</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">2</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">5.76877209868</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">3</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">5.11362294338</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">4</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">4.74870642433</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">5</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">4.50026077146</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">6</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">4.31127773737</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">7</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">4.16015043396</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">8</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">4.03561719785</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">9</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.93073282192</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">10</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.84087660929</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">11</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.76282560605</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">12</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.69425027154</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">13</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.6334178602</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">14</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.57900855837</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">15</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.52999671386</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">16</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.48557145163</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">17</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.44508206139</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">18</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.40799948447</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">19</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.3738885624</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">20</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.3423876581</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">21</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.31319343769</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">22</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.2860493346</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">23</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.2607366863</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">24</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.23706784091</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">25</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.21488073614</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">26</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.19403459259</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">27</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.17440646052</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">28</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.15588842703</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">29</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.13838533947</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">30</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.12181293595</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">31</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.10609629966</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">32</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.09116857282</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">33</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.07696988017</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">34</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.06344642288</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">35</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.05054971191</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">36</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.03823591619</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">37</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.02646530573</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">38</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.01520177394</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">39</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">3.00441242601</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">40</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">2.99406722296</span>
</span></span><span class="line"><span class="cl"><span class="n">Iteration</span><span class="p">:</span> <span class="mi">41</span> <span class="n">Log</span><span class="o">-</span><span class="n">likelihood</span> <span class="n">function</span><span class="p">:</span> <span class="o">-</span><span class="mf">2.98413867259</span>
</span></span></code></pre></div><p>一旦对数似然函数得分增加值小于0.01，迭代将终止。如果在500次迭代后还没有到达收敛， train 函数返回并抛出一个 AssertionError 。<br>
可选的关键字 typecode 几乎可以一直忽略。这个关键字允许用户选择要使用的数值矩阵类型。当为了避免大数据计算的内存问题时，可能有必要使用单精度浮点数（Float8，Float16等等）而不是默认的double型。</p>
<h3 id="使用logistic回归模型进行分类">使用logistic回归模型进行分类</h3>
<p>调用 classify 函数可以进行分类。给定一个logistic回归模型和 _x_1 和 _x_2 的值（例如，操纵子结构未知的基因对）， classify 函数返回 1 或 0 ，分别对应类别OP和NOP。例如，考虑基因对 <em>yxcE</em> ， <em>yxcD</em> 和 <em>yxiB</em> ， <em>yxiA</em> ：<br>
操纵子状态未知的相邻基因对</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="n">基因对</span>	<span class="n">基因间距离</span> <span class="n">x1</span>	<span class="n">基因表达得分</span> <span class="n">x2</span>
</span></span><span class="line"><span class="cl"><span class="n">yxcE</span> <span class="err">—</span> <span class="n">yxcD</span>	<span class="mi">6</span>	<span class="o">-</span><span class="mf">173.143442352</span>
</span></span><span class="line"><span class="cl"><span class="n">yxiB</span> <span class="err">—</span> <span class="n">yxiA</span>	<span class="mi">309</span>	<span class="o">-</span><span class="mf">271.005880394</span>
</span></span></code></pre></div><p>Logistic回归模型预测 <em>yxcE</em> ， <em>yxcD</em> 属于相同操纵子（类别OP），而 <em>yxiB</em> ， <em>yxiA</em> 属于不同操纵子:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="s2">&#34;yxcE, yxcD:&#34;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mf">173.143442352</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">yxcE</span><span class="p">,</span> <span class="n">yxcD</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="s2">&#34;yxiB, yxiA:&#34;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="mi">309</span><span class="p">,</span> <span class="o">-</span><span class="mf">271.005880394</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="n">yxiB</span><span class="p">,</span> <span class="n">yxiA</span><span class="p">:</span> <span class="mi">0</span>
</span></span></code></pre></div><p>这个结果和生物学文献报道的一致）。<br>
为了确定这个预测的可信度，我们可以调用 calculate 函数来获得类别OP和NOP的概率公式。对于 <em>yxcE</em>, <em>yxcD</em> 我们发现</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">q</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span><span class="o">-</span><span class="mf">173.143442352</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="s2">&#34;class OP: probability =&#34;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s2">&#34;class NOP: probability =&#34;</span><span class="p">,</span> <span class="n">q</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">OP</span><span class="p">:</span> <span class="n">probability</span> <span class="o">=</span> <span class="mf">0.993242163503</span> <span class="k">class</span> <span class="nc">NOP</span><span class="p">:</span> <span class="n">probability</span> <span class="o">=</span> <span class="mf">0.00675783649744</span>
</span></span></code></pre></div><p>对于 <em>yxiB</em> ， <em>yxiA</em></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">q</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="p">[</span><span class="mi">309</span><span class="p">,</span> <span class="o">-</span><span class="mf">271.005880394</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="s2">&#34;class OP: probability =&#34;</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="s2">&#34;class NOP: probability =&#34;</span><span class="p">,</span> <span class="n">q</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">OP</span><span class="p">:</span> <span class="n">probability</span> <span class="o">=</span> <span class="mf">0.000321211251817</span> <span class="k">class</span> <span class="nc">NOP</span><span class="p">:</span> <span class="n">probability</span> <span class="o">=</span> <span class="mf">0.999678788748</span>
</span></span></code></pre></div><p>为了确定回归模型的预测精确性，我们可以把模型应用到训练数据上：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span> <span class="s2">&#34;True:&#34;</span><span class="p">,</span> <span class="n">ys</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="s2">&#34;Predicted:&#34;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span></code></pre></div><p>这表示除一个基因对外其他所有预测都是正确的。Leave-one-out分析可以对预测精确性给出一个更可信的估计。Leave-one-out是指从训练数据中移除要预测的基因重新计算模型，再用该模型进行预测比对：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">xs</span><span class="p">[:</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:],</span> <span class="n">ys</span><span class="p">[:</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="n">ys</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:])</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span> <span class="s2">&#34;True:&#34;</span><span class="p">,</span> <span class="n">ys</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="s2">&#34;Predicted:&#34;</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span></code></pre></div><p>Leave-one-out分析显示这个logistic回归模型的预测只对两个基因对不正确，对应预测精确度为88%。</p>
<h2 id="logistic回归线性判别分析和支持向量机">Logistic回归，线性判别分析和支持向量机</h2>
<p>Logistic回归模型类似于线性判别分析。在线性判别分析中，类别概率同样可由方程(<a class="link" href="https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr16.html#eq-op"  target="_blank" rel="noopener"
    >16.2</a>) 和 (<a class="link" href="https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr16.html#eq-nop"  target="_blank" rel="noopener"
    >16.3</a>) 给出。但是，不是直接估计系数β，我们首先对预测变量 <em>x</em> 拟合一个正态分布。然后通过这个正态分布的平均值和方差计算系数β。如果 <em>x</em> 的分布确实是正态的，线性判别分析将比logistic回归模型有更好的性能。另一方面，logistic回归模型对于偏态到正态的广泛分布更加强健。<br>
另一个相似的方法是应用线性核函数的支持向量机。这样的SVM也使用一个预测变量的线性组合，但是是从靠近类别之间的边界区域的预测变量 <em>x</em> 来估计系数β。如果logistic回归模型(公式 (<a class="link" href="https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr16.html#eq-op"  target="_blank" rel="noopener"
    >16.2</a>) 和 (<a class="link" href="https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr16.html#eq-nop"  target="_blank" rel="noopener"
    >16.3</a>) )很好的描述了远离边界区域的 <em>x</em> ，我们可以期望logistic回归模型优于线性核函数SVM，因为它应用了更多数据。如果不是，SVM可能更好。<br>
Trevor Hastie, Robert Tibshirani, and Jerome Friedman: The Elements of Statistical Learning. Data Mining, Inference, and Prediction.(统计学习基础:数据挖掘、推理与预测) Springer Series in Statistics, 2001. 4.4章.</p>
<h2 id="k-最近邻居法knn">K-最近邻居法KNN</h2>
<h3 id="背景和目的-1">背景和目的</h3>
<p>最近邻居法是一种不需要将数据拟合到一个模型的监督学习算法。数据点是基于训练数据集的 <em>k</em> 个最近邻居类别进行分类的。<br>
在Biopython中， <em>KNN</em> 方法可在 Bio.KNN 中获得。我们使用 <a class="link" href="https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr16.html#sec-logisticregression"  target="_blank" rel="noopener"
    >16.1</a> 同样的操纵子数据集来说明Biopython中 <em>KNN</em> 方法的用法</p>
<h3 id="初始化一个knn模型">初始化一个KNN模型</h3>
<p>我们创建和初始化一个_KNN_模型：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">Bio</span> <span class="kn">import</span> <span class="n">kNN</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">model</span> <span class="o">=</span> <span class="n">kNN</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
</span></span></code></pre></div><p>这里 xs 和 ys 和 前面的相同。 k 是分类中的邻居数 <em>k</em> 。对于二分类，为 <em>k</em> 选择一个奇数可以避免tied votes。函数名 train 在这里有点不合适，因为就没有训练模型：这个函数仅仅是用来存储模型变量 xs ， ys 和 k 。</p>
<h3 id="使用knn模型来分类">使用KNN模型来分类</h3>
<p>应用 <em>KNN</em> 模型对新数据进行分类，我们使用 classify 函数。这个函数以一个数据点(_x_1,_x_2)为参数并在训练数据集 xs 中寻找 <em>k</em> -最近邻居。然后基于在这 <em>k</em> 个邻居中出现最多的类别（ ys ）来对数据点(_x_1,_x_2)进行分类。<br>
对于基因对 <em>yxcE</em> 、 <em>yxcD</em> 和 <em>yxiB</em> 、 <em>yxiA</em> 的例子，我们发现</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mf">173.143442352</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="s2">&#34;yxcE, yxcD:&#34;</span><span class="p">,</span> <span class="n">kNN</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">yxcE</span><span class="p">,</span> <span class="n">yxcD</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">309</span><span class="p">,</span> <span class="o">-</span><span class="mf">271.005880394</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="s2">&#34;yxiB, yxiA:&#34;</span><span class="p">,</span> <span class="n">kNN</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">yxiB</span><span class="p">,</span> <span class="n">yxiA</span><span class="p">:</span> <span class="mi">0</span>
</span></span></code></pre></div><p>和logistic回归模型一致，<em>yxcE</em>,_yxcD_被归为一类（类别OP），<em>yxiB</em>,_yxiA_属于不同操纵子。<br>
函数 classify 可以指定距离函数和权重函数作为可选参数。距离函数影响作为最近邻居的 <em>k</em> 个类别的选择，因为这些到查询点（ <em>x</em> ， <em>y</em> ）有最小距离的类别被定义为邻居。默认使用欧几里德距离。另外，我们也可以如示例中的使用曼哈顿距离：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="k">def</span> <span class="nf">cityblock</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl"><span class="o">...</span>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl"><span class="o">...</span>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl"><span class="o">...</span>    <span class="n">distance</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">x2</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">x2</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="o">...</span>    <span class="k">return</span> <span class="n">distance</span>
</span></span><span class="line"><span class="cl"><span class="o">...</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mf">173.143442352</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="s2">&#34;yxcE, yxcD:&#34;</span><span class="p">,</span> <span class="n">kNN</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">distance_fn</span> <span class="o">=</span> <span class="n">cityblock</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">yxcE</span><span class="p">,</span> <span class="n">yxcD</span><span class="p">:</span> <span class="mi">1</span>
</span></span></code></pre></div><p>权重函数可以用于权重投票。例如，相比于相邻较远的邻居，我们可能想给更近的邻居一个更高的权重：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="k">def</span> <span class="nf">weight</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
</span></span><span class="line"><span class="cl"><span class="o">...</span>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl"><span class="o">...</span>    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span>
</span></span><span class="line"><span class="cl"><span class="o">...</span>    <span class="k">return</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="nb">abs</span><span class="p">(</span><span class="n">x1</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">-</span><span class="n">x2</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="nb">abs</span><span class="p">(</span><span class="n">x1</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">x2</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</span></span><span class="line"><span class="cl"><span class="o">...</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mf">173.143442352</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="s2">&#34;yxcE, yxcD:&#34;</span><span class="p">,</span> <span class="n">kNN</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">weight_fn</span> <span class="o">=</span> <span class="n">weight</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="n">yxcE</span><span class="p">,</span> <span class="n">yxcD</span><span class="p">:</span> <span class="mi">1</span>
</span></span></code></pre></div><p>默认所有邻居有相同权重。<br>
为了确定这些预测的置信度，我们可以调用函数 calculate 来计算分配到类别OP和NOP的总权重。对于默认的加权方案，这样减少了每个分类的邻居数量。对于 <em>yxcE</em> ， <em>yxcD</em> ， 我们发现</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="o">-</span><span class="mf">173.143442352</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">weight</span> <span class="o">=</span> <span class="n">kNN</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="s2">&#34;class OP: weight =&#34;</span><span class="p">,</span> <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&#34;class NOP: weight =&#34;</span><span class="p">,</span> <span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">OP</span><span class="p">:</span> <span class="n">weight</span> <span class="o">=</span> <span class="mf">0.0</span> <span class="k">class</span> <span class="nc">NOP</span><span class="p">:</span> <span class="n">weight</span> <span class="o">=</span> <span class="mf">3.0</span>
</span></span></code></pre></div><p>这意味着 x1 ， x2 的所有三个邻居都属于NOP类别。对另一个例子 <em>yesK</em> ， <em>yesL</em> 我们发现</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">x</span> <span class="o">=</span> <span class="p">[</span><span class="mi">117</span><span class="p">,</span> <span class="o">-</span><span class="mf">267.14</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="n">weight</span> <span class="o">=</span> <span class="n">kNN</span><span class="o">.</span><span class="n">calculate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span> <span class="s2">&#34;class OP: weight =&#34;</span><span class="p">,</span> <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="s2">&#34;class NOP: weight =&#34;</span><span class="p">,</span> <span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</span></span><span class="line"><span class="cl"><span class="k">class</span> <span class="nc">OP</span><span class="p">:</span> <span class="n">weight</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="k">class</span> <span class="nc">NOP</span><span class="p">:</span> <span class="n">weight</span> <span class="o">=</span> <span class="mf">1.0</span>
</span></span></code></pre></div><p>这意思是两个邻居是操纵子对，另一个是非操纵子对<br>
对于_KNN_方法的预测精确性，我们对训练数据应用模型：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span> <span class="s2">&#34;True:&#34;</span><span class="p">,</span> <span class="n">ys</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="s2">&#34;Predicted:&#34;</span><span class="p">,</span> <span class="n">kNN</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span></code></pre></div><p>显示除了两个基因对所有预测都是正确的。Leave-one-out分析可以对预测精确性给出一个更可信的估计，这是通过从训练数据中移除要预测的基因，再重新计算模型实现：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="o">&gt;&gt;&gt;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ys</span><span class="p">)):</span>
</span></span><span class="line"><span class="cl">        <span class="n">model</span> <span class="o">=</span> <span class="n">kNN</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">xs</span><span class="p">[:</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:],</span> <span class="n">ys</span><span class="p">[:</span><span class="n">i</span><span class="p">]</span><span class="o">+</span><span class="n">ys</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:])</span>
</span></span><span class="line"><span class="cl">        <span class="nb">print</span> <span class="s2">&#34;True:&#34;</span><span class="p">,</span> <span class="n">ys</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="s2">&#34;Predicted:&#34;</span><span class="p">,</span> <span class="n">kNN</span><span class="o">.</span><span class="n">classify</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">1</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">0</span>
</span></span><span class="line"><span class="cl"><span class="kc">True</span><span class="p">:</span> <span class="mi">0</span> <span class="n">Predicted</span><span class="p">:</span> <span class="mi">1</span>
</span></span></code></pre></div><p>Leave-one-out分析显示这个 <em>KNN</em> 模型的预测正确17个基因对中的13个，对应预测精确度为76%。</p>
<h2 id="naive贝叶斯">Naive贝叶斯</h2>
<p>这部分将描述模块 Bio.NaiveBayes</p>
<h2 id="最大熵">最大熵</h2>
<p>这部分将描述模块 Bio.MaximumEntropy</p>
<h2 id="马尔科夫模型">马尔科夫模型</h2>
<p>这部分将描述模块 Bio.MarkovModel 和/或 Bio.HMM.MarkovModel .</p>

</section>


    <footer class="article-footer">
    
    <section class="article-tags">
        
            <a href="/tags/tutorials/">tutorials</a>
        
    </section>


    
    <section class="article-copyright">
        <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <circle cx="12" cy="12" r="9" />
  <path d="M14.5 9a3.5 4 0 1 0 0 6" />
</svg>



        <span>Licensed under CC BY-NC-SA 4.0</span>
    </section>
    </footer>


    
</article>

    <aside class="related-contents--wrapper">
    
    
</aside>

     
    
        
    <div class="disqus-container">
    <div id="disqus_thread"></div>
<script type="application/javascript">
    window.disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "hugo-theme-stack" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</div>

<style>
    .disqus-container {
        background-color: var(--card-background);
        border-radius: var(--card-border-radius);
        box-shadow: var(--shadow-l1);
        padding: var(--card-padding);
    }
</style>

<script>
    window.addEventListener('onColorSchemeChange', (e) => {
        if (DISQUS) {
            DISQUS.reset({
                reload: true
            });
        }
    })
</script>

    

    <footer class="site-footer">
    <section class="copyright">
        &copy; 
        
            2022 - 
        
        2023 叶宇浩随记博客
    </section>
    
    <section class="powerby">
        Built with <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> <br />
        Theme <b><a href="https://github.com/CaiJimmy/hugo-theme-stack" target="_blank" rel="noopener" data-version="3.5.0">Stack</a></b> designed by <a href="https://jimmycai.com" target="_blank" rel="noopener">Jimmy</a>
    </section>
</footer>


    
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    
    <div class="pswp__bg"></div>

    
    <div class="pswp__scroll-wrap">

        
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                
                
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo="crossorigin="anonymous"
                defer="true"
                >
            </script><script 
                src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU="crossorigin="anonymous"
                defer="true"
                >
            </script><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.css"integrity="sha256-c0uckgykQ9v5k&#43;IqViZOZKc47Jn7KQil4/MP3ySA3F8="crossorigin="anonymous"
            ><link 
                rel="stylesheet" 
                href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.css"integrity="sha256-SBLU4vv6CA6lHsZ1XyTdhyjJxCjPif/TRkjnsyGAGnE="crossorigin="anonymous"
            >

            </main>
    
        <aside class="sidebar right-sidebar sticky">
            <section class="widget archives">
                <div class="widget-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
  <path stroke="none" d="M0 0h24v24H0z"/>
  <line x1="5" y1="9" x2="19" y2="9" />
  <line x1="5" y1="15" x2="19" y2="15" />
  <line x1="11" y1="4" x2="7" y2="20" />
  <line x1="17" y1="4" x2="13" y2="20" />
</svg>



                </div>
                <h2 class="widget-title section-title">Table of contents</h2>
                
                <div class="widget--toc">
                    <nav id="TableOfContents">
  <ol>
    <li><a href="#logistic回归模型">Logistic回归模型</a>
      <ol>
        <li><a href="#背景和目的">背景和目的</a></li>
        <li><a href="#训练logistic回归模型">训练logistic回归模型</a></li>
        <li><a href="#使用logistic回归模型进行分类">使用logistic回归模型进行分类</a></li>
      </ol>
    </li>
    <li><a href="#logistic回归线性判别分析和支持向量机">Logistic回归，线性判别分析和支持向量机</a></li>
    <li><a href="#k-最近邻居法knn">K-最近邻居法KNN</a>
      <ol>
        <li><a href="#背景和目的-1">背景和目的</a></li>
        <li><a href="#初始化一个knn模型">初始化一个KNN模型</a></li>
        <li><a href="#使用knn模型来分类">使用KNN模型来分类</a></li>
      </ol>
    </li>
    <li><a href="#naive贝叶斯">Naive贝叶斯</a></li>
    <li><a href="#最大熵">最大熵</a></li>
    <li><a href="#马尔科夫模型">马尔科夫模型</a></li>
  </ol>
</nav>
                </div>
            </section>
        </aside>
    

        </div>
        <script 
                src="https://cdn.jsdelivr.net/npm/node-vibrant@3.1.5/dist/vibrant.min.js"integrity="sha256-5NovOZc4iwiAWTYIFiIM7DxKUXKWvpVEuMEPLzcm5/g="crossorigin="anonymous"
                defer="false"
                >
            </script><script type="text/javascript" src="/ts/main.js" defer></script>
<script>
    (function () {
        const customFont = document.createElement('link');
        customFont.href = "https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap";

        customFont.type = "text/css";
        customFont.rel = "stylesheet";

        document.head.appendChild(customFont);
    }());
</script>

    </body>
</html>
