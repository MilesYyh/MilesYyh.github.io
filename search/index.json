[{"content":" 偶然间在公众号的文章看到了在PPT中插入3D结构的展示方法[^学习来源1],感觉挺好玩的，遂写此post记录\n结构格式要求 结构文件格式要求为gltf格式\ngltf格式 glTF™ is a royalty-free specification for the efficient transmission and loading of 3D scenes and models by engines and applications. glTF minimizes the size of 3D assets, and the runtime processing needed to unpack and use them. glTF defines an extensible, publishing format that streamlines authoring workflows and interactive services by enabling the interoperable use of 3D content across the industry. [^学习来源2]简单来说，gltf就是一种3D结构格式的文件\n将结构保存为gtlf格式 使用完PyMOL调整完蛋白结构后，直接保存为gltf格式即可\nsave xxx.gltf 注意了，不管你PYMOL主窗口中显示的整个蛋白的主体还是局部地方，最后保存为gltf都是整体的展示，若想在PPT展示局部，需在PPT内部放缩\n使用PPT的载入3D结构的方法 插入 点击完此设备后，要注意点击“所有文件（类似的字体后）才可以选择上面保存为gltf的3D结构 实践展示 图片\n动图\nYour browser doesn't support HTML5 video. Here is a link to the video instead. 补充 我发现不管PyMOL中的背景是黑的还是白的，加载到PPT都是白色的 PPT载入后，我发现整体的色彩都有点暗淡，所以在PyMOL中本身调整时色彩就要深色的会好点 似乎在PPT只能旋转而已，当然了PPT中也可以放大（聚焦）到局部 （此时可以通过Ctrl+鼠标左键拖动蛋白主体） 学习来源 https://mp.weixin.qq.com/s/BYCf_bmMBPDa083UCVH9wQ https://www.khronos.org/gltf/ https://jerkwin.github.io/2015/06/13/%E5%9C%A8PPT%E4%B8%AD%E5%8A%A8%E6%80%81%E5%B1%95%E7%A4%BA%E5%88%86%E5%AD%90%E4%B8%89%E7%BB%B4%E7%BB%93%E6%9E%84%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E6%B3%95/ ","date":"2023-03-15T00:00:00Z","permalink":"https://example.com/p/ppt%E6%8F%92%E5%85%A5%E5%8F%AF%E4%BA%A4%E4%BA%92%E7%9A%843d%E8%9B%8B%E7%99%BD%E7%BB%93%E6%9E%84%E5%B1%95%E7%A4%BA/","title":"PPT插入可交互的3D蛋白结构展示"},{"content":"De novo design of small beta barrel proteins\nlink:https://www.pnas.org/doi/abs/10.1073/pnas.2207974120\n","date":"2023-03-14T00:00:00Z","permalink":"https://example.com/p/de-novo-design-of-small-beta-barrel-proteins/","title":"De novo design of small beta barrel proteins"},{"content":"说明 之前从PDB数据库上下载某个基因的野生型结构时，发生存在AA缺失的现象，即某些位点上的AA没有，碰巧这个缺失的位点在突变的范围内，位点的缺失在PyMOL中表现为断点，看了下PDB文件 (从PDB数据库上下载下来的原文件) 的missing信息 (只有从PDB数据库上下载的结构文件才会有missing字段，像结构预测的软件是没有的)，确实是缺失的（避免是因为PyMOL不能识别导致）\n位点在377上缺失 紧接着，使用trRosetta预测了野生型的蛋白结构，发现在使用gmx pdb2gmx 时发现这个新的结构文件上原子缺失了（并不等同于上面的氨基酸缺失），如下报错： 也就是说gmx发现这个蛋白结构文件23位点的色氨酸Trp缺失了CH2的原子信息（注意可能还存在其他原子的缺失，只是Tyr-23的缺失在前面，所以先报错了）\n遂记录下解决办法，针对AA缺失和原子缺失的修复方法\n氨基酸缺失 Chimera-modeller填补 注意了，这里一定是要用PDB数据库上下载下来的结构文件，因为只有PDB上的结构文件，有足够多的补充信息， 比如上面在Vscode中展示的missing字段\n导入结构文件，只显示蛋白主体 command-line中输入\n#导入文件 open 蛋白的IDcode(PDB数据库的) 可以看到这些377 residue确实是缺失的，断点线处\n#只展示蛋白主体，因为PDB上还含有配体、水分子等信息，这里巧用的chimera的内置的key-word delete ~protein #确保删除了配体 delete ligand 注意下，这里delete后的那些stick展现的残基实际上是这个蛋白的活性位点，chimera会默认突出显示\n使用Model Loops 顶部menu\nTools... Structure Editing... Model/Refine Loops 调整参数（主要调整这几个）：\nModel/remodel : 看自己的缺失位置去确定 Number of models to generate: 生成的结果数 Modeller license key: 这个key可以自己去modeller申请，免费的，MODELIRANJE 实际上这里用到的是Modeller同源建模 等待结果即可，chiemra主窗口的左下角可以看进度（不过这里使用的云端的Modeller的程序，当然了可以指定自己本地的Modeller） 保存最优结构pdb 运行完Modeller，Chimera会自动将结构比对在主窗口中 这里保存#1.1的结果\n单击Modeller Results中的#1.1 在点击Modeller Results中不同的结构时，chimera会自动比对这个结构和原来的结构，并且此时主窗口和model都会只展示（选择）他们两个，其他则被暂时hide了起来\n保存为PDB文件 顶部menu File... Save PDB... 注意，在Save models的选项中，选择自己想要保存的结果即可，比如这里的#1.1就是377位点上被补全后的氨基酸+整个其他位点的整体蛋白了，所以可以不用选择自己导入的那个结构文件了 确实补充上了\n手动复制位点PDB信息到文件 这个方法有个前提，就是，能同时找到同一个类型的蛋白，比如说A和B两个蛋白，他们实际上是同一个蛋白（不要在乎这里的名字A、B），（假设）A是通过X-ray的方法得到的结构信息，它缺失的位点信息是1-39，B是通过NMR的方法得到的结构信息，它缺失的位点信息是532-543，那么我想补全A的话，可以通过B的1-39的PDB坐标复制到A的文件中 这个产生的结果，相对其他办法，比较麻烦，而且误差会比较大，但是也很有用\nalign操作 因为A和B是不同的文件（即他们的坐标位置不一样，A.pdb 和B.pdb的坐标信息不是一回事），所以需要通过使用PyMOL的align去将他们对齐先，可以保证他们的坐标尽量一致\nA和B可以提前将结构中含有的配体和水分子都去除,然后再用PyMOL保存问新的pdb文件，用于下面的分析 pymol-cmd\nalign A,B_ 将B保存为一个新的PDB文件（这个新的B，坐标和A是差不多的）\n注意在PyMOL中导出新的B的结构文件时，要选择B！！ 这个方法不太行，因为我发现两个PDB的坐标还是不相似的，即没办法保证同时在一个坐标系中，那么，copy另一个的AA的坐标到另一个上是行不通的\n原子缺失 Swiss-PdbViewer 一键修复 现在似乎不能在mac上运行了\n输入文件 使用spbbviewer打开结构文件时会，自动补全缺失的原子信息，并且软件会给予提示 可以看到23位点上的色氨酸上的CH2原子补充上去了\n另存文件 将pdbviewer中的结构文件输出，得到了一个原子信息被补全的结构文件，可以用这个文件去进行其他的分析了 pdbviewer很方便，它能一键将所有缺失的原子信息都能补全\nPyMOL手动突变 输入结构文件 pymol-cmd输入以下的命令\ncd 目录 load 结构文件 使用PyMOL突变 PyMOL 顶部menu Wizard... Mutagenesis... Protein 鼠标左击目标氨基酸 （一定要选中，没选中则后续操作无效，而且这是针对Mutagenesis出现后去选中的） 右下角Mutagenesis No Mutation... 第一行... 选择对应的氨基酸 Apply Done 输出新的结构文件即可 为了方便做比较，这里展示了下没有使用PyMOL突变时的结构单元（以23TYR为例）和突变后的情况，可以看出CH2已经加上去了 比较麻烦的是当结构中有多个AA的原子缺失时，需要每个单独去突变，比较费时间，还是spdbviewer好用点\n参考资料 1.https://www.bilibili.com/video/BV1jo4y167Qh/?spm_id_from=333.999.0.0\u0026amp;vd_source=3379790417bee9bda9515122763010b6 2.https://spdbv.unil.ch/\n","date":"2023-03-11T00:00:00Z","permalink":"https://example.com/p/%E6%B0%A8%E5%9F%BA%E9%85%B8%E7%BB%93%E6%9E%84%E7%BC%BA%E5%A4%B1%E4%BF%AE%E5%A4%8D/","title":"氨基酸结构缺失修复"},{"content":"DOI: 10.1016/0022-2836(82)90515-0 这篇论文讲了Kyte-Doolittle-scale即蛋白疏水性的数值 ","date":"2023-03-10T00:00:00Z","permalink":"https://example.com/p/a-simple-method-for-displaying-the-hydropathic-character-of-a-protein/","title":"A simple method for displaying the hydropathic character of a protein"},{"content":" 最近参加了一次酶稳定性预测的比赛，挺折磨的，发现很多东西都不会（主要是深度学习方面，学的不怎么好，理论和实际都比较缺，打算今年考完研后再系统性的学习一遍深度学习，不过所幸下次的学习有前面的杂七杂八学过的基础，美滋滋），特写此post来记录第一次真正使用过深度学习解决问题。\n顺便吐槽一下结果，在比赛ddl的前一天排名很好的，反正有铜牌（相当满足），比赛结束后直掉800多名hh。 比赛背景介绍 本次比赛实际上是一个回归任务，即输入的数据为：AA sequence（分别是野生型的突变序列）；输出数据为：酶的热稳定性值；即有一个蛋白（野生型），它有很多突变的位点，然后，我们需要看这些位点的突变是有利否，即这些突变能否提高热稳定性\n简单介绍一些计算生物方面的知识： 我一开始特别想用深度学习的方法（比如写个bliLSTM）来着，但是其实也不一定非得到上深度学习，因为现有的很多计算方法的结果也挺可靠的\n蛋白质稳定性工作在应用机器学习方法之前，在相关领域内也有一些计算方法可以实现，如：ESM, EVE 和 Rosetta （Rosetta 真的神！）等 ，在这次比赛中，直接运用这些方法而非机器学习方法也是允许的 AlphaFold2的诞生：我们在这次比赛中的目标是利用一级结构预测热稳定性，而如何利用一级结构预测整体的三级 PDB 文件：蛋白质的整体结构是三维的，如何通过结构化数据的方式理解三维的关系，包括每个氨基酸的坐标、种类等，常用的是PDB文件，PDB文件是一个开放的数据库，可以从中查询蛋白质相应的数据，如果是数据库中没有的蛋白质，则可以用AlphaFold2、I-TASSER、trRosetta、Robetta等进行预测，预测结果也是一个类似PDB格式的描述三维原子三维坐标关系的文件。 数据集介绍 比赛提供的wild-type序列：\nVPVNPEPDATSVENVALKTGSGDSQSDPIKADLEVKGQSALPFDVDCWAILCKGAPNVLQRVNEKTKNSNRDRSGANKGPFKDPQKWGIKALPPKNPSWSAQDFKSPEEYAFASSLQGGTNAILAPVNLASQNSQGGVLNGFYSANKVAQFDPSKPQQTKGTWFQITKFTGAAGPYCKALGSNDKSVCDKNKNIAGDWGFDPAKWAYQYDEKNNKFNYVGK train.csv 文件： seq_id:每条序列的身份即ID，用以区分序列 protein_sequence:蛋白质序列（官方说他们人为的将大部分将test测试集中的蛋白序列大部分都保持在了固定的220AA，即测试集中的蛋白序列部分氨基酸是被删除了的，可能是留下了结构域的部分） pH:每条序列的pH最适条件值 data_scource:每条序列对应的来源（实际上是每条序列是从哪里收集来的信息） tm:熔融温度，值越高，表示酶稳定性越高 train_updates_20220929.csv： 也是训练数据，官方的Train.csv文件有一定的错误，因此在9月29日发布了这份修改了错误的版本 test.csv 测试集 (它这里的数据相比于train.csv，少了tm的值) sample_submission： 最终预测结果文件提交的格式示例 wildtype_structure_prediction_af2.pdb 本次比赛野生型蛋白的三维结构文件 wild-type结构长这样 评分函数 本次比赛的评估指标是spearman 相关系数，即反应的是我们预测的tm值与真实值之间的相关性，因此本次比赛的相对大小重要性大于预测的绝对大小\n前期准备 本次比赛的目标是通过氨基酸序列，预测酶的热稳定性，即回归任务，核心难点有以下几块：\n训练数据集的构建：本次比赛允许使用外部数据，因此有参赛者找寻了大量的可用外部数据，需要研究如何将不同来源的数据合并在一个训练数据集。同时本次的测试数据集是仅由一条原始序列，进行各种单点突变而成，因此也需要在构建训练数据集的过程中尽量相似于测试数据，即单点突变等。\ntm数据的缺乏。大量的外部数据中，直接给定温度:Tm的数据还是比较少的，因此也由许多参赛者找寻了跟TM的相关数据，如预测ΔΔG等\n思路 基本思路，目前核心解决这个比赛任务的思路有以下几种：\n传统的生物学方法解决:如blosum评分矩阵，rosetta等，这种方式可以不用训练数据，直接对测试集数据进行计算，给出稳定性分数，优势是计算方便，缺点是目前来看准确性一般 机器学习的预测方式(3D CNN):蛋白质在空间中最终是三维结构，因此可以用3DCNN进行预测 Transformer(序列预测)：直接将蛋白质的氨基酸序列，如AAAKL…，作为序列，运用如LSTM、Transformer等模型进行序列预测 传统建模（XGBOOST、LIGHTGBM）：运用各种方式进行特征工程，运用XGB、LGB进行预测 开源方案 赛开始之初，最重要的就是研究目前开源的代码，主流的方案和思路是哪些，我主要看了三个，如下：\nLB开源最高、0.603分： https://www.kaggle.com/code/seyered/eda-novozymes-enzyme-stability XGB：https://www.kaggle.com/code/cdeotte/xgboost-5000-mutations-200-pdb-files-lb-0-410 3DCNN：https://www.kaggle.com/code/vslaykovsky/nesp-thermonet-v2 LB-0.603 这个代码的核心本质是利用一系列传统的生物计算方法为主，兼顾一部分机器学习算法的结果，进行模型的加权融合。主要的方法包括：\nBlosum: BLOSUM (Blocks of Amino Acid Substitution Matrix）：传统的生物方法，会计算序列之间每一个氨基酸变化带来的影响大小 PLDDT：Alphafold2在预测出整体结构后，对每一个氨基酸会给出一个置信度打分，即PLDDT，代表预测的准确与否，与稳定性有一定相关性 DeepDDG：利用深度学习预测稳定性的算法，已开源 Demask：利用线性模型预测的氨基酸改变后的作用大小 此外还有RMSD、SASA、Rosetta等，不一一赘述了，原文中有比较清晰的介绍 XGBoost 作者在通过一系列特征工程，构建了每条序列的特征，并且利用XGB模型进行预测，其中特征包括：\nProtein Structure features (from atom positions in PDB file)（从PDB FILE中获取模型的3D结构）：包括氨基酸与中心点之间的距离，相邻氨基酸之间折叠的角度等 Protein Sequence features：序列，主要是单个氨基酸本身的一系列特征，包括molecular weight、 molecular function等 Substitution Matrix Features：其实就是BLOSUM评分矩阵，将其作为一种特征 Transformer ESM Features：利用ESM模型对序列进行EMBEDDING，也作为特征中的一种 最后用XGB进行训练 3DCNN 网路 我发现这个大佬它不仅预测了tm，还预测了ddG。具体来讲就是作者将蛋白质作为三维结构，即提取了蛋白质之前的位置信息，构建3DCNN模型进行预测。 预测的目标一部分是DDG,一部分是TM。DDG与TM有比较强的相关性，由于作者从外部数据获取的数据中大部分不包含TM，因此以DDG为主要的预测label，TM作为辅助损失 （666学到了还可以这样！）。 神经的主体如下图（激活函数是Relu函数） 看了很多的开源方案后，我发现本次最高的开源方案（结果没出来之前，结果一出后\u0026hellip;简直了）是部分生物方法、部分机器学习的加权融合结果。所以我打算在后期中，主要是加强机器学习部分。通过一系列模型的Embedding、特征的构建拼接、loss function的设计等尝试，来提高分数，打算从3DCNN的方案入手\n我的优化方案 主要为在3DCNN的基础上，加入更多维度的特征，并调整模型结构取得提高\n特征工程 加入每个氨基酸的性质 Protein Sequence features：序列，主要是单个氨基酸本身的一系列特征，包括molecular weight、 molecular function等 氨基酸是与氨基酸的对应字母相关联，即ABCD等字母，每个会对应相应的molecular weight等特征；分别提取出wild-type野生型序列的数据、mutant突变点的数据，并将两个数据相减计算DIFF，构造出新的特征 代码如下：\ndef features_engineer(df): ## features1:Amino Acid Features df[\u0026#39;length\u0026#39;] = df[\u0026#39;sequence\u0026#39;].apply(lambda x : len(str(x))) df[\u0026#39;position_rate\u0026#39;] = df[\u0026#39;seq_position\u0026#39;] / df[\u0026#39;length\u0026#39;] aa_map = {\u0026#39;VAL\u0026#39;: \u0026#39;V\u0026#39;, \u0026#39;PRO\u0026#39;: \u0026#39;P\u0026#39;, \u0026#39;ASN\u0026#39;: \u0026#39;N\u0026#39;, \u0026#39;GLU\u0026#39;: \u0026#39;E\u0026#39;, \u0026#39;ASP\u0026#39;: \u0026#39;D\u0026#39;, \u0026#39;ALA\u0026#39;: \u0026#39;A\u0026#39;, \u0026#39;THR\u0026#39;: \u0026#39;T\u0026#39;, \u0026#39;SER\u0026#39;: \u0026#39;S\u0026#39;, \u0026#39;LEU\u0026#39;: \u0026#39;L\u0026#39;, \u0026#39;LYS\u0026#39;: \u0026#39;K\u0026#39;, \u0026#39;GLY\u0026#39;: \u0026#39;G\u0026#39;, \u0026#39;GLN\u0026#39;: \u0026#39;Q\u0026#39;, \u0026#39;ILE\u0026#39;: \u0026#39;I\u0026#39;, \u0026#39;PHE\u0026#39;: \u0026#39;F\u0026#39;, \u0026#39;CYS\u0026#39;: \u0026#39;C\u0026#39;, \u0026#39;TRP\u0026#39;: \u0026#39;W\u0026#39;, \u0026#39;ARG\u0026#39;: \u0026#39;R\u0026#39;, \u0026#39;TYR\u0026#39;: \u0026#39;Y\u0026#39;, \u0026#39;HIS\u0026#39;: \u0026#39;H\u0026#39;, \u0026#39;MET\u0026#39;: \u0026#39;M\u0026#39;} aa_map_2 = {x:y for x,y in zip(np.sort(list(aa_map.values())),np.arange(20))} aa_map_2[\u0026#39;X\u0026#39;] = 20 aa_props = pd.read_csv(\u0026#39;/content/drive/MyDrive/Novozymes Enzyme Stability/data/train_data/aminoacids.csv\u0026#39;) PROPS = [\u0026#39;Letter\u0026#39;,\u0026#39;Molecular Weight\u0026#39;, \u0026#39;Residue Weight\u0026#39;, \u0026#39;pKa1\u0026#39;, \u0026#39;pKb2\u0026#39;, \u0026#39;pl4\u0026#39;, \u0026#39;H\u0026#39;, \u0026#39;VSC\u0026#39;, \u0026#39;P1\u0026#39;, \u0026#39;P2\u0026#39;, \u0026#39;SASA\u0026#39;, \u0026#39;NCISC\u0026#39;] aa_props = aa_props[PROPS] print(\u0026#39;Amino Acid properties dataframe. Shape:\u0026#39;, aa_props.shape ) # print(aa_props.loc[aa_props[\u0026#39;Letter\u0026#39;] == \u0026#39;A\u0026#39; ,\u0026#39;Molecular Weight\u0026#39;][0]) def get_props(df,cols): df[f\u0026#39;{cols}_x\u0026#39;] = df[\u0026#39;wildtype\u0026#39;].apply(lambda x : aa_props.loc[aa_props[\u0026#39;Letter\u0026#39;] == x , cols].values[0]) df[f\u0026#39;{cols}_y\u0026#39;] = df[\u0026#39;mutant\u0026#39;].apply(lambda x : aa_props.loc[aa_props[\u0026#39;Letter\u0026#39;] == x , cols].values[0]) return df for i in PROPS[1:]: df = get_props(df,i) df[f\u0026#39;{i}_diff\u0026#39;] = df[f\u0026#39;{i}_y\u0026#39;] - df[f\u0026#39;{i}_x\u0026#39;] # df = df.merge(aa_props,left_on = [\u0026#39;wildtype\u0026#39;],right_on = [\u0026#39;Letter\u0026#39;]).merge(aa_props,left_on = [\u0026#39;mutant\u0026#39;],right_on = [\u0026#39;Letter\u0026#39;]) 加入氨基酸变化打分矩阵（blosum系列） Substitution Matrix Features：为氨基酸变化的评分矩阵，该数值一定程度上与变化后的稳定性存在相关性；主要利用biopython包中的get_sub_matrix函数，提取出blosum100、blosum80、blosum60、blosum40的评分数值，以及读取预先下载的demask矩阵数值，作为特征加入数据 代码如下：\ndef get_sub_matrix(matrix_name=\u0026#34;blosum100\u0026#34;): sub_matrix = getattr(MatrixInfo, matrix_name) sub_matrix.update({(k[1], k[0]):v for k,v in sub_matrix.items() if (k[1], k[0]) not in list(sub_matrix.keys())}) return sub_matrix sub_mat_b100 = get_sub_matrix(\u0026#34;blosum100\u0026#34;) sub_mat_b80 = get_sub_matrix(\u0026#34;blosum80\u0026#34;) sub_mat_b60 = get_sub_matrix(\u0026#34;blosum60\u0026#34;) sub_mat_b40 = get_sub_matrix(\u0026#34;blosum40\u0026#34;) # DEMASK SUBSTITUTION MATRICES dff = pd.read_csv(\u0026#39;/content/drive/MyDrive/Novozymes Enzyme Stability/data/train_data/matrix.txt\u0026#39;, sep=\u0026#39;\\t\u0026#39;) letters = list( dff.columns ) l_dict = {x:y for x,y in zip(letters,range(20))} sub_mat_demask = {} for x in letters: for y in letters: sub_mat_demask[(x,y)] = dff.iloc[l_dict[x],l_dict[y]] df[\u0026#39;sub_mat_100\u0026#39;] = [sub_mat_b100[(df.loc[i,\u0026#39;wildtype\u0026#39;],df.loc[i,\u0026#39;mutant\u0026#39;])] for i in range(len(df))] df[\u0026#39;sub_mat_80\u0026#39;] = [sub_mat_b80[(df.loc[i,\u0026#39;wildtype\u0026#39;],df.loc[i,\u0026#39;mutant\u0026#39;])] for i in range(len(df))] df[\u0026#39;sub_mat_60\u0026#39;] = [sub_mat_b60[(df.loc[i,\u0026#39;wildtype\u0026#39;],df.loc[i,\u0026#39;mutant\u0026#39;])] for i in range(len(df))] df[\u0026#39;sub_mat_40\u0026#39;] = [sub_mat_b40[(df.loc[i,\u0026#39;wildtype\u0026#39;],df.loc[i,\u0026#39;mutant\u0026#39;])] for i in range(len(df))] df[\u0026#39;sub_mat_demask\u0026#39;] = [sub_mat_demask[(df.loc[i,\u0026#39;wildtype\u0026#39;],df.loc[i,\u0026#39;mutant\u0026#39;])] for i in range(len(df))] df[\u0026#39;wildtype_encoder\u0026#39;] = df[\u0026#39;wildtype\u0026#39;].map(wildtype_dict) df[\u0026#39;mutant_encoder\u0026#39;] = df[\u0026#39;mutant\u0026#39;].map(mutant_dict) return df Embedding向量化序列 EMBEDDING特征：简单而言，将一些分类型特征作为EMBEDDING向量，放入模型中进行学习；将wild-type和mutant的氨基酸，转换成数字encoder,用于之后进行EMBEDDING操作 代码放到了第2部分get_sub_matrix函数中，具体为以下几行：\ndf[\u0026#39;wildtype_encoder\u0026#39;] = df[\u0026#39;wildtype\u0026#39;].map(wildtype_dict) df[\u0026#39;mutant_encoder\u0026#39;] = df[\u0026#39;mutant\u0026#39;].map(mutant_dict) return df 模型结构 模型结构方面，对于Protein Sequence features 特征和blosum特征，分别进行了batchnorm1d归一化操作，并进行两次全连接层，转化到64维度\n代码中表现为：\nclass ThermoNet2(nn.Module): def __init__(self, params): super().__init__() ## 3DCNN CONV_LAYER_SIZES = [14, 16, 24, 32, 48, 78, 128] FLATTEN_SIZES = [0, 5488, 5184, 4000, 3072, 2106, 1024] dropout_rate = params[\u0026#39;dropout_rate\u0026#39;] dropout_rate_dt = params[\u0026#39;dropout_rate_dt\u0026#39;] dense_layer_size = int(params[\u0026#39;dense_layer_size\u0026#39;]) layer_num = int(params[\u0026#39;conv_layer_num\u0026#39;]) silu = params[\u0026#39;SiLU\u0026#39;] self.params = params if silu: activation = nn.SiLU() else: activation = nn.ReLU() self.activation = activation model = [ nn.Sequential( *[nn.Sequential( nn.Conv3d(in_channels=CONV_LAYER_SIZES[l], out_channels=CONV_LAYER_SIZES[l + 1], kernel_size=(3, 3, 3)), activation ) for l in range(layer_num)] ), nn.MaxPool3d(kernel_size=(2,2,2)), nn.Flatten(), ] flatten_size = FLATTEN_SIZES[layer_num] if self.params[\u0026#39;LayerNorm\u0026#39;]: model.append(nn.LayerNorm(flatten_size)) self.model = nn.Sequential(*model) # flatten_size += len(features_wide) self.Dropout = nn.Dropout(p=0.5) self.ddG = nn.Sequential( nn.Dropout(p=dropout_rate), nn.Linear(in_features=flatten_size + 64 * 3 , out_features=dense_layer_size), activation, nn.Dropout(p=dropout_rate), nn.Linear(in_features=dense_layer_size, out_features=1) ) self.dT = nn.Sequential( nn.Dropout(p=dropout_rate_dt), nn.Linear(in_features=flatten_size + 64 * 3 , out_features=dense_layer_size), activation, nn.Dropout(p=dropout_rate_dt), nn.Linear(in_features=dense_layer_size, out_features=1) ) self.batchnorm1d = nn.BatchNorm1d(len(features_wide)) self.wide_linear1 = nn.Linear(in_features=len(features_wide),out_features = 128) self.wide_linear2 = nn.Linear(in_features=128,out_features = 64) self.embedding_wild = nn.Embedding(22, 64, sparse=False) self.embedding_mutant = nn.Embedding(22, 64, sparse=False) def forward(self, x0, x1 ,x2): if self.params[\u0026#39;diff_features\u0026#39;]: x0[:, 7:, ...] -= x0[:, :7, ...] x0 = self.model(x0) x1 = self.batchnorm1d(x1) x1 = self.wide_linear1(x1) x1 = self.Dropout(x1) x1 = self.activation(x1) x1 = self.wide_linear2(x1) x1 = self.Dropout(x1) x2_wild = self.embedding_wild(x2[:,0]) x2_mutant = self.embedding_wild(x2[:,1]) x = torch.concat([x0,x1,x2_wild,x2_mutant],axis=1) x = self.Dropout(x) ddg = self.ddG(x) dt = self.dT(x) return ddg.squeeze(), dt.squeeze() 具体为这几行：\nself.batchnorm1d = nn.BatchNorm1d(len(features_wide)) self.wide_linear1 = nn.Linear(in_features=len(features_wide),out_features = 128) self.wide_linear2 = nn.Linear(in_features=128,out_features = 64) #前向传播 x1 = self.batchnorm1d(x1) x1 = self.wide_linear1(x1) x1 = self.Dropout(x1) x1 = self.activation(x1) x1 = self.wide_linear2(x1) x1 = self.Dropout(x1) 注意了，我这里的Embedding向量上，初始化为64维的向量特征，具体代码是这几行：\nself.embedding_wild = nn.Embedding(22, 64, sparse=False) self.embedding_mutant = nn.Embedding(22, 64, sparse=False) #前向传播 x2_wild = self.embedding_wild(x2[:,0]) x2_mutant = self.embedding_wild(x2[:,1]) 最终将三维结构3DCNN特征X0、 Protein Sequence features 特征blosum特征、E\u0008mbedding特征都进行拼接，通过全连接层进行输出（实际上是完整的前向传播的代码了，代码如下：）\ndef forward(self, x0, x1 ,x2): if self.params[\u0026#39;diff_features\u0026#39;]: x0[:, 7:, ...] -= x0[:, :7, ...] x0 = self.model(x0) x1 = self.batchnorm1d(x1) x1 = self.wide_linear1(x1) x1 = self.Dropout(x1) x1 = self.activation(x1) x1 = self.wide_linear2(x1) x1 = self.Dropout(x1) x2_wild = self.embedding_wild(x2[:,0]) x2_mutant = self.embedding_wild(x2[:,1]) x = torch.concat([x0,x1,x2_wild,x2_mutant],axis=1) x = self.Dropout(x) ddg = self.ddG(x) dt = self.dT(x) return ddg.squeeze(), dt.squeeze() 模型评估函数的构造 注意了，本次比赛官方要求的spearman相关系数\n我这里构建了模型的评估函数，会根据交叉验证的方式，对模型的训练集进行CV分数的评估，主要是看相关系数，与这次比赛的评估结果一致\n代码如下：\nIS_DDG_TARGET = True def valid(models): kfold = GroupKFold(N_FOLDS) oof_predictions = np.zeros((df_train.shape[0],1)) if params[\u0026#39;GroupKFold\u0026#39;]: print(\u0026#39;group_k_fold\u0026#39;) groups = df_train.sequence else: groups = range(len(df_train)) for fold, (train_idx, val_idx) in enumerate(tqdm(kfold.split(df_train, groups=groups), total=N_FOLDS, desc=\u0026#34;Folds\u0026#34;)): true = df_train.loc[val_idx,\u0026#39;ddG\u0026#39;] with torch.no_grad(): model = models[fold] model.eval() model = model.to(DEVICE) dl = DataLoader(ThermoNet2Dataset(df_train.iloc[val_idx]), batch_size=64) if IS_DDG_TARGET: pred = np.concatenate([model.forward(x[0].to(DEVICE),x[1].to(DEVICE),x[2].to(DEVICE))[0].cpu().numpy() for x in tqdm(dl, desc=\u0026#39;ThermoNet2 ddg predict\u0026#39;, disable=True)]) oof_predictions[val_idx] += pred.reshape(-1,1) print(f\u0026#39;fold {fold} , spearman {scipy.stats.spearmanr(true,pred)[0]} , pearson {scipy.stats.pearsonr(true,pred)[0]}\u0026#39;) true = df_train.loc[:,\u0026#39;ddG\u0026#39;].values pred = oof_predictions.reshape(-1,) print(f\u0026#39;all , spearman {scipy.stats.spearmanr(true,pred)[0]} , pearson {scipy.stats.pearsonr(true,pred)[0]}\u0026#39;) XGBoost模型融合 到此已经完成了基本的深度学习的建模了，接下来打算利用XGBoost决策树模型进行建模，主要是模型融合操作\n特征层面 Protein Sequence features 和 Substitution Matrix Features，这两个与深度学习模型的特征是完全一样的，在XGBoost模型中特征层面针对Embedding的方法改变了下，具体如下：\nSequence embedding:sequence本身是一系列字母组成的序列，使用ESM模型，将其转换成Embedding格式\n#构建mutants sequences def create_mut_seq(row): mut_seq = list(row.sequence) mut_seq[row.seq_position] = row.mutant mut_seq = \u0026#39;\u0026#39;.join(mut_seq) row[\u0026#39;mutant_seq\u0026#39;] = mut_seq return row df_train = df_train.apply(lambda x : create_mut_seq(x),axis= 1) #获取sequences embedding def get_esm_embedding(seq,t_model): data = [(\u0026#34;protein1\u0026#34;, seq)] batch_labels, batch_strs, batch_tokens = batch_converter(data) batch_tokens = batch_tokens.to(device) with torch.no_grad(): results = t_model(batch_tokens, repr_layers=[33]) results = results[\u0026#34;representations\u0026#34;][33].detach().cpu().numpy() return np.mean( results[0,:,:],axis=0 ) df_train[\u0026#39;sequence_embedding\u0026#39;] = [get_esm_embedding(i,t_model) for i in tqdm(df_train[\u0026#39;mutant_seq\u0026#39;])] 3D坐标结构，由其本身是三维的，无法直接放入XGB模型训练，因此采用了上一步深度学习预测后拉平的embedding,作为特征 (输出3DCNN输出的EMBEDDING) def forward(self, x0, x1 ,x2): if self.params[\u0026#39;diff_features\u0026#39;]: x0[:, 7:, ...] -= x0[:, :7, ...] x = self.model(x0) # ddg = self.ddG(x) # dt = self.dT(x) return x IS_DDG_TARGET = True def valid(models): kfold = GroupKFold(N_FOLDS) oof_predictions = np.zeros((df_train.shape[0],1024)) if params[\u0026#39;GroupKFold\u0026#39;]: print(\u0026#39;group_k_fold\u0026#39;) groups = df_train.sequence else: groups = range(len(df_train)) for fold, (train_idx, val_idx) in enumerate(tqdm(kfold.split(df_train, groups=groups), total=N_FOLDS, desc=\u0026#34;Folds\u0026#34;)): true = df_train.loc[val_idx,\u0026#39;ddG\u0026#39;] preds = [] with torch.no_grad(): model = models[fold] model.eval() DEVICE = \u0026#39;cuda\u0026#39; model = model.to(DEVICE) dl = DataLoader(ThermoNet2Dataset(df_train.iloc[val_idx]), batch_size=64) for x0 , x1 , x2 , ddg , dt in dl: pred = model.forward(x0.to(DEVICE),x1.to(DEVICE),x2.to(DEVICE)) preds += pred.cpu().numpy().tolist() oof_predictions[val_idx,:] = preds return oof_predictions 模型序列 同样利用GROUPKFOLD （cv） 进行分割，通过XGBoost模型，输入坐标EMBEDDING、SEQUENCE EMBEDDING和其他特征\nN_FOLDS = 10 kfold = GroupKFold(N_FOLDS) groups = range(len(df_train)) for fold, (train_idx, val_idx) in enumerate(tqdm(kfold.split(df_train, groups=groups), total=N_FOLDS, desc=\u0026#34;Folds\u0026#34;)): print(df_train.loc[train_idx][\u0026#39;ddG\u0026#39;].mean(),df_train.loc[val_idx][\u0026#39;ddG\u0026#39;].mean()) X_train = np.concatenate([tmp[train_idx,:],df_train.loc[train_idx, features_wide],np.concatenate(df_train.loc[train_idx, \u0026#39;sequence_embedding\u0026#39;].values).reshape(len(train_idx),1280)],axis=1) y_train = df_train.loc[train_idx,\u0026#39;ddG\u0026#39;] X_valid = np.concatenate([tmp[val_idx,:],df_train.loc[val_idx, features_wide],np.concatenate(df_train.loc[val_idx, \u0026#39;sequence_embedding\u0026#39;].values).reshape(len(val_idx),1280)],axis=1) y_valid = df_train.loc[val_idx, \u0026#39;ddG\u0026#39;] dtrain = xgb.DMatrix(data=X_train, label=y_train) dvalid = xgb.DMatrix(data=X_valid, label=y_valid) # TRAIN MODEL FOLD K model = xgb.train(xgb_parms, dtrain=dtrain, evals=[(dtrain,\u0026#39;train\u0026#39;),(dvalid,\u0026#39;valid\u0026#39;)], num_boost_round=9999, early_stopping_rounds=10, verbose_eval=500) model.save_model(f\u0026#39;./XGB_fold{fold}.xgb\u0026#39;) 最后将测试集方面的3D坐标特征，利用训练好的模型预测10次取平均值获得\ndef predict_embedding(models): test_predictions = np.zeros((df_test_replace.shape[0],1024)) for fold in range(10): preds = [] with torch.no_grad(): model = models[fold] model.eval() DEVICE = \u0026#39;cuda\u0026#39; model = model.to(DEVICE) dl = DataLoader(ThermoNet2Dataset(df_test_replace), batch_size=64) for x0 , x1 , x2 in dl: pred = model.forward(x0.to(DEVICE),x1.to(DEVICE),x2.to(DEVICE)) preds += pred.cpu().numpy().tolist() test_predictions += preds return test_predictions / 10 完整代码展示 （在kaggle上训练的）\n# !unzip -q /content/enezmy-train.zip !unzip -q \u0026#39;/content/drive/MyDrive/Novozymes Enzyme Stability/data/novozymes-enzyme-stability-prediction.zip\u0026#39; !unzip -q \u0026#39;/content/drive/MyDrive/Novozymes Enzyme Stability/data/thermonet-features.zip\u0026#39; !unzip -q \u0026#39;/content/drive/MyDrive/Novozymes Enzyme Stability/data/enezmy-train.zip\u0026#39; 导库 #导包 import pandas as pd import os from tqdm.notebook import tqdm import numpy as np import plotly.express as px import torch.nn as nn import torch from torch.utils.data import Dataset from torch.utils.data import DataLoader !pip install -q Levenshtein import Levenshtein import copy from sklearn.model_selection import GroupKFold,StratifiedKFold from collections import defaultdict import copy from torch.optim import AdamW from ast import literal_eval import re import joblib import scipy.stats !pip install biopython==1.79 import Bio.SubsMat from Bio.SubsMat import MatrixInfo from sklearn.preprocessing import MinMaxScaler !pip -q install biopandas -q from biopandas.pdb import PandasPdb from biopandas.mmcif import PandasMmcif import random 随机数种子 #随机数种子 def seed(seed=0): np.random.seed(seed) # Numpy module. random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.cuda.manual_seed_all(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False seed(19990829) 读取数据集 #读取dataset TRAIN_FEATURES_DIR = \u0026#39;/content/drive/MyDrive/Novozymes Enzyme Stability/data/train_data\u0026#39; WITH_PUCCI_SOURCE = True WITH_KAGGLE_DDG_SOURCE = True DESTABILIZING_MUTATIONS_ONLY = True AUGMENT_DESTABILIZING_MUTATIONS = False df_train = pd.read_csv(\u0026#39;./df_train.csv\u0026#39;) features = joblib.load(\u0026#39;/content/drive/MyDrive/Novozymes Enzyme Stability/data/train_data/features/features.pkl\u0026#39;) df_train[\u0026#39;features\u0026#39;] = [v for v in tqdm(features.values())] wildtype_dict = {v:k for k,v in enumerate(df_train[\u0026#39;wildtype\u0026#39;].unique())} mutant_dict = {v:k for k,v in enumerate(df_train[\u0026#39;mutant\u0026#39;].unique())} 特征工程 def features_engineer(df): ## features1:Amino Acid Features df[\u0026#39;length\u0026#39;] = df[\u0026#39;sequence\u0026#39;].apply(lambda x : len(str(x))) df[\u0026#39;position_rate\u0026#39;] = df[\u0026#39;seq_position\u0026#39;] / df[\u0026#39;length\u0026#39;] aa_map = {\u0026#39;VAL\u0026#39;: \u0026#39;V\u0026#39;, \u0026#39;PRO\u0026#39;: \u0026#39;P\u0026#39;, \u0026#39;ASN\u0026#39;: \u0026#39;N\u0026#39;, \u0026#39;GLU\u0026#39;: \u0026#39;E\u0026#39;, \u0026#39;ASP\u0026#39;: \u0026#39;D\u0026#39;, \u0026#39;ALA\u0026#39;: \u0026#39;A\u0026#39;, \u0026#39;THR\u0026#39;: \u0026#39;T\u0026#39;, \u0026#39;SER\u0026#39;: \u0026#39;S\u0026#39;, \u0026#39;LEU\u0026#39;: \u0026#39;L\u0026#39;, \u0026#39;LYS\u0026#39;: \u0026#39;K\u0026#39;, \u0026#39;GLY\u0026#39;: \u0026#39;G\u0026#39;, \u0026#39;GLN\u0026#39;: \u0026#39;Q\u0026#39;, \u0026#39;ILE\u0026#39;: \u0026#39;I\u0026#39;, \u0026#39;PHE\u0026#39;: \u0026#39;F\u0026#39;, \u0026#39;CYS\u0026#39;: \u0026#39;C\u0026#39;, \u0026#39;TRP\u0026#39;: \u0026#39;W\u0026#39;, \u0026#39;ARG\u0026#39;: \u0026#39;R\u0026#39;, \u0026#39;TYR\u0026#39;: \u0026#39;Y\u0026#39;, \u0026#39;HIS\u0026#39;: \u0026#39;H\u0026#39;, \u0026#39;MET\u0026#39;: \u0026#39;M\u0026#39;} aa_map_2 = {x:y for x,y in zip(np.sort(list(aa_map.values())),np.arange(20))} aa_map_2[\u0026#39;X\u0026#39;] = 20 aa_props = pd.read_csv(\u0026#39;/content/drive/MyDrive/Novozymes Enzyme Stability/data/train_data/aminoacids.csv\u0026#39;) PROPS = [\u0026#39;Letter\u0026#39;,\u0026#39;Molecular Weight\u0026#39;, \u0026#39;Residue Weight\u0026#39;, \u0026#39;pKa1\u0026#39;, \u0026#39;pKb2\u0026#39;, \u0026#39;pl4\u0026#39;, \u0026#39;H\u0026#39;, \u0026#39;VSC\u0026#39;, \u0026#39;P1\u0026#39;, \u0026#39;P2\u0026#39;, \u0026#39;SASA\u0026#39;, \u0026#39;NCISC\u0026#39;] aa_props = aa_props[PROPS] print(\u0026#39;Amino Acid properties dataframe. Shape:\u0026#39;, aa_props.shape ) # print(aa_props.loc[aa_props[\u0026#39;Letter\u0026#39;] == \u0026#39;A\u0026#39; ,\u0026#39;Molecular Weight\u0026#39;][0]) def get_props(df,cols): df[f\u0026#39;{cols}_x\u0026#39;] = df[\u0026#39;wildtype\u0026#39;].apply(lambda x : aa_props.loc[aa_props[\u0026#39;Letter\u0026#39;] == x , cols].values[0]) df[f\u0026#39;{cols}_y\u0026#39;] = df[\u0026#39;mutant\u0026#39;].apply(lambda x : aa_props.loc[aa_props[\u0026#39;Letter\u0026#39;] == x , cols].values[0]) return df for i in PROPS[1:]: df = get_props(df,i) df[f\u0026#39;{i}_diff\u0026#39;] = df[f\u0026#39;{i}_y\u0026#39;] - df[f\u0026#39;{i}_x\u0026#39;] # df = df.merge(aa_props,left_on = [\u0026#39;wildtype\u0026#39;],right_on = [\u0026#39;Letter\u0026#39;]).merge(aa_props,left_on = [\u0026#39;mutant\u0026#39;],right_on = [\u0026#39;Letter\u0026#39;]) def get_sub_matrix(matrix_name=\u0026#34;blosum100\u0026#34;): sub_matrix = getattr(MatrixInfo, matrix_name) sub_matrix.update({(k[1], k[0]):v for k,v in sub_matrix.items() if (k[1], k[0]) not in list(sub_matrix.keys())}) return sub_matrix sub_mat_b100 = get_sub_matrix(\u0026#34;blosum100\u0026#34;) sub_mat_b80 = get_sub_matrix(\u0026#34;blosum80\u0026#34;) sub_mat_b60 = get_sub_matrix(\u0026#34;blosum60\u0026#34;) sub_mat_b40 = get_sub_matrix(\u0026#34;blosum40\u0026#34;) # DEMASK SUBSTITUTION MATRICES dff = pd.read_csv(\u0026#39;/content/drive/MyDrive/Novozymes Enzyme Stability/data/train_data/matrix.txt\u0026#39;, sep=\u0026#39;\\t\u0026#39;) letters = list( dff.columns ) l_dict = {x:y for x,y in zip(letters,range(20))} sub_mat_demask = {} for x in letters: for y in letters: sub_mat_demask[(x,y)] = dff.iloc[l_dict[x],l_dict[y]] df[\u0026#39;sub_mat_100\u0026#39;] = [sub_mat_b100[(df.loc[i,\u0026#39;wildtype\u0026#39;],df.loc[i,\u0026#39;mutant\u0026#39;])] for i in range(len(df))] df[\u0026#39;sub_mat_80\u0026#39;] = [sub_mat_b80[(df.loc[i,\u0026#39;wildtype\u0026#39;],df.loc[i,\u0026#39;mutant\u0026#39;])] for i in range(len(df))] df[\u0026#39;sub_mat_60\u0026#39;] = [sub_mat_b60[(df.loc[i,\u0026#39;wildtype\u0026#39;],df.loc[i,\u0026#39;mutant\u0026#39;])] for i in range(len(df))] df[\u0026#39;sub_mat_40\u0026#39;] = [sub_mat_b40[(df.loc[i,\u0026#39;wildtype\u0026#39;],df.loc[i,\u0026#39;mutant\u0026#39;])] for i in range(len(df))] df[\u0026#39;sub_mat_demask\u0026#39;] = [sub_mat_demask[(df.loc[i,\u0026#39;wildtype\u0026#39;],df.loc[i,\u0026#39;mutant\u0026#39;])] for i in range(len(df))] df[\u0026#39;wildtype_encoder\u0026#39;] = df[\u0026#39;wildtype\u0026#39;].map(wildtype_dict) df[\u0026#39;mutant_encoder\u0026#39;] = df[\u0026#39;mutant\u0026#39;].map(mutant_dict) return df df_train = features_engineer(df_train) 挂载模型和文件 !pip install fair-esm -q from scipy.special import softmax from scipy.stats import entropy import esm # t_model, alphabet = torch.hub.load(\u0026#34;/content/drive/MyDrive/Novozymes Enzyme Stability/data/esm2_t33_650M_UR50D.pt\u0026#34;, \u0026#34;esm2_t33_650M_UR50D\u0026#34;) !mkdir \u0026#39;/root/.cache/torch/hub/checkpoints/\u0026#39; !cp \u0026#39;/content/drive/MyDrive/Novozymes Enzyme Stability/data/esm2_t33_650M_UR50D.pt\u0026#39; \u0026#39;/root/.cache/torch/hub/checkpoints/\u0026#39; t_model, alphabet = esm.pretrained.esm2_t33_650M_UR50D() batch_converter = alphabet.get_batch_converter() t_model.eval() device = torch.device(\u0026#39;cuda\u0026#39; if torch.cuda.is_available() else \u0026#39;cpu\u0026#39;) t_model.to(device) 蛋白序列编码函数 def create_mut_seq(row): mut_seq = list(row.sequence) mut_seq[row.seq_position] = row.mutant mut_seq = \u0026#39;\u0026#39;.join(mut_seq) row[\u0026#39;mutant_seq\u0026#39;] = mut_seq return row df_train = df_train.apply(lambda x : create_mut_seq(x),axis= 1) def get_esm_embedding(seq,t_model): data = [(\u0026#34;protein1\u0026#34;, seq)] batch_labels, batch_strs, batch_tokens = batch_converter(data) batch_tokens = batch_tokens.to(device) with torch.no_grad(): results = t_model(batch_tokens, repr_layers=[33]) results = results[\u0026#34;representations\u0026#34;][33].detach().cpu().numpy() return np.mean( results[0,:,:],axis=0 ) df_train[\u0026#39;sequence_embedding\u0026#39;] = [get_esm_embedding(i,t_model) for i in tqdm(df_train[\u0026#39;mutant_seq\u0026#39;])] # from sklearn.decomposition import PCA # pca_pool = PCA(n_components=32) # df_train[\u0026#39;sequence_embedding_pca\u0026#39;] = pca_pool.fit_transform(df_train[\u0026#39;sequence_embedding\u0026#39;].values) features_1 = [\u0026#39;Molecular Weight_x\u0026#39;, \u0026#39;Residue Weight_x\u0026#39;, \u0026#39;pKa1_x\u0026#39;, \u0026#39;pKb2_x\u0026#39;,\u0026#39;pl4_x\u0026#39;, \u0026#39;H_x\u0026#39;, \u0026#39;VSC_x\u0026#39;, \u0026#39;P1_x\u0026#39;, \u0026#39;P2_x\u0026#39;, \u0026#39;SASA_x\u0026#39;, \u0026#39;NCISC_x\u0026#39;,\u0026#39;Molecular Weight_y\u0026#39;, \u0026#39;Residue Weight_y\u0026#39;, \u0026#39;pKa1_y\u0026#39;, \u0026#39;pKb2_y\u0026#39;, \u0026#39;pl4_y\u0026#39;, \u0026#39;H_y\u0026#39;, \u0026#39;VSC_y\u0026#39;, \u0026#39;P1_y\u0026#39;, \u0026#39;P2_y\u0026#39;, \u0026#39;SASA_y\u0026#39;, \u0026#39;NCISC_y\u0026#39;,\u0026#39;Molecular Weight_diff\u0026#39;, \u0026#39;Residue Weight_diff\u0026#39;, \u0026#39;pKa1_diff\u0026#39;, \u0026#39;pKb2_diff\u0026#39;, \u0026#39;pl4_diff\u0026#39;, \u0026#39;H_diff\u0026#39;, \u0026#39;VSC_diff\u0026#39;, \u0026#39;P1_diff\u0026#39;, \u0026#39;P2_diff\u0026#39;, \u0026#39;SASA_diff\u0026#39;, \u0026#39;NCISC_diff\u0026#39;] features_2 = [\u0026#39;sub_mat_100\u0026#39;,\u0026#39;sub_mat_80\u0026#39;,\u0026#39;sub_mat_60\u0026#39;,\u0026#39;sub_mat_40\u0026#39;,\u0026#39;sub_mat_demask\u0026#39;] features_3 = [\u0026#39;sequence_embedding\u0026#39;] features_wide = features_1 + features_2 features_embedding = [\u0026#39;wildtype_encoder\u0026#39;,\u0026#39;mutant_encoder\u0026#39;] XGBoost模型的定义 import xgboost as xgb print(\u0026#39;XGB Version\u0026#39;,xgb.__version__) FOLDS = 11 SEED = 123 # XGB MODEL PARAMETERS xgb_parms = { \u0026#39;max_depth\u0026#39;:100, \u0026#39;learning_rate\u0026#39;:0.01, \u0026#39;subsample\u0026#39;:0.6, \u0026#39;colsample_bytree\u0026#39;:0.2, \u0026#39;eval_metric\u0026#39;:(\u0026#39;rmse\u0026#39;), \u0026#39;objective\u0026#39;:\u0026#39;reg:squarederror\u0026#39;, # \u0026#39;tree_method\u0026#39;:\u0026#39;gpu_hist\u0026#39;, # \u0026#39;predictor\u0026#39;:\u0026#39;gpu_predictor\u0026#39;, \u0026#39;random_state\u0026#39;:19990829 } 深度学习模型 class ThermoNet2Dataset(Dataset): def __init__(self, df=None, features=None): self.df = df self.features = features self.features_wide = features_wide self.features_embedding = features_embedding def __getitem__(self, item): if self.df is not None: r = self.df.iloc[item] if \u0026#39;ddG\u0026#39; in self.df.columns: return torch.as_tensor(r.features, dtype=torch.float),torch.as_tensor(r[self.features_wide], dtype=torch.float) , torch.as_tensor(r[self.features_embedding],dtype=torch.int) , torch.tensor(r.ddG, dtype=torch.float), torch.tensor(r.dT, dtype=torch.float) else: return torch.as_tensor(r.features, dtype=torch.float),torch.as_tensor(r[self.features_wide], dtype=torch.float) , torch.as_tensor(r[self.features_embedding],dtype=torch.int) else: return torch.as_tensor(self.features_deep[item], dtype=torch.float) def __len__(self): return len(self.df) if self.df is not None else len(self.features_deep) class ThermoNet2(nn.Module): def __init__(self, params): super().__init__() ## 3DCNN CONV_LAYER_SIZES = [14, 16, 24, 32, 48, 78, 128] FLATTEN_SIZES = [0, 5488, 5184, 4000, 3072, 2106, 1024] dropout_rate = params[\u0026#39;dropout_rate\u0026#39;] dropout_rate_dt = params[\u0026#39;dropout_rate_dt\u0026#39;] dense_layer_size = int(params[\u0026#39;dense_layer_size\u0026#39;]) layer_num = int(params[\u0026#39;conv_layer_num\u0026#39;]) silu = params[\u0026#39;SiLU\u0026#39;] self.params = params if silu: activation = nn.SiLU() else: # activation = nn.LeakyReLU() activation = nn.ReLU() self.activation = activation model = [ nn.Sequential( *[nn.Sequential( nn.Conv3d(in_channels=CONV_LAYER_SIZES[l], out_channels=CONV_LAYER_SIZES[l + 1], kernel_size=(3, 3, 3)), activation ) for l in range(layer_num)] ), nn.MaxPool3d(kernel_size=(2,2,2)), nn.Flatten(), ] flatten_size = FLATTEN_SIZES[layer_num] if self.params[\u0026#39;LayerNorm\u0026#39;]: model.append(nn.LayerNorm(flatten_size)) self.model = nn.Sequential(*model) # flatten_size += len(features_wide) self.Dropout = nn.Dropout(p=0.5) self.ddG = nn.Sequential( nn.Dropout(p=dropout_rate), nn.Linear(in_features=flatten_size , out_features=dense_layer_size), activation, nn.Dropout(p=dropout_rate), nn.Linear(in_features=dense_layer_size, out_features=1) ) self.dT = nn.Sequential( nn.Dropout(p=dropout_rate_dt), nn.Linear(in_features=flatten_size , out_features=dense_layer_size), activation, nn.Dropout(p=dropout_rate_dt), nn.Linear(in_features=dense_layer_size, out_features=1) ) self.batchnorm1d = nn.BatchNorm1d(len(features_wide)) self.wide_linear1 = nn.Linear(in_features=len(features_wide),out_features = 128) self.wide_linear2 = nn.Linear(in_features=128,out_features = 64) self.embedding_wild = nn.Embedding(22, 64, sparse=False) self.embedding_mutant = nn.Embedding(22, 64, sparse=False) def forward(self, x0, x1 ,x2): if self.params[\u0026#39;diff_features\u0026#39;]: x0[:, 7:, ...] -= x0[:, :7, ...] x = self.model(x0) # ddg = self.ddG(x) # dt = self.dT(x) return x DEFAULT_PARAMS = { \u0026#39;SiLU\u0026#39;: False, \u0026#39;diff_features\u0026#39;: True, \u0026#39;LayerNorm\u0026#39;: False, \u0026#39;GroupKFold\u0026#39;: False, # only use for hyperopt \u0026#39;epochs\u0026#39;: 100, \u0026#39;AdamW\u0026#39;: False, } BEST_PARAMS = {**DEFAULT_PARAMS, **{\u0026#39;AdamW\u0026#39;: True, \u0026#39;C_dt_loss\u0026#39;: 0.01, \u0026#39;OneCycleLR\u0026#39;: False, \u0026#39;batch_size\u0026#39;: 256, \u0026#39;AdamW_decay\u0026#39;: 1.3994535042337082, \u0026#39;dropout_rate\u0026#39;: 0.06297340526648805, \u0026#39;learning_rate\u0026#39;: 0.00020503764745082723, \u0026#39;conv_layer_num\u0026#39;: 6, \u0026#39;dropout_rate_dt\u0026#39;: 0.3153179929570238, \u0026#39;dense_layer_size\u0026#39;: 74.1731281147114}} params = BEST_PARAMS def get_model(): thermonet_models = [] for fold in range(10): model = ThermoNet2(params) model.load_state_dict(torch.load(f\u0026#39;/content/drive/MyDrive/Novozymes Enzyme Stability/data/weights/enzyme_fold{fold}_best_model_v5.pkl\u0026#39;)) thermonet_models.append(model) return thermonet_models thermonet_models = get_model() IS_DDG_TARGET = True def valid(models): kfold = GroupKFold(N_FOLDS) oof_predictions = np.zeros((df_train.shape[0],1024)) if params[\u0026#39;GroupKFold\u0026#39;]: print(\u0026#39;group_k_fold\u0026#39;) groups = df_train.sequence else: groups = range(len(df_train)) for fold, (train_idx, val_idx) in enumerate(tqdm(kfold.split(df_train, groups=groups), total=N_FOLDS, desc=\u0026#34;Folds\u0026#34;)): true = df_train.loc[val_idx,\u0026#39;ddG\u0026#39;] preds = [] with torch.no_grad(): model = models[fold] model.eval() DEVICE = \u0026#39;cuda\u0026#39; model = model.to(DEVICE) dl = DataLoader(ThermoNet2Dataset(df_train.iloc[val_idx]), batch_size=64) for x0 , x1 , x2 , ddg , dt in dl: pred = model.forward(x0.to(DEVICE),x1.to(DEVICE),x2.to(DEVICE)) preds += pred.cpu().numpy().tolist() oof_predictions[val_idx,:] = preds return oof_predictions tmp = valid(thermonet_models) 交叉验证 N_FOLDS = 10 kfold = GroupKFold(N_FOLDS) groups = range(len(df_train)) for fold, (train_idx, val_idx) in enumerate(tqdm(kfold.split(df_train, groups=groups), total=N_FOLDS, desc=\u0026#34;Folds\u0026#34;)): print(df_train.loc[train_idx][\u0026#39;ddG\u0026#39;].mean(),df_train.loc[val_idx][\u0026#39;ddG\u0026#39;].mean()) X_train = np.concatenate([tmp[train_idx,:],df_train.loc[train_idx, features_wide],np.concatenate(df_train.loc[train_idx, \u0026#39;sequence_embedding\u0026#39;].values).reshape(len(train_idx),1280)],axis=1) y_train = df_train.loc[train_idx,\u0026#39;ddG\u0026#39;] X_valid = np.concatenate([tmp[val_idx,:],df_train.loc[val_idx, features_wide],np.concatenate(df_train.loc[val_idx, \u0026#39;sequence_embedding\u0026#39;].values).reshape(len(val_idx),1280)],axis=1) y_valid = df_train.loc[val_idx, \u0026#39;ddG\u0026#39;] dtrain = xgb.DMatrix(data=X_train, label=y_train) dvalid = xgb.DMatrix(data=X_valid, label=y_valid) # TRAIN MODEL FOLD K model = xgb.train(xgb_parms, dtrain=dtrain, evals=[(dtrain,\u0026#39;train\u0026#39;),(dvalid,\u0026#39;valid\u0026#39;)], num_boost_round=9999, early_stopping_rounds=10, verbose_eval=500) model.save_model(f\u0026#39;./XGB_fold{fold}.xgb\u0026#39;) def valid(): N_FOLDS = 10 kfold = GroupKFold(N_FOLDS) oof_predictions = np.zeros((df_train.shape[0],1)) groups = range(len(df_train)) for fold, (train_idx, val_idx) in enumerate(tqdm(kfold.split(df_train, groups=groups), total=N_FOLDS, desc=\u0026#34;Folds\u0026#34;)): print(df_train.loc[train_idx][\u0026#39;ddG\u0026#39;].mean(),df_train.loc[val_idx][\u0026#39;ddG\u0026#39;].mean()) X_train = np.concatenate([tmp[train_idx,:],df_train.loc[train_idx, features_wide],np.concatenate(df_train.loc[train_idx, \u0026#39;sequence_embedding\u0026#39;].values).reshape(len(train_idx),1280)],axis=1) y_train = df_train.loc[train_idx,\u0026#39;ddG\u0026#39;] X_valid = np.concatenate([tmp[val_idx,:],df_train.loc[val_idx, features_wide],np.concatenate(df_train.loc[val_idx, \u0026#39;sequence_embedding\u0026#39;].values).reshape(len(val_idx),1280)],axis=1) y_valid = df_train.loc[val_idx, \u0026#39;ddG\u0026#39;] model = xgb.Booster() model.load_model(f\u0026#39;./XGB_fold{fold}.xgb\u0026#39;) dvalid = xgb.DMatrix(data=X_valid) preds = model.predict(dvalid) oof_predictions[val_idx] += preds.reshape(-1,1) print(f\u0026#39;fold {fold} , spearman {scipy.stats.spearmanr(y_valid,preds)[0]} , pearson {scipy.stats.pearsonr(y_valid,preds)[0]}\u0026#39;) true = df_train.loc[:,\u0026#39;ddG\u0026#39;].values pred = oof_predictions.reshape(-1,) print(f\u0026#39;all , spearman {scipy.stats.spearmanr(y_valid,preds)[0]} , pearson {scipy.stats.pearsonr(y_valid,preds)[0]}\u0026#39;) valid() valid() def gen_mutations(name, df, wild=\u0026#34;VPVNPEPDATSVENVALKTGSGDSQSDPIKADLEVKGQSALPFDVDCWAILCKGAPNVLQ\u0026#34;\u0026#34;RVNEKTKNSNRDRSGANKGPFKDPQKWGIKALPPKNPSWSAQDFKSPEEYAFASSLQGGT\u0026#34;\u0026#34;NAILAPVNLASQNSQGGVLNGFYSANKVAQFDPSKPQQTKGTWFQITKFTGAAGPYCKAL\u0026#34;\u0026#34;GSNDKSVCDKNKNIAGDWGFDPAKWAYQYDEKNNKFNYVGK\u0026#34;): result = [] for _, r in df.iterrows(): ops = Levenshtein.editops(wild, r.protein_sequence) assert len(ops) \u0026lt;= 1 if len(ops) \u0026gt; 0 and ops[0][0] == \u0026#39;replace\u0026#39;: idx = ops[0][1] result.append([ops[0][0], idx + 1, wild[idx], r.protein_sequence[idx]]) elif len(ops) == 0: result.append([\u0026#39;same\u0026#39;, 0, \u0026#39;\u0026#39;, \u0026#39;\u0026#39;]) elif ops[0][0] == \u0026#39;insert\u0026#39;: assert False, \u0026#34;Ups\u0026#34; elif ops[0][0] == \u0026#39;delete\u0026#39;: idx = ops[0][1] result.append([\u0026#39;delete\u0026#39;, idx + 1, wild[idx], \u0026#39;-\u0026#39;]) else: assert False, \u0026#34;Ups\u0026#34; df = pd.concat([df, pd.DataFrame(data=result, columns=[\u0026#39;op\u0026#39;, \u0026#39;idx\u0026#39;, \u0026#39;wild\u0026#39;, \u0026#39;mutant\u0026#39;])], axis=1) df[\u0026#39;mut\u0026#39;] = df[[\u0026#39;wild\u0026#39;, \u0026#39;idx\u0026#39;, \u0026#39;mutant\u0026#39;]].astype(str).apply(lambda v: \u0026#39;\u0026#39;.join(v), axis=1) df[\u0026#39;name\u0026#39;] = name return df df_test = gen_mutations(\u0026#39;wildtypeA\u0026#39;, pd.read_csv(\u0026#39;./test.csv\u0026#39;)) display(df_test) df_test = df_test.rename(columns = {\u0026#39;wild\u0026#39;:\u0026#39;wildtype\u0026#39;}) test_features = np.load(\u0026#39;/content/nesp_features.npy\u0026#39;) df_test_replace = df_test.loc[df_test.op == \u0026#39;replace\u0026#39;] df_test_replace[\u0026#39;features\u0026#39;] = [v for v in test_features] # df_test_replace[\u0026#39;length\u0026#39;] = df_test_replace[\u0026#39;protein_sequence\u0026#39;].apply(lambda x : len(str(x))) df_test_replace[\u0026#39;seq_position\u0026#39;] = df_test_replace[\u0026#39;idx\u0026#39;] - 1 df_test_replace[\u0026#39;sequence\u0026#39;] = df_test_replace[\u0026#39;protein_sequence\u0026#39;] # df_test_replace[\u0026#39;position_rate\u0026#39;] = df_test_replace[\u0026#39;seq_position\u0026#39;] / df_test_replace[\u0026#39;length\u0026#39;] df_test_replace = df_test_replace.reset_index(drop=True) df_test_replace = features_engineer(df_test_replace) df_test_replace[\u0026#39;sequence_embedding\u0026#39;] = [get_esm_embedding(i,t_model) for i in tqdm(df_test_replace[\u0026#39;sequence\u0026#39;])] 预测结果 def predict_embedding(models): test_predictions = np.zeros((df_test_replace.shape[0],1024)) for fold in range(10): preds = [] with torch.no_grad(): model = models[fold] model.eval() DEVICE = \u0026#39;cuda\u0026#39; model = model.to(DEVICE) dl = DataLoader(ThermoNet2Dataset(df_test_replace), batch_size=64) for x0 , x1 , x2 in dl: pred = model.forward(x0.to(DEVICE),x1.to(DEVICE),x2.to(DEVICE)) preds += pred.cpu().numpy().tolist() test_predictions += preds return test_predictions / 10 test_tmp = predict_embedding(thermonet_models) IS_DDG_TARGET = True def predict(): preds = [] N_FOLDS = 10 for fold in range(N_FOLDS): model = xgb.Booster() model.load_model(f\u0026#39;./XGB_fold{fold}.xgb\u0026#39;) dvalid = xgb.DMatrix(data=np.concatenate([test_tmp,df_test_replace.loc[:, features_wide],np.concatenate(df_test_replace.loc[:, \u0026#39;sequence_embedding\u0026#39;].values).reshape(len(df_test_replace),1280)],axis=1)) pred = model.predict(dvalid) preds.append(pred) return preds preds = predict() preds[0].shape 输出模型预测结果 # replacement mutations df_test_replace[\u0026#39;ddg\u0026#39;] = np.array(preds).mean(axis=0) df_test = df_test.merge(df_test_replace[[\u0026#39;seq_id\u0026#39;,\u0026#39;ddg\u0026#39;]],on=\u0026#39;seq_id\u0026#39;,how=\u0026#39;left\u0026#39;) # df_test.loc[df_test.op == \u0026#39;replace\u0026#39;, \u0026#39;ddg\u0026#39;] = test_ddg # deletion mutations df_test.loc[df_test[\u0026#39;op\u0026#39;] == \u0026#34;delete\u0026#34;, \u0026#39;ddg\u0026#39;] = df_test[df_test[\u0026#34;op\u0026#34;]==\u0026#34;replace\u0026#34;][\u0026#34;ddg\u0026#34;].quantile(q=0.25) # no mutations df_test.loc[df_test[\u0026#39;op\u0026#39;] == \u0026#34;same\u0026#34;, \u0026#39;ddg\u0026#39;] = 0. df_test.rename(columns={\u0026#39;ddg\u0026#39;: \u0026#39;tm\u0026#39;})[[\u0026#39;seq_id\u0026#39;, \u0026#39;tm\u0026#39;]].to_csv(\u0026#39;submission.csv\u0026#39;, index=False) !head submission.csv #!kaggle competitions submit -c novozymes-enzyme-stability-prediction -f submission.csv -m \u0026#34;Message\u0026#34; That is all. 计算生物学真有意思。\n","date":"2023-03-05T00:00:00Z","permalink":"https://example.com/p/kaggle%E9%85%B6%E7%9A%84%E7%83%AD%E7%A8%B3%E5%AE%9A%E6%80%A7%E9%A2%84%E6%B5%8B%E6%AF%94%E8%B5%9B_%E6%88%91%E7%9A%84%E5%8F%82%E8%B5%9B%E7%AC%94%E8%AE%B0/","title":"Kaggle酶的热稳定性预测比赛_我的参赛笔记"},{"content":"De novo design of luciferases using deep learning\nlink:https://www.nature.com/articles/s41586-023-05696-3\n","date":"2023-03-02T00:00:00Z","permalink":"https://example.com/p/de-novo-design-of-luciferases-using-deep-learning/","title":"De novo design of luciferases using deep learning"},{"content":" ","date":"2023-02-25T00:00:00Z","permalink":"https://example.com/p/protein-anatomy/","title":"《PROTEIN ANATOMY》"},{"content":"Macromolecular modeling and design in Rosetta: recent methods and frameworks\nlink:https://www.nature.com/articles/s41592-020-0848-2\n","date":"2023-01-19T00:00:00Z","permalink":"https://example.com/p/macromolecular-modeling-and-design-in-rosetta-recent-methods-and-frameworks/","title":"Macromolecular modeling and design in Rosetta: recent methods and frameworks"},{"content":"df.sort_values() DataFrame.sort_values(by, axis=0, ascending=True, inplace=False, kind=\u0026lsquo;quicksort\u0026rsquo;, na_position=\u0026lsquo;last\u0026rsquo;)\n参数说明：\naxis：{0 or ‘index’, 1 or ‘columns’}, default 0 by：str or list of str；如果axis=0，那么by=\u0026ldquo;列名\u0026rdquo;；如果axis=1，那么by=\u0026ldquo;行名\u0026rdquo; ascending：布尔型，True则升序，如果by=[\u0026lsquo;列名1\u0026rsquo;,\u0026lsquo;列名2\u0026rsquo;]，则该参数可以是[True, False]， 即第一字段升序，第二个降序 1 2 2 注意：指定多列（多行）排序时，先按排在前面的列（行）排序，如果内部有相同数据，再对相同 数据内部用下一个列（行）排序，以此类推。如何内部无重复数据，则后续排列不执行。即首先满 足排在前面的参数的排序，再排后面参数 inplace：布尔型，是否用排序后的数据框替换现有的数据框 kind：排序方法，{‘quicksort’, ‘mergesort’, ‘heapsort’}, default ‘quicksort’。似 乎不用太关心 na_position：{‘first’, ‘last’}, default ‘last’，默认缺失值排在最后面。 df.sort_index() 功能和sort_values一致，但是它默认根据行标签（是所有行标签本身，而不是行对应的数据）对所有行排序，或根据列标签对所有列排序\nsort_index(axis=0, level=None, ascending=True, inplace=False, kind=\u0026lsquo;quicksort\u0026rsquo;, na_position=\u0026lsquo;last\u0026rsquo;, sort_remaining=True, by=None)\nimport pandas as pd df = pd.DataFrame({\u0026#39;b\u0026#39;:[1,2,3,2],\u0026#39;a\u0026#39;:[4,3,2,1],\u0026#39;c\u0026#39;:[1,3,8,2]},index=[2,0,1,3]) df result: code:\ndf.sort_values(by=\u0026#34;c\u0026#34;) result: code:\ndf.sort_values(by=[\u0026#34;a\u0026#34;,\u0026#34;c\u0026#34;],ascending=[True,False]) result: code:\ndf result: code:\ndf.sort_index() #3102 变成 0123 result: code:\ndf.sort_index(axis=1)#bac 变成 abc result: ","date":"2023-01-19T00:00:00Z","permalink":"https://example.com/p/pandas%E6%95%B0%E6%8D%AE%E6%8E%92%E5%BA%8Fsort_values%E5%92%8Csort_index/","title":"pandas数据排序sort_values()和sort_index()"},{"content":" 为什么写这一页？因为中间换电脑忘记把之前关于机器学习和深度学习的个人笔记复制到新的电脑上了，旧laptop被我格式化。属于是无语了，所以特写此页做导航页，方便自己检索知识\n机器学习 基础知识：\n吴恩达老师的课程 李航老师的统计学习方法 （已学完） 简博士的机器学习课程（学完了当时更新到的知识） 小康老师的机器学习课程（这个实际讲解的李航老师的那本书，当时是靠这个理解的） 刘老师的博客（常用！）： https://www.cnblogs.com/pinard/ 框架方面只学了个sklearn，通过代码实践学习 深度学习 李宏毅老师的课程 （已学完） pytorch 基础课程 小土堆 （已学完） 李沐老师的动手深度学习（已学完） 因为自己笔记全删除了（很痛心），所以找来两个很好的笔记分享（常用）：\nhttps://github.com/ShusenTang/Dive-into-DL-PyTorch （本地安装运行会形成一个知识网页） https://blog.csdn.net/weixin_42306148/article/details/123754540?spm=1001.2014.3001.5501 ","date":"2022-12-29T00:00:00Z","permalink":"https://example.com/p/%E6%88%91%E7%9A%84%E7%AE%97%E6%B3%95%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%88%AA%E9%A1%B5/","title":"我的算法学习导航页"},{"content":" 这个post是自己去跟着阿里云天池上的机器学习的一个案例跟着敲了一遍代码，并且加了自己的理解，放到这里来随时回顾\n读取数据 import pandas as pd import numpy as np import matplotlib.pyplot as plt import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) import seaborn as sns #scipy 是一个统计学习的库 from scipy import stats train_data = pd.read_csv(\u0026#34;./zhengqi_train.txt\u0026#34;,sep=\u0026#34;\\t\u0026#34;,encoding=\u0026#34;utf-8\u0026#34;) test_data = pd.read_csv(\u0026#34;./zhengqi_test.txt\u0026#34;,sep=\u0026#34;\\t\u0026#34;,encoding=\u0026#34;utf-8\u0026#34;) 查看训练集特征变量信息 train_data.head() result: code:\ntrain_data.info result 此训练集数据共有2888个样本，数据中有V0-V37共计38个特征变量，变量类型都为数值类型，所有数据特征没有缺失值数据； 数据字段由于采用了脱敏处理，删除了特征数据的具体含义；target字段为标签变量\ncode:\ntest_data.info result: 测试集数据共有1925个样本，数据中有V0-V37共计38个特征变量，变量类型都为数值类型\n查看数据统计信息 train_data.describe() result: code:\ntest_data.describe() result: 上面数据显示了数据的统计信息，例如样本数，数据的均值mean，标准差std，最小值，最大值等\n查看数据字段信息 code:\ntrain_data.head() result: 上面显示训练集前5条数据的基本信息，可以看到数据都是浮点型数据，数据都是数值型连续型特征\ncode:\ntest_data.head() result: 画箱形图探索数据 code:\n#指定绘图对象的宽和高 fig = plt.figure(figsize=(4,8)) # orient：\u0026#34;v\u0026#34;|\u0026#34;h\u0026#34; 用于控制图像使水平还是竖直显示 sns.boxplot(train_data[\u0026#34;V0\u0026#34;],orient=\u0026#34;v\u0026#34;,width=0.5) 对于箱形图的分析： #画箱式图 columns = train_data.columns.tolist()[:39] fig = plt.figure(figsize=(20,40)) for _ in range(38): #13行3列的子图，然后i+1表示是其中的第几张 plt.subplot(13,3,_+1) sns.boxplot(train_data[columns[_]],orient=\u0026#34;v\u0026#34;,width=0.5) plt.ylabel(columns[_],fontsize=8) plt.show() 查看数据分布图 查看特征变量‘V0’的数据分布直方图，并绘制Q-Q图查看数据是否近似于正态分布 code:\nplt.figure(figsize=(10,5)) ax = plt.subplot(1,2,1) #displot绘制直方图 #fit=stats.norm 统计数据 sns.distplot(train_data[\u0026#34;V0\u0026#34;],fit=stats.norm) ax = plt.subplot(1,2,2) #stats.proplot 绘制Q-Q图 res = stats.probplot(train_data[\u0026#34;V0\u0026#34;],plot=plt) result: 查看查看所有数据的直方图和Q-Q图，查看训练集的数据是否近似于正态分布\n#子图的数量 train_cols = 6 train_rows = len(train_data.columns) plt.figure(figsize=(4*train_cols,4*train_rows)) i = 0 for col in train_data.columns: i+=1 ax = plt.subplot(train_rows,train_cols,i) sns.distplot(train_data[col]) i+=1 ax = plt.subplot(train_rows,train_cols,i) reg = stats.probplot(train_data[col],plot=plt) plt.show() result: 由上面的数据分布图信息可以看出，很多特征变量（如\u0026rsquo;V1\u0026rsquo;,\u0026lsquo;V9\u0026rsquo;,\u0026lsquo;V24\u0026rsquo;,\u0026lsquo;V28\u0026rsquo;等）的数据分布不是正态的，数据并不跟随对角线，后续可以使用数据变换对数据进行转换。\n对比同一特征变量‘V0’下，训练集数据和测试集数据的分布情况，查看数据分布是否一致\nkdeplot\n核密度估计是概率论上用来估计未知的密度函数，属于非参数检验，通过核密度估计图可以比较直观的看出样本数据本身的分布特征\ncode\nax = sns.kdeplot(train_data[\u0026#34;V0\u0026#34;],color=\u0026#34;Red\u0026#34;,shade=True) ax = sns.kdeplot(test_data[\u0026#34;V0\u0026#34;],color=\u0026#34;Blue\u0026#34;,shade=True) ax.set_xlabel(\u0026#34;V0\u0026#34;) ax.set_ylabel(\u0026#34;Frequency\u0026#34;) ax = ax.legend([\u0026#34;train\u0026#34;,\u0026#34;test\u0026#34;]) result 查看所有特征变量下，训练集数据和测试集数据的分布情况，分析并寻找出数据分布不一致的特征变量。\ncode:\ndist_col = 6 dist_row = len(test_data.columns) plt.figure(figsize=(4*dist_col,4*dist_row)) i = 1 for col in test_data.columns: ax = plt.subplot(dist_row,dist_col,i) ax = sns.kdeplot(train_data[col],color=\u0026#34;Red\u0026#34;,shade=True) ax = sns.kdeplot(test_data[col],color=\u0026#34;Blue\u0026#34;,shade=True) ax.set_xlabel = str(col) ax.set_ylabel = \u0026#34;Frequency\u0026#34; ax = ax.legend([\u0026#34;train\u0026#34;,\u0026#34;test\u0026#34;]) i+=1 plt.show() 由上图的数据分布可以看到特征\u0026rsquo;V5\u0026rsquo;,\u0026lsquo;V9\u0026rsquo;,\u0026lsquo;V11\u0026rsquo;,\u0026lsquo;V17\u0026rsquo;,\u0026lsquo;V22\u0026rsquo;,\u0026lsquo;V28\u0026rsquo; 训练集数据与测试集数据分布不一致，会导致模型泛化能力差，采用删除此类特征方法。\ncode:\ndrop_colums = [\u0026#39;V5\u0026#39;,\u0026#39;V9\u0026#39;,\u0026#39;V11\u0026#39;,\u0026#39;V17\u0026#39;,\u0026#39;V22\u0026#39;,\u0026#39;V28\u0026#39;] result: 可视化线性回归关系 查看特征变量‘V0’与\u0026rsquo;target\u0026rsquo;变量的线性回归关系\nsns.regplot()绘图数据和线性回归模型拟合\ncode:\nfcols = 2 frows = 1 plt.figure(figsize=(8,4)) ax = plt.subplot(1,2,1) #以字典的形式赋予直线和点的属性 sns.regplot(x=\u0026#34;V0\u0026#34;,y=\u0026#34;target\u0026#34;,data=train_data,ax=ax, scatter_kws={\u0026#34;marker\u0026#34;:\u0026#34;.\u0026#34;,\u0026#34;s\u0026#34;:3,\u0026#34;alpha\u0026#34;:0.3}, line_kws={\u0026#34;color\u0026#34;:\u0026#34;k\u0026#34;}) plt.xlabel(\u0026#34;V0\u0026#34;) plt.ylabel(\u0026#34;target\u0026#34;) ax = plt.subplot(1,2,2) sns.distplot(train_data[\u0026#34;V0\u0026#34;].dropna()) plt.xlabel(\u0026#34;V0\u0026#34;) plt.show() result: 查看所有特征变量与\u0026rsquo;target\u0026rsquo;变量的线性回归关系\nfcols = 6 frows = len(train_data.columns) #记得调整这个，否则图挤在一起 plt.figure(figsize=(5*fcols,4*frows)) i = 0 for col in train_data.columns: i+=1 ax = plt.subplot(frows,fcols,i) ax = sns.regplot(x=col,y=\u0026#34;target\u0026#34;,data=train_data, scatter_kws={\u0026#34;marker\u0026#34;:\u0026#34;.\u0026#34;,\u0026#34;s\u0026#34;:3,\u0026#34;alpha\u0026#34;:0.3}, line_kws={\u0026#34;color\u0026#34;:\u0026#34;k\u0026#34;}) ax.set_xlabel(col) ax.set_ylabel(\u0026#34;target\u0026#34;) i+=1 ax = plt.subplot(frows,fcols,i) ax = sns.distplot(train_data[col].dropna()) ax.set_xlabel(col) plt.show() result: 查看特征变量的相关性 code:\ndata_train1 = train_data.drop([\u0026#39;V5\u0026#39;,\u0026#39;V9\u0026#39;,\u0026#39;V11\u0026#39;,\u0026#39;V17\u0026#39;,\u0026#39;V22\u0026#39;,\u0026#39;V28\u0026#39;],axis=1) #corr()可以自动计算相关性 train_corr = data_train1.corr() train_corr result: code:\n#画出相关性热力图 ax = plt.subplots(figsize=(20,16)) ax = sns.heatmap(train_corr,vmax=.8,square=True,annot=True) result: # 找出相关程度 plt.figure(figsize=(20,16)) colnm = data_train1.columns.tolist() mcorr = data_train1[colnm].corr(method=\u0026#34;spearman\u0026#34;)#相关系数矩阵，即给出了任意两个变量之间的相关系数 mask = np.zeros_like(mcorr,dtype=np.bool) mask[np.triu_indices_from(mask)] = True# 角分线右侧为True #sns.diverging_palette 生成一个调色板 cmap = sns.diverging_palette(220,10,as_cmap=True) g = sns.heatmap(mcorr,mask=mask,cmap=cmap,square=True,annot=True,fmt=\u0026#34;0.2f\u0026#34;) plt.show() result: 上图为所有特征变量和target变量两两之间的相关系数，由此可以看出各个特征变量V0-V37之间的相关性以及特征变量V0-V37与target的相关性。\n查找出特征变量和target变量相关系数大于0.5的特征变量 #寻找K个最相关的特征信息 k = 10 #nlargest()可以返回多个最大值 #的第一个参数就是截取的行数。第二个参数就是依据的列名。 cols = train_corr.nlargest(k,\u0026#34;target\u0026#34;)[\u0026#34;target\u0026#34;].index ax = plt.subplots(figsize=(10,10)) ax = sns.heatmap(train_data[cols].corr(),annot=True,square=True) plt.show() result: code:\ntrain_data.corr()[\u0026#34;target\u0026#34;]\u0026gt;0.5 result: code:\nthreshold = 0.5 corrmat = train_data.corr() #corrmat.index[abs(corrmat[\u0026#34;target\u0026#34;])\u0026gt;threshold] 可以使得大于0.5的特征名出来 top_corr_feaature = corrmat.index[abs(corrmat[\u0026#34;target\u0026#34;])\u0026gt;threshold] plt.figure(figsize=(10,10)) g = sns.heatmap(train_data[top_corr_feaature].corr(),annot=True,cmap=\u0026#34;RdYlGn\u0026#34;) result: code:\nthreshold = 0.5 corr_matrix = data_train1.corr().abs() drop_col = corr_matrix[corr_matrix[\u0026#34;target\u0026#34;]\u0026lt;threshold].index drop_col result: 由于\u0026rsquo;V14\u0026rsquo;, \u0026lsquo;V21\u0026rsquo;, \u0026lsquo;V25\u0026rsquo;, \u0026lsquo;V26\u0026rsquo;, \u0026lsquo;V32\u0026rsquo;, \u0026lsquo;V33\u0026rsquo;, \u0026lsquo;V34\u0026rsquo;特征的相关系数值小于0.5，故认为这些特征与最终的预测target值不相关，删除这些特征变量；\n","date":"2022-12-24T00:00:00Z","permalink":"https://example.com/p/%E9%98%BF%E9%87%8C%E4%BA%91_ml_02_%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2/","title":"阿里云_ML_02_数据探索"},{"content":" 这个post是自己去跟着阿里云天池上的机器学习的一个案例跟着敲了一遍代码，并且加了自己的理解，放到这里来随时回顾\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) #从scipy中导入stats统计函数 from scipy import stats plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = \u0026#34;SimHei\u0026#34; plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False train_data = pd.read_csv(\u0026#34;./zhengqi_train.txt\u0026#34;,sep=\u0026#34;\\t\u0026#34;,encoding=\u0026#34;utf-8\u0026#34;) test_data = pd.read_csv(\u0026#34;./zhengqi_test.txt\u0026#34;,sep=\u0026#34;\\t\u0026#34;,encoding=\u0026#34;utf-8\u0026#34;) train_data.describe() result: 异常值分析 plt.figure(figsize=(18,10)) #x传入的每一列的特征值（数值），labels传入的是每个特征值的名字即列名 就是图中的x轴的名字 plt.boxplot(x=train_data.values,labels=train_data.columns) plt.hlines([7.5,-7.5],0,40,colors=\u0026#34;r\u0026#34;) plt.show() result: 删除异常值 train_data = train_data[train_data[\u0026#34;V9\u0026#34;]\u0026gt;-7.5] train_data.describe() result: code:\ntrain_data.head() result: 最大最小值归一化 code:\nfrom sklearn import preprocessing feature_columns = [col for col in train_data.columns if col not in [\u0026#34;target\u0026#34;]] #注意MinScaler传入的是每一列的数据 min_max_scaler = preprocessing.MinMaxScaler() min_max_scaler = min_max_scaler.fit(train_data[feature_columns]) train_data_scaler = min_max_scaler.transform(train_data[feature_columns]) test_data_scaler = min_max_scaler.transform(test_data[feature_columns]) test_data_scaler result: code:\ntrain_data_scaler = pd.DataFrame(train_data_scaler) test_data_scaler = pd.DataFrame(test_data_scaler) train_data_scaler.columns = feature_columns test_data_scaler.columns = feature_columns train_data_scaler[\u0026#34;target\u0026#34;] = train_data[\u0026#34;target\u0026#34;] # test_data_scaler[\u0026#34;target\u0026#34;] = test_data[\u0026#34;target\u0026#34;] train_data_scaler.head() result: code:\ntest_data_scaler.head() result: 查看训练集数据和测试集数据分布情况 code:\ncols = 6 rows = len(test_data_scaler.columns) plt.figure(figsize=(4*cols,4*rows)) for i,col in enumerate(test_data_scaler.columns): ax = plt.subplot(rows,cols,i+1) ax = sns.kdeplot(train_data_scaler[col],color=\u0026#34;Red\u0026#34;,shade=True) ax = sns.kdeplot(test_data_scaler[col],color=\u0026#34;Blue\u0026#34;,shade=True) ax.set_xlabel(col) ax.set_ylabel(\u0026#34;Frequency\u0026#34;) ax = ax.legend([\u0026#34;train\u0026#34;,\u0026#34;test\u0026#34;]) plt.show() result: 查看特征\u0026rsquo;V5\u0026rsquo;, \u0026lsquo;V17\u0026rsquo;, \u0026lsquo;V28\u0026rsquo;, \u0026lsquo;V22\u0026rsquo;, \u0026lsquo;V11\u0026rsquo;, \u0026lsquo;V9\u0026rsquo;数据的数据分布\n这几个特征下，训练集的数据和测试集的数据分布不一致，会影响模型的泛化能力，故删除这些特征\n特征相关性 plt.figure(figsize=(20,16)) colmns = train_data_scaler.columns.tolist() mcorr = train_data_scaler[colmns].corr(method=\u0026#34;spearman\u0026#34;) mask = np.zeros_like(mcorr,dtype=np.bool) mask[np.tril_indices_from(mask)] = True cmap = sns.diverging_palette(220,10,as_cmap=True) sns.heatmap(mcorr,mask=mask,cmap=cmap,square=True,annot=True,fmt=\u0026#34;0.2f\u0026#34;) plt.show() result: 这前面的跟02数据探索的没啥区别，接下来的才是区别\n特征降维 相关性分析 #target的相关 mcorr = mcorr.abs() numerical_corr = mcorr[mcorr[\u0026#34;target\u0026#34;]\u0026gt;=0.1][\u0026#34;target\u0026#34;] print(numerical_corr.sort_values(ascending=False)) #获取相关性排名在前面的数据的 相关性 index0 = numerical_corr.sort_values(ascending=False).index print(train_data_scaler[index0].corr(method=\u0026#34;spearman\u0026#34;)) result: 相关性初筛 code:\nfeature_corr = numerical_corr.sort_values(ascending=False).reset_index() feature_corr.columns = [\u0026#34;features_and_target\u0026#34;,\u0026#34;corr\u0026#34;] #筛选出相关性大于0.3的特征 feature_corr_select = feature_corr[feature_corr[\u0026#34;corr\u0026#34;]\u0026gt;0.3] print(feature_corr_select) select_feature = [col for col in feature_corr_select[\u0026#34;features_and_target\u0026#34;] if col not in [\u0026#34;target\u0026#34;]] new_train_data_corr_select = train_data_scaler[select_feature+[\u0026#34;target\u0026#34;]]#注意可以直接这样相加 new_test_data_corr_select = test_data_scaler[select_feature] result: 多重线性分析 code:\n#多重共线性方差膨胀因子 from statsmodels.stats.outliers_influence import variance_inflation_factor #多重共线性 new_numerical=[\u0026#39;V0\u0026#39;, \u0026#39;V2\u0026#39;, \u0026#39;V3\u0026#39;, \u0026#39;V4\u0026#39;, \u0026#39;V5\u0026#39;, \u0026#39;V6\u0026#39;, \u0026#39;V10\u0026#39;,\u0026#39;V11\u0026#39;, \u0026#39;V13\u0026#39;, \u0026#39;V15\u0026#39;, \u0026#39;V16\u0026#39;, \u0026#39;V18\u0026#39;, \u0026#39;V19\u0026#39;, \u0026#39;V20\u0026#39;, \u0026#39;V22\u0026#39;,\u0026#39;V24\u0026#39;,\u0026#39;V30\u0026#39;, \u0026#39;V31\u0026#39;, \u0026#39;V37\u0026#39;] X = np.matrix(train_data_scaler[new_numerical]) VIF_list=[variance_inflation_factor(X,i) for i in range(X.shape[1])] VIF_list result: PCA去除多重共线性 降维 code:\nfrom sklearn.decomposition import PCA #PCA降维 保持90%的信息 pca = PCA(n_components=0.9) #注意直接用了fit_transform 就相当于fit和transform了 new_train_pca_90 = pca.fit_transform(train_data_scaler.iloc[:,0:-1]) new_test_pca_90 = pca.transform(test_data_scaler) new_train_pca_90 = pd.DataFrame(new_train_pca_90) new_test_pca_90 = pd.DataFrame(new_test_pca_90) new_test_pca_90 = pd.DataFrame(new_test_pca_90) new_train_pca_90[\u0026#34;target\u0026#34;] = train_data_scaler[\u0026#34;target\u0026#34;] new_train_pca_90.describe() result: code:\ntrain_data_scaler.describe() result: code:\npca = PCA(n_components=0.95) #注意直接用了fit_transform 就相当于fit和transform了 new_train_pca_16 = pca.fit_transform(train_data_scaler.iloc[:,0:-1]) new_test_pca_16 = pca.transform(test_data_scaler) new_train_pca_16 = pd.DataFrame(new_train_pca_16) new_test_pca_16 = pd.DataFrame(new_test_pca_16) new_test_pca_16 = pd.DataFrame(new_test_pca_16) new_train_pca_16[\u0026#34;target\u0026#34;] = train_data_scaler[\u0026#34;target\u0026#34;] new_train_pca_16.describe() result: 线性回归 导入相关库 from sklearn.linear_model import LinearRegression #线性回归 from sklearn.neighbors import KNeighborsRegressor #K近邻回归 from sklearn.tree import DecisionTreeClassifier #决策树回归 from sklearn.ensemble import RandomForestRegressor #随机森林回归 from sklearn.svm import SVR #支持向量回归 import lightgbm as lgb #lightGB模型 from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error 切分训练数据和线下验证数据 # 采用pca的0.95的数据 #填充下NAN值 new_train_pca_16 = new_train_pca_16.fillna(0) train = new_train_pca_16[new_test_pca_16.columns]#这一步好像没有什么用 target = new_train_pca_16[\u0026#34;target\u0026#34;] #训练数据0.8，验证0.2 train_data,test_data,train_target,test_target=train_test_split(train,target,test_size=0.2,random_state=22) train_data.head() result: 多元线性回归模型 #这里逻辑就是用训练集来训练模型参数，然后这些参数去得出验证集的结果， #再与实际的结果作比较，得出score clf = LinearRegression() clf.fit(train_data,train_target) score = mean_squared_error(test_target,clf.predict(test_data)) print(\u0026#34;LinearRegression: \u0026#34;,score) result:\nLinearRegression: 0.37000386014473896 code:\ntrain_score = [] test_score = [] #给予不同的数据量，查看模型的学习效果 for i in range(10,len(train_data)+1,10): lin_reg = LinearRegression() lin_reg.fit(train_data[:i],train_target[:i]) #查看模型的预测情况：两种 #模型基于训练数据集预测的情况(可以理解为模型拟合训练数据集的情况) #模型基于测试数据集预测的情况 #次处用第一种 y_train_predict = lin_reg.predict(train_data[:i]) train_score.append(mean_squared_error(train_target[:i],y_train_predict)) y_test_predict = lin_reg.predict(test_data) test_score.append(mean_squared_error(y_test_predict,test_target)) #一个点一个score对应 plt.plot([i for i in range(1,len(train_score)+1)],train_score,label=\u0026#34;train\u0026#34;) plt.plot([i for i in range(1,len(test_score)+1)],test_score,label=\u0026#34;test\u0026#34;) plt.legend() plt.show() result: code:\n#可以将上面这个封装成一个方法去调用 def plot_learning_curve(algo,X_train,X_test,y_train,y_test): \u0026#34;\u0026#34;\u0026#34;绘制学习曲线：只需要传入算法(或实例对象)、X_train、X_test、y_train、y_test\u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34;当使用该函数时传入算法，该算法的变量要进行实例化，如：PolynomialRegression(degree=2)，变量 degree 要进 行实例化\u0026#34;\u0026#34;\u0026#34; train_score = [] test_score = [] for i in range(10,len(X_train)+1,10): algo.fit(X_train[:i],y_train[:i]) y_train_predict = algo.predict(X_train[:i]) train_score.append(mean_squared_error(y_train_predict,y_train[:i])) y_test_predict = algo.predict(X_test) test_score.append(mean_squared_error(y_test_predict,y_test)) plt.plot([i for i in range(1,len(train_score)+1)],train_score,label=\u0026#34;train\u0026#34;) plt.plot([i for i in range(1,len(test_score)+1)],test_score,label=\u0026#34;test\u0026#34;) plt.legend() plt.show() K近邻回归 for i in range(3,20): clf = KNeighborsRegressor(n_neighbors=i) clf.fit(train_data,train_target) score = mean_squared_error(clf.predict(test_data),test_target) print(\u0026#34;KNeighborsRegressor: \u0026#34;,score) result: code:\nplot_learning_curve(KNeighborsRegressor(n_neighbors=5) , train_data, test_data, train_target, test_target) result: 决策树回归 from sklearn.tree import DecisionTreeRegressor clf = DecisionTreeRegressor() clf.fit(train_data,train_target) score = mean_squared_error(clf.predict(test_data),test_target) print(\u0026#34;DecisionTreeRegressor: \u0026#34;,score) result:\nDecisionTreeRegressor: 0.6178948252595156 code:\nplot_learning_curve(DecisionTreeRegressor(),train_data,test_data,train_target,test_target) result: 随机森林回归 clf = RandomForestRegressor(n_estimators=200) clf.fit(train_data,train_target) score = mean_squared_error(test_target,clf.predict(test_data)) print(\u0026#34;随机森林： \u0026#34;,score) result:\n随机森林： 0.31464010194169545 code:\nplot_learning_curve(RandomForestRegressor(),train_data,test_data,train_target,test_target) result: 提升树回归 Gradient Boosting from sklearn.ensemble import GradientBoostingRegressor myGBR = GradientBoostingRegressor(alpha=0.9,criterion=\u0026#34;friedman_mse\u0026#34;,init=None, learning_rate=0.03,loss=\u0026#34;huber\u0026#34;,max_depth=14, max_features=\u0026#34;sqrt\u0026#34;,max_leaf_nodes=None, min_impurity_decrease=0.9,random_state=22,n_estimators=300, subsample=0.8) myGBR.fit(train_data,train_target) score = mean_squared_error(test_target,clf.predict(test_data)) print(score) result:\n0.31464010194169545 code:\nplot_learning_curve(myGBR,train_data,test_data,train_target,test_target) result: ","date":"2022-12-24T00:00:00Z","permalink":"https://example.com/p/%E9%98%BF%E9%87%8C%E4%BA%91_ml_03_%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/","title":"阿里云_ML_03_特征工程"},{"content":" 这个post是自己去跟着阿里云天池上的机器学习的一个案例跟着敲了一遍代码，并且加了自己的理解，放到这里来随时回顾\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) from scipy import stats %matplotlib inline 读取数据 train_data = pd.read_csv(\u0026#34;./zhengqi_train.txt\u0026#34;,sep=\u0026#34;\\t\u0026#34;,encoding=\u0026#34;utf-8\u0026#34;) test_data = pd.read_csv(\u0026#34;./zhengqi_test.txt\u0026#34;,sep=\u0026#34;\\t\u0026#34;,encoding=\u0026#34;utf-8\u0026#34;) train_data.describe() result: 异常值分析 其实就是画给box图看离散的点\nplt.figure(figsize=(18,10)) plt.boxplot(x=train_data.values,labels=train_data.columns) plt.hlines([-7.5,7.5],0,40,colors=\u0026#34;Blue\u0026#34;) plt.show() result: 删除异常值 train_data = train_data[train_data[\u0026#34;V9\u0026#34;]\u0026gt;-7.5] train_data.describe() result: code:\ntest_data.describe() result: 最大值最小值归一化处理 from sklearn import preprocessing features_columns = [col for col in train_data.columns if col not in [\u0026#34;target\u0026#34;]] min_max_scaler = preprocessing.MinMaxScaler() min_max_scaler = min_max_scaler.fit(train_data[features_columns]) train_data_scaler = min_max_scaler.transform(train_data[features_columns]) test_data_scaler = min_max_scaler.transform(test_data[features_columns]) train_data_scaler = pd.DataFrame(train_data_scaler) train_data_scaler.columns = features_columns test_data_scaler = pd.DataFrame(test_data_scaler) test_data_scaler.columns = features_columns train_data_scaler[\u0026#34;target\u0026#34;] = train_data[\u0026#34;target\u0026#34;] train_data_scaler.describe() result: code:\ntest_data_scaler.describe() result: 查看训练集和测试集的分布情况 cols = 6 rows = len(test_data_scaler.columns) plt.figure(figsize=(4*cols,4*rows)) for i,col in enumerate(test_data_scaler.columns): #注意是（行，列） ax = plt.subplot(rows,cols,i+1) ax = sns.kdeplot(train_data_scaler[col],shade=True,color=\u0026#34;Red\u0026#34;) ax = sns.kdeplot(test_data_scaler[col],shade=True,color=\u0026#34;Blue\u0026#34;) ax.set_xlabel(col) ax.set_ylabel(\u0026#34;Frequency\u0026#34;) ax = ax.legend([\u0026#34;train\u0026#34;,\u0026#34;test\u0026#34;]) plt.show() result: 查看特征\u0026rsquo;V5\u0026rsquo;, \u0026lsquo;V17\u0026rsquo;, \u0026lsquo;V28\u0026rsquo;, \u0026lsquo;V22\u0026rsquo;, \u0026lsquo;V11\u0026rsquo;, \u0026lsquo;V9\u0026rsquo;数据的数据分布\n这几个特征下，训练集的数据和测试集的数据分布不一致，会影响模型的泛化能力，故删除这些特征\n特征相关性 code:\nplt.figure(figsize=(20,16)) #这样可以算出列与列之间的相关性 column = train_data_scaler.columns.tolist() mcorr = train_data_scaler[column].corr(method=\u0026#34;spearman\u0026#34;) mask = np.zeros_like(mcorr,dtype=np.bool) mask[np.triu_indices_from(mask)] = True cmap = sns.diverging_palette(220,10,as_cmap=True) sns.heatmap(mcorr,mask=mask,cmap=cmap,square=True,annot=True,fmt=\u0026#34;0.2f\u0026#34;) result: 特征降维 相关性分析\n#记得取相关性的绝对值 mcorr = mcorr.abs() #提取特征和target 大于0.1的数据 numerical_corr = mcorr[mcorr[\u0026#34;target\u0026#34;]\u0026gt;0.1][\u0026#34;target\u0026#34;] print(numerical_corr.sort_values(ascending=False)) #取出索引 index0 = numerical_corr.sort_values(ascending=False).index print(train_data_scaler[index0].corr(\u0026#34;spearman\u0026#34;)) result: 相关性初筛 code:\n#reset_index()重置索引 features_corr = numerical_corr.sort_values(ascending=False).reset_index() features_corr.columns = [\u0026#34;features_and_target\u0026#34;,\u0026#34;corr\u0026#34;] #筛选出大于相关性大于0.3的特征 features_corr_select = features_corr[features_corr[\u0026#34;corr\u0026#34;]\u0026gt;0.3] print(features_corr_select) #只提取出特征 selcet_features = [col for col in features_corr_select[\u0026#34;features_and_target\u0026#34;] if col not in [\u0026#34;target\u0026#34;]] new_train_data_corr_select = train_data_scaler[selcet_features+[\u0026#34;target\u0026#34;]] new_train_data_corr_select = test_data_scaler[selcet_features] result: 多重线性分析 code\n#VIF多重线性方差膨胀因子 from statsmodels.stats.outliers_influence import variance_inflation_factor #多重共线性 new_numerical=[\u0026#39;V0\u0026#39;, \u0026#39;V2\u0026#39;, \u0026#39;V3\u0026#39;, \u0026#39;V4\u0026#39;, \u0026#39;V5\u0026#39;, \u0026#39;V6\u0026#39;, \u0026#39;V10\u0026#39;,\u0026#39;V11\u0026#39;, \u0026#39;V13\u0026#39;, \u0026#39;V15\u0026#39;, \u0026#39;V16\u0026#39;, \u0026#39;V18\u0026#39;, \u0026#39;V19\u0026#39;, \u0026#39;V20\u0026#39;, \u0026#39;V22\u0026#39;,\u0026#39;V24\u0026#39;,\u0026#39;V30\u0026#39;, \u0026#39;V31\u0026#39;, \u0026#39;V37\u0026#39;] #将每个特征的所有值弄成一个matrix了 X = np.matrix(train_data_scaler[new_numerical]) # [X.shape for i in range(X.shape[1])] VIF_list = [variance_inflation_factor(X,i) for i in range(X.shape[1])] VIF_list result: PCA去除多重共线性 降维 from sklearn.decomposition import PCA pca = PCA(n_components=16) new_train_pca_16 = pca.fit_transform(train_data_scaler.iloc[:,0:-1]) new_test_pca_16 = pca.transform(test_data_scaler) new_train_pca_16 = pd.DataFrame(new_train_pca_16) new_test_pca_16 = pd.DataFrame(new_test_pca_16) new_train_pca_16[\u0026#34;target\u0026#34;] = train_data_scaler[\u0026#34;target\u0026#34;] new_train_pca_16.describe() result: 模型训练 code\nfrom sklearn.linear_model import LinearRegression from sklearn.tree import DecisionTreeRegressor from sklearn.neighbors import KNeighborsRegressor from sklearn.ensemble import RandomForestRegressor from sklearn.svm import SVR import lightgbm as lgb#lightGBM模型 from sklearn.model_selection import train_test_split from sklearn.metrics import mean_squared_error from sklearn.model_selection import learning_curve from sklearn.model_selection import ShuffleSplit 切分训练数据和验证数据 new_train_pca_16 = new_train_pca_16.fillna(0) #因为train_test_split 划分数据集时需要分别传入特征和target train = new_train_pca_16[new_test_pca_16.columns] target = new_train_pca_16[\u0026#34;target\u0026#34;] train_data,test_data,train_target,test_target = train_test_split(train,target,test_size=0.2,random_state=22) # 定义绘制模型学习曲线 #np.linspace() 在指定的间隔内返回均匀间隔的数字，返回num均匀分布的样本，在[start, stop] def plot_learning_curve(estimator,title,X,y,ylim=None,cv=None, n_jobs=1,train_sizes=np.linspace(.1,1.0,5)): plt.figure() plt.title(title) if ylim is not None: plt.ylim(*ylim) plt.xlabel(\u0026#34;Training example\u0026#34;) plt.ylabel(\u0026#34;Score\u0026#34;) train_sizes,train_scores,test_scores = learning_curve(estimator,X,y,cv=cv,n_jobs=n_jobs, train_sizes=train_sizes) train_score_mean = np.mean(train_scores,axis=1) train_scoes_std = np.std(train_scores,axis=1) test_scores_mean = np.mean(test_scores,axis=1) test_scores_std = np.std(test_scores,axis=1) print(train_score_mean) print(test_scores_mean) #网格 plt.grid() #两曲线的阴影 plt.fill_between(train_sizes,train_score_mean-train_scoes_std, train_score_mean+train_scoes_std,alpha=0.1, color=\u0026#34;r\u0026#34;) plt.fill_between(train_sizes,test_scores_mean-test_scores_std, test_scores_mean+test_scores_std,alpha=0.1,color=\u0026#34;g\u0026#34;) plt.plot(train_sizes,train_score_mean,\u0026#34;o-\u0026#34;,color=\u0026#34;r\u0026#34;, label=\u0026#34;Cross-validation score\u0026#34;) plt.plot(train_sizes,test_scores_mean,\u0026#34;o-\u0026#34;,color=\u0026#34;g\u0026#34;, label=\u0026#34;Cross-validation score\u0026#34;) plt.legend(loc=\u0026#34;best\u0026#34;) return plt 多元线性回归模型 clf = LinearRegression() clf.fit(train_data,train_target) score = mean_squared_error(test_target,clf.predict(test_data)) print(score) result:\n0.37912142408268473 绘制线性回归模型学习曲线 X = train_data.values y = train_target.values #图一 title = \u0026#34;LinearRegression\u0026#34; cv = ShuffleSplit(n_splits=100,test_size=0.2,random_state=0) estimator = LinearRegression() plot_learning_curve(estimator,title,X,y,ylim=(0.5,0.8),cv=cv,n_jobs=-1) result: KNN code\nX = train_data.values y = train_target.values #图二 title = \u0026#34;KNeighborRegressor\u0026#34; cv = ShuffleSplit(n_splits=100,test_size=0.2,random_state=0) estimator = KNeighborsRegressor() plot_learning_curve(estimator,title,X,y,ylim=(0.3,0.9),cv=cv,n_jobs=-1) result: 决策树回归 code:\nX = train_data.values y = train_target.values #图二 title = \u0026#34;DecisionTreeRegressor\u0026#34; cv = ShuffleSplit(n_splits=100,test_size=0.2,random_state=0) estimator = DecisionTreeRegressor() plot_learning_curve(estimator,title,X,y,cv=cv,n_jobs=-1) result: 随机森林回归 X = train_data.values y = train_target.values #图三 title = \u0026#34;RandomForestRegressor\u0026#34; cv = ShuffleSplit(n_splits=100,test_size=0.2,random_state=0) estimator = RandomForestRegressor(n_estimators=200) plot_learning_curve(estimator=estimator,title=title,X=X,y=y,cv=cv,n_jobs=-1) result: 绘制lgb回归学习曲线 X = train_data.values y = train_target.values title = r\u0026#34;LGBMRegressor\u0026#34; cv = ShuffleSplit(n_splits=100,test_size=0.2,random_state=22) estimator = lgb.LGBMRegressor(learning_rate=0.01, max_depth=-1, n_estimators=100, boosting_type=\u0026#34;gbdt\u0026#34;, random_state=22, objective=\u0026#34;regression\u0026#34;) plot_learning_curve(estimator,title=title,cv=cv,X=X,y=y,n_jobs=-1) result: ","date":"2022-12-24T00:00:00Z","permalink":"https://example.com/p/%E9%98%BF%E9%87%8C%E4%BA%91_ml_04_%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83/","title":"阿里云_ML_04_模型训练"},{"content":" 这个post是自己去跟着阿里云天池上的机器学习的一个案例跟着敲了一遍代码，并且加了自己的理解，放到这里来随时回顾\n过拟合与欠拟合的问题 获取并绘制数据集 import numpy as np import matplotlib.pyplot as plt import pandas as pd np.random.seed(22) x = np.random.uniform(-3.0,3.0,size=100) X = x.reshape(-1,1)#-1表示系统自动计算行 #np.random.normal()产生正态分布的数 y = 0.5 * x**2 + x + 2 + np.random.normal(0,1,size=100) plt.scatter(x,y) plt.show() result: 使用线性回归拟合数据 from sklearn.linear_model import LinearRegression lin_reg = LinearRegression() lin_reg.fit(X,y) lin_reg.score(X,y)#score返回的是准确率 result:\n0.4340452690750729 准确率为 0.434，比较低，直线拟合数据的程度较低\n使用均方误差判断拟合程度 from sklearn.metrics import mean_squared_error y_predict = lin_reg.predict(X) mean_squared_error(y_predict,y) result:\n2.7365298290204287 绘制拟合效果 plt.scatter(x,y) plt.plot(np.sort(x),y_predict[np.argsort(x)],color=\u0026#34;red\u0026#34;) plt.show() result: 使用多项式回归拟合:\n封装Pipeline管道 #Pipeline封装算法流 from sklearn.pipeline import Pipeline from sklearn.preprocessing import PolynomialFeatures from sklearn.preprocessing import StandardScaler def PolynomialRegression(degree): return Pipeline([ (\u0026#34;poly\u0026#34;,PolynomialFeatures(degree=degree)), (\u0026#34;std_scaler\u0026#34;,StandardScaler()), (\u0026#34;lin_reg\u0026#34;,LinearRegression()) ]) 使用 Pipeline 拟合数据：degree = 2 poly2_reg = PolynomialRegression(2) poly2_reg.fit(X,y) y2_predict = poly2_reg.predict(X) mean_squared_error(y,y2_predict) result:\n0.9687659188513672 绘制拟合效果 plt.scatter(x,y) plt.plot(np.sort(x),y2_predict[np.argsort(x)],color=\u0026#34;red\u0026#34;) plt.show() 调整degree为10 poly10_reg = PolynomialRegression(10) poly10_reg.fit(X,y) y10_predict = poly10_reg.predict(X) print(mean_squared_error(y,y10_predict)) plt.scatter(x,y) plt.plot(np.sort(x),y10_predict[np.argsort(x)],color=\u0026#34;r\u0026#34;) plt.show() result:\n0.8144736129032114 调整degree为100 poly100_reg = PolynomialRegression(100) poly100_reg.fit(X,y) y100_predict = poly100_reg.predict(X) print(mean_squared_error(y,y100_predict)) plt.scatter(x,y) plt.plot(np.sort(x),y100_predict[np.argsort(x)],color=\u0026#34;red\u0026#34;) plt.show() result: 分析 degree=2：均方误差为 1.0987392142417856； degree=10：均方误差为 1.0508466763764164； degree=100：均方误差为 0.6874357783433694； degree 越大拟合的效果越好，因为样本点是一定的，我们总能找到一条曲线将所有的样本点拟合，也就是说将所有的样本点都完全落在这根曲线上，使得整体的均方误差为 0； 红色曲线并不是所计算出的拟合曲线，而此红色曲线只是原有的数据点对应的 y 的预测值连接出来的结果，而且有的地方没有数据点，因此连接的结果和原来的曲线不一样； 交叉验证 交叉验证迭代器 K折交叉验证： KFold 将所有的样例划分为 k 个组，称为折叠 (fold) （如果 k = n， 这等价于 Leave One Out（留一） 策略），都具有相同的大小（如果可能）。预测函数学习时使用 k - 1 个折叠中的数据，最后一个剩下的折叠会用于测试。\nK折重复多次： RepeatedKFold 重复 K-Fold n 次。当需要运行时可以使用它 KFold n 次，在每次重复中产生不同的分割。\n留一交叉验证： LeaveOneOut (或 LOO) 是一个简单的交叉验证。每个学习集都是通过除了一个样本以外的所有样本创建的，测试集是被留下的样本。 因此，对于 n 个样本，我们有 n 个不同的训练集和 n 个不同的测试集。这种交叉验证程序不会浪费太多数据，因为只有一个样本是从训练集中删除掉的:\n留P交叉验证： LeavePOut 与 LeaveOneOut 非常相似，因为它通过从整个集合中删除 p 个样本来创建所有可能的 训练/测试集。对于 n 个样本，这产生了 {n \\choose p} 个 训练-测试 对。与 LeaveOneOut 和 KFold 不同，当 p \u0026gt; 1 时，测试集会重叠。\n用户自定义数据集划分： ShuffleSplit 迭代器将会生成一个用户给定数量的独立的训练/测试数据划分。样例首先被打散然后划分为一对训练测试集合。\n设置每次生成的随机数相同： 可以通过设定明确的 random_state ，使得伪随机生成器的结果可以重复。\n基于类标签、具有分层的交叉验证迭代器 如何解决样本不平衡问题？ 使用StratifiedKFold和StratifiedShuffleSplit 分层抽样。 一些分类问题在目标类别的分布上可能表现出很大的不平衡性：例如，可能会出现比正样本多数倍的负样本。在这种情况下，建议采用如 StratifiedKFold 和 StratifiedShuffleSplit 中实现的分层抽样方法，确保相对的类别频率在每个训练和验证 折叠 中大致保留。\nStratifiedKFold是 k-fold 的变种，会返回 stratified（分层） 的折叠：每个小集合中， 各个类别的样例比例大致和完整数据集中相同。\nStratifiedShuffleSplit是 ShuffleSplit 的一个变种，会返回直接的划分，比如： 创建一个划分，但是划分中每个类的比例和完整数据集中的相同。\n用于分组数据的交叉验证迭代器 如何进一步测试模型的泛化能力？ 留出一组特定的不属于测试集和训练集的数据。有时我们想知道在一组特定的 groups 上训练的模型是否能很好地适用于看不见的 group 。为了衡量这一点，我们需要确保验证对象中的所有样本来自配对训练折叠中完全没有表示的组。\nGroupKFold是 k-fold 的变体，它确保同一个 group 在测试和训练集中都不被表示。 例如，如果数据是从不同的 subjects 获得的，每个 subject 有多个样本，并且如果模型足够灵活以高度人物指定的特征中学习，则可能无法推广到新的 subject 。 GroupKFold 可以检测到这种过拟合的情况。\nLeaveOneGroupOut是一个交叉验证方案，它根据第三方提供的 array of integer groups （整数组的数组）来提供样本。这个组信息可以用来编码任意域特定的预定义交叉验证折叠。\n每个训练集都是由除特定组别以外的所有样本构成的。\nLeavePGroupsOut类似于 LeaveOneGroupOut ，但为每个训练/测试集删除与 P 组有关的样本。\nGroupShuffleSplit迭代器是 ShuffleSplit 和 LeavePGroupsOut 的组合，它生成一个随机划分分区的序列，其中为每个分组提供了一个组子集。\n时间序列分割 TimeSeriesSplit是 k-fold 的一个变体，它首先返回 k 折作为训练数据集，并且 (k+1) 折作为测试数据集。 请注意，与标准的交叉验证方法不同，连续的训练集是超越前者的超集。 另外，它将所有的剩余数据添加到第一个训练分区，它总是用来训练模型。\n#交叉验证所需的函数 from sklearn.model_selection import train_test_split,cross_val_score,cross_validate #交叉验证所需的子集划分方法 from sklearn.model_selection import KFold,LeaveOneOut,LeavePOut,ShuffleSplit #分层分割 from sklearn.model_selection import StratifiedKFold,StratifiedShuffleSplit #分组分隔 from sklearn.model_selection import GroupKFold,LeaveOneGroupOut,LeavePGroupsOut,GroupShuffleSplit #时间序列分割 from sklearn.model_selection import TimeSeriesSplit from sklearn import datasets #用sklearn自带数据集 from sklearn import svm from sklearn import preprocessing from sklearn.metrics import recall_score #模型度量 召回率 iris = datasets.load_iris() print(\u0026#34;样本集大小：\u0026#34;,iris.data.shape,iris.target.shape) result:\n样本集大小： (150, 4) (150,) code\n#数据集划分,训练模型 X_train,X_test,y_train,y_test = train_test_split(iris.data,iris.target,test_size=0.4,random_state=22) print(\u0026#34;训练集大小：\u0026#34;,X_train.shape,y_train.shape) print(\u0026#34;测试集大小：\u0026#34;,X_test.shape,y_test.shape) #使用训练集训练模型 clf = svm.SVC(kernel=\u0026#34;linear\u0026#34;,C=1).fit(X_train,y_train) print(\u0026#34;准确率：\u0026#34;,clf.score(X_test,y_test)) result:\n训练集大小： (90, 4) (90,) 测试集大小： (60, 4) (60,) 准确率： 0.9833333333333333 code:\n#归一化，则在测试集上也要使用训练集模型提取的归一化函数。 # 通过训练集获得归一化函数模型。（也就是先减几，再除以几的函数）。在训练集和测试集上都使用这个归一化函数 scaler = preprocessing.StandardScaler().fit(X_train) #再用transform处理数据 X_train_transform = scaler.transform(X_train) clf = svm.SVC(kernel=\u0026#34;linear\u0026#34;,C=1).fit(X_train_transform,y_train) X_test_transform = scaler.transform(X_test) print(clf.score(X_test_transform,y_test)) result:\n0.9666666666666667 # 直接调用交叉验证评估模型 clf = svm.SVC(kernel=\u0026#34;linear\u0026#34;,C=1) #就是定义cv scores = cross_val_score(clf,iris.data,iris.target,cv=5) print(scores) # 获取置信区间。（也就是均值和方差） print(\u0026#34;Accuracy: %0.2f (+/- %0.2f)\u0026#34; % (scores.mean(),scores.std()*2)) result:\n[0.96666667 1. 0.96666667 0.96666667 1. ] Accuracy: 0.98 (+/- 0.03) # 多种度量结果 #precision_macro 精度，recall_macro召回率 scoring = [\u0026#34;precision_macro\u0026#34;,\u0026#34;recall_macro\u0026#34;] scores = cross_validate(clf,iris.data,iris.target,scoring=scoring,cv=5,return_train_score=True) # scores类型为字典。包含训练得分，拟合次数， score-times （得分次数） sorted(scores.keys()) print(\u0026#34;测试结果：\u0026#34;,scores) result: ","date":"2022-12-24T00:00:00Z","permalink":"https://example.com/p/%E9%98%BF%E9%87%8C%E4%BA%91_ml_05_%E6%A8%A1%E5%9E%8B%E9%AA%8C%E8%AF%81/","title":"阿里云_ML_05_模型验证"},{"content":"Computer-aided design of functional protein interactions https://www.nature.com/articles/nchembio.251\n","date":"2022-12-22T00:00:00Z","permalink":"https://example.com/p/computer-aided-design-of-functional-protein-interactions/","title":"Computer-aided design of functional protein interactions"},{"content":" ","date":"2022-12-16T00:00:00Z","permalink":"https://example.com/p/protein-anatomy/","title":"《PROTEIN ANATOMY》"},{"content":"sklearn是利用python进行机器学习中一个非常全面和好用的第三方库，用过的都说好。今天主要记录一下sklearn中关于交叉验证的各种用法，主要是对sklearn官方文档 https://scikit-learn.org/stable/modules/cross_validation.html\nimport numpy as np from sklearn.model_selection import train_test_split from sklearn.datasets import load_iris from sklearn import svm iris = load_iris() iris.data.shape,iris.target.shape result:\n((150, 4), (150,)) train_test_split 对数据集进行快速打乱（分为训练集和测试集）, 这里相当于对数据集进行了shuffle后按照给定的test_size进行数据集划分\n这里是按照6:4对训练集测试集进行划分 X_train,X_test,y_train,y_test = train_test_split(iris.data,iris.target,test_size=.4,random_state=22)\ncode:\nX_train.shape,y_train.shape result:\n((90, 4), (90,)) code:\niris.data[:5] result: code:\nX_train[:5] result: code:\nclf = svm.SVC(kernel=\u0026#34;linear\u0026#34;,C=1) clf.fit(X_train,y_train) result:\nSVC(C=1, kernel=\u0026#39;linear\u0026#39;) clf.score(X_test,y_test) result:\n0.9833333333333333 cross_val_score 对数据集进行指定次数的交叉验证并为每次验证效果评测 其中，score 默认是以 scoring=\u0026lsquo;f1_macro’进行评测的，余外针对分类或回归还有： 这需要from　sklearn import metrics ,通过在cross_val_score 指定参数来设定评测标准； 当cv 指定为int 类型时，默认使用KFold 或StratifiedKFold 进行数据集打乱，下面会对KFold 和StratifiedKFold 进行介绍\nfrom sklearn.model_selection import cross_val_score clf = svm.SVC(kernel=\u0026#34;linear\u0026#34;,C=1) scores = cross_val_score(clf,iris.data,iris.target,cv=5) scores result:\narray([0.96666667, 1. , 0.96666667, 0.96666667, 1. ]) code:\nscores.mean() result:\n0.9800000000000001 在cross_val_score 中同样可使用pipeline 进行流水线操作 from sklearn import preprocessing from sklearn.pipeline import make_pipeline clf = make_pipeline(preprocessing.StandardScaler(),svm.SVC(C=1)) cross_val_score(clf,iris.data,iris.target,cv=5) result:\narray([0.96666667, 0.96666667, 0.96666667, 0.93333333, 1. ]) cross_val_predict cross_val_predict 与 cross_val_score 很相像，不过不同于返回的是评测效果，cross_val_predict 返回的是estimator 的分类结果（或回归值），这个对于后期模型的改善很重要，可以通过该预测输出对比实际目标值，准确定位到预测出错的地方，为我们参数优化及问题排查十分的重要\nfrom sklearn.model_selection import cross_val_predict from sklearn import metrics predicted = cross_val_predict(clf,iris.data,iris.target,cv=10) predicted result: code\nmetrics.accuracy_score(iris.target,predicted) result:\n0.9666666666666667 KFold K折交叉验证，这是将数据集分成K份的官方给定方案，所谓K折就是将数据集通过K次分割，使得所有数据既在训练集出现过，又在测试集出现过，当然，每次分割中不会有重叠。相当于无放回抽样\nfrom sklearn.model_selection import KFold X = [\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;,\u0026#34;d\u0026#34;] kf = KFold(n_splits=2) for train,test in kf.split(X): print(train,test) print(np.array(X)[train],np.array(X)[test]) print(\u0026#34;\\n\u0026#34;) result: LeaveOneOut LeaveOneOut 其实就是KFold 的一个特例，因为使用次数比较多，因此独立的定义出来，完全可以通过KFold 实现\nfrom sklearn.model_selection import LeaveOneOut X = [1,2,3,4] loo = LeaveOneOut() for train,test in loo.split(X): print(train,test) result: #使用KFold实现LeaveOneOut kf = KFold(n_splits=len(X)) for train,test in kf.split(X): print(train,test) result: LeavePOut 这个也是KFold 的一个特例，用KFold 实现起来稍麻烦些，跟LeaveOneOut 也很像。\nfrom sklearn.model_selection import LeavePOut X = np.ones(4) lpo = LeavePOut(p=2) for train,test in lpo.split(X): print(train,test) result: ShuffleSplit ShuffleSplit 咋一看用法跟LeavePOut 很像，其实两者完全不一样，LeavePOut 是使得数据集经过数次分割后，所有的测试集出现的元素的集合即是完整的数据集，即无放回的抽样，而ShuffleSplit 则是有放回的抽样，只能说经过一个足够大的抽样次数后，保证测试集出现了完成的数据集的倍数\nfrom sklearn.model_selection import ShuffleSplit X = np.arange(5) ss = ShuffleSplit(n_splits=3,test_size=.25,random_state=22) for train,test in ss.split(X): print(train,test) result: StratifiedKFold 这个就比较好玩了，通过指定分组，对测试集进行无放回抽样。\nfrom sklearn.model_selection import StratifiedKFold X = np.ones(10) y = [0,0,0,0,1,1,1,1,1,1] skf = StratifiedKFold(n_splits=3) for train,test in skf.split(X,y): print(train,test) result: GroudKFold 这个跟StratifiedKFold 比较像，不过测试集是按照一定分组进行打乱的，即先分堆，然后把这些堆打乱，每个堆里的顺序还是固定不变的\nfrom sklearn.model_selection import GroupKFold X = [.1, .2, 2.2, 2.4, 2.3, 4.55, 5.8, 8.8, 9, 10] y = [\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;d\u0026#39;,\u0026#39;d\u0026#39;,\u0026#39;d\u0026#39;] groups = [1,1,1,2,2,2,3,3,3,3] gkf = GroupKFold(n_splits=3) for train,test in gkf.split(X,y,groups=groups): print(train,test) result: LeaveOneGroupOut 这个是在GroupKFold 上的基础上混乱度又减小了，按照给定的分组方式将测试集分割下来\nfrom sklearn.model_selection import LeaveOneGroupOut X = [1, 5, 10, 50, 60, 70, 80] y = [0, 1, 1, 2, 2, 2, 2] groups = [1, 1, 2, 2, 3, 3, 3] logo = LeaveOneGroupOut() for train,test in logo.split(X,y,groups=groups): print(train,test) result: LeavePGroupsOut 这个没啥可说的，跟上面那个一样，只是一个是单组，一个是多组\nfrom sklearn.model_selection import LeavePGroupsOut X = np.arange(6) y = [1, 1, 1, 2, 2, 2] groups = [1, 1, 2, 2, 3, 3] lpgo = LeavePGroupsOut(n_groups=2) for train,test in lpgo.split(X,y,groups=groups): print(train,test) result: GroupShuffleSplot 这个是有放回抽样\nfrom sklearn.model_selection import GroupShuffleSplit X = [.1, .2, 2.2, 2.4, 2.3, 4.55, 5.8, .001] y = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;,\u0026#39;b\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;a\u0026#39;] groups = [1,1,2,2,3,3,4,4] gss = GroupShuffleSplit(n_splits=4,test_size=.5,random_state=0) for train,test in gss.split(X,y,groups=groups): print(train,test) result: TimeSeriesSplit 针对时间序列的处理，防止未来数据的使用，分割时是将数据进行从前到后切割（这个说法其实不太恰当，因为切割是延续性的。。）\nfrom sklearn.model_selection import TimeSeriesSplit X = np.array([[1,2],[3,4],[1,2],[3,4],[1,2],[3,4]]) tscv = TimeSeriesSplit(n_splits=3) for train,test in tscv.split(X): print(train,test) result: ","date":"2022-12-16T00:00:00Z","permalink":"https://example.com/p/sklearn%E4%B8%AD%E7%9A%84%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81cross-validation/","title":"sklearn中的交叉验证Cross-Validation"},{"content":"Obtaining protein foldability information from computational models of AlphaFold2 and RoseTTAFold\nlink:https://www.sciencedirect.com/science/article/pii/S2001037022003683\n","date":"2022-12-15T00:00:00Z","permalink":"https://example.com/p/obtaining-protein-foldability-information-from-computational-models-of-alphafold2-and-rosettafold/","title":"Obtaining protein foldability information from computational models of AlphaFold2 and RoseTTAFold"},{"content":"Accurate positioning of functional residues with robotics-inspired computational protein design\nlink:https://www.pnas.org/doi/abs/10.1073/pnas.2115480119\n","date":"2022-12-01T00:00:00Z","permalink":"https://example.com/p/accurate-positioning-of-functional-residues-with-robotics-inspired-computational-protein-design/","title":"Accurate positioning of functional residues with robotics-inspired computational protein design "},{"content":"Robust deep learning–based protein sequence design using ProteinMPNN\nlink:https://www.science.org/doi/abs/10.1126/science.add2187\n","date":"2022-12-01T00:00:00Z","permalink":"https://example.com/p/robust-deep-learningbased-protein-sequence-design-using-proteinmpnn/","title":"Robust deep learning–based protein sequence design using ProteinMPNN"},{"content":"Control of protein signaling using a computationally designed GTPase/GEF orthogonal pair\nlink:https://www.pnas.org/cai/doi/10.1073/pnas.1114487109\n","date":"2022-11-29T00:00:00Z","permalink":"https://example.com/p/control-of-protein-signaling-using-a-computationally-designed-gtpase/gef-orthogonal-pair/","title":"Control of protein signaling using a computationally designed GTPase/GEF orthogonal pair"},{"content":"在pandas库里面，我们常常关心的是最大的前几个，比如销售最好的几个产品，几个店，等。之前讲到的head(), 能够看到看到DF里面的前几行，如果需要看到最大或者最小的几行就需要先进行排序。max()和min()可以看到最大或者最小值，但是只能看到一个值。 所以我们可以使用nlargest()函数，nlargest()的优点就是能一次看到最大的几行，而且不需要排序。缺点就是只能看到最大的，看不到最小的。 单价排在前十的数据\nnlargest()的第一个参数就是截取的行数。第二个参数就是依据的列名。\n这样就可以筛选出单价最高的前十行，而且是按照单价从最高到最低进行排列的，所以还是按照之前的索引。\n还可以按照total_price来进行排名 按照total_price排名\nnlargest还有一个参数，keep=\u0026lsquo;first\u0026rsquo;或者\u0026rsquo;last\u0026rsquo;。当出现重复值的时候，keep=\u0026lsquo;first\u0026rsquo;,会选取在原始DataFrame里排在前面的，keep=\u0026lsquo;last\u0026rsquo;则去排后面的。\n由于nlagerst()不能去最小的多个值，如果我们一定要使用这个函数进行选取也是可以的.\n先设置一个辅助列 然后在进行选取： 以辅助列进行选取\n当然了，也可以通过head()加上排序进行选取的。\n那以前这些操作都可以通过其它函数来进行替代的话，nlargest()有什么必要介绍吗？或者说学不学这个函数有什么关系吗？\n这就是我们今天要重点介绍的，如果说要选择不同location_road下的前五名要怎么操作呢？\n很多人可能第一反应会想到先分组然后进行max()操作，但是这样的操作只能选择最大的一列： 使用max()\n但是使用max有一个问题，就是选取的是每一列的最大值，而不是选取最大值的那一行，也就是说只能在选取单列的最大值的时候才是准确的。\n这个时候我们就要想到apply和lambda的自定义函数了\n选取多个指标的TOP(N)\n这样就选出了不同loaction_road的price排在前五的行了。\nnlargest()函数在这种场景下使用是非常方便的，而且结果也已经默认排好顺序了。\n还有一些场景下需要计算分组的前几名，然后在进行求和的，这个我们也可以使用nlargest进行操作： 分组之后进行求和\n使用这种方法会出现报错提示，这个因为在列和索引都存在loaction_road，有重复，系统有警告，在实际使用时可以先改列名再操作。我们也可以换一种方式直接按照索引进行求和，这样就没有警告了： ","date":"2022-11-07T00:00:00Z","permalink":"https://example.com/p/pandas%E7%9A%84%E5%88%86%E7%BB%84%E5%8F%96%E6%9C%80%E5%A4%A7%E5%A4%9A%E8%A1%8C%E5%B9%B6%E6%B1%82%E5%92%8C%E5%87%BD%E6%95%B0nlargest/","title":"pandas的分组取最大多行并求和函数nlargest()"},{"content":" 这是我在别人的基础上，然后自行调整了很多细节信息，较为满意的结构图片的展示，特写此post来记录分享（take an example），当然了，能帮到大家也是一件极好的事 （主要用的都是cmd的命令，所以本记录不适合零基础）\n载入蛋白结构 这里我使用的是PDB code 4MDS\nload 4MDS 去除水分子和配体 这一步根据自己的需要去做 （其实做不做都无所谓）\n去水 去水（蛋白主体旁边的红点其实就是水分子）\nremove solvent 展示序列信息 set seq_view,1 可以看到这里有三个配体，名字分别为23H，DMS\n去除配体结构 resn 后面的名字 （23H，DMS）根据自己研究蛋白的配体名字来修改\nremove resn 23H remove resn DMS 复制主要参数 这里的参数来源于参考资料[^1]，我略作了修改\nselect resn HOH remove sele select hydrophobic, (resn ala+gly+val+ile+leu+phe+met) show sticks, (hydrophobic and (!name c+n+o)) color yelloworange, hydrophobic select hydrophilic, !hydrophobic and (!name c+n+o) show sticks, hydrophilic color lightblue, hydrophilic color white, bb. color oxygen, elem o color nitrogen, elem N color sulfur, elem S set ray_trace_mode,3 set stick_radius, 0.4 set cartoon_loop_radius, 0.4 set cartoon_oval_width, 0.4 set cartoon_rect_width, 0.4 set fog,0 bg_color white set valence, 0 set ray_shadow, 0 zoom orient color gray80 hide sticks 样图展示，最后会得到一个灰色带白的protein cartoon 个性化目标残基 这一步也是要根据自己感兴趣的残基来，我这里以位点ARG-191为例\n方便看结构，先将背景调回黑色\nbg_color black 将目标残基，赋予到一个新的名为test的object上\nselect test, resi 191 将主窗口的视觉集中到目标残基上\nzoom test 将目标残基展示为棍sticks状\nshow sticks,test 修改残基的颜色\ncolor tv_blue,test 保存一个pse文件 (因为在PyMOL中没有”撤回“的操作，但是可以利用pse文件乘坐时光机回到保存为pse时的那个样子，后面的步骤哪里不小心出错了，可以打开pse文件重新操作)\nsave 01.pse 展示氢键作用 寻找与目标氨基酸形成了氢键的周围氨基酸（这里使用鼠标操作会比命令行更方便）\n生成氢键 命令行版本 (生成的是”test_hbond“的object) cmd.dist(\u0026#34;test_hbond\u0026#34;,\u0026#34;(test)\u0026#34;,\u0026#34;(byobj (test)) and (not (test))\u0026#34;,quiet=1,mode=2,label=0,reset=1) Or\n鼠标操作 （生成的是”test_polar_conts“的object） 样图展示 (我这里使用的是命令行的操作)\n寻找作用的氨基酸 个人习惯，为了方便寻找目标氨基酸形成氢键的另一个氨基酸，我会先把整个蛋白都展示出棍状先\nshow sticks 然后再通过鼠标的操作，选择另一个残基，并且将其重命名为Hbond_AA 接下来，个性化Hbond_AA（重新赋色） 因为上面将整个蛋白的sticks都展现了出来，现在已经不需要了\n#隐藏整体的sticks hide sticks #展示目标氨基酸和作用氨基酸的sticks show sticks,test show sticks,Hbond_AA #修改Hbond_AA这个氨基酸的颜色 color paleyellow,Hbond_AA #将背景改回白色 bg_color white save 02.pse 修改氢键的展示形式 PyMOL默认氢键的展示形式为黄色、断点线\n修改线条形式 set dash_gap 0 参数说明： dash_gap后面的数值越大，短点先之间的间隔就越大 修改线条的粗细 set dash_radius 0.05 参数说明： dash_radius 的值越大，线条越粗 修改线条的颜色 #test_hbond名字，根据自己的修改 color red,test_hbond 个性化label的展现形式 显示label show label,Hbond_AA show label,test 当然了也可以直接使用鼠标点击”L“\n调整label大小 set label_size,36 #cmd.set(\u0026#39;label_size\u0026#39;,36,\u0026#39;\u0026#39;,0) 调整label颜色 (有需要的话，我这里还是会黑色好了) set label_color,black 调整label的字体形式 细节化label （这一部分都是用鼠标操作） Mouse Mode 改为 Editing 模式 通过鼠标将label移动到适合的位置，目前到这步是这样的效果 删除氨基酸three-letter 和 position之间的”-“，对每个label鼠标右键，进行删除即可 添加黑框 (也是对每个氨基酸鼠标右键，选择Background Color \u0026ndash; front ) 添加 label的指向对象（这个也是右键edit出现的，connector中选择show即可），这条黑线会跟着lable的移动而移动 到此目前的效果如下 导出图片 这一步使用的是ray (个人更喜欢ray)or draw，不太考虑截图，使用ray时有一个比较在意的点就是，如果你设置背景的东西的透明度则ray导出图片中是没有背景的（比如cartoon的透明度）\nA:设置cartoon的透明度为80%，使用ray渲染 使用ray渲染完后，马上使用png保存图片，如果ray完后再点击任何地方都会取消ray的效果\nset cartoon_transparency,0.8 ray png A.png B:设置cartoon的透明度为40%，使用ray渲染 set cartoon_transparency,0.4 ray png B.png C、D分别是对应使用ray而改成了draw的渲染下的情况 最后的样图展示 在透明度达到80时，ray渲染的图片是没有了背景的 同等下ray渲染的质感高于draw 当然了可以搭配上surface之类的展现形式，混搭也好看 参考资料 1.https://mp.weixin.qq.com/s/lW4n7g1VUFEsJAnllc9kRg\n","date":"2022-10-20T00:00:00Z","permalink":"https://example.com/p/%E8%9B%8B%E7%99%BD%E7%BB%93%E6%9E%84%E5%9B%BE%E7%89%87%E7%BE%8E%E5%8C%96/","title":"蛋白结构图片美化"},{"content":" PyMOL的功能十分强大，特别是命令行的参数，它可以对蛋白结构的细节展示进行许多调整，方便使用，这里集合一些参数，直接复制进pymol的cmd即可（本post会持续更新）\nPyMOL-wiki PyMOL官网本身有gallery（https://pymolwiki.org/index.php/Gallery），这里会有一堆大佬去上传自己的展示参数，只不过，我发现似乎有点少 下面的大部分是转载的[^学习来源2],当然了，用到自己的结构上时，可以自行参数调整，比如不喜欢侧链的信息，就可以自行hide起来，自行修改color等\n细节侧链美漫风 参数\nreinitialize select resn HOH remove sele select hydrophobic, (resn ala+gly+val+ile+leu+phe+met) show sticks, (hydrophobic and (!name c+n+o)) color yelloworange, hydrophobic select hydrophilic, !hydrophobic and (!name c+n+o) show sticks, hydrophilic color lightblue, hydrophilic color white, bb. color oxygen, elem o color nitrogen, elem N color sulfur, elem S set ray_trace_mode,3 set stick_radius, 0.4 set cartoon_loop_radius, 0.4 set cartoon_oval_width, 0.4 set cartoon_rect_width, 0.4 set fog,0 bg_color white set valence, 0 set ray_shadow, 0 zoom orient ray 图片展示（take PDB code:7f52，删除了水分子，图片背景实际上是白色的，放在word中会更明显） 个人很喜欢这个风格，不过我经常会将其他的侧链信息都隐藏起来，从而突出某些位点的sticks\n单色扁平莫兰迪 参数\nselect resn HOH remove sele set cartoon_loop_radius, 0.2 set cartoon_oval_width, 0.2 set cartoon_rect_width, 0.2 set specular, off set ray_trace_mode, 1 select name ca show spheres, sele set sphere_scale, 0 set cartoon_side_chain_helper, 1 bg_color white color gray80 set ray_trace_disco_factor, 1.0 set ray_trace_gain, 0.0 set ambient, 0.66 set ray_shadow, 0 zoom orient ray 样图展示\n饱和质感彩虹糖 参数\nselect resn HOH remove sele cartoon loop set cartoon_loop_radius, 1.5 spectrum set fog,0 bg_color white set valence, 0 set ray_shadow, 0 set ray_trace_mode, 3 zoom orient ray 样图展示\nAlphaFold的新美学 bg_color white set spec_reflect, 0 set ray_trace_mode, 0 set_color high_lddt_c, [0,0.325490196078431,0.843137254901961 ] set_color normal_lddt_c, [0.341176470588235,0.792156862745098,0.976470588235294] set_color medium_lddt_c, [1,0.858823529411765,0.070588235294118] set_color low_lddt_c, [1,0.494117647058824,0.270588235294118] color high_lddt_c, (b \u0026gt; 90) color normal_lddt_c, (b \u0026lt; 90 and b \u0026gt; 70) color medium_lddt_c, (b \u0026lt; 70 and b \u0026gt; 50) color low_lddt_c, (b \u0026lt; 50) space rgb set ray_shadow, 0 set fog, 0 zoom orient ray 样图展示\nSuntyle hide show cartoon select resn HOH remove sele set ambient, 1 set spec_reflect, 0 select resn HOH remove sele bg_color white util.cbc(selection=\u0026#39;(all)\u0026#39;,first_color=7,quiet=1,legacy=0,_self=cmd) set fog, 0 set cartoon_loop_radius, 1 cartoon loop zoom orient set ray_trace_mode, 3 ray 样图展示 (这个样式在PyMOL里面看可能很不好看，但是在ray之后，边缘化特别好看)\nsurface背景+氨基酸细节风-1 [^学习来源3、4]\n主要参数\nbg_color white set antialias,5 set ambient, 0.7 set ray_trace_mode,1 set transparency,0.6 ray 上面的参数是主要的，更多的信息如surface需要自己去调整去\n学习来源 1.https://pymolwiki.org/index.php/Gallery 2.https://mp.weixin.qq.com/s/lW4n7g1VUFEsJAnllc9kRg 3.https://twitter.com/tonets/status/1631207549260242946?cxt=HHwWhIC9ifSnm6MtAAAA 4.https://twitter.com/tonets/status/1631540589920493568?cxt=HHwWgIC9rbbhsqQtAAAA\n","date":"2022-10-12T00:00:00Z","permalink":"https://example.com/p/pymol%E8%9B%8B%E7%99%BD%E7%BB%93%E7%BE%8E%E5%8C%96%E5%8F%82%E6%95%B0%E6%95%B4%E5%90%88/","title":"PyMOL蛋白结美化参数(整合)"},{"content":" 在stackflow上看到了两个生动的卷积运算的动画，感觉很明了，来源：http://ww1.machinelearninguru.com/\nYour browser doesn't support HTML5 video. Here is a link to the video instead. Your browser doesn't support HTML5 video. Here is a link to the video instead. ","date":"2022-09-24T00:00:00Z","permalink":"https://example.com/p/%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97%E5%8A%A8%E7%94%BB/","title":"有意思的卷积运算动画"},{"content":" ","date":"2022-09-07T00:00:00Z","permalink":"https://example.com/p/%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%BB%93%E6%9E%84%E5%AF%BC%E8%AE%BA/","title":"《蛋白质结构导论》"},{"content":"参考资料：\nhttps://www.zhihu.com/question/67763556\nhttps://www.jianshu.com/p/a3cb124679df\n首先来了解下什么是分位数，实际上就是用概率作为依据将一批数据分开的那个点\n举个实例：\n分位数是数据分析中常用的一个统计量，经过抽样得到一个样本值，以学生成成绩为例：\n60,70,87,56,35,64,28,84,89,65.\np分位数\n如果想在这10位同学中淘汰至少35%,同时让至少65%的同学晋级，你怎么选？\n当然的想法是找一个数，小于等于这个数的同学至少有35%,大于等于这个数的同学至少有65%,\n要想顺利地找到这个数，需要将数据排序：\n28, 35, 56, 60，64, 65, 70, 84, 87, 89\n排序后上面十个数分别记为x(1)到x(10).\n至少有35%,即至少有10*35%=3.5个学生，所以x_0.35 ≥60=x(4); （从左到右）\n至少有65%，即至少有10*65%=6.5个学生，所以x_0.35≤60=x(4); （从右到左）\n故二者取交集，令x_0.35 =60.\n以上是np不为整数的情况，如果np为整数，不妨设p=0.3\n至少有30%,即至少有10*30%=3个学生，所以x_0.3 ≥56=x(3); （从左到右）\n至少有70%，即至少有10*70%=7个学生，所以x_0.3≤60=x(4); （从右到左）\n二者取交集，有两个值，一个是56,一个是60,如何选取？就取二者的平均值\nx_0.3＝（56+60）/2=58.\n补充：\n常见的四分位数：\n四分位数（Quartile) 也称四分位点，是指在统计学中把所有数值由小到大排列并分成四等份，处于三个分割点位置的数值\n第一四分位数 (Q1)，又称“较小四分位数”，等于该样本中所有数值由小到大排列后第25%的数字\n第二四分位数 (Q2)，又称“中位数”，等于该样本中所有数值由小到大排列后第50%的数字。\n第三四分位数 (Q3)，又称“较大四分位数”，等于该样本中所有数值由小到大排列后第75%的数字。\n第三四分位数与第一四分位数的差距又称四分位距（InterQuartile Range,IQR）\n正如上文所言，四分位数 就是将数据从小到大排成4等分，然后取出3个分割点的数值。百分位数则以此类推，通过分位数 我们可以对数据的分布有更深的了解\n分位数的计算方法有两种，以四分位数为例\nn+1 方法 n是项数 n+1 算出来的结果会比实际稍高一些\nn = 10 Q1 = (n+1) * 0.25 Q2 = (n+1) * 0.50 Q3 = (n+1) * 0.75 1+(n-1)的方法\n这种算法，预期的结果会比实际低一些\nn = 10 Q1 = 1 + (n-1) * 0.25 Q2 = 1 + (n-1) * 0.50 Q3 = 1 + (n-1) * 0.75 n方法\n传统统计学并没有这种方法，但在实际计算时有时候会需要该方法（(_)）。毕竟直接取才是最符 合辑的。\nn = 10 Q1 = n * 0.25 Q2 = n + (n-1) * 0.50 Q3 = n + (n-1) * 0.75 现在，回过头来看下pandas的quantile方法,他这里用的是1+(n-1)的方法\n这里建议直接用开头的去理解\nimport pandas as pd df = pd.DataFrame(([28,35,56,60,64,65,70,84,87,89]),columns=[\u0026#34;test\u0026#34;]) df result: code:\ndf.quantile(q=0.3) result: code:\ndf.quantile(q=0.35) result: code:\n#np中也有对应的方法,不过传进去的数是整数，不是小数 ，即0.3==》30 import numpy as np np.percentile(df,30) result:\n58.8 ","date":"2022-09-04T00:00:00Z","permalink":"https://example.com/p/pd.quantile%E5%88%86%E4%BD%8D%E6%95%B0%E6%96%B9%E6%B3%95/","title":"pd.quantile分位数方法"},{"content":"Highly accurate protein structure prediction with AlphaFold\nlink:https://www.nature.com/articles/s41586-021-03819-2\n","date":"2022-08-25T00:00:00Z","permalink":"https://example.com/p/de-novo-design-of-small-beta-barrel-proteins/","title":"De novo design of small beta barrel proteins"},{"content":" ","date":"2022-08-19T00:00:00Z","permalink":"https://example.com/p/python-cookbook%E7%AC%AC%E4%B8%89%E7%89%88%E4%B8%AD%E6%96%87v2.0.0/","title":"《Python Cookbook》第三版中文v2.0.0"},{"content":"Accurate prediction of protein structures and interactions using a 3-track neural network\nlink:https://www.science.org/doi/abs/10.1126/science.abj8754\n","date":"2022-08-17T00:00:00Z","permalink":"https://example.com/p/accurate-prediction-of-protein-structures-and-interactions-using-a-3-track-neural-network/","title":"Accurate prediction of protein structures and interactions using a 3-track neural network"},{"content":"Automated Design of Efficient and Functionally Diverse Enzyme Repertoires\nlink:https://www.sciencedirect.com/science/article/pii/S1097276518306932\n","date":"2022-08-11T00:00:00Z","permalink":"https://example.com/p/automated-design-of-efficient-and-functionally-diverse-enzyme-repertoires/","title":"Automated Design of Efficient and Functionally Diverse Enzyme Repertoires "},{"content":" import numpy as np import pandas as pd df = pd.DataFrame(np.arange(6).reshape((2,3)),index=[\u0026#34;street1\u0026#34;,\u0026#34;street2\u0026#34;],columns=[\u0026#34;one\u0026#34;,\u0026#34;two\u0026#34;,\u0026#34;sthree\u0026#34;]) df result: code:\ndata2 = df.stack() data2 result: code:\ndata3 = data2.unstack() data3 result: ","date":"2022-08-10T00:00:00Z","permalink":"https://example.com/p/pandas-stack%E5%92%8Cunstack%E5%87%BD%E6%95%B0/","title":"pandas stack和unstack函数"},{"content":"Computational Thermostabilization of an Enzyme\nlink:https://www.science.org/doi/abs/10.1126/science.1107387\n","date":"2022-08-09T00:00:00Z","permalink":"https://example.com/p/computational-thermostabilization-of-an-enzyme/","title":"Computational Thermostabilization of an Enzyme"},{"content":"sklearn.preprocessing.StandardScaler数据标准化 如果某个特征的方差远大于其它特征的方差，那么它将会在算法学习中占据主导位置，导致我们的学习器不能像我们期望的那样，去学习其他的特征，这将导致最后的模型收敛速度慢甚至不收敛，因此我们需要对这样的特征数据进行标准化/归一化\nStandarScaler 标准化数据通过减去均值然后除以方差（或标准差），这种数据标准化方法经过处理后数据符合标准正态分布，即均值为0，标准差为1，转化函数为：x =(x - 𝜇)/𝜎\nimport numpy as np from sklearn.preprocessing import StandardScaler \u0026#34;\u0026#34;\u0026#34; scale_ : 缩放比列，同时也是标准差 mean_ : 每个特征的平均值 var_ : 每个特征的方差 n_samples_seen_ : 样本数量 \u0026#34;\u0026#34;\u0026#34; x = np.array(range(1,10)).reshape(-1,1) ss = StandardScaler() ss.fit(X=x) print(x) print(ss.n_samples_seen_) print(ss.mean_) print(ss.var_) print(ss.scale_) print(\u0026#34;标准化后的数据：\u0026#34;) y = ss.fit_transform(x) print(y) result: ","date":"2022-08-08T00:00:00Z","permalink":"https://example.com/p/sklearn.preprocessing.standardscaler%E6%95%B0%E6%8D%AE%E6%A0%87%E5%87%86%E5%8C%96/","title":"sklearn.preprocessing.StandardScaler数据标准化"},{"content":" ","date":"2022-07-09T00:00:00Z","permalink":"https://example.com/p/%E7%BB%93%E6%9E%84%E7%94%9F%E7%89%A9%E5%AD%A6/","title":"《结构生物学》"},{"content":"De Novo Computational Design of Retro-Aldol Enzymes\nlink:https://www.science.org/doi/abs/10.1126/science.1152692\n","date":"2022-07-02T00:00:00Z","permalink":"https://example.com/p/de-novo-computational-design-of-retro-aldol-enzymes/","title":"De Novo Computational Design of Retro-Aldol Enzymes"},{"content":" ","date":"2022-06-12T00:00:00Z","permalink":"https://example.com/p/%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%BB%93%E6%9E%84%E9%A2%84%E6%B5%8B%E5%AE%9E%E9%AA%8C%E6%8C%87%E5%8D%97/","title":"《蛋白质结构预测实验指南》"},{"content":"Part 2 - Molecular Representations and Surfaces Fectch 结构 这次使用的是1d86 （PDB code）,打开后发现是个DNA结构，长这样，Chimera默认对DNA对象展示的标准是：\nribbon 带状 个性化展示核糖核酸和碱基 养成好习惯，把Side View也打开下 如果Fetch时不成功，可以手动去RCSB上下载蛋白的结构文件，再使用Menu去open\nPresets: 快速修改表现形式 打开presets的选择 顶部menu 这里选择第二种看看\nPresets... Interactive 2 (all atoms) presets 可供选择的套装说明 参考[^学习来源2] presets 总的来说就是已经设定好的一系列配置，它可以一键快速调整你的结构的展现形式，颜色等\nInteractive模式 这个模式是为了方便展示”交互作用的“（官方的描述是Interactive presets are meant for interactive manipulation and analysis. They may change which items (atoms, ribbons, surfaces) are displayed and how they are colored.）\nInteractive 1 (ribbons) shows most peptide and nucleic acid chains as ribbons, plus atomic detail (excluding hydrogens on carbon atoms) for residues within 3.6 Å of a ligand residue or metal ion. Atomic detail is also used for chains that are very short. Nucleic acids may be shown with special sugar and base representations (as produced by Nucleotides), with the level of abstraction dependent on size. 蛋白或者核酸主链会展示为带状 重要残基或者底物3.6A距离内的残基或者金属离子细节得到加强 核酸的话还会特别表现核糖和碱基的样式 Interactive 2 (all atoms) displays all atoms and bonds as wires, with heteroatoms colored by element. Carbons are shown in the model colors so that models will be distinguishable from one another. 原子以及所成的键展示为wire线状 原子会以element赋色 如有不同的结构体时，主要的色调会展现为不同的颜色，以区分多个结构体 Interactive 3 (hydrophobicity surface) shows amino acid hydrophobicity in the Kyte-Doolittle scale with colors ranging from dodger blue for the most hydrophilic to white at 0.0 to to orange red for the most hydrophobic (different color-codings can be applied with rangecolor or Render by Attribute). Surfaces of nonpeptides will be colored to match the underlying atoms instead. 会根据氨基酸的亲水性去赋予色彩（白到橙红色），每个AA的亲水性是根据如下的表去判断的 图片来源[^学习来源3]\n注意下，在表现出surface时，同时会添加上这个结构的\u0008surface的model ID\nPublication 模式 presets are intended for generating images for presentation and publication. They do not change which items are displayed or their colors, but may change the styles of the items. For example, ribbons are not hidden/shown, but any existing ribbon will be adjusted to rounded ribbon or licorice according to the chosen preset. Similarly, atoms and bonds are not hidden/shown, but any wire will be changed to sticks. The background color is set to white. Publication presets may decrease interactive performance because they increase smoothness by using finer divisions to depict curved objects (ribbons, molecular surfaces, etc.). Individual display parameters are discussed in more detail in the tips on preparing images. \u0026gt; 总的来说就是Publication，是为了出版图片的，它会强制性一些调整，比如背景颜色修改为白色等\nPublication 1 (silhouette,rounded ribbon) silhouette 轮廓\nPublication 2 (silhouette, licorice) licorice 甘草色？ Publication 3 (depth-cued, rounded ribbon) depth-cued 深度感 Publication 4 (depth-cued, licorice) Publication 5 (flat ribbon) with both silhouettes and depth-cueing, and ribbon inside color set to white 可以到这个ribbon inside color white 实际上就是带状展示的背面(后面)为白色\n注意了 不同的模式之间会在不影响其模式的参数下，允许叠加，比如你上次是红色的，但是这次模式色彩中没有设置，则仍为红色，若模式有定义好的，则会强制转换\n选中不同的C原子，赋予色彩，隐藏水分子 在上一步中选择了 int 2的展现模式\nPresets... Interactive 2 (all atoms) select/color 将C元素改为白色 Select... Chemistry... element... C Actions... Color... white select 可以看看右下角的那个放大镜是否变为了绿色 hide 将水分子隐藏 当然也可以选择将水分子删除掉，但是如果后续还有用的话，将其隐藏就好了 另外，水分子，是在Structure那里选择的\n顶部menu\nSelect... Structure... solvent** Actions... Atoms/Bonds... hide** 注意了，此时我这里select时，selection model 是replace不是append，所以不会也选中上面选中的C元素 可以看到红色的小点（水分子）没有了\n同样，选中不同的碱基并且修改颜色 碱基的选择是在Residue中的\nDA DNA上的腺嘌呤 顶部menu，选中A碱基，赋予蓝色\nSelect... Residue... DA Actions... Color... blue DC DNA上的胞嘧啶 顶部menu，选中C碱基，赋予cyan(蓝绿色)\nSelect... Residue... DC Actions... Color... cyan DG DNA上的鸟嘌呤 顶部menu，选中G碱基，赋予黄色\nSelect... Residue... DG Actions... Color... yellow DT DNA上的胸腺嘧啶 顶部menu，选中T碱基，赋予粉色\nSelect... Residue... DT Actions... Color... magenta 注意下，selection model是replace\n换种molecular display 顶部menu 所有原子都展现为球状\nActions... Atoms/Bonds... sphere 将A链的原子展现为球棍状，其他的不变\nSelect... Chain... A Actions... Atoms/Bonds... ball \u0026amp; stick 将所有的原子都表现为棍状\n#清空选中的部分，这会导致选择整体 Select... Clear Selection Actions... Atoms/Bonds... stick 将ribbon带状更具圆滑感 顶部menu\n#展示带状 Actions... Ribbon... show #edged,将带状的边缘更加边缘化，感觉上是更方 Actions... Ribbon... edged #rounded,将带状的边缘变得圆滑一点 Actions... Ribbon... rounded 将DNA的碱基变得有层次感 在Chimera中DNA有特殊的 nucleotide objects,对此我们可以使得它的表现更加具有层次\n打开 Nucleotides 窗口 顶部menu\nActions... Atoms/Bonds... nucleotide objects... settings... 调整参数 set Show side (sugar/base) as to tube/slab set Show base orientation to false click Slab Style tab, set slab style to skinny click Slab Options tab, set Slab object to ellipsoid click Apply; these are the “lollipops” 这些参数可以获得如下的效果 使用sequences和Depiction进行美化 使用sequences工具去选中部分碱基 打开sequences窗口\nFavorites... Sequence 选中A链上的部分碱基 使用Nucleotides去调整 打开Nucleotides窗口（这种打开方式和上面的action中打开是一样的） 顶部menu\nTools... Depiction... Nucletides 进行如下，参数调整\nset Show base orientation to true set Slab object to box click Apply; base orientations are shown with “bumps” 可以看到之前选中的碱基变成了另一种类似泵的展现形式 将DNA的整体展现为阶梯状 清除已选部分，恢复整体全选 顶部menu\nSelect... Clear Selection （可以看右下角的那个放大镜是否是绿色的，是绿色的，则不是全选整体） 或者 Ctrl + 鼠标左键空白处\nchimera 默认是全选\n打开Nucleotides，调整参数 顶部menu\nTools... Depiction... Nucletides 进行如下参数调整：\nset Show side (sugar/base) as to ladder in the Ladder Options, set Rung(阶梯的意思) radius to 0.3 Å click OK (which will also dismiss the dialog) 一键DNA碱基的展示去除 顶部menu\nActions... Atoms/Bonds... nucleotide objects... off 顺便展示会球棍模型吧 顶部menu\n#把ribbon带状的展示隐藏 Actions... Ribbon... hide #显示为球棍模型 Actions... Atoms/Bonds... ball \u0026amp; stick 通过surface美化 内置的对象 在chimera中有内置了的一些对象可以快速选中[^参考学习来源4]（即不管是结构，不管是哪个PDB，chimera都能一下划分出这些关键词，比如ligand，我们一下就可以通过ligand这个关键词，一下就选中它） 展示整体(main)的surface 顶部menu\nActions... Surface... show 可以看到，当没有选中特定对象事，即chimera默认为全体，这个全体（DNA or protein）即是main，注意了，这个main不包含配体ligand\n展示配体的surface 顶部menu\n注意下，ligand 是在Structure中选中的\n#将main的surface hide起来 Actions... Surface... hide #选中配体 Select... Structure... ligand #展示配体的surface Actions... Surface... show #将surface以网状的形式展现出来 Actions... Surface... mesh 修改surface的颜色 将上一步中的ligand的surface修改为红色 顶部menu\nActions... Color... all options 选择 Coloring applies to surface（因为想要将ligand的surface的颜色修改为red，而不是本身ball\u0026amp;stick的颜色，surface默认为本身这个selection所在主窗口带有的颜色）（这时候selection还是ligand的对象） 和 红色red 最后顺便将surface清除吧，为下面的教程做准备\n#清除selection，全选 Select... Clear Selection #其实这里是将mesh弄回了全包裹的那个形式，这里更像是为了下面的展示做准备，比较直观看出区别 Actions... Surface... solid Actions... Surface... hide 现在恢复成这样了\n修改surface的透明度 顶部menu\nActions... Surface... transparency... 50% 不同的selection model selection model有不同的模式可以选择 append 模式 追加模式，其实就是叠加，在这种模式下，选择不同的AA之类的，可以叠加在一起为一个selection，就是可以连续选择不同的对象\nreplace 模式 和追加模式相反，它只允许选择单个对象，当你选择一个后，再去选择另一个，原来的那一个对象selection会被新的替代\nintersect 这个可以实现，去交集的作用，比如我想选择B链上的A和T碱基，看实例吧\nsubtract 后续再补充这个\n实例 选择B链上的A、T碱基，并且展示它们的surface\n顶部menu\n#选择append的模式去select Select... Selection Mode... append #这样会同时选中整体（A+B链）上的A和T全部碱基 Select... Residue... DA Select... Residue... DT #将模式调为intersect Select... Selection Mode... intersect #选择B链，因为上面之前已经选中了全部的A、T碱基（更换模式时不会清空selection,已选的仍会继承下去），又因为此时是intersect的模式，所以此时已选就变为了，B链上的A、T全部碱基，这就是intersect的作用 Select... Chain... B Actions... Surface... show 注意了，当使用命令行操作时，selection model选啥对命令行的命令运行没影响。因为在命令中可以直接针对性指定（我还是更喜欢命令行的操作hh），如下（效果同上一张图片）\n#在commond line中运行以下命令 #以surface的形式展B链上的A、T碱基 surf :da.b,dt.b 学习来源 1.https://www.cgl.ucsf.edu/chimera/current/docs/UsersGuide/frametut.html 2.https://www.cgl.ucsf.edu/chimera/current/docs/UsersGuide/frametut.html 3.https://www.cgl.ucsf.edu/chimera/current/docs/UsersGuide/midas/hydrophob.html 4.https://www.cgl.ucsf.edu/chimera/current/docs/UsersGuide/midas/surface.html#surfcats\n","date":"2022-06-12T00:00:00Z","permalink":"https://example.com/p/1_getting_started_menu_version-2/","title":"1_Getting_Started_Menu_Version-2"},{"content":" ","date":"2022-06-11T00:00:00Z","permalink":"https://example.com/p/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-%E9%82%B1%E8%80%81%E5%B8%88/","title":"《神经网络与深度学习-邱老师》"},{"content":" 不久前，别人发给了我github上一个蛋白质分类的深度学习模型，在运行作者的代码时，发现模型似乎有点问题（反正在我的laptop上运行了不了，应该不是电脑的问题，因为我仔细看了后发现作者使用的biLSTM有问题，经过千辛万苦，终于让我改好了hhh）,特此写一篇markdown来记录（其实我对LSTM一点都不熟悉\u0026hellip;属于是误打误撞修好的）\n项目地址：https://github.com/jgbrasier/protein-classification 作者还提供了一个PDF（IDL_Projet.pdf）里面有详细说明模型的构造 PDF主要内容 数据的预处理 模型的结构 融合了两个模型的结构，分别是CNN卷积网络和BiLSTM。 超参数的设置 预测的准确率和混淆矩阵 我对代码的bug修改记录 数据读取部分 作者是在google lab上运行的，所以有数据上传部分的代码，但是我发现代码上传的好慢，于是自己手动上传了 作者原code: 我的修改： 直接注释就好了，将dataset下载到本地的目录，再读取\n数据加载部分 后面训练时，发现数据的维度对不上，而且我觉得他这里的数据加载器有问题，所以，我自己加了个数据的处理器（主要是加在了Model前面）： CNN_BiLSTM融合模型的问题 发现这里训练是也有问题（当时没截图），作者原代码：\nclass CNN_BiLSTM(nn.Module): def __init__(self, vocab_size, embedding_size, hidden_size, n_filters, filter_sizes, num_layers, num_classes, batch_size): \u0026#34;\u0026#34;\u0026#34; vocab_size: int, number of words in vocbulary emedding_size: int, embedding dimension hidden_size: int, size of hidden layer num_layers: int, number of LSTM layers num_classes: number of classes batch_size: size of mini batches \u0026#34;\u0026#34;\u0026#34; super(CNN_BiLSTM, self).__init__() self.hidden_size = hidden_size self.num_layers = num_layers self.batch_size = batch_size self.embedding = nn.Embedding(vocab_size, embedding_size) self.convs = nn.ModuleList([nn.Conv1d(in_channels=1, out_channels=n_filters, kernel_size=(fs, embedding_size)) for fs in filter_sizes]) self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True, bidirectional=True) self.dropout = nn.Dropout(0.3) l = n_filters*len(filter_sizes) + hidden_size*2 self.fc1 = nn.Linear(l, l//2) self.fc2 = nn.Linear(l//2, num_classes) def init_hidden(self): # initialise hidden \u0026amp; cell state h0 = Variable(th.zeros(self.num_layers*2, self.batch_size, self.hidden_size)).to(device) c0 = Variable(th.zeros(self.num_layers*2, self.batch_size, self.hidden_size)).to(device) return (h0, c0) def forward(self, inputs, hidden): x = self.embedding(inputs) # x=[batch_size, seq_len, embedding_size] cnn_x = x.unsqueeze(1) # x=[batch_size, 1, seq_len, embedding_size] cnn_x = [F.relu(conv(cnn_x)).squeeze(3) for conv in self.convs] # conv_n = [batch size, n_filters, seq_len - filter_sizes[n]] cnn_x = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in cnn_x] cnn_out = self.dropout(th.cat(cnn_x, dim = 1)) lstm_out, hidden = self.lstm(x, hidden) lstm_out = lstm_out.permute(1,0,2) # lstm_out=[sequence_len, batch_size, hidden_size] because batch_first = True # feature extraction 1 using maxpool1d over all states # lstm_out = th.transpose(lstm_out, 0, 1) # lstm_out = th.transpose(lstm_out, 1, 2) # lstm_out = F.max_pool1d(lstm_out, lstm_out.size(2)).squeeze(2) # feature extraction 2 taking last hidden state lstm_out = lstm_out[-1] lstm_out = F.relu(lstm_out) # concatenate CNN and LSTM output before FC layers cnn_out = th.transpose(cnn_out, 0, 1) lstm_out = th.transpose(lstm_out, 0, 1) cnn_lstm_out = th.cat((cnn_out, lstm_out), 0) cnn_lstm_out = th.transpose(cnn_lstm_out, 0, 1) # FC layers y = F.relu(cnn_lstm_out) y = self.fc1(y) y = self.dropout(y) y = F.relu(y) y = self.fc2(y) probs = F.log_softmax(y, dim=1) # probs=[batch_size, num_classes] return probs # generate model instance # cnnbilstm_model = CNN_BiLSTM(vocab_size, embedding_size, hidden_size, n_filters, filter_sizes, num_layers, num_classes, batch_size).to(device) # print(cnnbilstm_model) 我的修改：\nclass CNN_BiLSTM(nn.Module): def __init__(self, vocab_size, embedding_size, hidden_size, n_filters, filter_sizes, num_layers, num_classes, batch_size): \u0026#34;\u0026#34;\u0026#34; vocab_size: int, number of words in vocbulary emedding_size: int, embedding dimension hidden_size: int, size of hidden layer num_layers: int, number of LSTM layers num_classes: number of classes batch_size: size of mini batches \u0026#34;\u0026#34;\u0026#34; super(CNN_BiLSTM, self).__init__() self.hidden_size = hidden_size self.num_layers = num_layers self.batch_size = batch_size self.embedding = nn.Embedding(vocab_size, embedding_size) # ,11 self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs, embedding_size)) for fs in filter_sizes]) self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, batch_first=True, bidirectional=True) self.dropout = nn.Dropout(0.3) l = n_filters*len(filter_sizes) + hidden_size*2 self.fc1 = nn.Linear(l, l//2) self.fc2 = nn.Linear(l//2, num_classes) def init_hidden(self): # initialise hidden \u0026amp; cell state h0 = Variable(th.zeros(self.num_layers*2, self.batch_size, self.hidden_size)).to(device) c0 = Variable(th.zeros(self.num_layers*2, self.batch_size, self.hidden_size)).to(device) return (h0, c0) def forward(self, inputs, hidden): x = self.embedding(inputs) # x=[batch_size, seq_len, embedding_size] print(x.shape) cnn_x = x.unsqueeze(1) # x=[batch_size, 1, seq_len, embedding_size] cnn_x = [F.relu(conv(cnn_x)).squeeze(3) for conv in self.convs] # conv_n = [batch size, n_filters, seq_len - filter_sizes[n]] cnn_x = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in cnn_x] cnn_out = self.dropout(th.cat(cnn_x, dim = 1)) lstm_out, hidden = self.lstm(x, hidden) lstm_out = lstm_out.permute(1,0,2) # lstm_out=[sequence_len, batch_size, hidden_size] because batch_first = True # feature extraction 1 using maxpool1d over all states # lstm_out = th.transpose(lstm_out, 0, 1) # lstm_out = th.transpose(lstm_out, 1, 2) # lstm_out = F.max_pool1d(lstm_out, lstm_out.size(2)).squeeze(2) # feature extraction 2 taking last hidden state lstm_out = lstm_out[-1] lstm_out = F.relu(lstm_out) # concatenate CNN and LSTM output before FC layers cnn_out = th.transpose(cnn_out, 0, 1) lstm_out = th.transpose(lstm_out, 0, 1) cnn_lstm_out = th.cat((cnn_out, lstm_out), 0) cnn_lstm_out = th.transpose(cnn_lstm_out, 0, 1) # FC layers y = F.relu(cnn_lstm_out) y = self.fc1(y) y = self.dropout(y) y = F.relu(y) y = self.fc2(y) probs = F.log_softmax(y, dim=1) # probs=[batch_size, num_classes] return probs # generate model instance cnnbilstm_model = CNN_BiLSTM(vocab_size, embedding_size, hidden_size, n_filters, filter_sizes, num_layers, num_classes, batch_size).to(device) print(cnnbilstm_model) result： 确确实实ok的\n除了其他文件路径外，其他部分的代码和作者本人的没啥区别。\n","date":"2022-06-11T00:00:00Z","permalink":"https://example.com/p/protein_class%E6%A8%A1%E5%9E%8B%E4%BF%AE%E5%A4%8D%E8%AE%B0%E5%BD%95/","title":"protein_class模型修复记录"},{"content":" ","date":"2022-05-22T00:00:00Z","permalink":"https://example.com/p/pyrosetta-textbook/","title":"《PyRosetta Textbook》"},{"content":" ","date":"2022-05-20T00:00:00Z","permalink":"https://example.com/p/%E8%9B%8B%E7%99%BD%E8%B4%A8%E7%BB%93%E6%9E%84%E9%A2%84%E6%B5%8B%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%9A%84%E5%BA%94%E7%94%A8/","title":"《蛋白质结构预测：支持向量机的应用》"},{"content":" 第一个学的python的包就是matplotlib，特地整理了去年的学习笔记\n箱形图（Box-plot）：\n又称为盒须图、盒式图或箱线图，是一种用作显示一组数据分散情况资料的统计图。它能显示出一组数据的最大值、最小值、中位数及上下四分位数\n参数如下：\nseaborn.boxplot(x=None, y=None, hue=None, data=None, order=None, hue_order=None, orient=None, color=None, palette=None, saturation=0.75, width=0.8, dodge=True, fliersize=5, linewidth=None, whis=1.5, notch=False, ax=None, **kwargs)\nx，y：dataframe中的列名（str）或者矢量数据\ndata：dataframe或者数组\nhue（str）：dataframe的列名，按照列名中的值分类形成分类的条形图\npalette：调色板，控制图像的色调\norder, hue_order (lists of strings)：用于控制条形图的顺序\norient：“v”|“h” 用于控制图像使水平还是竖直显示（这通常是从输入变量的dtype推断出来的，此参数一般当不传入x、y，只传入data的时候使用）\nfliersize：float，用于指示离群值观察的标记大小\nwhis： 确定离群值的上下界（IQR超过低和高四分位数的比例），此范围之外的点将被识别为异常值。IQR指的是上下四分位的差值。\nwidth： float，控制箱型图的宽度\n箱型图的作用：\n1.直观明了地识别数据批中的异常值 其实箱线图判断异常值的标准以四分位数和四分位距为基础，四分位数具有一定的耐抗性，多达25%的数据可以变得任意远而不会很大地扰动四分位数，所以异常值不会影响箱形图的数据形状，箱线图识别异常值的结果比较客观。由此可见，箱型图在识别异常值方面有一定的优越性。\n2.利用箱型图判断数据批的偏态和尾重 对于标准正态分布的样本，只有极少值为异常值。异常值越多说明尾部越重，自由度越小（即自由变动的量的个数）；而偏态表示偏离程度，异常值集中在较小值一侧，则分布呈左偏态；异常值集中在较大值一侧，则分布呈右偏态。 code:\n#使用iris数据集作为例子 import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) from sklearn.datasets import load_iris plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = \u0026#34;SimHei\u0026#34; data = pd.DataFrame(load_iris().data,columns=load_iris().feature_names) data[\u0026#34;sample\u0026#34;] = load_iris().target code\ndata.columns result\nIndex([\u0026#39;sepal length (cm)\u0026#39;, \u0026#39;sepal width (cm)\u0026#39;, \u0026#39;petal length (cm)\u0026#39;, \u0026#39;petal width (cm)\u0026#39;, \u0026#39;sample\u0026#39;], dtype=\u0026#39;object\u0026#39;) code\nsns.boxplot(x=data[\u0026#34;sepal length (cm)\u0026#34;],data=data) result code:\n# y参数（竖着放） sns.boxplot(y=data[\u0026#34;sepal length (cm)\u0026#34;],data=data) result: code:\n#直接是data sns.boxplot(data=data,palette=\u0026#34;Set3\u0026#34;) result: code:\nsns.boxplot(data=data,orient=\u0026#34;h\u0026#34;,palette=\u0026#34;Set2\u0026#34;) result: ","date":"2022-05-12T00:00:00Z","permalink":"https://example.com/p/sns.boxplot%E7%AE%80%E5%8D%95%E7%94%A8%E6%B3%95/","title":"sns.boxplot()简单用法"},{"content":" 第一个学的python的包就是matplotlib，特地整理了去年的学习笔记\n核密度估计是概率论上用来估计未知的密度函数，属于非参数检验，通过核密度估计图可以比较直观的看出样本数据本身的分布特征\n参数如下： sns.kdeplot(data,data2=None,shade=False,vertical=False,kernel=\u0026lsquo;gau\u0026rsquo;,bw=\u0026lsquo;scott\u0026rsquo;,gridsize=100,cut=3,clip=None,legend=True,cumulative=False,shade_lowest=True,cbar=False, cbar_ax=None, cbar_kws=None, ax=None, *kwargs)\n主要用来绘制特征变量y值的分布，看看数据符合哪种分布 用的地方不多，了解为主，不需要深入研究\ncode\nimport numpy as np import matplotlib.pyplot as plt import seaborn as sns import pandas as pd sns.set() x = np.random.randn(100) plt.plot(x)#这样是无法看出分布 sns.kdeplot(x) result code:\n#cumulative ：是否绘制累积分布 sns.kdeplot(x,cumulative=True) result: code:\n#shade：若为True，则在kde曲线下面的区域中进行阴影处理，color控制曲线及阴影的颜色 sns.kdeplot(x,shade=True,color=\u0026#34;g\u0026#34;) result: vertical：表示以X轴进行绘制还是以Y轴进行绘制\ncode:\n#y轴画图 import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) sns.kdeplot(x,vertical=True) result: 二元kde图像，很少使用，稍微了解一下即可\ncode:\n#x,y y = np.random.randn(100) sns.kdeplot(x,y) code:\n#cbar:参数位True，则会添加一个颜色棒（颜色棒在二元kde图像中才有） sns.kdeplot(x,y,shade=True,cbar=True) ","date":"2022-05-12T00:00:00Z","permalink":"https://example.com/p/sns.kdeplot%E6%A0%B8%E5%AF%86%E5%BA%A6%E4%BC%B0%E8%AE%A1%E5%9B%BE/","title":"sns.kdeplot()核密度估计图"},{"content":" 第一个学的python的包就是matplotlib，特地整理了去年的学习笔记\nsns.regplot()：绘图数据和线性回归模型拟合\n参数 seaborn.regplot(x, y, data=None, x_estimator=None, x_bins=None, x_ci=\u0026lsquo;ci\u0026rsquo;, scatter=True, fit_reg=True, ci=95, n_boot=1000, units=None, order=1, logistic=False, lowess=False, robust=False, logx=False, x_partial=None, y_partial=None, truncate=False, dropna=True, x_jitter=None, y_jitter=None, label=None, color=None, marker=\u0026lsquo;o\u0026rsquo;, scatter_kws=None, line_kws=None, ax=None)\n参数说明\nx,y：就是x,y轴的值\ndata：x,y所属的df\nx_estimator：将此函数应用于x的每个唯一值并绘制结果估计值。当x是离散变量时，这很有用。如果给定x_ci，则此估计值将自举并绘制置信区间\nx_bins：将x分成多少段\ncode:\n#使用定义为numpy数组的两个变量绘制；使用不同的颜色 import numpy as np import seaborn as sns mean,cov = [4,6],[(1.5,.7),(.7,1)] x,y = np.random.multivariate_normal(mean,cov,88).T sns.regplot(x=x,y=y,color=\u0026#34;g\u0026#34;) result: code:\n#使用pd.Series的两个变量绘制；使用不同的标记 import pandas as pd x,y = pd.Series(x,name=\u0026#34;x_var\u0026#34;),pd.Series(y,name=\u0026#34;y_var\u0026#34;) sns.regplot(x=x,y=y,marker=\u0026#34;+\u0026#34;) result: code:\n#使用68%的置信区间，这与估计的标准误差相对应: sns.regplot(x,y,ci=68) result: code:\n#使用离散x变量绘制并添加一些抖动 sns.regplot(x=\u0026#34;size\u0026#34;,y=\u0026#34;total_bill\u0026#34;,data=tips,x_jitter=1) result: code:\n#用离散x变量绘制图，显示唯一值的平均值和置信区间 sns.regplot(x=\u0026#34;size\u0026#34;,y=\u0026#34;total_bill\u0026#34;,data=tips,x_estimator=np.mean) result: code:\n#用一个连续变量划分为几个独立的区域 sns.regplot(x=x,y=y,x_bins=4) result: code:\n#用log（x）拟合回归模型并截断模型预测 sns.regplot(x=\u0026#34;size\u0026#34;,y=\u0026#34;total_bill\u0026#34;,data=tips,x_estimator=np.mean,logx=True,truncate=True) result: ","date":"2022-05-12T00:00:00Z","permalink":"https://example.com/p/sns.regplot%E7%9A%84%E7%94%A8%E6%B3%95/","title":"sns.regplot()的用法"},{"content":" 第一个学的python的包就是matplotlib，特地整理了去年的学习笔记\n这是一种检验样本数据概率分布(例如正态分布)的方法。 使用方法如下：\ncode:\nimport matplotlib.pyplot as plt from scipy import stats fig = plt.figure() res = stats.probplot(train[\u0026#34;SalePrice\u0026#34;], plot=plt) #默认检测是正态分布 plt.show() ![](picture/stats.proplot(QQ图）.png)\n红色线条表示正态分布，蓝色线条表示样本数据，蓝色越接近红色参考线，说明越符合预期分布（这是是正态分布）\nq-q 图是通过比较数据和正态分布的分位数是否相等来判断数据是不是符合正态分布\n","date":"2022-05-12T00:00:00Z","permalink":"https://example.com/p/stats.proplotqq%E5%9B%BE/","title":"stats.proplot(QQ图）"},{"content":" 第一个学的python的包就是matplotlib，特地整理了去年的学习笔记\ncode：\nimport numpy as np import seaborn as sns import matplotlib.pyplot as plt #随机创建10行12列的数组，定义一个子图宽高为9和6，应用到热力图中 np.random.seed(22) sns.set() uniform_data = np.random.randn(10,12) f,ax = plt.subplots(figsize=(16,10)) ax = sns.heatmap(uniform_data) plt.show() result: code:\n#现在在上图的基础上改变一下色彩图的上下界 ax = sns.heatmap(uniform_data,vmin=0,vmax=1) #和上图对比就会发现色彩图的上下界更明确了 result: 使用发散色图绘制以0为中心的数据的热力图 这里使用的是np.random.randn()函数，和上面的np.random.rand()函数不一样的。因为这个函数可以返回一个或一组服从标准正态分布的随机样本值，上面的np.random.rand()函数返回一个或一组服从0~1均匀分布的随机样本值，随机样本取值范围是[0,1)，不包括1\ncode:\nuniform_data = np.random.randn(10,12) f,ax = plt.subplots(figsize=(9,6)) ax = sns.heatmap(uniform_data,center=0) plt.show() result: 为行和列加上有意义的标签,使用sns.load_dataset(\u0026ldquo;flights\u0026rdquo;)自带的数据集，数据集的部分截图如下,共143行数据: code:\nflights = sns.load_dataset(\u0026#34;flights\u0026#34;) 接着使用了一个特别高效的函数pivot()，该函数有三个参数(index,columns,values)，第一个参数index是指新表的索引，第二个参数columns是新表的列名，第三个参数values是指新表中的值，看效果就比较明确了\nflights = flights.pivot(\u0026#34;month\u0026#34;,\u0026#34;year\u0026#34;,\u0026#34;passengers\u0026#34;) flights 由表可以看出第一个参数就是行标，第二个参数是列标，第三个参数是表中的值。 显示一下热力图 code:\nax = sns.heatmap(flights) plt.show() result: 使用整型格式的数值为每个单元格注释\nheatmap中的参数annot为True时，为每个单元格写入数据值。如果数组具有与数据相同的形状，则使用它来注释热力图而不是原始数据。参数fmt是指添加注释时要使用的字符串格式代码 code:\nax = sns.heatmap(flights,annot=True,fmt=\u0026#34;d\u0026#34;) result: 为每个单元格之间添加行\nheatmap函数中的参数linewidths是指划分每个单元格的行的宽度 code\nax = sns.heatmap(flights,linewidths=.5) result 换一个不同的色彩图\nheatmap函数中的参数cmap是指色彩颜色的选择，可选的颜色还有很多，比如:Accent, Accent_r, Blues, Blues_r, BrBG, BrBG_r, BuGn, BuGn_r, BuPu, BuPu_r, CMRmap, CMRmap_r, Dark2, Dark2_r, GnBu, GnBu_r, Greens, Greens_r, Greys, Greys_r, OrRd, OrRd_r, Oranges, Oranges_r, PRGn, PRGn_r, Paired, Paired_r, Pastel1, Pastel1_r, Pastel2, Pastel2_r, PiYG, PiYG_r, PuBu, PuBuGn, PuBuGn_r, PuBu_r, PuOr, PuOr_r, PuRd, PuRd_r, Purples, Purples_r, RdBu, RdBu_r, RdGy, RdGy_r, RdPu, RdPu_r, RdYlBu, RdYlBu_r, RdYlGn, RdYlGn_r, Reds, Reds_r, Set1, Set1_r, Set2, Set2_r, Set3, Set3_r, Spectral, Spectral_r, Wistia, Wistia_r, YlGn, YlGnBu, YlGnBu_r, YlGn_r, YlOrBr, YlOrBr_r, YlOrRd, YlOrRd_r\u0026hellip;其中末尾加r是颜色取反\ncode\nax = sns.heatmap(flights,cmap=\u0026#34;RdPu\u0026#34;) result: 将色彩图特定值设为中心 code:\nax = sns.heatmap(flights,center=flights.loc[\u0026#34;Jan\u0026#34;,1955]) result: 只绘制列标签不绘制行标签\nheatmap函数中的参数xticklabels,yticklabels如果是True则绘制数据框的列名称;如果是False则不绘制列名称，如果是列表则将这些替代标签绘制为xticklabels;如果是整数则使用列名称也只是绘制n个标签;如果是自动的，请尝试密集绘制不重叠的标签。\ncode:\nax = sns.heatmap(flights,xticklabels=2,yticklabels=False) result: 不绘制颜色条\nheatmap函数中的参数cbar为TRUE即绘制颜色条，为False就不绘制颜色条\ncode:\nax = sns.heatmap(flights,cbar=False) result: 使用 不同的轴作为颜色条\nsubplots函数中的参数gridspec_kw是将字典的关键字传递给GridSpec构造函数创建子图放在网格里。heatmap函数中的参数ax指绘制图的轴，否则使用当前活动的轴，cbar_ax用于绘制颜色条的轴，否则从主轴获取；cbar_kwsfig.colorbar的关键字参数.这部分不太会用大白话解释，直接上图吧\ncode:\ngrid_kws = {\u0026#34;height_ratios\u0026#34;:(.9,.09),\u0026#34;hspace\u0026#34;:.3} f,(ax,cbar_ax) = plt.subplots(2,gridspec_kw=grid_kws) ax = sns.heatmap(flights,ax=ax,cbar_ax=cbar_ax,cbar_kws={\u0026#34;orientation\u0026#34;:\u0026#34;horizontal\u0026#34;}) result: 仅绘制矩阵的一部分\n首先创建一组服从正太分布的矩阵，并计算其每一列的相关系数记为corr，创建这么多0矩阵。np.triu_indices_from(mask)这个函数一直不太理解什么意思，把输出放在这里，以后有机会遇到再再补充和纠正\nheatmap函数中的参数mask如果通过，则数据不会显示在mask为True的单元格中，具有缺失值的单元格将自动被屏蔽。参数square为Ture，则将Axes方面设置为相等，并使其每个单元格为方形。参数vmax用于锚定色彩图的值，否则会从数据和其他关键字参数推断出来，但是这里为3还是不太理解。直接贴出热力图吧\n掩码是一个与数据同样大小的数组，掩码只有真假二值，mask中为1的单元格，画图后，data对应的单元格不显示。mask中为0的单元格，画图后，data对应的单元格显示热力图\ncode\ncorr = np.corrcoef(np.random.randn(10,200)) mask = np.zeros_like(corr) print(mask) mask[np.triu_indices_from(mask)] = True with sns.axes_style(\u0026#34;white\u0026#34;): ax = sns.heatmap(corr,mask=mask,vmax=.3,square=True) result np.triu_indices_from(mask)是返回上三角矩阵的元素的坐标。The indices for the triangle. The returned tuple contains two arrays, each with the indices along one dimension of the array. Can be used to slice a ndarray of shape(n, n). 返回的是一个二元组，元组第一个元素是所有元素的行坐标，第二个元素是所有元素的列坐标。可以执行下面代码观察：\ncode:\nmask = np.zeros((4,4)) print(mask) print(np.triu_indices_from(mask)) mask[np.triu_indices_from(mask)] = True print(mask) result:\n[[0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.] [0. 0. 0. 0.]] (array([0, 0, 0, 0, 1, 1, 1, 2, 2, 3], dtype=int64), array([0, 1, 2, 3, 1, 2, 3, 2, 3, 3], dtype=int64)) [[1. 1. 1. 1.] [0. 1. 1. 1.] [0. 0. 1. 1.] [0. 0. 0. 1.]] mask : boolean array or DataFrame, optional If passed, data will not be shown in cells where mask is True. Cells with missing values are automatically masked. mask是掩码。passed是传入该参数。如果传入该参数，掩码位置是true的单元格不显示。如果某单元格无值，也自动不显示。加入下面行，可以看到效果。\n掩码是一个与数据同样大小的数组，掩码只有真假二值，mask中为1的单元格，画图后，data对应的单元格不显示。mask中为0的单元格，画图后，data对应的单元格显示热力图\n","date":"2022-05-11T00:00:00Z","permalink":"https://example.com/p/pseaborn.heatmap%E7%BB%98%E5%88%B6%E7%83%AD%E5%9B%BE/","title":"pseaborn.heatmap绘制热图"},{"content":" 第一个学的python的包就是matplotlib，特地整理了去年的学习笔记\nseaborn.diverging_palette(h_neg, h_pos, s=75, l=50, sep=10, n=6, center=\u0026lsquo;light\u0026rsquo;, as_cmap=False)\n在两个 HUSL 颜色直接建立一个发散调色板。\n如果您在使用 IPython notebook，您还可以通过 choose_diverging_palette() 函数交互式选择调色板。\n参数：h_neg, h_pos：float in [0, 359]\n图的正负范围的锚定色调\ns：[0, 100] 范围内的浮点数，可选\n图的两个范围的锚定饱和度\nl：[0, 100] 范围内的浮点数，可选\n图的两个范围的锚定亮度\nn：int，可选\n调色板中的颜色数（如果为not，返回一个colormap）\ncenter：{“light”, “dark”}, 可选\n调色板中心为亮或暗\nas_cmap：bool, 可选\n如果为 true，返回一个 matplotlib colormap 而不是一个颜色列表。\n返回值：palette or cmap：seaborn color palette or matplotlib colormap\n类似列表的颜色对象的 RGB 元组，或者可以将连续值映射到颜色的 colormap 对象，具体取决于 as_cmap 参数的值。\n另外\n创建具有暗值的连续调色板。创建具有亮值的连续调色板\ncode\n#生成一个蓝白红调色板 import seaborn as sns sns.palplot(sns.diverging_palette(240,10,n=9)) result: code:\n#生成一个更亮的绿-白-紫调色板： sns.palplot(sns.diverging_palette(150, 275, s=80, l=55, n=9)) result: code\n#生成一个蓝-黑-红调色板: sns.palplot(sns.diverging_palette(250,15,s=75,l=40,n=9,center=\u0026#34;dark\u0026#34;)) result: code:\n#生成一个蓝-黑-红调色板: from numpy import arange x = arange(25).reshape(5,5) cmap = sns.diverging_palette(220,20,sep=20,as_cmap=True) ax = sns.heatmap(x,cmap=cmap) result: ","date":"2022-05-11T00:00:00Z","permalink":"https://example.com/p/seaborn.diverging_palette%E5%8F%91%E6%95%A3%E8%B0%83%E8%89%B2%E6%9D%BF/","title":"seaborn.diverging_palette发散调色板"},{"content":" 第一个学的python的包就是matplotlib，特地整理了去年的学习笔记\nsns.distplot()集合了matplotlib的hist()于sns.kdeplot()功能，增了rugplot分布观测显示与理由scipy库fit拟合参数分布的新颖用途\n参数如下 sns.distplot(a, bins=None, hist=True, kde=True, rug=False, fit=None, hist_kws=None, kde_kws=None, rug_kws=None, fit_kws=None, color=None, vertical=False, norm_hist=False, axlabel=None, label=None, ax=None)\n直方图：先分箱，然后计算每个分箱频数的数据分布，\n和条形图的区别，条形图有空隙，直方图没有，条形图一般用于类别特征，直方图一般用于数字特征（连续型） 多用于y值和数字（连续型）特征的分布画图\ncode:\nimport matplotlib.pyplot as plt import numpy as np import pandas as pd import seaborn as sns sns.set() #切换到sns的默认运行配置 x = np.random.randn(100) sns.distplot(x) result: code:\nsns.displot(x) 通过hist和kde参数调节是否显示直方图及核密度估计(默认hist,kde均为True)\nimport warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) fig,axes = plt.subplots(1,3)#创建一个1行三列的图片 sns.displot(x,ax=axes[0]) sns.distplot(x,hist=False,ax=axes[1])#不显示直方图 sns.distplot(x,kde=False,ax=axes[2])#不显示核密度 bins：int或list，控制直方图的划分\n#bins fig,axes = plt.subplots(1,2) sns.distplot(x,kde=False,bins=20,ax=axes[0])#分成20个区间 ##以0,1,2,3为分割点，形成区间[0,1],[1,2],[2,3]，区间外的值不计入 sns.distplot(x,kde=False,bins=[x for x in range(4)],ax=axes[1]) code:\nfrom scipy.stats import * sns.distplot(x,hist=False,fit=norm)#拟合标准正态分布 code:\nsns.distplot(x,kde_kws={\u0026#34;label\u0026#34;:\u0026#34;KDE\u0026#34;},vertical=True,color=\u0026#34;y\u0026#34;) norm_hist：若为True, 则直方图高度显示密度而非计数(含有kde图像中默认为True)\ncode:\nfig,axes = plt.subplots(1,2) sns.distplot(x,norm_hist=True,kde=False,ax=axes[0]) #左图 sns.distplot(x,kde=False,ax=axes[1]) #右图 ","date":"2022-05-11T00:00:00Z","permalink":"https://example.com/p/sns.distplot%E7%94%A8%E6%B3%95/","title":"sns.distplot()用法"},{"content":" 第一个学的python的包就是matplotlib，特地整理了去年的学习笔记\nimport matplotlib.pyplot as plt matplotlib.pyplot.contourf([X,Y],Z) 是来绘制等高线的，contour和contourf都是画三维等高线图的，不同点在于contour() 是绘制轮廓线，contourf()会填充轮廓。\n在传入X,Y时会先进np.meshgrid生成坐标 X,Y对应的网格数据以及此网格对应的高度值，因此我们调用np.meshgrid(x,y)把x,y值转换成网格数据 重要参数说明：\nX,Y为数组，是在Z中的坐标值 当 X,Y,Z 都是 2 维数组时，它们的形状必须相同。如果都是 1 维数组时，len(X)是 Z 的列数， 而 len(Y) 是 Z 中的行数。（例如，经由创建numpy.meshgrid()）\nZ：类似矩阵\n确定轮廓线/区域的数量和位置 其实就是X,Y的函数高度值\nc 这里在机器学习中一般会传入y，即输出的类别，因为Colormap用于将数据值（浮点数）从间隔转 换为相应Colormap表示的RGBA颜色。用于将数据缩放到间隔中看 。\n无论contour还是contourf，都是绘制三维图，其中前两个参数x和y为两个等长一维数组，第三个参数z为二维数组（表示平面点xi,yi映射的函数值）。\n正是由于contourf可以填充等高线之间的空隙颜色，呈现出区域的分划状，所以很多分类机器学习模型的可视化常会借助其展现。\ncode:\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt def height(x,y): return(1-x/2+x**5+y**3)*np.exp(-x**2-y**2) x = np.linspace(0, 3, 256) y = np.linspace(0, 3, 256) X,Y = np.meshgrid(x,y)#把X，Y传入网格中 X.shape=256,256 Y.shape=256,256 print(X.shape) result:\n(256, 256) code:\nX[:5] #可以看见meshgrid中每一行的数组都是一样的，这里就相当于x轴上的点平移而成的，网格 #Y同理 result:\narray([[0. , 0.01176471, 0.02352941, ..., 2.97647059, 2.98823529, 3. ], [0. , 0.01176471, 0.02352941, ..., 2.97647059, 2.98823529, 3. ], [0. , 0.01176471, 0.02352941, ..., 2.97647059, 2.98823529, 3. ], [0. , 0.01176471, 0.02352941, ..., 2.97647059, 2.98823529, 3. ], [0. , 0.01176471, 0.02352941, ..., 2.97647059, 2.98823529, 3. ]]) code:\nplt.contourf(X,Y,height(X,Y),8,alpha=0.75,cmap=plt.cm.hot) plt.show() result: code\nC = plt.contourf(X,Y,height(X,Y),8,alpha=0.75,cmap=plt.cm.hot) plt.clabel(C,inline=True,fontsize=10) #这杨可以显示边界值 即将contourf传入到clabel result: code:\nplt.contour(X,Y,height(X,Y),8,alpha=0.75,cmap=plt.cm.hot) result ","date":"2022-05-10T00:00:00Z","permalink":"https://example.com/p/matplotlib.pyplot_contourf-%E7%BB%98%E5%88%B6%E7%AD%89%E9%AB%98%E7%BA%BF/","title":"matplotlib.pyplot_contourf 绘制等高线"},{"content":" 第一个学的python的包就是matplotlib，特地整理了去年的学习笔记\nimport matplotlib.pyplot as plt plt.plot() fig.add_subplot plt.subplot plt.subplots ","date":"2022-05-10T00:00:00Z","permalink":"https://example.com/p/plt_plot-plt_subplot-plt_subplots%E5%8C%BA%E5%88%AB/","title":"plt_plot+plt_subplot+plt_subplots区别"},{"content":" 第一个学的python的包就是matplotlib，特地整理了去年的学习笔记\nimport numpy as np import matplotlib.pyplot as plt import torch import torch.nn.functional as F x = torch.linspace(-10,10,60) y = torch.sigmoid(x) ax = plt.gca() #将最上方的边框颜色置为none ax.spines[\u0026#34;top\u0026#34;].set_color(\u0026#34;none\u0026#34;) #右边的边框颜色置为none ax.spines[\u0026#34;right\u0026#34;].set_color(\u0026#34;none\u0026#34;) #要移动底部x轴，所以要先锁定x轴 ax.xaxis.set_ticks_position(\u0026#34;bottom\u0026#34;) #data表示按数值挪动，其后数字代表挪动到Y轴的刻度值 ax.spines[\u0026#34;bottom\u0026#34;].set_position((\u0026#34;data\u0026#34;,0)) # #同上 ax.yaxis.set_ticks_position(\u0026#34;left\u0026#34;) #同上 ax.spines[\u0026#34;left\u0026#34;].set_position((\u0026#34;data\u0026#34;,0)) plt.plot(x.numpy(),y.numpy()) plt.show() ","date":"2022-05-10T00:00:00Z","permalink":"https://example.com/p/plt.gca%E5%9D%90%E6%A0%87%E8%BD%B4%E7%A7%BB%E5%8A%A8/","title":"plt.gca()坐标轴移动"},{"content":" 第一个学的python的包就是matplotlib，特地整理了去年的学习笔记\nmatplotlib库的axiss模块中的Axes.set_xscale()函数用于设置x轴比例\n用法： Axes.set_xscale(self, value, **kwargs)\n参数：此方法接受以下参数。\nvalue:此参数是要应用的轴比例类型。\n**kwargs:有不同的关键字参数可以接受，并且取决于规模\n以下示例说明了matplotlib.axes中的matplotlib.axes.Axes.set_xscale()函数：\n实例1 #实例1 import matplotlib.pyplot as plt import numpy as np from matplotlib.ticker import EngFormatter#使用工程工程符号标记刻度线 val = np.random.RandomState(19680801) xs = np.logspace(1,9,100) ys = (0.8 + 4 * val.uniform(size=100)) * np.log10(xs)**2 fig,ax0 = plt.subplots() ax0.set_xscale(\u0026#34;log\u0026#34;) formatter0 = EngFormatter(unit=\u0026#34;Hz\u0026#34;) ax0.xaxis.set_major_formatter(formatter0) ax0.plot(xs,ys) ax0.set_xlabel(\u0026#34;Frequency\u0026#34;) fig.suptitle(\u0026#34;%matplotlib.axes.Axes.set_xscale() \\ function Example\\n\u0026#34;,fontweight=\u0026#34;bold\u0026#34;) plt.show() 实例2 #实例2 import numpy as np import matplotlib.pyplot as plt import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) fig,ax4 = plt.subplots() x = 10.0**np.linspace(0.0,2.0,15) y = x**2.0 ax4.set_xscale(\u0026#34;log\u0026#34;,nonposx=\u0026#34;clip\u0026#34;) ax4.set_yscale(\u0026#34;log\u0026#34;,nonposy=\u0026#34;clip\u0026#34;) ax4.errorbar(x,y,xerr=0.1*x,yerr = 2.0 + 1.75 * y,color=\u0026#34;green\u0026#34;) ax4.set_ylim(bottom = 0.1) fig.suptitle(\u0026#34;matplotlib.axes.Axes.set_xscale() \\ function Example \\n\u0026#34;,fontweight = \u0026#34;bold\u0026#34;) plt.show() ","date":"2022-05-09T00:00:00Z","permalink":"https://example.com/p/axes.set_xscale%E5%87%BD%E6%95%B0%E7%94%A8%E4%BA%8E%E8%AE%BE%E7%BD%AEx%E8%BD%B4%E6%AF%94%E4%BE%8B/","title":"Axes.set_xscale()函数用于设置x轴比例"},{"content":" 第一个学的python的包就是matplotlib，特地整理了去年的学习笔记\nmatplotlib.colors.ListedColormap类属于matplotlib.colors模块。 matplotlib.colors模块用于将颜色或数字参数转换为RGBA或RGB。此模块用于将数字映射到颜色或以一维颜色数组(也称为colormap)进行颜色规格转换。\nmatplotlib.colors.ListedColormap类用于从颜色列表创建colarmap对象。这对于直接索引到颜色表中很有用，也可以用于为法线贴图创建特殊的颜色表\n用法： class matplotlib.colors.ListedColormap(colors, name=’from_list’, N=None)\n参数：\n颜色：它是Matplotlib颜色规格的数组或列表，或等于N x 3或N x 4浮点数组(N rgb或rgba值) 名称：它是一个可选参数，它接受一个字符串来标识颜色图。 N:它是一个可选参数，它接受表示映射中条目数的整数值。它的默认值为“无”，其中颜色列表中的每个元素都有一个颜色表条目。如果N小于len(colors)，则列表将在N处截断，而如果N大于len，则列表将重复进行扩展。\n该类的方法： 1)reversed()：这用于创建Colormap的反向实例。\n用法： reversed(self, name=None)\n参数：\nname:它是一个可选参数，表示反转的颜色图的名称。如果将其设置为“无”，则名称将为父色图的名称+ “_r”。\n返回值：它返回颜色图的反向实例\n实例1 import matplotlib.pyplot as plt import numpy as np import matplotlib.colors a = np.linspace(-3,3) A,B = np.meshgrid(a,a) X = np.exp(-(A**2 + B**2)) figure, (axes1,axes2) = plt.subplots(ncols=2) colors = [\u0026#34;green\u0026#34;,\u0026#34;orange\u0026#34;,\u0026#34;gold\u0026#34;,\u0026#34;blue\u0026#34;,\u0026#34;k\u0026#34;,\u0026#34;#550011\u0026#34;,\u0026#34;purple\u0026#34;,\u0026#34;red\u0026#34;] axes1.set_title(\u0026#34;color list\u0026#34;) contour = axes1.contourf(A,B,X,colors=colors) axes2.set_title(\u0026#34;white colormap\u0026#34;) cmap = matplotlib.colors.ListedColormap(colors) contour = axes2.contourf(A,B,X,cmap=cmap) figure.colorbar(contour) plt.show() 实例2 #实列2 import matplotlib.pyplot as plt import numpy as np import matplotlib.colors as colors from mpl_toolkits.axes_grid1 import make_axes_locatable res = np.array([[0,2],[3,5]],dtype=int) u = np.unique(res) bounds = np.concatenate(([res.min()-1], u[:-1]+np.diff(u)/2., [res.max()+1])) norm = colors.BoundaryNorm(bounds, len(bounds)-1) color_map1 = [\u0026#39;#7fc97f\u0026#39;, \u0026#39;#ffff99\u0026#39;, \u0026#39;#386cb0\u0026#39;, \u0026#39;#f0027f\u0026#39;] color_map = colors.ListedColormap(color_map1) fig, axes = plt.subplots() img = axes.imshow(res, cmap = color_map, norm = norm) divider = make_axes_locatable(axes) cax = divider.append_axes(\u0026#34;right\u0026#34;, size =\u0026#34;5 %\u0026#34;) color_bar = plt.colorbar(img, cmap = color_map, norm = norm, cax = cax) color_bar.set_ticks(bounds[:-1]+np.diff(bounds)/2.) color_bar.ax.set_yticklabels(color_map1) color_bar.ax.tick_params(labelsize = 10) plt.show() ","date":"2022-05-09T00:00:00Z","permalink":"https://example.com/p/matplotlib.colors.listedcolormap/","title":"Matplotlib.colors.ListedColormap"},{"content":" 我发现一直好像对PDB文件中氨基酸的原子编码规则不是很清楚，然后我去看了网上的帖子，发现都好像有点不太对，故整理此post来弄懂\n氨基酸的结构通式 注意哪个是α原子 PDB文件中原子编码具体的命令规则 规则 基于PyMOL 和 PDB文件\n氨基(NH)和羧基上(CO)的所有原子都采用本名，C, N, O, H，如果一个氨基酸的侧链基团也出现了氨基和羧基，侧链基团R上的氨基和羧基上的原子信息不是采用的本名，否则无法与主链的肽基区分开 侧链基团上的氨基和羧基的原子也采用下面提到的规则 首先α-C被命名为CA，注意生物学上哪个C才能被定义为CA原子，另外，PDB文件中会显示，但是PyMOL中有时候不会显示出CA的标签，其后的C按照成键关系逐级递推，名字后缀依次为 B-G-D-E-Z-H（此为希腊字母表顺序：α(A)，β(B)，γ(G)，δ(D)，ε(E)，ζ(Z)，η(H)……） 注意这个”成键逐级递推“的意思，例如，当一个CD，下面的直接相连的节点若是非H原子，且含有多个其他原子（比如含有两个C），则这多个原子都是隶属于CE的，为了区分要加上不同的数字编号(从1开始)（CE1,CE2） 对于氢原子，命名采用“H+所连上级原子后缀(如有数字也要带上)+数字编号”形式 对于处于与”中间CX(X代表任意字母or数字)原子“相连氢原子，且成键上一级的原子不是成环的，数字编号是从”2“开始的 若成键的上一个原子处于中间C，且这个C一端是H原子，另一端是其他原子，则这个H原子不用写数字，另一端的那个非H原子形成下一个节点的H原子数字编号从1开始 若成键的上一级注意处于环结构中时，因为成键的上一级所属的逐级的字母是同一个字母（环的原子，加上逐级递增的原则），为了区分，H的数字编号是从”1“开始的 CA相连的一个H原子就不用加上最后的数字编号 若成键的上一个原子是最后一个原子，对于处于上末尾的H原子（存在多个H原子时）的数字编号是从1开始的 处于最后末端的H原子（只存在一个H原子时），不管上一级成键的原子是否在环结构中，不用加数字编号，因为就它一个 本质就是标号法，注意H上的那些数字都是为了区分，不同的H才加上去的描述 例子 不含环的氨基酸 按照上面这张图的的绿色线条的箭头方向前进 进行说明\n第一处的C、O是位于羧基上（非侧链）的，所以直接写出C、O原子即可 第二处的N、H是位于氨基上（非侧链）的，所以直接写出所有原子N、H即可 HA 右侧其实是CA，前面规则了CA在PyMOL的标签中是不会显示出来的，则这个HA的H原子是与CA相连，所以直接写HA即可 第三处的C，根据逐级递推，这个C写作CB，与它相连的两个H，又因为CB是处于中间的C原子，所以两个H原子写作HB2、HB3 第四处CG同理 第五处CD同理，只是下面成键相连的不是H原子了 第六处的氧原子0E1和第七处的氮原子NE2，因为它成键的上一级是CD，且是本身是氧原子，不是H原子，故按照逐级递推的原则，分别写作OE、NE，但是又因为他们两个成键的都是CD，所以加上个序号，分别为OE1,NE2 第八处和第九处的H原子，成键相连的上一级为NE2，且是H原子，不用逐级递推，又位于侧链的末端所以，写作HE21,HE22 (E2是上一级的编号) 含环的氨基酸 注意逐级递增，特别是那个苯环中的CE1和CE2、（字母的编号是按逐级的顺序来的），HE1和HE2以及最后的HH\n最后附上20种氨基酸的相关信息 （下面的信息部分来自于学习来源3作者提供的excel表格）\n丙氨酸 英文 Alanine 简写 ALA - A SMILES C[C@H](N)C(O)=O 结构 异亮氨酸 英文 Isoleucine 简写 ILE - I SMILES CC[C@H](C)[C@H](N)C(O)=O 结构式 亮氨酸 英文 Leucine 简写 LEU-L SMILES CC(C)C[C@H](N)C(O)=O 结构式 缬氨酸 英文 Valine 简写 VAL - V SMILES CC(C)[C@H](N)C(O)=O 结构式 甲硫氨酸 英文 Methionine 简写 MET - M SMILES O=C(O)[C@@H](N)CCSC 结构式 苯丙氨酸 英文 Phenylalanine 简写 PHE - F SMILES O=C(O)[C@@H](N)CC1=CC=CC=C1 结构式 色氨酸 英文 Tryptophan 简写 TRP - W SMILES O=C(O)[C@@H](N)CC1=CNC2=CC=CC=C12 结构式 酪氨酸 英文 Tyrosine 简写 TYR - Y SMILES O=C(O)[C@H](CC1=CC=C(O)C=C1)N 结构式 天冬酰胺 英文 Asparagine 简写 ASN - N SMILES O=C(O)[C@H](CC(N)=O)N 结构式 半胱氨酸 英文 Cysteine 简写 CYS - C SMILES O=C(O)[C@H](CS)N 结构式 谷氨酰胺 英文 Glutamine 简写 GLN - Q SMILES O=C(O)[C@H](CCC(N)=O)N 结构式 丝氨酸 英文 Serine 简写 SER - S SMILES O=C(O)[C@H](CO)N 结构式 苏氨酸 英文 Threonine 简写 THR - T SMILES O=C(O)[C@H]([C@H](O)C)N 结构式 精氨酸 英文 Arginine 简写 ARG - R SMILES O=C(O)[C@H](CCCNC(N)=N)N 结构式 组氨酸 英文 Histidine 简写 HIS - H SMILES O=C(O)[C@H](CC1=CNC=N1)N 结构式 赖氨酸 英文 Lysine 简写 LYS - K SMILES O=C([C@@H](N)CCCCN)O 结构式 天冬氨酸 英文 Aspartic acid 简写 ASP - D SMILES O=C(O)[C@H](CC(O)=O)N 结构式 谷氨酸 英文 Glutamic acid 简写 GLU - E SMILES O=C(O)[C@H](CCC(O)=O)N 结构式 甘氨酸 英文 Glycine 简写 GLY - G SMILES O=C(O)CN 结构式 脯氨酸 英文 Proline 简写 PRO - P SMILES O=C([C@@H]1CCCN1)O 结构式 注意下，因为每个氨基酸的3D结构都是在整个蛋白中抽出来展示的，此时氨基酸是残基了，所以不想2d结构式那样是一个真正完整的氨基酸（比如COOH已经形成了肽键）\n学习来源 前面两个帖子感觉上都有问题，反正我自己去PDB文件和PyMOL中显示的似乎规则匹配不上，当然不排除是软件或者PDB文件格式变化过\n1.https://wap.sciencenet.cn/blog-3387981-1118283.html?mobile=1 2.https://cloud.tencent.com/developer/article/2026465 3.https://zhuanlan.zhihu.com/p/420481239\n","date":"2022-05-07T00:00:00Z","permalink":"https://example.com/p/pdb%E6%96%87%E4%BB%B6%E4%B8%AD%E6%B0%A8%E5%9F%BA%E9%85%B8%E5%8E%9F%E5%AD%90%E7%BC%96%E7%A0%81%E7%9A%84%E8%A7%84%E5%88%99/","title":"PDB文件中氨基酸原子编码的规则"},{"content":"、\n第一个学的python的包就是matplotlib，特地整理了去年的学习笔记\nmatplotlib.cm matplotlib.cm是matplotlib库中内置的色彩映射函数\nmatplotlib.cm.色彩即对[数据集]应用[色彩] https://matplotlib.org/stable/api/cm_api.html\n内置色彩映射的列表 ","date":"2022-05-07T00:00:00Z","permalink":"https://example.com/p/%E5%AF%B9matplotlib.cm.rdylbu%E7%9A%84%E7%90%86%E8%A7%A3/","title":"对matplotlib.cm.RdYlBu()的理解"},{"content":" 第一个学的python的包就是matplotlib，特地整理了去年的学习笔记\n作用：在图中带有指向型文本注释信息，突显细节\nmatplotlib.pyplot.annotate() 参数说明：\ns : string 内容 xy : (float,float) 箭头点所在的坐标位置 xytext : s内容所在的坐标位置 weight : str/int 设置字体线型，其中字符串从小到大可选项有{\u0026lsquo;ultralight\u0026rsquo;, \u0026rsquo;light\u0026rsquo;, \u0026rsquo;normal\u0026rsquo;, \u0026lsquo;regular\u0026rsquo;, \u0026lsquo;book\u0026rsquo;, \u0026lsquo;medium\u0026rsquo;, \u0026lsquo;roman\u0026rsquo;, \u0026lsquo;semibold\u0026rsquo;, \u0026lsquo;demibold\u0026rsquo;, \u0026lsquo;demi\u0026rsquo;, \u0026lsquo;bold\u0026rsquo;, \u0026lsquo;heavy\u0026rsquo;, \u0026rsquo;extra bold\u0026rsquo;, \u0026lsquo;black\u0026rsquo;} color ： str/tuple 设置字体颜色，单个字符候选项{\u0026lsquo;b\u0026rsquo;, \u0026lsquo;g\u0026rsquo;, \u0026lsquo;r\u0026rsquo;, \u0026lsquo;c\u0026rsquo;, \u0026rsquo;m\u0026rsquo;, \u0026lsquo;y\u0026rsquo;, \u0026lsquo;k\u0026rsquo;, \u0026lsquo;w\u0026rsquo;}，也可以\u0026rsquo;black\u0026rsquo;,\u0026lsquo;red\u0026rsquo;等，tuple时用[0,1]之间的浮点型数据，RGB或者RGBA, 如: (0.1, 0.2, 0.5)、(0.1, 0.2, 0.5, 0.3)等 arrowprops : dict 设置指向箭头的参数，字典中key值有①arrowstyle：设置箭头的样式，其value候选项如\u0026rsquo;-\u0026gt;\u0026rsquo;,\u0026rsquo;|-|\u0026rsquo;,\u0026rsquo;-|\u0026gt;\u0026rsquo;,也可以用字符串\u0026rsquo;simple\u0026rsquo;,\u0026lsquo;fancy\u0026rsquo; ; ②connectionstyle：设置箭头的形状，为直线或者曲线，候选项有\u0026rsquo;arc3\u0026rsquo;,\u0026lsquo;arc\u0026rsquo;,\u0026lsquo;angle\u0026rsquo;,\u0026lsquo;angle3\u0026rsquo;，可以防止箭头被曲线内容遮挡; ③color：设置箭头颜色，见前面的color参数 bbox: dict code:\nimport matplotlib.pyplot as plt import numpy as np import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) x = np.linspace(0,10,200) y = np.sin(x) plt.plot(x,y,linestyle=\u0026#34;-.\u0026#34;,color=\u0026#34;purple\u0026#34;) plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = \u0026#34;SimHei\u0026#34; plt.annotate(s=\u0026#34;这是测试\u0026#34;,xy=(3,np.sin(3)),xytext=(4,-0.5),weight=\u0026#34;heavy\u0026#34;,color=\u0026#34;red\u0026#34;, arrowprops=dict(arrowstyle=\u0026#34;-|\u0026gt;\u0026#34;,connectionstyle=\u0026#34;arc3\u0026#34;,color=\u0026#34;blue\u0026#34;), bbox=dict(boxstyle=\u0026#34;round,pad=0.5\u0026#34;,fc=\u0026#34;green\u0026#34;,ec=\u0026#34;k\u0026#34;,lw=2,alpha=0.5)) plt.show() result: code:\nimport matplotlib.pyplot as plt import numpy as np import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) x = np.linspace(0,10,200) y = np.sin(x) plt.plot(x,y,linestyle=\u0026#34;-.\u0026#34;,color=\u0026#34;purple\u0026#34;) plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = \u0026#34;SimHei\u0026#34; plt.annotate(s=\u0026#34;这是测试\u0026#34;,xy=(3,np.sin(3)),xytext=(4,-0.5),weight=\u0026#34;heavy\u0026#34;,color=\u0026#34;red\u0026#34;, arrowprops=dict(arrowstyle=\u0026#34;|-|\u0026#34;,connectionstyle=\u0026#34;arc3\u0026#34;,color=\u0026#34;blue\u0026#34;), bbox=dict(boxstyle=\u0026#34;round,pad=0.5\u0026#34;,fc=\u0026#34;green\u0026#34;,ec=\u0026#34;k\u0026#34;,lw=2,alpha=0.5)) plt.show() result: matplotlib.pyplot.text() 无指向型的注释文本 matplotlib.pyplot.text()函数，即只会在图中添加注释内容而无指向箭头。\n用法同annotate x,y代表注释内容位置 s代表注释文本内容 family设置字体，自带的可选项有{\u0026lsquo;serif\u0026rsquo;, \u0026lsquo;sans-serif\u0026rsquo;, \u0026lsquo;cursive\u0026rsquo;, \u0026lsquo;fantasy\u0026rsquo;, \u0026lsquo;monospace\u0026rsquo;} fontsize字体大小 style设置字体样式，可选项{\u0026rsquo;normal\u0026rsquo;, \u0026lsquo;italic\u0026rsquo;(斜体), \u0026lsquo;oblique\u0026rsquo;(也是斜体)} code:\nimport matplotlib.pyplot as plt import numpy as np import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) x = np.linspace(0,10,200) y = np.sin(x) plt.plot(x,y,linestyle=\u0026#34;-.\u0026#34;,color=\u0026#34;purple\u0026#34;) plt.text(4,0,\u0026#34;这是一个测试\u0026#34;,family=\u0026#34;sans-serif\u0026#34;,fontsize=12,style=\u0026#34;italic\u0026#34;,color=\u0026#34;red\u0026#34;) plt.show() result: ","date":"2022-05-06T00:00:00Z","permalink":"https://example.com/p/annotatetext--%E6%B3%A8%E9%87%8A%E6%96%87%E6%9C%AC/","title":"annotate(),text()--注释文本"},{"content":" 第一个学的python的包就是matplotlib，特地整理了去年的学习笔记\nimport matplotlib.pyplot as plt import matplotlib #用style定义 matplotlib.style.use(\u0026#34;ggplot\u0026#34;) ","date":"2022-05-06T00:00:00Z","permalink":"https://example.com/p/%E7%94%BB%E5%9B%BE%E9%A3%8E%E6%A0%BC%E5%AE%9A%E4%B9%89%E4%B8%BAggplot/","title":"画图风格定义为ggplot"},{"content":" 第一个学的python的包就是matplotlib，特地整理了去年的学习笔记\n配置图形参数 使用面向对象的绘图接口时会创建figure和axes对象。figure实例可以看成是一个能够容纳各种坐标轴、图形、文字和标签的容器，axes是一个带有刻度和标签的矩形，最终会包含所有可视化的图形元素\nimport matplotlib.pyplot as plt import matplotlib as mpl import numpy as np import pandas as pd plt.style.use(\u0026#34;seaborn-whitegrid\u0026#34;) fig = plt.figure() ax = plt.axes() 调整图形：线条的颜色和风格 自定义线条的颜色可以设置plot()方法中的color属性。可以使用标准颜色名称，如‘blue’、’red‘等，也可以使用缩写颜色代码（rgbcmyk） 自定义线条的风格可以设置plot()方法中的linestyle属性。可以使用标准风格名称，如‘solid’、‘dashed’、‘dashdot’和‘dotted’，也可以使用简写形式，如‘-’、‘–’、‘-.’或者‘:‘\nx = np.linspace(0,10,100) plt.plot(x,np.sin(x-0),color=\u0026#34;blue\u0026#34;, linestyle=\u0026#34;solid\u0026#34;) plt.plot(x,np.cos(x-1),color=\u0026#34;g\u0026#34;,linestyle=\u0026#34;dashdot\u0026#34;) #更加简洁的方式，将linestyle和color编码结合起来，作为plt.plot()函数的一个非关键参数使用 plt.plot(x,x-3,\u0026#34;--r\u0026#34;) plt.show() 调整图形：坐标上下限——axis()方法 荐使用plt.axis()方法。通过传入[xmin, xmax, ymin, ymax]对应的值，plt.axis()方法可以让你用一行代码设置x和y的限值\n#设置x轴和y轴的限值 plt.plot(x,np.sin(x)) plt.axis([-1,11,-1.5,1.5]) #按照图形的内容自动收紧坐标轴，不留空白区域 plt.axis(\u0026#34;tight\u0026#34;) #让x轴和y轴单位长度相等，即分辨率相等 plt.axis(\u0026#34;equal\u0026#34;) 设置图形的标签 #简单设置方法 plt.title(\u0026#34;A Sine Curve\u0026#34;) plt.xlabel(\u0026#34;X\u0026#34;) plt.ylabel(\u0026#34;sin(x)\u0026#34;) #当单个坐标轴上显示多条线时，创建图例显示每条线是很有效的方法。Matplotlib内置了一个简单快速的方法——plt.legend() #在plt.plot()方法中显示设置label参数，配合plt.legend()函数可以方便的制作图例 plt.plot(x,np.sin(x),\u0026#34;-g\u0026#34;,label=\u0026#34;sin(x)\u0026#34;) plt.plot(x,np.cos(x),\u0026#34;:b\u0026#34;,label=\u0026#34;cos(x)\u0026#34;) plt.axis(\u0026#34;equal\u0026#34;) plt.legend() plt.title(\u0026#34;triangle function curve\u0026#34;) plt.show() ","date":"2022-05-06T00:00:00Z","permalink":"https://example.com/p/%E9%85%8D%E7%BD%AE%E5%9B%BE%E5%BD%A2%E5%8F%82%E6%95%B0/","title":"配置图形参数"},{"content":"Practically Useful: What the Rosetta Protein Modeling Suite Can Do for You\n","date":"2022-05-04T00:00:00Z","permalink":"https://example.com/p/practically-useful-what-the-rosetta-protein-modeling-suite-can-do-for-you/","title":"Practically Useful: What the Rosetta Protein Modeling Suite Can Do for You"},{"content":"PyRosetta: a script-based interface for implementing molecular modeling algorithms using Rosetta\nlink:https://academic.oup.com/bioinformatics/article/26/5/689/212442?login=false\n","date":"2022-05-03T00:00:00Z","permalink":"https://example.com/p/pyrosetta-a-script-based-interface-for-implementing-molecular-modeling-algorithms-using-rosetta/","title":"PyRosetta: a script-based interface for implementing molecular modeling algorithms using Rosetta"},{"content":"Free-Energy Landscape of Enzyme Catalysis\n","date":"2022-04-21T00:00:00Z","permalink":"https://example.com/p/free-energy-landscape-of-enzyme-catalysis/","title":"Free-Energy Landscape of Enzyme Catalysis"},{"content":" 这是跟学刘二老师的的课程ppt，特地放这里用来随时随地看\n","date":"2022-04-21T00:00:00Z","permalink":"https://example.com/p/lecture_13_rnn_classifier/","title":"Lecture_13_RNN_Classifier"},{"content":" 这是跟学刘二老师的的课程ppt，特地放这里用来随时随地看\n","date":"2022-04-21T00:00:00Z","permalink":"https://example.com/p/ppt_lecture_01_overview/","title":"ppt_Lecture_01_Overview"},{"content":" 这是跟学刘二老师的的课程ppt，特地放这里用来随时随地看\n","date":"2022-04-21T00:00:00Z","permalink":"https://example.com/p/ppt_lecture_02_linear_model/","title":"ppt_Lecture_02_Linear_Model"},{"content":" 这是跟学刘二老师的的课程ppt，特地放这里用来随时随地看\n","date":"2022-04-21T00:00:00Z","permalink":"https://example.com/p/ppt_lecture_03_gradient_descent/","title":"ppt_Lecture_03_Gradient_Descent"},{"content":" 这是跟学刘二老师的的课程ppt，特地放这里用来随时随地看\n","date":"2022-04-21T00:00:00Z","permalink":"https://example.com/p/ppt_lecture_04_back_propagation/","title":"ppt_Lecture_04_Back_Propagation"},{"content":" 这是跟学刘二老师的的课程ppt，特地放这里用来随时随地看\n","date":"2022-04-21T00:00:00Z","permalink":"https://example.com/p/ppt_lecture_05_linear_regression_with_pytorch/","title":"ppt_Lecture_05_Linear_Regression_with_PyTorch"},{"content":" 这是跟学刘二老师的的课程ppt，特地放这里用来随时随地看\n","date":"2022-04-21T00:00:00Z","permalink":"https://example.com/p/ppt_lecture_06_logistic_regression/","title":"ppt_Lecture_06_Logistic_Regression"},{"content":" 这是跟学刘二老师的的课程ppt，特地放这里用来随时随地看\n","date":"2022-04-21T00:00:00Z","permalink":"https://example.com/p/ppt_lecture_07_multiple_dimension_input/","title":"ppt_Lecture_07_Multiple_Dimension_Input"},{"content":" 这是跟学刘二老师的的课程ppt，特地放这里用来随时随地看\n","date":"2022-04-21T00:00:00Z","permalink":"https://example.com/p/ppt_lecture_08_dataset_and_dataloader/","title":"ppt_Lecture_08_Dataset_and_Dataloader"},{"content":" 这是跟学刘二老师的的课程ppt，特地放这里用来随时随地看\n","date":"2022-04-21T00:00:00Z","permalink":"https://example.com/p/ppt_lecture_09_softmax_classifier/","title":"ppt_Lecture_09_Softmax_Classifier"},{"content":" 这是跟学刘二老师的的课程ppt，特地放这里用来随时随地看\n","date":"2022-04-21T00:00:00Z","permalink":"https://example.com/p/ppt_lecture_10_basic_cnn/","title":"ppt_Lecture_10_Basic_CNN"},{"content":" 这是跟学刘二老师的的课程ppt，特地放这里用来随时随地看\n","date":"2022-04-21T00:00:00Z","permalink":"https://example.com/p/ppt_lecture_11_advanced_cnn/","title":"ppt_Lecture_11_Advanced_CNN"},{"content":" 这是跟学刘二老师的的课程ppt，特地放这里用来随时随地看\n","date":"2022-04-21T00:00:00Z","permalink":"https://example.com/p/ppt_lecture_12_basic_rnn/","title":"ppt_Lecture_12_Basic_RNN"},{"content":" 这是Python sklearn机器学习各种评价指标——sklearn.metrics简介及应用示例 （文中图片来源于知乎，但是因为是很久之前的图片，今天整理起来找不到原作者的知乎账号了）\n补充，找到了，但是记错了，不是知乎。。\nhttps://scikit-learn.org/stable/modules/classes.html https://www.cnblogs.com/mindy-snail/p/12445973.html # 有两种方式导入： #方式一： from sklearn.metrics import mean_squared_error from sklearn.metrics import r2_score # 此时的调用方式直接调用即可 mean_squared_error(y_test,y_pred) #方式二： from sklearn import metrics #此时的调用方式 metrics.mean_squared_error(y_test,y_pred) 来看scikit-learn.metrics里各种指标简介\n回归指标 1.explained_variance_score(y_true, y_pred, sample_weight=None, multioutput=‘uniform_average’)：回归方差(反应自变量与因变量之间的相关程度) 2.mean_absolute_error(y_true,y_pred,sample_weight=None,multioutput=‘uniform_average’)：平均绝对误差 3.mean_squared_error(y_true, y_pred, sample_weight=None, multioutput=‘uniform_average’)：均方差 4.median_absolute_error(y_true, y_pred) 中值绝对误差 5.r2_score(y_true, y_pred,sample_weight=None,multioutput=‘uniform_average’) ：R平方值 分类指标 1.accuracy_score(y_true,y_pred):精度 2.auc(x,y,reorder=False):ROC曲线下的面积;较大的AUC代表了较好的performance 3.average_precision_score(y_true, y_score, average=‘macro’, sample_weight=None):根据预测得分计算平均精度(AP) 4.brief_score_loss(y_true, y_prob, sample_weight=None, pos_label=None):The smaller the Brier score, the better. 5.confusion_matrix(y_true, y_pred, labels=None, sample_weight=None):通过计算混淆矩阵来评估分类的准确性 返回混淆矩阵 6.f1_score(y_true, y_pred, labels=None, pos_label=1, average=‘binary’, sample_weight=None):F1值 7.log_loss(y_true, y_pred, eps=1e-15, normalize=True, sample_weight=None, labels=None):对数损耗，又称逻辑损耗或交叉熵损耗 8.precison_score(y_true, y_pred, labels=None, pos_label=1, average=‘binary’)查准率或者精度； precision(查准率)=TP/(TP+FP) 9.recall_score(y_true, y_pred, labels=None, pos_label=1, average=‘binary’, sample_weight=None):查全率 10.roc_auc_score(y_true, y_score, average=‘macro’, sample_weight=None):计算ROC曲线下的面积就是AUC的值，the larger the better 11.roc_curve(y_true, y_score, pos_label=None, sample_weight=None, drop_intermediate=True):计算ROC曲线的横纵坐标值，TPR，FPR TPR = TP/(TP+FN) = recall(真正例率，敏感度) FPR = FP/(FP+TN)(假正例率，1-特异性) 补充： 混淆矩阵 代码1 from sklearn.metrics import confusion_matrix from sklearn.metrics import classification_report y_test=[\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;p\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;p\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;p\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;p\u0026#34;,\u0026#34;p\u0026#34;,\u0026#34;p\u0026#34;,\u0026#34;a\u0026#34;] y_pred=[\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;p\u0026#34;,\u0026#34;p\u0026#34;,\u0026#34;p\u0026#34;,\u0026#34;p\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;p\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;p\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;a\u0026#34;,\u0026#34;a\u0026#34;,\u0026#34;p\u0026#34;,\u0026#34;b\u0026#34;] confusion_matrix(y_test,y_pred,labels=[\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;p\u0026#34;]) result: print(classification_report(y_test,y_pred)) result: 准确率分数 import numpy as np from sklearn.metrics import accuracy_score y_pred = [1, 9, 9, 5,1,0,2,2] y_true = [1,9,9,8,0,6,1,2] print(accuracy_score(y_true,y_pred)) print(accuracy_score(y_true,y_pred,normalize=False)) result:\n0.5 4 ROC曲线 #API实现 from sklearn.metrics import roc_curve roc_curve(y_true,y_score,pos_label=None,sample_weight=None,drop_intermediate=True) AUC曲线 #API from sklearn.metrics import auc auc(x,y,reorder=False) recall 召回曲线 hold-out set流出法 MSE\u0026amp;RMSE ","date":"2022-04-19T00:00:00Z","permalink":"https://example.com/p/sklearn.metrics%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%90%84%E7%A7%8D%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87/","title":"Sklearn.metrics机器学习各种评价指标"},{"content":" str.startswith(substr,start,end) 判断substr是否在str中开头 ,当然了可以指定从str哪里开始检索 str.endswith(substr,start,end) 判断substr是否在str中结尾,当然了可以指定从str哪里开始检索 code:\nstr_long = \u0026#34;This is a test now\u0026#34; str_long.startswith(\u0026#34;This\u0026#34;) result:\nTrue code:\nstr_long.startswith(\u0026#34;This\u0026#34;,5,9) result:\nFlase code:\nstr_long.endswith(\u0026#34;now\u0026#34;) result:\nTrue code:\nstr_long.endswith(\u0026#34;test\u0026#34;,0,14) result:\nTrue ","date":"2022-04-19T00:00:00Z","permalink":"https://example.com/p/startwithsendswith%E5%88%A4%E6%96%AD%E5%BC%80%E5%A4%B4%E7%BB%93%E5%B0%BE/","title":"startwiths\u0026endswith()判断开头\u0026结尾 "},{"content":"理性设计提高产量 link:https://onlinelibrary.wiley.com/doi/epdf/10.1002/bit.27919 ","date":"2022-04-09T00:00:00Z","permalink":"https://example.com/p/rational-design-of-a-highly-efficient-catalytic-system-for-the-production-of-paps-from-atp-and-its-application-in-the-synthesis-of-chondroitin-sulfate/","title":"Rational design of a highly efficient catalytic system for the production of PAPS from ATP and its application in the synthesis of chondroitin sulfate"},{"content":" 整理了下 win AutoDock的学习记录（主要方便自己看），看的是blibli和其他网址上别人的做法，因为是很久之前学的，所以暂时没找到链接（找到了再放上来）\n对接的完整步骤 修改默认工作目录 工作目录下要有adt.bat/autodock4.exe/autogrid4.exe 以及所要进行对接的受体和配体\n导入受体pdb File\u0026mdash;read Mol \u0026ndash;选择 （小红点代表水分子，要去除）\n去除受体的水分子 Edit \u0026ndash; delete water\n受体加氢 Edit \u0026ndash; Hydro \u0026ndash; add 选为受体 Grid \u0026ndash; Macro \u0026ndash; choose \u0026ndash; 选择 会生成一个受体pdbqt\n导入配体 同上\n加氢 同上\n选为配体 Ligand \u0026ndash; input \u0026ndash;choose\n检查下它的扭转键和中心 ligand \u0026ndash; Torsion Tree -Detect Root ligand \u0026ndash; Torsion Tree -Choose Torsion 红色的是不可以被扭转的，绿色的是可以被扭转的\n接下来导出为配体的pdbqt ligand \u0026ndash; output \u0026ndash; save as pdbqt (完成后记得删除DM中的对象)\n准备对接 导入受体 ： Grid \u0026ndash; Macromolecule \u0026ndash; open \u0026ndash;选择 导入配体： Grid \u0026ndash; Set Map Types \u0026ndash; open ligand \u0026ndash;选择 准备盒子运行的大小： Grid \u0026ndash; Grid Box \u0026ndash; 调整盒子大小（包裹核心位点或者整个protein） 将配体从box中取出 勾去掉后，不要关闭那个GUI，右键拖出小分子，拖出后将勾打上\n保存盒子的大小 在调整盒子的那个GUI，fiie \u0026ndash; close saving current\ngrid \u0026ndash; output \u0026ndash; save GPF (自行加上后缀.GPF)\n运行AutoGrad run \u0026ndash; run autograd 仔细核查里面的参数 Parameter Filename 那里要自己去手动选下 无误后点 Launch 运行完毕后会生成一堆/glg/map等文件\n准备autodock的运行参数 Docking \u0026ndash; Macromoldecule \u0026ndash; set rigid filename \u0026ndash; 选择受体的pdbqt Docking \u0026ndash; Ligand \u0026ndash; choose \u0026ndash; 选择配体 Docking \u0026ndash; Search parameter \u0026ndash; Genetic Algorithm Docking \u0026ndash; output \u0026ndash; LGA （自行加上后缀.dpf）\n运行autodock Run \u0026ndash; Run AutoDock (过程同运行autograd) 会生成dlg文件\n查看对接结果 在此之前记得把view中的分子弄走先 （Edit \u0026ndash; Delete \u0026ndash; Delete Molecule） Analyze \u0026ndash; docking \u0026ndash; open \u0026ndash; 选择刚刚生成的dlg文件 Analyze \u0026ndash; Macromolecule open \u0026ndash; open (为了好看点，可以修改形式) Analyze \u0026ndash; Conformations \u0026ndash; play ranked by energy 会出现小条可以调整小分子的位置 点击小条的倒数第二个按钮， Build H-bonds 和 show info 可以查看每次的具体结果 Analyze \u0026ndash; conformations \u0026ndash; load 可以查看其他的结合能\nWrite Complex 可以将某次的对接结果的文件保存为pdbqt，然后再利用openlabel将pdbqt转化pdb，然后就可以在pymol中查看这个pdb了\n可能存在的问题 蛋白质去水的方式 蛋白质加氢是全氢还是极性氢 极性氢的添加方式：Edit \u0026ndash; Hydro \u0026ndash; add \u0026ndash; Polar Only (全氢：All Hydro)\n官网的回答：无论是在选为配体还是受体之前都要加全氢 3.蛋白质是否要设置原子类型 官方的回答 就是用adt选为受体文件时会自动加上G电荷，柔和非极性氢，会自动识别原子类型\n小分子加全氢还是极性氢？该给残基分配好电荷吗？？ 加全氢！！！！ 怎么均匀分配电荷呢？Edit \u0026ndash; Charges \u0026ndash; Check Total on Resdiues 在打开小分子的时，adt会自动加上电荷，如果再去加上上面操作会报错。 是否给小分子计算静电荷compute Gasteiger 就是说在加完氢后选择配体时会报错 官方的回答： 就是说他这里有可能属于第二三种情况，即它本身部分原子有部分电荷或者全部原子电荷为零，所以通过计算静电荷来解决，（没报错的话不要计算静电荷，对结合能结果会有影响） Edit \u0026ndash; charges \u0026ndash; Computes Gasteiger; 再次选为配体就可以成功了\n是否给大分子计算静电荷compute Gasteiger 图片中后者的操作相当于给没有带电的残基加上了电荷，而前者是不管有没有都会加上 yes 对于一些会冒出这个纠正（就是说这里是给这四个残基加上了静电荷） 官方的回答： 如果分子本身有一些没有电荷，adt会自动加上静电荷；如果分子本身有电荷，adt会询问是否保存还是添加静电荷\n到底什么时候点击compute Gasteiger呢 Nice level是什么？ 在点击 run \u0026ndash; run autogrid（run aotodock） 会出现Nice leve 官方的回答： 如何获取小分子mol2或pdb文件 注意是否下载的3D结构\nzinc：https://zinc15.docking.org/substances/home/ TCMSP: https://old.tcmsp-e.com/tcmsp.php PubChem:https://pubchem.ncbi.nlm.nih.gov/ (他这里的sdf是三级结构) sdf的格式要通过openbabel转为mol2/pdb 为什么两次同样的操作会有不同的结果？ 如何解决呢？ 文件名有空格的错误 此时在运行 run \u0026ndash; autograd时会不太行（即不会生成对应的map文件）\n给AD4_parameters.dat参数文件添加金属离子参数 具体地看另一份md\n如何找到最优构象 其实这个问题跟前面的为什么运行的结果不一样里降讲过了\n对接结果结合能的转换 adt中对接结果（看dlg文件）的结合能的单位是 用PYMOL计算配体的RMSD 需要的文件：对接结果的pdbqt 和 用来对接的小分子配体pdbqt 导入到PYMOL中 在命令行中输入 align 对接结果pdbqt名,配体pdbqt名 因为文件名含有空格 报错 运行run autogrid时 这里就是因为工作目录中出现了空格\n用R语言png函数输出高分辨率的图片 看对应的md\n官方建议对接50次 docking \u0026ndash; search parameter \u0026ndash; GA \u0026ndash; Number of GA Runs 调整次数为50\n19.用PYMOL给蛋白质小分子添加标签 具体看对应的md\n20.ADT的引用 去官方网站找它的引用\n21. 点击Run AutoDock一闪而过的问题 看是否命名有空格\n即使是一闪而过，也会生成dlg文件，去dlg看相关的内容报错，如： 可以看出是找不到map\n但是工作目录下有这个文件，但是它的报错的指向文件后面有个#，此时去看dpf（这个相当于ADT的参数文件了） 去里面将找不到的那个文件的 后面的 # 调远点（就是说# 与前面的 有个空格间隔 否则蛋白质名字太长的话会与#连在一起，所以蛋白质的命名不要太长好点）\n重新运行 run \u0026ndash; autodock 运行成功\n知道活性中心坐标后怎么确定grid box 具体看对应的md\n官方文档的解读 byatmo type 黄色：S原子 ； 蓝色：N原子 ； 红色：O原子 ； 灰色：不可以被旋转的C原子；\n快捷键 Choose Torsions 需要注意的一点 最好保证此时只有一个分子，避免其他分子的影响 Gird生成的gpf 如果有金属离子，则要edit这个gpf\n算法的优越性 ","date":"2022-04-08T00:00:00Z","permalink":"https://example.com/p/window-autodock%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","title":"window AutoDock学习笔记"},{"content":" 整理了下 win AutoDock的学习记录（主要方便自己看），看的是blibli和其他网址上别人的做法，因为是很久之前学的，所以暂时没找到链接（找到了再放上来）\n在mpl文件夹中找到AD4_parameters.dat文件 并放置到工作目录下\n先看下缺乏的情况 顺便提下，在从RCSB PDB中下载pdb时 在下面可以看到一些详情；如什么东西在什么链\n去除相关链（可以直接在pymol中操作）\n命令行：remove chain B (C/D)\n去除水分子： A \u0026ndash; remove water\n如有小分子配体则删除 （怎么看是不是小分子配体呢？在seq 不是连续拼接上的，就是小分子配体；金属离子不要去除）\nFile \u0026ndash; export Molecule \u0026ndash; pdb\n在ADT中导入刚才pymol生成的pdb，因为在pymol中去过水了，所以直接加氢就好了\n将NI.pdb 选为受体生成pdbqt 可以看出系统会自动提醒这个离子没有电荷（因为AD4_parameters.dat中没有这个金属离子的参数）\n然后就可以一波操作（如Grid导入受体/配体/确认对接box的大小）运行autogrid/autodock， 此时会有报错的内容： 报错的解决方案 修改AD4_parameters.dat的离子内容（去网上找一份比较齐全的，然后复制过去） 一定要记得修改gpf/dpf的文件 gpf（盒子的那个文件） / dpf是docking时生成那个文件\n在开头新增一行 paramter_file AD4_parameters.dat # force field default paramter_file 再次运行run \u0026ndash; autogrid\n","date":"2022-04-07T00:00:00Z","permalink":"https://example.com/p/ad4_parameters%E9%87%91%E5%B1%9E%E7%A6%BB%E5%AD%90%E5%8F%82%E6%95%B0%E7%9A%84%E4%BF%AE%E6%94%B9/","title":"AD4_parameters金属离子参数的修改"},{"content":" 整理了下 win AutoDock的学习记录（主要方便自己看），看的是blibli和其他网址上别人的做法，因为是很久之前学的，所以暂时没找到链接（找到了再放上来）\n导入 对接结果pdbqt 和 小分子配体的pdbqt PYMOL 背景改成白色\nDisplay \u0026ndash; Background \u0026ndash; White\n选一个和对接位点比较远的残基（即和对接位点不太相关的残基）\n选中残基 \u0026ndash; 鼠标右键 \u0026ndash; edit label 将其名字改为我们想要告诉别人的残基名字 同理再来一个\n将右下角的3-Button 的模式改为Editing\nCtrl 就可以拖动标签了\n后面还想修改，先把3-Button改回Viewing模式\n如果想删除标签，去点击sele的L \u0026ndash; clear\n","date":"2022-04-07T00:00:00Z","permalink":"https://example.com/p/%E7%94%A8pymol%E7%BB%99%E8%9B%8B%E7%99%BD%E8%B4%A8%E5%B0%8F%E5%88%86%E5%AD%90%E6%B7%BB%E5%8A%A0%E6%A0%87%E7%AD%BE/","title":"用PYMOL给蛋白质小分子添加标签"},{"content":" 整理了下 win AutoDock的学习记录（主要方便自己看），看的是blibli和其他网址上别人的做法，因为是很久之前学的，所以暂时没找到链接（找到了再放上来）\n引言 scale()数据标准化、中心化处理 使用heatmap画热图函数 heapmap(a)\n使用png函数绘制 首先 查看下R的工作目录 getwd()\npng(filename= \u0026ldquo;HEATMAP.png\u0026rdquo;,width = 1700, height = 1500, units = \u0026ldquo;px\u0026rdquo;, bg=\u0026ldquo;white\u0026rdquo;,res = 300) 参数值自己调整\nheapmap(a)\ndev.off() 这就画完了，但是要看看数据是否完整被画出来，再进行png（）命令中的参数进行调整\n记得关闭Rstudio ","date":"2022-04-07T00:00:00Z","permalink":"https://example.com/p/%E7%94%A8r%E8%AF%AD%E8%A8%80png%E5%87%BD%E6%95%B0%E8%BE%93%E5%87%BA%E9%AB%98%E5%88%86%E8%BE%A8%E7%8E%87%E7%9A%84%E5%9B%BE%E7%89%87/","title":"用R语言png函数输出高分辨率的图片"},{"content":" 整理了下 win AutoDock的学习记录（主要方便自己看），看的是blibli和其他网址上别人的做法，因为是很久之前学的，所以暂时没找到链接（找到了再放上来）\nGird \u0026ndash; Grid box \u0026ndash; View \u0026ndash; show box as lines View中还有去掉中心坐标以及修改箱子尺寸的 通过某种软件知道了蛋白质的活性中心后，把这个box尽可能地锁定到那里去 在那个软件中知道了两个 参考点，然后简单运算到 ADT地坐标系上 找原子法： 找到活性中心的一个残基 这就可以看到原子了 注意要输入全称才会锁定上 这就蹦到这了 肉眼法 肉眼观察，自行调整到那里,如果不好看的话，可以调整蛋白模型的表现形式 ","date":"2022-04-07T00:00:00Z","permalink":"https://example.com/p/%E7%9F%A5%E9%81%93%E6%B4%BB%E6%80%A7%E4%B8%AD%E5%BF%83%E5%9D%90%E6%A0%87%E5%90%8E%E6%80%8E%E4%B9%88%E7%A1%AE%E5%AE%9Agrid-box/","title":"知道活性中心坐标后怎么确定grid box"},{"content":"RosettaScripts: A Scripting Language Interface to the Rosetta Macromolecular Modeling Suite\nlink:https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0020161\n","date":"2022-03-19T00:00:00Z","permalink":"https://example.com/p/rosettascripts-a-scripting-language-interface-to-the-rosetta-macromolecular-modeling-suite/","title":"RosettaScripts: A Scripting Language Interface to the Rosetta Macromolecular Modeling Suite"},{"content":"我们首先生成三类三维特征的数据，代码如下：\nimport numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D from sklearn.datasets import make_classification X,y = make_classification(n_samples=1000,n_features=3,n_redundant=0, n_classes=3,n_informative=2,n_clusters_per_class=1,class_sep=0.5, random_state=22) fig = plt.figure(figsize=(15,8)) ax = Axes3D(fig,rect=[0,0,1,1],elev=30,azim=20) ax.scatter(X[:,0],X[:,1],X[:,2],marker=\u0026#34;o\u0026#34;,c=y) 首先我们看看使用PCA降维到二维的情况，注意PCA无法使用类别信息来降维\nfrom sklearn.decomposition import PCA pca = PCA(n_components=2) pca.fit(X) print(pca.explained_variance_ratio_) print(pca.explained_variance_) X_nex = pca.transform(X) plt.scatter(X_nex[:,0],X_nex[:,1],marker=\u0026#34;o\u0026#34;,c=y) plt.show() result: 由于PCA没有利用类别信息，我们可以看到降维后，样本特征和类别的信息关联几乎完全丢失。\n现在我们再看看使用LDA的效果\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis lda = LinearDiscriminantAnalysis(n_components=2) lda.fit(X,y) X_new = lda.transform(X) plt.scatter(X_new[:,0],X_new[:,1],marker=\u0026#34;o\u0026#34;,c=y) plt.show() result: 可以看出降维后样本特征和类别信息之间的关系得以保留。\n一般来说，如果我们的数据是有类别标签的，那么优先选择LDA去尝试降维；当然也可以使用PCA做很小幅度的降维去消去噪声，然后再使用LDA降维。如果没有类别标签，那么肯定PCA是最先考虑的一个选择了。\n","date":"2022-03-07T00:00:00Z","permalink":"https://example.com/p/%E7%94%A8scikit-learn%E8%BF%9B%E8%A1%8Clda%E9%99%8D%E7%BB%B4/","title":"用scikit-learn进行LDA降维"},{"content":"import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) # matplotlib.style.use(\u0026#34;ggplot\u0026#34;) #这个用来绘制三维图 from mpl_toolkits.mplot3d import Axes3D # from sklearn.datasets.samples_generator import make_blobs from sklearn.datasets import make_blobs # X为样本特征，Y为样本簇类别， 共1000个样本，每个样本3个特征，共4个簇 X,y = make_blobs(n_samples=10000,n_features=3,centers=[[3,3,3],[0,0,0],[1,1,1],[2,2,2]], cluster_std=[0.2,0.1,0.2,0.2],random_state=22) fig = plt.figure(figsize=(15,5))#之所以要这样是为了传给Axes3D一个画布 ax = Axes3D(fig,rect=[0,0,1,1],elev=30,azim=20) plt.scatter(X[:,0],X[:,1],X[:,2],marker=\u0026#34;o\u0026#34;) result: 我们先不降维，只对数据进行投影，看看投影后的三个维度的方差分布，代码如下：\nfrom sklearn.decomposition import PCA pca = PCA(n_components=3) pca.fit(X) print(pca.explained_variance_) print(pca.explained_variance_ratio_) result:\n[3.78352072 0.03342374 0.03210098] [0.98297637 0.00868364 0.00833998] 投影后第一个特征占了绝大多数的主成分比例。\n现在我们来进行降维，从三维降到2维，代码如下：\npca = PCA(n_components=2) pca.fit(X) print(pca.explained_variance_) print(pca.explained_variance_ratio_) result:\n[3.78352072 0.03342374] [0.98297637 0.00868364] 投影到二维后选择的肯定是前两个特征，而抛弃第三个特征。\n为了有个直观的认识，我们看看此时转化后的数据分布，代码如下：\n#降维之后的数据用transform获得 X_nex = pca.transform(X) plt.scatter(X_nex[:,0],X_nex[:,1],marker=\u0026#34;o\u0026#34;) plt.show() result: 可见降维后的数据依然可以很清楚的看到我们之前三维图中的4个簇。\n现在我们看看不直接指定降维的维度，而指定降维后的主成分方差和比例。\npca = PCA(n_components=0.95) pca.fit(X) print(pca.explained_variance_ratio_) print(pca.explained_variance_) print(pca.n_components_) result:\n[0.98297637] [3.78352072] 1 可见只有第一个投影特征被保留。这也很好理解，我们的第一个主成分占投影特征的方差比例高达98%。只选择这一个特征维度便可以满足95%的阈值。我们现在选择阈值99%看看，代码如下：\npca = PCA(n_components=0.99) pca.fit(X) print(pca.explained_variance_ratio_) print(pca.explained_variance_) print(pca.n_components) result:\n[0.98297637 0.00868364] [3.78352072 0.03342374] 0.99 这个结果也很好理解，因为我们第一个主成分占了98.3%的方差比例，第二个主成分占了0.8%的方差比例，两者一起可以满足我们的阈值。 最后我们看看让MLE算法自己选择降维维度的效果，代码如下：\npca = PCA(n_components=\u0026#34;mle\u0026#34;) pca.fit(X) print(pca.explained_variance_) print(pca.explained_variance_ratio_) print(pca.n_components_) result:\n[3.78352072] [0.98297637] 1 可见由于我们的数据的第一个投影特征的方差占比高达98.3%，MLE算法只保留了我们的第一个特征。\n","date":"2022-03-05T00:00:00Z","permalink":"https://example.com/p/%E7%94%A8scikit-learn%E5%AD%A6%E4%B9%A0%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90pca/","title":"用scikit-learn学习主成分分析(PCA)"},{"content":"LLE用于降维可视化实践\n下面我们用一个具体的例子来使用scikit-learn进行LLE降维并可视化。\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D #manifold是用来导入LLE from sklearn import manifold,datasets from sklearn.utils import check_random_state 我们接着生成随机数据，由于LLE必须要基于流形不能闭合，因此我们生成了一个缺一个口的三维球体。生成数据并可视化的代码如下：\nn_samples = 500 #check_random_state 的作用是 Turn seed into a np.random.RandomState instance random_state = check_random_state(0) print(random_state) result:\nRandomState(MT19937) #作用体现在这里了 p = random_state.rand(n_samples)*(2*np.pi-0.55) t = random_state.rand(n_samples)*np.pi print(p,t) result: # 让球体不闭合，符合流形定义 indices = ((t \u0026lt; (np.pi - (np.pi / 8))) \u0026amp; (t \u0026gt; ((np.pi / 8)))) colors = p[indices] x, y, z = np.sin(t[indices]) * np.cos(p[indices]), \\ np.sin(t[indices]) * np.sin(p[indices]), \\ np.cos(t[indices]) fig = plt.figure(figsize=(15,8)) ax = Axes3D(fig,elev=30,azim=-20) ax.scatter(x,y,z,c=p[indices],marker=\u0026#34;o\u0026#34;,cmap = plt.cm.rainbow) result: 现在我们简单的尝试用LLE将其从三维降为2维并可视化，近邻数设为30，用标准的LLE算法。\ntrain_data = np.array([x,y,z]).T train_data = manifold.LocallyLinearEmbedding(n_neighbors=30,n_components=2, method=\u0026#34;standard\u0026#34;).fit_transform(train_data) plt.scatter(train_data[:,0],train_data[:,1],marker=\u0026#34;o\u0026#34;,c=colors) result: 可以看出从三维降到了2维后，我们大概还是可以看出这是一个球体。 现在我们看看用不同的近邻数时，LLE算法降维的效果图，代码如下：\nfor index,k in enumerate((10,20,30,40)): plt.subplot(2,2,index+1) trans_data = manifold.LocallyLinearEmbedding(n_components=2,n_neighbors=k, method=\u0026#34;standard\u0026#34;).fit_transform(train_data) plt.scatter(trans_data[:,0],trans_data[:,1],marker=\u0026#34;o\u0026#34;,c=colors) plt.text(.99,.01,(\u0026#34;LLE: k=%d\u0026#34; % (k)),transform=plt.gca().transAxes,size=10, horizontalalignment=\u0026#34;right\u0026#34;) plt.show() 现在我们看看还是这些k近邻数，用HLLE的效果。\nfor index,k in enumerate((10,20,30,40)): plt.subplot(2,2,index+1) trans_data = manifold.LocallyLinearEmbedding(n_components=2,n_neighbors=k, method=\u0026#34;hessian\u0026#34;).fit_transform(train_data) plt.scatter(trans_data[:,0],trans_data[:,1],marker=\u0026#34;o\u0026#34;,c=colors) plt.text(.99,.01,(\u0026#34;LLE: k=%d\u0026#34; % (k)),transform=plt.gca().transAxes,size=10, horizontalalignment=\u0026#34;right\u0026#34;) plt.show() result: 可见在同样的近邻数的时候，HLLE降维后的数据分布特征效果要比LLE更好。\n我们接着看看MLLE和LTSA的效果。由于代码类似，这里就只给出效果图。\nfor index,k in enumerate((10,20,30,40)): plt.subplot(2,2,index+1) trans_data = manifold.LocallyLinearEmbedding(n_components=2,n_neighbors=k, method=\u0026#34;modified\u0026#34;).fit_transform(train_data) plt.scatter(trans_data[:,0],trans_data[:,1],marker=\u0026#34;o\u0026#34;,c=colors) plt.text(.99,.01,(\u0026#34;LLE: k=%d\u0026#34; % (k)),transform=plt.gca().transAxes,size=10, horizontalalignment=\u0026#34;right\u0026#34;) plt.show() result: for index,k in enumerate((10,20,30,40)): plt.subplot(2,2,index+1) trans_data = manifold.LocallyLinearEmbedding(n_components=2,n_neighbors=k, method=\u0026#34;ltsa\u0026#34;).fit_transform(train_data) plt.scatter(trans_data[:,0],trans_data[:,1],marker=\u0026#34;o\u0026#34;,c=colors) plt.text(.99,.01,(\u0026#34;LLE: k=%d\u0026#34; % (k)),transform=plt.gca().transAxes,size=10, horizontalalignment=\u0026#34;right\u0026#34;) plt.show() result: ","date":"2022-03-05T00:00:00Z","permalink":"https://example.com/p/%E7%94%A8scikit-learn%E7%A0%94%E7%A9%B6%E5%B1%80%E9%83%A8%E7%BA%BF%E6%80%A7%E5%B5%8C%E5%85%A5lle/","title":"用scikit-learn研究局部线性嵌入(LLE)"},{"content":" ","date":"2022-03-03T00:00:00Z","permalink":"https://example.com/p/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/","title":"《动手学深度学习》"},{"content":"itertools包中的方法product 函数\nproduct(A, B)函数，返回A、B中的元素的笛卡尔积的元组\ncode:\nfrom itertools import product A = [1,2,3,4] B = [5,6,7,8] list(product(A,B)) result:\n[(1, 5), (1, 6), (1, 7), (1, 8), (2, 5), (2, 6), (2, 7), (2, 8), (3, 5), (3, 6), (3, 7), (3, 8), (4, 5), (4, 6), (4, 7), (4, 8)] 可以看出返回的是A和B中每一个元素的元组组合\n就是说它会将A中的第一个元素与B中的每一个元素构成一个元组组合，以此类推下去\n","date":"2022-03-02T00:00:00Z","permalink":"https://example.com/p/python%E4%B8%AD%E7%9A%84itertools.product/","title":"Python中的itertools.product"},{"content":" 这是学习pytorch基本使用的记录（我记得看的是B站刘二老师的视频）\n注意几个参数\n输入和隐层（输出）维度 序列长度 批处理大小 注 调用RNNCell这个需要循环，循环长度就是序列长度 import torch batch_size = 1 seq_len = 3 #序列长度 input_size = 4 #输入维度 hidden_size = 2 #隐层维度 cell = torch.nn.RNNCell(input_size=input_size,hidden_size=hidden_size) dataset = torch.randn(seq_len,batch_size,input_size) hidden = torch.zeros(batch_size,hidden_size) #for循环处理seq_len长度的数据 for idx,data in enumerate(dataset): print(\u0026#34;=\u0026#34;*20,idx,\u0026#34;=\u0026#34;*20) print(\u0026#34;Input size:\u0026#34;,data.shape,data) hidden = cell(data,hidden) print(\u0026#34;hidden size:\u0026#34;,hidden.shape,hidden) print(hidden) result:\n==================== 0 ==================== Input size: torch.Size([1, 4]) tensor([[-0.5352, 1.8843, -0.0926, 0.5294]]) hidden size: torch.Size([1, 2]) tensor([[ 0.3160, -0.5305]], grad_fn=\u0026lt;TanhBackward0\u0026gt;) tensor([[ 0.3160, -0.5305]], grad_fn=\u0026lt;TanhBackward0\u0026gt;) ==================== 1 ==================== Input size: torch.Size([1, 4]) tensor([[-1.6617, 0.4429, -0.1435, 0.9613]]) hidden size: torch.Size([1, 2]) tensor([[-0.9231, 0.6875]], grad_fn=\u0026lt;TanhBackward0\u0026gt;) tensor([[-0.9231, 0.6875]], grad_fn=\u0026lt;TanhBackward0\u0026gt;) ==================== 2 ==================== Input size: torch.Size([1, 4]) tensor([[-0.1785, 0.3038, -1.1640, -1.6320]]) hidden size: torch.Size([1, 2]) tensor([[ 0.9599, -0.9934]], grad_fn=\u0026lt;TanhBackward0\u0026gt;) tensor([[ 0.9599, -0.9934]], grad_fn=\u0026lt;TanhBackward0\u0026gt;) In [ ]: 1 ​ ","date":"2022-02-28T00:00:00Z","permalink":"https://example.com/p/pytorch%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B12_%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E8%B7%AF%E5%9F%BA%E7%A1%80%E7%AF%87/","title":"pytorch基本模型12_循环神经网路（基础篇）"},{"content":"Protein structure prediction and analysis using the Robetta server\nlink:https://academic.oup.com/nar/article/32/suppl_2/W526/1040731?login=false\n","date":"2022-02-27T00:00:00Z","permalink":"https://example.com/p/protein-structure-prediction-and-analysis-using-the-robetta-server/","title":"Protein structure prediction and analysis using the Robetta server"},{"content":" 教程看的是这里的 https://www.runoob.com/python/os-file-methods.html\nos.access(path,mode) 用来检验权限模式 os.access() 方法使用当前的uid/gid尝试访问路径。大部分操作使用有效的 uid/gid, 因此运行环境可以在 suid/sgid 环境尝试\n参数\npath \u0026ndash; 要用来检测是否有访问权限的路径\nmode \u0026ndash; mode为F_OK，测试存在的路径，或者它可以是包含R_OK, W_OK和X_OK或者R_OK, W_OK和X_OK其中之一或者更多\nos.F_OK: 作为access()的mode参数，测试path是否存在 os.R_OK: 包含在access()的mode参数中 ， 测试path是否可读 os.W_OK 包含在access()的mode参数中 ， 测试path是否可写 os.X_OK 包含在access()的mode参数中 ，测试path是否可执行 code:\nimport os #文件是否存在 os.access(\u0026#34;./os_test.txt\u0026#34;,mode=os.F_OK) result:\nTrue code:\n#文件是否可读 os.access(\u0026#34;./os_test.txt\u0026#34;,mode=os.R_OK) result:\nTrue code\n#文件是否可写 os.access(\u0026#34;./os_test.txt\u0026#34;,mode=os.W_OK) result:\nTrue code:\n#文件是否可执行 os.access(\u0026#34;./os_test.txt\u0026#34;,mode=os.X_OK) result:\nTrue os.chdir() os.chdir() 方法用于改变当前工作目录到指定的路径\n参数： path \u0026ndash; 要切换到的新路径 返回值 如果允许访问返回 True , 否则返回False code:\n#先看看看当前目录在哪里 os.getcwd() result:\n\u0026#39;F:\\\\newjupyter\\\\py进阶点\u0026#39; code:\nos.chdir(\u0026#34;../3D-QSAR生物效应工具/\u0026#34;) os.getcwd() result:\n\u0026#39;F:\\\\newjupyter\\\\3D-QSAR生物效应工具\u0026#39; code:\nos.chdir(\u0026#34;../py进阶点/\u0026#34;) os.chflags(path,flags) os.chflags() 方法用于设置路径的标记为数字标记。多个标记可以使用 OR 来组合起来;只支持在 Unix 下使用\npath \u0026ndash; 文件名路径或目录路径 flags \u0026ndash; 可以是以下值 stat.UF_NODUMP: 非转储文件 stat.UF_IMMUTABLE: 文件是只读的 stat.UF_APPEND: 文件只能追加内容 stat.UF_NOUNLINK: 文件不可删除 / 重命名 stat.UF_OPAQUE: 目录不透明，需要通过联合堆栈查看 stat.SF_ARCHIVED: 可存档文件(超级用户可设) stat.SF_IMMUTABLE: 文件是只读的(超级用户可设) stat.SF_APPEND: 文件只能追加内容(超级用户可设) stat.SF_NOUNLINK: 文件不可删除(超级用户可设) stat.SF_SNAPSHOT: 快照文件(超级用户可设) os.chmod(path,mode) os.chmod() 方法用于更改文件或目录的权限\npath \u0026ndash; 文件路径 flags \u0026ndash; 可用以下选项按位或操作生成， 目录的读权限表示可以获取目录里文件名列表， ，执行权限表示可以把工作目录切换到此目录 ，删除添加目录里的文件必须同时有写和执行权限 ，文件权限以用户id-\u0026gt;组id-\u0026gt;其它顺序检验,最先匹配的允许或禁止权限被应用。 os.chown(path,uid,gid) os.chown() 方法用于更改文件所有者，如果不修改可以设置为 -1, 你需要超级用户权限来执行权限修改操作 只支持在 Unix 下使用\n参数 path \u0026ndash; 设置权限的文件路径 uid \u0026ndash; 所属用户 ID gid \u0026ndash; 所属用户组 ID os.chroot(path) os.chroot() 方法用于更改当前进程的根目录为指定的目录，使用该函数需要管理员权限\n感觉很多都是用不上的，所以后面的仅列出命名的作用，要用的时再去教程看\nos.close(fd) 关闭文件描述符 fd\nos.closerange(fd_low,fd_high) 关闭所有文件描述符，从fd_low(包含)到fd_high(不包含)，错误会忽略\nos.dup(fd) 复制文件描述符 fd\nos.dup2(fd,fd2) 将一个文件描述符fd复制到另一个fd2\nos.fchdir(fd) 通过文件描述符改变当前的工作目录\nos.fchmod(fd,mode) 改变一个文件的访问权限，该文件由参数fd指定，参数mode是Unix下的文件访问权限\nos.fchown(fd,uid,gid) 修改一个文件的所有权，这个函数修改一个文件的用户ID和用户组ID，该文件由文件描述符fd指定\nos.fdatasync(fd) 强制将文件写入磁盘，该文件由文件描述符fd指定，但是不强制更新文件的状态信息\nos.fdopen(fd) 通过文件描述符 fd 创建一个文件对象，并返回这个文件对象\nos.fpathconf(fd, name) 返回一个打开的文件的系统配置信息。name为检索的系统配置的值，它也许是一个定义系统值的字符串，这些名字在很多标准中指定（POSIX.1, Unix 95, Unix 98, 和其它）\nos.fstat(fd) 返回文件描述符fd的状态，像stat()\nos.fstatvfs(fd) 返回包含文件描述符fd的文件的文件系统信息，像statfs()\nos.fsync(fd) 强制将文件描述符为fd的文件写入硬盘\nftruncate(fd,length) 裁剪文件描述符fd对应的文件，所以它最大不能超过文件大小\nos.getcwd() 返回当前工作目录\nos.getcwdu() 返回一个当前工作目录的Unicode对象\n太多了，挑几个常用的出来\nos.link(src,dst) os.link() 方法用于创建硬链接，名为参数 dst，指向参数 src。 该方法对于创建一个已存在文件的拷贝是非常有用的。 只支持在 Unix, Windows 下使用\n参数 src 用于创建硬连接的源地址 dst 用于创建硬连接的目标地址 os.listdir(path) os.listdir() 方法用于返回指定的文件夹包含的文件或文件夹的名字的列表。\n它不包括 . 和 .. 即使它在文件夹中。\n只支持在 Unix, Windows 下使用。 os.remove(path) os.remove() 方法用于删除指定路径的文件。如果指定的路径是一个目录，将抛出OSError。\n删除目录则用 os.removedirs(path)\n在Unix, Windows中有效 os.rename(src,dst) os.rename() 方法用于命名文件或目录，从 src 到 dst,如果dst是一个存在的目录, 将抛出OSError\n修改目录名则用 os.renames(src,dst) os.rmdir(path) os.rmdir() 方法用于删除指定路径的目录。仅当这文件夹是空的才可以, 否则, 抛出OSError\n注意是删除空目录\nos.path()模块 os.path 模块主要用于获取文件的属性\n","date":"2022-02-25T00:00:00Z","permalink":"https://example.com/p/py_os%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93/","title":"py_os方法总结"},{"content":" 这是学习pytorch基本使用的记录（我记得看的是B站刘二老师的视频）\n1、卷积核超参数选择困难，自动找到卷积的最佳组合。 2、1x1卷积核，不同通道的信息融合。使用1x1卷积核虽然参数量增加了，但是能够显著的降低计算量(operations) 3、Inception Moudel由4个分支组成，要分清哪些是在Init里定义，哪些是在forward里调用。4个分支在dim=1(channels)上进行concatenate。24+16+24+24 = 88 4、GoogleNet的Inception(Pytorch实现) 5、1乘28乘28 → 10乘24乘24 → 10乘12乘12 → 88乘12乘12 → 20乘8乘8 → 88乘4乘4=1408 代码说明：\n1、先使用类对Inception Moudel进行封装\n2、先是1个卷积层(conv,maxpooling,relu)，然后inceptionA模块(输出的channels是24+16+24+24=88)，接下来又是一个卷积层(conv,mp,relu),然后inceptionA模块，最后一个全连接层(fc)。\n3、1408这个数据可以通过x = x.view(in_size, -1)后调用x.shape得到。\nimport torch import torch.nn as nn from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader import torch.nn.functional as F batch_size = 64 transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307),(0.3081,))]) train_dataset = datasets.MNIST(root=\u0026#34;../dataset/minis/\u0026#34;,train=True,transform=transform) train_loader = DataLoader(dataset=train_dataset,shuffle=True,batch_size=batch_size) test_dataset = datasets.MNIST(root=\u0026#34;../dataset/minis/\u0026#34;,train=False,transform=transform) test_loader = DataLoader(dataset=test_dataset,shuffle=True,batch_size=batch_size) class InceptionA(torch.nn.Module): def __init__(self,in_channels): super(InceptionA,self).__init__() self.branch1x1 = torch.nn.Conv2d(in_channels,16,kernel_size=1) self.branch5x5_1 = torch.nn.Conv2d(in_channels,16,kernel_size=1) self.branch5x5_2 = torch.nn.Conv2d(16,24,kernel_size=5,padding=2) self.branch3x3_1 = torch.nn.Conv2d(in_channels,16,kernel_size=1) self.branch3x3_2 = torch.nn.Conv2d(16,24,kernel_size=3,padding=1) self.branch3x3_3 = torch.nn.Conv2d(24,24,kernel_size=3,padding=1) self.branch_pool = torch.nn.Conv2d(in_channels,24,kernel_size=1) def forward(self,x): branch1x1 = self.branch1x1(x) branch5x5 = self.branch5x5_1(x) branch5x5 = self.branch5x5_2(branch5x5) branch3x3 = self.branch3x3_1(x) branch3x3 = self.branch3x3_2(branch3x3) branch3x3 = self.branch3x3_3(branch3x3) branch_pool = F.avg_pool2d(x,kernel_size=3,stride=1,padding=1) branch_pool = self.branch1x1(branch_pool) outputs = [branch1x1,branch5x5,branch3x3,branch_pool] #torch.cat 拼接作用 #b,c,w,h c对应的是dim=1 return torch.cat(outputs,dim=1) class Net(torch.nn.Module): def __init__(self): super(Net,self).__init__() self.conv1 = torch.nn.Conv2d(1,10,kernel_size=5) self.conv2 = torch.nn.Conv2d(88,20,kernel_size=5)# 88 = 24x3 + 16 self.incep1 = InceptionA(in_channels=10)# 与conv1 中的10对应 self.incep2 = InceptionA(in_channels=20)# 与conv1 中的20对应 self.mp = torch.nn.MaxPool2d(2) self.fc = torch.nn.Linear(1480,10) def forward(self,x): in_size = x.size(0) x = F.relu(self.mp(self.conv1(x))) x = self.incep1(x) x = F.relu(self.mp(self.conv2(x))) x = self.incep2(x) x = x.view(in_size,-1) x = self.fc(x) return x model = Net() criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.5) def train(epoch): running_loss = 0.0 for batch_idx,data in enumerate(train_loader,0): inputs,target = data optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs,target) loss.backward() optimizer.step() running_loss += loss.item() if batch_idx % 300 == 299: print(\u0026#34;[%d, %5d] loss: %.3f\u0026#34; % (epoch+1,batch_idx+1,running_loss/300)) running_loss = 0.0 def test(): correct = 0 total = 0 with torch.no_grad(): for data in test_loader: images,labels = data outputs = model(images) _, predicted = torch.max(outputs.data,dim=1) total += labels.size(0) correct += (predicted == labels).sum().item() print(\u0026#34;accuracy on test set: %d %%\u0026#34; % (100*correct/total)) if __name__ == \u0026#34;__main__\u0026#34;: for epoch in range(10): train(epoch) test() result:\n--------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_10908/1354835192.py in \u0026lt;module\u0026gt; 1 if __name__ == \u0026#34;__main__\u0026#34;: 2 for epoch in range(10): ----\u0026gt; 3 train(epoch) 4 test() ~\\AppData\\Local\\Temp/ipykernel_10908/1826242549.py in train(epoch) 5 optimizer.zero_grad() 6 ----\u0026gt; 7 outputs = model(inputs) 8 loss = criterion(outputs,target) 9 loss.backward() F:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py in _call_impl(self, *input, **kwargs) 1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1109 or _global_forward_hooks or _global_forward_pre_hooks): -\u0026gt; 1110 return forward_call(*input, **kwargs) 1111 # Do not call functions when jit is used 1112 full_backward_hooks, non_full_backward_hooks = [], [] ~\\AppData\\Local\\Temp/ipykernel_10908/3171288132.py in forward(self, x) 15 x = F.relu(self.mp(self.conv1(x))) 16 x = self.incep1(x) ---\u0026gt; 17 x = F.relu(self.mp(self.conv2(x))) 18 x = self.incep2(x) 19 x = x.view(in_size,-1) F:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py in _call_impl(self, *input, **kwargs) 1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1109 or _global_forward_hooks or _global_forward_pre_hooks): -\u0026gt; 1110 return forward_call(*input, **kwargs) 1111 # Do not call functions when jit is used 1112 full_backward_hooks, non_full_backward_hooks = [], [] F:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py in forward(self, input) 445 446 def forward(self, input: Tensor) -\u0026gt; Tensor: --\u0026gt; 447 return self._conv_forward(input, self.weight, self.bias) 448 449 class Conv3d(_ConvNd): F:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py in _conv_forward(self, input, weight, bias) 441 weight, bias, self.stride, 442 _pair(0), self.dilation, self.groups) --\u0026gt; 443 return F.conv2d(input, weight, bias, self.stride, 444 self.padding, self.dilation, self.groups) 445 RuntimeError: Given groups=1, weight of size [20, 88, 5, 5], expected input[64, 80, 12, 12] to have 88 channels, but got 80 channels instead 说明：\n1、要解决的问题：梯度消失\n2、跳连接，H(x) = F(x) + x,张量维度必须一样，加完后再激活。不要做pooling，张量的维度会发生变化。 代码说明：\n1、先是1个卷积层(conv,maxpooling,relu)，然后ResidualBlock模块，接下来又是一个卷积层(conv,mp,relu),然后esidualBlock模块模块，最后一个全连接层(fc)。\nimport torch from torchvision import datasets,transforms from torch.utils.data import DataLoader import torch.nn.functional as F batch_size = 64 transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3081,))]) train_dataset = datasets.MNIST(root=\u0026#34;../dataset/minis/\u0026#34;,transform=transform,train=True) train_loader = DataLoader(dataset=train_dataset,shuffle=True,batch_size=batch_size) test_dataset = datasets.MNIST(root=\u0026#34;../dataset/minis/\u0026#34;,transform=transform,train=True) test_loader = DataLoader(dataset=test_dataset,shuffle=True,batch_size=batch_size) class ResidualBlock(torch.nn.Module): def __init__(self,channels): super(ResidualBlock,self).__init__() self.channels = channels self.conv1 = torch.nn.Conv2d(channels,channels,kernel_size=3,padding=1) self.conv2 = torch.nn.Conv2d(channels,channels,kernel_size=3,padding=1) def forward(self,x): y = F.relu(self.conv1(x)) y = self.conv2(y) return F.relu(x+y) class Net(torch.nn.Module): def __init__(self): super(Net,self).__init__() self.conv1 =torch.nn.Conv2d(1,16,kernel_size=5) self.conv2 = torch.nn.Conv2d(16,32,kernel_size=5) ## 88 = 24x3 + 16 self.rblock1 = ResidualBlock(16) self.rblock2 = ResidualBlock(32) self.mp = nn.MaxPool2d(2) self.fc = nn.Linear(512,10) def forward(self,x): in_size = x.size(0) x = self.mp(F.relu(self.conv1(x))) x = self.rblock1(x) x = self.mp(F.relu(self.conv2(x))) x = self.rblock2(x) x = x.view(in_size,-1) x = self.fc(x) return x model = Net() criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.5) def train(epoch): running_loss = 0.0 for batch_idx,data in enumerate(train_loader,0): inputs,target = data optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs,target) loss.backward() optimizer.step() running_loss += loss.item() if batch_idx % 300 == 299: print(\u0026#34;[%d, %5d] loss: %.3f \u0026#34; % (epoch+1, batch_idx+1, running_loss/300)) running_loss = 0.0 def test(): correct = 0 total = 0 with torch.no_grad(): for data in test_loader: images,labels = data outputs = model(images) _,predicted = torch.max(outputs.data,dim=1) total += labels.size(0) correct += (predicted == labels).sum().item() print(\u0026#34;accuracy on test set: %d %% \u0026#34; % (100*correct/total)) if __name__ == \u0026#34;__main__\u0026#34;: for epoch in range(10): train(epoch) test() result:\n[1, 300] loss: 0.498 [1, 600] loss: 0.158 [1, 900] loss: 0.109 accuracy on test set: 97 % [2, 300] loss: 0.095 [2, 600] loss: 0.081 [2, 900] loss: 0.073 accuracy on test set: 97 % [3, 300] loss: 0.063 [3, 600] loss: 0.065 [3, 900] loss: 0.053 accuracy on test set: 98 % [4, 300] loss: 0.052 [4, 600] loss: 0.049 [4, 900] loss: 0.042 accuracy on test set: 98 % [5, 300] loss: 0.043 [5, 600] loss: 0.038 [5, 900] loss: 0.039 accuracy on test set: 98 % [6, 300] loss: 0.031 [6, 600] loss: 0.037 [6, 900] loss: 0.038 accuracy on test set: 99 % [7, 300] loss: 0.034 [7, 600] loss: 0.026 [7, 900] loss: 0.030 accuracy on test set: 99 % [8, 300] loss: 0.029 [8, 600] loss: 0.026 [8, 900] loss: 0.027 accuracy on test set: 99 % [9, 300] loss: 0.023 [9, 600] loss: 0.025 [9, 900] loss: 0.025 accuracy on test set: 99 % [10, 300] loss: 0.021 [10, 600] loss: 0.022 [10, 900] loss: 0.023 accuracy on test set: 99 % ​ ","date":"2022-02-25T00:00:00Z","permalink":"https://example.com/p/pytorch%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B11_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E9%AB%98%E7%BA%A7%E7%AF%87/","title":"pytorch基本模型11_卷积神经网络（高级篇）"},{"content":" 这是学习pytorch基本使用的记录（我记得看的是B站刘二老师的视频）\n1、每一个卷积核它的通道数量要求和输入通道是一样的。这种卷积核的总数有多少个和你输出通道的数量是一样的\n2 卷积(convolution)后，C(Channels)变，W(width)和H(Height)可变可不变，取决于是否padding。subsampling(或pooling)后，C不变，W和H变\n3 卷积层：保留图像的空间信息。\n4 卷积层要求输入输出是四维张量(B,C,W,H)，全连接层的输入与输出都是二维张量(B,Input_feature)\n5 卷积(线性变换)，激活函数(非线性变换)，池化；这个过程若干次后，view打平，进入全连接层 1、torch.nn.Conv2d(1,10,kernel_size=3,stride=2,bias=False)\n1是指输入的Channel，灰色图像是1维的；10是指输出的Channel，也可以说第一个卷积层需要10个卷积核；kernel_size=3,卷积核大小是3x3；stride=2进行卷积运算时的步长，默认为1；bias=False卷积运算是否需要偏置bias，默认为False。padding = 0，卷积操作是否补0。\n2、self.fc = torch.nn.Linear(320, 10)，这个320获取的方式，可以通过x = x.view(batch_size, -1) # print(x.shape)可得到(64,320),64指的是batch，320就是指要进行全连接操作时，输入的特征维度。\nimport torch from torchvision import datasets from torchvision import transforms from torch.utils.data import DataLoader import torch.nn.functional as F batch_size = 64 transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3081,))]) #简单来说就是使用datasets读取数据 #DataLoader来处理加入batch train_dataset = datasets.MNIST(root=\u0026#34;../dataset/minis/\u0026#34;,train=True,transform=transform) train_loader = DataLoader(train_dataset,shuffle=True,batch_size=batch_size) test_dataset = datasets.MNIST(root=\u0026#34;../dataset/minis/\u0026#34;,train=False,transform=transform) test_loader = DataLoader(test_dataset,shuffle=True,batch_size=batch_size) class Net(torch.nn.Module): def __init__(self): super(Net,self).__init__() self.conv1 = torch.nn.Conv2d(1,10,kernel_size=5) self.conv2 = torch.nn.Conv2d(10,20,kernel_size=5) self.pooling = torch.nn.MaxPool2d(2) self.fc = torch.nn.Linear(320,10) def forward(self,x): batch_size = x.size(0) x = F.relu(self.pooling(self.conv1(x))) x = F.relu(self.pooling(self.conv2(x))) x = x.view(batch_size,-1)# -1 此处自动算出的是320 x = self.fc(x) return x model = Net() #如果要使用GPU device = torch.device(\u0026#34;cuda\u0026#34; if torch.cuda.is_available() else \u0026#34;cpu\u0026#34;) model.to(device) criterion = torch.nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model.parameters(),lr=0.01,momentum=0.5) def train(epoch): running_loss = 0.0 for batch_idx,data in enumerate(train_loader,0): inputs,target = data optimizer.zero_grad() outputs = model(inputs) loss = criterion(outputs,target) loss.backward() optimizer.step() running_loss += loss.item() if batch_idx % 300 == 200: print(\u0026#34;[%d, %5d] loss: %.3f\u0026#34; % (epoch+1,batch_idx+1,runing_loss/300)) runing_loss = 0.0 def test(): correct = 0 total = 0 with torch.no_grad(): for data in test_loader: images,labels = data outputs = model(images) _,predicted = torch.max(outputs.data,dim=1) total += labels.size(0) correct += (predicted == labels).sum().item() print(\u0026#34;accuracy on test set: %d %%\u0026#34; %(100*correct/total)) if __name__ == \u0026#34;__main__\u0026#34;: for epoch in range(10): train(epoch) test() result:\naccuracy on test set: 10 % accuracy on test set: 10 % accuracy on test set: 10 % accuracy on test set: 12 % accuracy on test set: 11 % accuracy on test set: 14 % accuracy on test set: 17 % accuracy on test set: 22 % accuracy on test set: 28 % accuracy on test set: 34 % ","date":"2022-02-24T00:00:00Z","permalink":"https://example.com/p/pytorch%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B10_%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%E7%AF%87/","title":"pytorch基本模型10_卷积神经网络（基础篇）"},{"content":" 这是学习pytorch基本使用的记录（我记得看的是B站刘二老师的视频）\nimport torch import numpy as np xy = np.loadtxt(\u0026#34;../ppt\u0026amp;vedio/diabetes.csv.gz\u0026#34;,delimiter=\u0026#34;,\u0026#34;,dtype=np.float32) x_data = torch.from_numpy(xy[:,:-1]) # [-1] 最后得到的是个矩阵 y_data = torch.from_numpy(xy[:,[-1]]) x_data result:\ntensor([[-0.2941, 0.4874, 0.1803, ..., 0.0015, -0.5312, -0.0333], [-0.8824, -0.1457, 0.0820, ..., -0.2072, -0.7669, -0.6667], [-0.0588, 0.8392, 0.0492, ..., -0.3055, -0.4927, -0.6333], ..., [-0.4118, 0.2161, 0.1803, ..., -0.2191, -0.8574, -0.7000], [-0.8824, 0.2663, -0.0164, ..., -0.1028, -0.7686, -0.1333], [-0.8824, -0.0653, 0.1475, ..., -0.0939, -0.7976, -0.9333]]) In [11]: 1 class Model(torch.nn.Module): code\nclass Model(torch.nn.Module): def __init__(self): super(Model,self).__init__() #注意输入输出特征数的对应 self.linear1 = torch.nn.Linear(8,6) self.linear2 = torch.nn.Linear(6,4) self.linear3 = torch.nn.Linear(4,1) self.sigmoid = torch.nn.Sigmoid() def forward(self,x): x = self.sigmoid(self.linear1(x)) x = self.sigmoid(self.linear2(x)) x = self.sigmoid(self.linear3(x)) return x model = Model() criterion = torch.nn.BCELoss(reduction=\u0026#34;mean\u0026#34;) optimizer = torch.optim.SGD(model.parameters(),lr=0.1) for epoch in range(1000000): y_pred = model(x_data) loss = criterion(y_data,y_pred) print(epoch,loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() result:\n0 53.0677490234375 1 nan 2 nan 3 nan 4 nan 5 nan 6 nan 7 nan code:\nprint(model.linear1.weight.data) print(model.linear1.bias.data) result:\ntensor([[nan, nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan, nan, nan, nan]]) tensor([nan, nan, nan, nan, nan, nan]) ","date":"2022-02-22T00:00:00Z","permalink":"https://example.com/p/pytorch%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B07_%E5%A4%84%E7%90%86%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81%E7%9A%84%E8%BE%93%E5%85%A5/","title":"pytorch基本模型07_处理多维特征的输入"},{"content":" 这是学习pytorch基本使用的记录（我记得看的是B站刘二老师的视频）\nimport numpy as np import torch from torch.utils.data import Dataset,DataLoader #DiabetesDataset集成了Dataset的属性 class DiabetesDataset(Dataset): def __init__(self,filepath): xy = np.loadtxt(filepath,delimiter=\u0026#34;,\u0026#34;,dtype=np.float32) self.len = xy.shape[0] #from_numpy 可以将numpy对象转为Tensor self.x_data = torch.from_numpy(xy[:,:-1]) self.y_data = torch.from_numpy(xy[:,[-1]]) def __getitem__(self,index): return self.x_data[index],self.y_data[index] def __len__(self): return self.len dataset = DiabetesDataset(\u0026#34;../ppt\u0026amp;vedio/diabetes.csv.gz\u0026#34;) train_loader = DataLoader(dataset=dataset, batch_size=32, shuffle=True, num_workers=0) class Model(torch.nn.Module): def __init__(self): super(Model,self).__init__() self.linear1 = torch.nn.Linear(8,6) self.linear2 = torch.nn.Linear(6,4) self.linear3 = torch.nn.Linear(4,1) self.sigmoid = torch.nn.Sigmoid() def forward(self,x): x = self.sigmoid(self.linear1(x)) x = self.sigmoid(self.linear2(x)) x = self.sigmoid(self.linear3(x)) return x model = Model() criterion = torch.nn.BCELoss(reduction=\u0026#34;mean\u0026#34;) optimizer = torch.optim.SGD(model.parameters(),lr=0.01) for epoch in range(100): for i,data in enumerate(train_loader,0): inputs,labels =data y_pred = model(inputs) loss = criterion(y_pred,labels) print(epoch,i,loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() result:\n0 0 0.7238109111785889 0 1 0.734155535697937 0 2 0.735148549079895 0 3 0.7151535153388977 0 4 0.7010735273361206 0 5 0.7042306065559387 0 6 0.7132471203804016 0 7 0.7032475471496582 0 8 0.691323459148407 0 9 0.7086484432220459 0 10 0.7053466439247131 0 11 0.7048277854919434 0 12 0.696583092212677 0 13 0.7092943787574768 0 14 0.7132148742675781 0 15 0.6938268542289734 0 16 0.7091550827026367 0 17 0.705946147441864 0 18 0.7031751275062561 0 19 0.6971657276153564 0 20 0.7075298428535461 0 21 0.7028775811195374 0 22 0.6948002576828003 0 23 0.7060260772705078 1 0 0.7041218280792236 1 1 0.6952049732208252 1 2 0.6952813863754272 1 3 0.6941629648208618 1 4 0.699323296546936 1 5 0.6982346177101135 1 6 0.6976686120033264 1 7 0.6967370510101318 1 8 0.6927669048309326 1 9 0.6944431066513062 1 10 0.6935222744941711 1 11 0.6937293410301208 1 12 0.6927419900894165 1 13 0.6919858455657959 1 14 0.6903647184371948 1 15 0.6886408925056458 1 16 0.6904481053352356 1 17 0.6901053786277771 1 18 0.6863846182823181 1 19 0.6932711601257324 1 20 0.6921823024749756 1 21 0.6871242523193359 1 22 0.6864039897918701 1 23 0.6924161314964294 2 0 0.6908227205276489 2 1 0.6854847073554993 2 2 0.684808611869812 2 3 0.6918825507164001 2 4 0.679256021976471 2 5 0.6812565326690674 2 6 0.682233452796936 2 7 0.6815920472145081 2 8 0.6915656328201294 2 9 0.6766581535339355 2 10 0.6777160167694092 2 11 0.6963716745376587 2 12 0.6961408853530884 2 13 0.6914422512054443 2 14 0.6843780279159546 2 15 0.6816650629043579 2 16 0.688967227935791 2 17 0.6913697123527527 2 18 0.6785143613815308 2 19 0.6752075552940369 2 20 0.6632001996040344 2 21 0.6670174598693848 2 22 0.6686815619468689 2 23 0.6688615083694458 3 0 0.6596735119819641 3 1 0.6541830897331238 3 2 0.6833596229553223 3 3 0.6872202157974243 3 4 0.667326807975769 3 5 0.6747707724571228 3 6 0.6951794028282166 3 7 0.6744949221611023 3 8 0.6784132719039917 3 9 0.6868323683738708 3 10 0.6825138330459595 3 11 0.6824738383293152 3 12 0.6735210418701172 3 13 0.6822521686553955 3 14 0.6730729937553406 3 15 0.6681510210037231 3 16 0.6723111271858215 3 17 0.6624507904052734 3 18 0.6814053654670715 3 19 0.6764410734176636 3 20 0.6511460542678833 3 21 0.6706039905548096 3 22 0.6756730079650879 3 23 0.6560348868370056 4 0 0.6641905307769775 4 1 0.6637734770774841 4 2 0.6690414547920227 4 3 0.6629841923713684 4 4 0.6683598160743713 4 5 0.6680396199226379 4 6 0.6975505352020264 4 7 0.6679826974868774 4 8 0.6557121872901917 4 9 0.630560576915741 4 10 0.6665669679641724 4 11 0.6727769374847412 4 12 0.6661531925201416 4 13 0.6529167294502258 4 14 0.6920093297958374 4 15 0.6656674146652222 4 16 0.6787740588188171 4 17 0.6854212284088135 4 18 0.6518775820732117 4 19 0.6648255586624146 4 20 0.6715539693832397 4 21 0.6853976845741272 4 22 0.6366860866546631 4 23 0.684538722038269 5 0 0.6640180945396423 5 1 0.6781076788902283 5 2 0.6708680987358093 5 3 0.6853482127189636 5 4 0.6707643270492554 5 5 0.6706784963607788 5 6 0.6411705017089844 5 7 0.7001084089279175 5 8 0.6407696604728699 5 9 0.6624833345413208 5 10 0.6547479629516602 5 11 0.6313338875770569 5 12 0.6537962555885315 5 13 0.6931388974189758 5 14 0.6772197484970093 5 15 0.6693583726882935 5 16 0.6293537616729736 5 17 0.6690284013748169 5 18 0.6607130169868469 5 19 0.6605002284049988 5 20 0.6520935893058777 5 21 0.6517869234085083 5 22 0.6598515510559082 5 23 0.6490010619163513 6 0 0.676692008972168 6 1 0.6939624547958374 6 2 0.6337894201278687 6 3 0.6504641771316528 6 4 0.6676177978515625 6 5 0.6675586104393005 6 6 0.6586876511573792 6 7 0.6674678325653076 6 8 0.6315858960151672 6 9 0.6398857235908508 6 10 0.611860990524292 6 11 0.6948724389076233 6 12 0.6481903791427612 6 13 0.6854329109191895 6 14 0.6573666334152222 6 15 0.6948724985122681 6 16 0.6854835748672485 6 17 0.657301127910614 6 18 0.6571990847587585 6 19 0.6285296082496643 6 20 0.6663212776184082 6 21 0.6372913122177124 6 22 0.6661078929901123 6 23 0.6166301965713501 7 0 0.636046826839447 7 1 0.615567147731781 7 2 0.6249083876609802 7 3 0.6036498546600342 7 4 0.6756086945533752 7 5 0.6441784501075745 7 6 0.7280608415603638 7 7 0.6340582370758057 7 8 0.6545341610908508 7 9 0.7389792203903198 7 10 0.6547161340713501 7 11 0.6650868654251099 7 12 0.6544781923294067 7 13 0.6333034038543701 7 14 0.6647482514381409 7 15 0.6647932529449463 7 16 0.696911096572876 7 17 0.6540780663490295 7 18 0.6219121813774109 7 19 0.6538030505180359 7 20 0.588437557220459 7 21 0.6753298044204712 7 22 0.6754135489463806 7 23 0.6855016946792603 8 0 0.6422207951545715 8 1 0.6753576993942261 8 2 0.6531792879104614 8 3 0.6753762364387512 8 4 0.6640760898590088 8 5 0.6193313002586365 8 6 0.664100170135498 8 7 0.6640655994415283 8 8 0.6072323322296143 8 9 0.6982581615447998 8 10 0.6068139672279358 8 11 0.6521663665771484 8 12 0.6288632154464722 8 13 0.6987525224685669 8 14 0.640417754650116 8 15 0.6283869743347168 8 16 0.6634187698364258 8 17 0.6279183030128479 8 18 0.7110573649406433 8 19 0.6634540557861328 8 20 0.6396898627281189 8 21 0.675296425819397 8 22 0.6157262921333313 8 23 0.6695317029953003 9 0 0.7235534191131592 9 1 0.651442289352417 9 2 0.663252592086792 9 3 0.6512454152107239 9 4 0.6511242985725403 9 5 0.6269345283508301 9 6 0.7240882515907288 9 7 0.6027435064315796 9 8 0.6630622744560242 9 9 0.6017908453941345 9 10 0.6134974956512451 9 11 0.6379880905151367 9 12 0.6504077911376953 9 13 0.5873461365699768 9 14 0.6500006914138794 9 15 0.6626847386360168 9 16 0.7135129570960999 9 17 0.6626099944114685 9 18 0.6371610164642334 9 19 0.6114543080329895 9 20 0.649660587310791 9 21 0.72718346118927 9 22 0.6752955913543701 9 23 0.5799316763877869 10 0 0.6882656812667847 10 1 0.6496299505233765 10 2 0.6624820828437805 10 3 0.7014215588569641 10 4 0.6624214053153992 10 5 0.6236253380775452 10 6 0.6623857617378235 10 7 0.5839900970458984 10 8 0.6623408794403076 10 9 0.6490362882614136 10 10 0.6489733457565308 10 11 0.675641655921936 10 12 0.6754695773124695 10 13 0.6224534511566162 10 14 0.58207768201828 10 15 0.6891053915023804 10 16 0.5679411888122559 10 17 0.6483359336853027 10 18 0.66209477186203 10 19 0.6209486722946167 10 20 0.6895554065704346 10 21 0.6757334470748901 10 22 0.6483609676361084 10 23 0.6500270366668701 11 0 0.6481527090072632 11 1 0.5791332721710205 11 2 0.6897921562194824 11 3 0.7035629749298096 11 4 0.6064831614494324 11 5 0.6339586973190308 11 6 0.6618896126747131 11 7 0.6337881088256836 11 8 0.6617703437805176 11 9 0.6900659799575806 11 10 0.7038231492042542 11 11 0.7316666841506958 11 12 0.6481291055679321 11 13 0.6338856220245361 11 14 0.6620185375213623 11 15 0.6478383541107178 11 16 0.647846519947052 11 17 0.6056873798370361 11 18 0.6476273536682129 11 19 0.6333498358726501 11 20 0.6334037184715271 11 21 0.6474665403366089 11 22 0.6330724358558655 11 23 0.5693375468254089 12 0 0.647082507610321 12 1 0.6179332137107849 12 2 0.6470595002174377 12 3 0.6322972178459167 12 4 0.6762519478797913 12 5 0.6174972057342529 12 6 0.6468405723571777 12 7 0.6468695402145386 12 8 0.6171333193778992 12 9 0.720962643623352 12 10 0.6615203619003296 12 11 0.6024335622787476 12 12 0.6168980598449707 12 13 0.6914325952529907 12 14 0.6616049408912659 12 15 0.6465579271316528 12 16 0.5867887139320374 12 17 0.7066691517829895 12 18 0.676554262638092 12 19 0.6165215969085693 12 20 0.6915771961212158 12 21 0.6463576555252075 12 22 0.6614757776260376 12 23 0.606574296951294 13 0 0.6765707731246948 13 1 0.600888729095459 13 2 0.646346926689148 13 3 0.6309623718261719 13 4 0.6462646126747131 13 5 0.5849079489707947 13 6 0.6768302917480469 13 7 0.6152694225311279 13 8 0.6150890588760376 13 9 0.6769771575927734 13 10 0.6305424571037292 13 11 0.5680438280105591 13 12 0.6769883632659912 13 13 0.6614216566085815 13 14 0.7083064317703247 13 15 0.6146556735038757 13 16 0.6612513661384583 13 17 0.724049985408783 13 18 0.6768670678138733 13 19 0.6614719033241272 13 20 0.5991166234016418 13 21 0.6456809639930725 13 22 0.6457703709602356 13 23 0.7132522463798523 14 0 0.6457196474075317 14 1 0.6144103407859802 14 2 0.692805826663971 14 3 0.8023262023925781 14 4 0.6149206757545471 14 5 0.6769888997077942 14 6 0.6147246956825256 14 7 0.6302101612091064 14 8 0.7395233511924744 14 9 0.5682003498077393 14 10 0.6302297711372375 14 11 0.6456767320632935 14 12 0.598553478717804 14 13 0.6614272594451904 14 14 0.6297085285186768 14 15 0.6454963088035583 14 16 0.7091476917266846 14 17 0.6297441720962524 14 18 0.6930999159812927 14 19 0.6455128788948059 14 20 0.5821403861045837 14 21 0.5974255800247192 14 22 0.6131952404975891 14 23 0.647381603717804 15 0 0.6451981067657471 15 1 0.5645837187767029 15 2 0.5800583362579346 15 3 0.6777629256248474 15 4 0.5961548089981079 15 5 0.6449284553527832 15 6 0.6778580546379089 15 7 0.6613126993179321 15 8 0.6778783202171326 15 9 0.5465599298477173 15 10 0.6117131114006042 15 11 0.6613501906394958 15 12 0.744498074054718 15 13 0.5624729990959167 15 14 0.6115515232086182 15 15 0.5779907703399658 15 16 0.5775327682495117 15 17 0.6783391237258911 15 18 0.695258617401123 15 19 0.6445622444152832 15 20 0.812835693359375 15 21 0.6947230696678162 15 22 0.6615110635757446 15 23 0.7390678524971008 16 0 0.7109143733978271 16 1 0.5957066416740417 16 2 0.6945281624794006 16 3 0.6613753437995911 16 4 0.645072877407074 16 5 0.5954307317733765 16 6 0.6613692045211792 16 7 0.6614701747894287 16 8 0.5786951780319214 16 9 0.6780018210411072 16 10 0.6614130139350891 16 11 0.5616026520729065 16 12 0.577769935131073 16 13 0.7119166851043701 16 14 0.6783175468444824 16 15 0.6278899908065796 16 16 0.5775600671768188 16 17 0.6275492310523987 16 18 0.7462076544761658 16 19 0.6446069478988647 16 20 0.6444383859634399 16 21 0.6950806379318237 16 22 0.6446076035499573 16 23 0.6235194802284241 17 0 0.7290958166122437 17 1 0.6949761509895325 17 2 0.6948339343070984 17 3 0.7115582227706909 17 4 0.6115227937698364 17 5 0.5946512222290039 17 6 0.5944704413414001 17 7 0.6445977091789246 17 8 0.6446580290794373 17 9 0.5939019918441772 17 10 0.6275533437728882 17 11 0.6444928646087646 17 12 0.6614480018615723 17 13 0.5934413075447083 17 14 0.5589055418968201 17 15 0.6959425806999207 17 16 0.7473934888839722 17 17 0.6954304575920105 17 18 0.610569417476654 17 19 0.5933464765548706 17 20 0.6442946791648865 17 21 0.5757419466972351 17 22 0.6960783004760742 17 23 0.6466059684753418 18 0 0.6788512468338013 18 1 0.5584567785263062 18 2 0.6960599422454834 18 3 0.6443184018135071 18 4 0.6787434816360474 18 5 0.6443182229995728 18 6 0.6960911750793457 18 7 0.5927281379699707 18 8 0.6788966059684753 18 9 0.6097809672355652 18 10 0.6442120671272278 18 11 0.6788821816444397 18 12 0.6617140769958496 18 13 0.6788751482963562 18 14 0.7133956551551819 18 15 0.6270840167999268 18 16 0.6269832253456116 18 17 0.5924397110939026 18 18 0.6788857579231262 18 19 0.6096579432487488 18 20 0.6094979047775269 18 21 0.6266997456550598 18 22 0.6266236901283264 18 23 0.6464288830757141 19 0 0.5740959644317627 19 1 0.626436710357666 19 2 0.6793640851974487 19 3 0.62645024061203 19 4 0.6263408064842224 19 5 0.6440450549125671 19 6 0.6616758108139038 19 7 0.6440485119819641 19 8 0.5555465221405029 19 9 0.6617172956466675 19 10 0.6261833310127258 19 11 0.6082172989845276 19 12 0.6796674728393555 19 13 0.6974465250968933 19 14 0.6795427799224854 19 15 0.6972837448120117 19 16 0.661614179611206 19 17 0.644037663936615 19 18 0.6084772944450378 19 19 0.7331360578536987 19 20 0.5908582806587219 19 21 0.6439712047576904 19 22 0.6617007851600647 19 23 0.6710373759269714 20 0 0.6793779134750366 20 1 0.7149509191513062 20 2 0.6440614461898804 20 3 0.5733366012573242 20 4 0.6616223454475403 20 5 0.6974049806594849 20 6 0.5554745197296143 20 7 0.6261683106422424 20 8 0.6440428495407104 20 9 0.6439739465713501 20 10 0.6797004342079163 20 11 0.6795982718467712 20 12 0.6796493530273438 20 13 0.5728002786636353 20 14 0.7332776188850403 20 15 0.5906317234039307 20 16 0.6439292430877686 20 17 0.5544609427452087 20 18 0.6618521213531494 20 19 0.6796758770942688 20 20 0.5899679064750671 20 21 0.60768723487854 20 22 0.6798528432846069 20 23 0.7215574383735657 21 0 0.6077879667282104 21 1 0.6437510848045349 21 2 0.6257487535476685 21 3 0.6980717182159424 21 4 0.769891619682312 21 5 0.6438487768173218 21 6 0.6438585519790649 21 7 0.6439067125320435 21 8 0.6618906259536743 21 9 0.643928587436676 21 10 0.7155277729034424 21 11 0.6618247628211975 21 12 0.6083424687385559 21 13 0.6439008116722107 21 14 0.6616871356964111 21 15 0.6261052489280701 21 16 0.6080047488212585 21 17 0.6618409752845764 21 18 0.6439000964164734 21 19 0.6258761882781982 21 20 0.6077943444252014 21 21 0.5716148018836975 21 22 0.6073914766311646 21 23 0.6715044975280762 22 0 0.6438251733779907 22 1 0.6983233690261841 22 2 0.5892598628997803 22 3 0.5892065763473511 22 4 0.6620799899101257 22 5 0.6802330613136292 22 6 0.6619439125061035 22 7 0.6800962686538696 22 8 0.6802374720573425 22 9 0.6073141098022461 22 10 0.6801384091377258 22 11 0.6436730027198792 22 12 0.5162310004234314 22 13 0.6620140671730042 22 14 0.5885504484176636 22 15 0.6252371668815613 22 16 0.6436125040054321 22 17 0.6437293887138367 22 18 0.6251316070556641 22 19 0.6251013278961182 22 20 0.699214518070221 22 21 0.6807697415351868 22 22 0.6803693175315857 22 23 0.6972894072532654 23 0 0.6986659169197083 23 1 0.7169358730316162 23 2 0.7531160116195679 23 3 0.6981171369552612 23 4 0.6077329516410828 23 5 0.553068995475769 23 6 0.6072377562522888 23 7 0.6070693135261536 23 8 0.6070226430892944 23 9 0.7539509534835815 23 10 0.6071333885192871 23 11 0.6618294715881348 23 12 0.6435985565185547 23 13 0.6986878514289856 23 14 0.6803224682807922 23 15 0.6619590520858765 23 16 0.6073663234710693 23 17 0.6254253387451172 23 18 0.5889462232589722 23 19 0.6254507899284363 23 20 0.6252644062042236 23 21 0.5514920949935913 23 22 0.6436285376548767 23 23 0.6716964244842529 24 0 0.6623282432556152 24 1 0.6992008090019226 24 2 0.6990067958831787 24 3 0.662121057510376 24 4 0.6252532005310059 24 5 0.6988453269004822 24 6 0.6253753900527954 24 7 0.643642783164978 24 8 0.7356311082839966 24 9 0.698523759841919 24 10 0.588935136795044 24 11 0.6253370642662048 24 12 0.6069749593734741 24 13 0.662041962146759 24 14 0.5884769558906555 24 15 0.6250705718994141 24 16 0.6436711549758911 24 17 0.662153959274292 24 18 0.6249357461929321 24 19 0.699183464050293 24 20 0.6436450481414795 24 21 0.532624363899231 24 22 0.6248939037322998 24 23 0.5940379500389099 25 0 0.6436192989349365 25 1 0.5873870253562927 25 2 0.7187070846557617 25 3 0.6247256994247437 25 4 0.6061162352561951 25 5 0.7185966968536377 25 6 0.624691367149353 25 7 0.6060001850128174 25 8 0.6246823072433472 25 9 0.5870606899261475 25 10 0.6056756973266602 25 11 0.6813386082649231 25 12 0.6435341835021973 25 13 0.7002807855606079 25 14 0.5680513978004456 25 15 0.7002434730529785 25 16 0.7191183567047119 25 17 0.7374480962753296 25 18 0.5686672925949097 25 19 0.699745774269104 25 20 0.6434667706489563 25 21 0.6248607635498047 25 22 0.6623232960700989 25 23 0.5675272941589355 26 0 0.6434169411659241 26 1 0.6057072281837463 26 2 0.624376118183136 26 3 0.6245797276496887 26 4 0.6244075894355774 26 5 0.5294697880744934 26 6 0.6626608967781067 26 7 0.6625264286994934 26 8 0.6625062227249146 26 9 0.681541383266449 26 10 0.5672833323478699 26 11 0.6625096201896667 26 12 0.6624827980995178 26 13 0.6050584316253662 26 14 0.8349971175193787 26 15 0.719215452671051 26 16 0.567950427532196 26 17 0.6435233354568481 26 18 0.6623919606208801 26 19 0.6433983445167542 26 20 0.643479585647583 26 21 0.6625416278839111 26 22 0.6244349479675293 26 23 0.672391414642334 27 0 0.6245945692062378 27 1 0.6624173521995544 27 2 0.6434527039527893 27 3 0.6434618830680847 27 4 0.5863687992095947 27 5 0.6622607707977295 27 6 0.6815505623817444 27 7 0.6244121789932251 27 8 0.7193164825439453 27 9 0.6433542966842651 27 10 0.6435132026672363 27 11 0.6244502067565918 27 12 0.6434428691864014 27 13 0.6813478469848633 27 14 0.5866858959197998 27 15 0.6435239315032959 27 16 0.5674052834510803 27 17 0.6242780685424805 27 18 0.7199230790138245 27 19 0.7006818056106567 27 20 0.6433901786804199 27 21 0.6815592050552368 27 22 0.605502188205719 27 23 0.6194592714309692 28 0 0.6434324979782104 28 1 0.6053040027618408 28 2 0.7390142679214478 28 3 0.5862155556678772 28 4 0.6433273553848267 28 5 0.7388811707496643 28 6 0.5675077438354492 28 7 0.6243470311164856 28 8 0.6815800070762634 28 9 0.566983163356781 28 10 0.7201063632965088 28 11 0.6245001554489136 28 12 0.6434064507484436 28 13 0.7198615074157715 28 14 0.6625063419342041 28 15 0.7002873420715332 28 16 0.7192820310592651 28 17 0.5303250551223755 28 18 0.662501871585846 28 19 0.6434462070465088 28 20 0.6053900718688965 28 21 0.6241967082023621 28 22 0.6243990063667297 28 23 0.5926589369773865 29 0 0.5858487486839294 29 1 0.6624592542648315 29 2 0.7394248843193054 29 3 0.6625291109085083 29 4 0.6053105592727661 29 5 0.6435593366622925 29 6 0.6433475017547607 29 7 0.6816805005073547 29 8 0.6242318153381348 29 9 0.7009531855583191 29 10 0.643263041973114 29 11 0.6816092729568481 29 12 0.6625613570213318 29 13 0.5862560868263245 29 14 0.5477388501167297 29 15 0.7010923027992249 29 16 0.6433932781219482 29 17 0.6434466242790222 29 18 0.5472902059555054 29 19 0.6240485906600952 29 20 0.5468974709510803 29 21 0.7599672079086304 29 22 0.6047376394271851 29 23 0.7803211212158203 30 0 0.6818005442619324 30 1 0.6815543174743652 30 2 0.5476623177528381 30 3 0.6241247057914734 30 4 0.7204768657684326 30 5 0.7201346158981323 30 6 0.7007417678833008 30 7 0.5862920880317688 30 8 0.6434628963470459 30 9 0.5667922496795654 30 10 0.5856943130493164 30 11 0.6627310514450073 30 12 0.5855647325515747 30 13 0.6238826513290405 30 14 0.623948872089386 30 15 0.6239582300186157 30 16 0.6045222282409668 30 17 0.6432942748069763 30 18 0.7408514618873596 30 19 0.6627610921859741 30 20 0.6819806098937988 30 21 0.6819625496864319 30 22 0.7011937499046326 30 23 0.5655948519706726 31 0 0.6626311540603638 31 1 0.5855148434638977 31 2 0.6819380521774292 31 3 0.6240769624710083 31 4 0.7013075947761536 31 5 0.6625874638557434 31 6 0.7398068904876709 31 7 0.605073869228363 31 8 0.6241337656974792 31 9 0.6432441473007202 31 10 0.7011726498603821 31 11 0.7202576994895935 31 12 0.643356442451477 31 13 0.5859516859054565 31 14 0.7009406089782715 31 15 0.6625134348869324 31 16 0.6626006364822388 31 17 0.5860682725906372 31 18 0.5284472107887268 31 19 0.7012509703636169 31 20 0.6434870958328247 31 21 0.6048876047134399 31 22 0.5661695003509521 31 23 0.6457337737083435 32 0 0.7210468649864197 32 1 0.6627835035324097 32 2 0.604812502861023 32 3 0.6821236610412598 32 4 0.7011620998382568 32 5 0.6242126822471619 32 6 0.7012038230895996 32 7 0.6050071120262146 32 8 0.6623995304107666 32 9 0.6050068140029907 32 10 0.662589430809021 32 11 0.6239742636680603 32 12 0.6818875074386597 32 13 0.7010570168495178 32 14 0.5666103959083557 32 15 0.5662730932235718 32 16 0.6046814918518066 32 17 0.7209436893463135 32 18 0.6433735489845276 32 19 0.6044849157333374 32 20 0.68205726146698 32 21 0.6045637130737305 32 22 0.6238501667976379 32 23 0.6188703179359436 33 0 0.6238220930099487 33 1 0.623663067817688 33 2 0.6627169847488403 33 3 0.6044061183929443 33 4 0.6042500734329224 33 5 0.6431055068969727 33 6 0.6824242472648621 33 7 0.6041886806488037 33 8 0.6825766563415527 33 9 0.7410497665405273 33 10 0.5267040133476257 33 11 0.6629874110221863 33 12 0.6434097290039062 33 13 0.6432785987854004 33 14 0.6041312217712402 33 15 0.6433602571487427 33 16 0.5845111608505249 33 17 0.7219359874725342 33 18 0.6824836134910583 33 19 0.6040821671485901 33 20 0.702208399772644 33 21 0.7215750217437744 33 22 0.6044358611106873 33 23 0.6729347109794617 34 0 0.62381911277771 34 1 0.5847979187965393 34 2 0.6236233711242676 34 3 0.6040453910827637 34 4 0.6628611087799072 34 5 0.7217805981636047 34 6 0.6627245545387268 34 7 0.604332685470581 34 8 0.7411515712738037 34 9 0.6432942152023315 34 10 0.662671685218811 34 11 0.623855710029602 34 12 0.6239822506904602 34 13 0.643197238445282 34 14 0.6822646856307983 34 15 0.6824656128883362 34 16 0.6434045433998108 34 17 0.5851026177406311 34 18 0.6432411074638367 34 19 0.7409734129905701 34 20 0.585043728351593 34 21 0.6434481739997864 34 22 0.6626616716384888 34 23 0.5647035837173462 35 0 0.6236823201179504 35 1 0.7800705432891846 35 2 0.6432623863220215 35 3 0.6239332556724548 35 4 0.5849385857582092 35 5 0.721335768699646 35 6 0.6239705085754395 35 7 0.6821501851081848 35 8 0.6628753542900085 35 9 0.6822757124900818 35 10 0.6239802837371826 35 11 0.5851340889930725 35 12 0.6044121980667114 35 13 0.5262690782546997 35 14 0.5843429565429688 35 15 0.6038345694541931 35 16 0.6038172841072083 35 17 0.7026576399803162 35 18 0.7815307378768921 35 19 0.6629156470298767 35 20 0.6629080772399902 35 21 0.6041462421417236 35 22 0.6824670433998108 35 23 0.6185359358787537 36 0 0.604238748550415 36 1 0.5645692348480225 36 2 0.5840722322463989 36 3 0.6629714965820312 36 4 0.6235837340354919 36 5 0.6630687713623047 36 6 0.6036087274551392 36 7 0.5639231204986572 36 8 0.6633909344673157 36 9 0.6235551834106445 36 10 0.7028086185455322 36 11 0.623353898525238 36 12 0.6829712986946106 36 13 0.6434535384178162 36 14 0.7623586654663086 36 15 0.6433229446411133 36 16 0.6236631870269775 36 17 0.6432740092277527 36 18 0.7616209983825684 36 19 0.5843974947929382 36 20 0.6629169583320618 36 21 0.6433728933334351 36 22 0.7217936515808105 36 23 0.6184332370758057 37 0 0.6824032068252563 37 1 0.6824020147323608 37 2 0.6237924695014954 37 3 0.6237685084342957 37 4 0.7215815782546997 37 5 0.6823676824569702 37 6 0.5849452018737793 37 7 0.5261578559875488 37 8 0.7021639347076416 37 9 0.7805138230323792 37 10 0.662914514541626 37 11 0.6238207221031189 37 12 0.7015342712402344 37 13 0.5658211708068848 37 14 0.6629443764686584 37 15 0.6628196239471436 37 16 0.6237396001815796 37 17 0.6238019466400146 37 18 0.6238201260566711 37 19 0.6042914986610413 37 20 0.6824421882629395 37 21 0.6040985584259033 37 22 0.6236847043037415 37 23 0.5914288759231567 38 0 0.6629559397697449 38 1 0.6433844566345215 38 2 0.603979229927063 38 3 0.7024663686752319 38 4 0.60411137342453 38 5 0.6628589630126953 38 6 0.7022923827171326 38 7 0.6041023135185242 38 8 0.6236183643341064 38 9 0.7023335695266724 38 10 0.6823348999023438 38 11 0.6629058718681335 38 12 0.623599648475647 38 13 0.60407555103302 38 14 0.6433300971984863 38 15 0.6824789047241211 38 16 0.5453081727027893 38 17 0.6038774251937866 38 18 0.663110077381134 38 19 0.6630046963691711 38 20 0.6235629916191101 38 21 0.68281090259552 38 22 0.6235635876655579 38 23 0.6732412576675415 39 0 0.6627931594848633 39 1 0.5843873023986816 39 2 0.6827585697174072 39 3 0.5645370483398438 39 4 0.6235134601593018 39 5 0.6234009265899658 39 6 0.5639718770980835 39 7 0.7029914855957031 39 8 0.663129448890686 39 9 0.6234713792800903 39 10 0.6433194875717163 39 11 0.6829876899719238 39 12 0.722622275352478 39 13 0.6236127614974976 39 14 0.6234003305435181 39 15 0.6432444453239441 39 16 0.6828628182411194 39 17 0.6235028505325317 39 18 0.5839515924453735 39 19 0.5837802886962891 39 20 0.7226803302764893 39 21 0.6631343364715576 39 22 0.7422187328338623 39 23 0.6458119750022888 40 0 0.6629328727722168 40 1 0.6432738900184631 40 2 0.6236621141433716 40 3 0.6234069466590881 40 4 0.6630692481994629 40 5 0.6039854288101196 40 6 0.6037296056747437 40 7 0.6433466672897339 40 8 0.6433624029159546 40 9 0.7223809957504272 40 10 0.7614911794662476 40 11 0.6041245460510254 40 12 0.6433984041213989 40 13 0.5647538900375366 40 14 0.5840160846710205 40 15 0.623522937297821 40 16 0.6629215478897095 40 17 0.6432861685752869 40 18 0.7026154398918152 40 19 0.6037902235984802 40 20 0.702769935131073 40 21 0.5841466784477234 40 22 0.6828117966651917 40 23 0.7005552053451538 41 0 0.7024226784706116 41 1 0.564690887928009 41 2 0.5839846730232239 41 3 0.7025492191314697 41 4 0.6826156377792358 41 5 0.6236241459846497 41 6 0.6236433982849121 41 7 0.6629761457443237 41 8 0.6234136819839478 41 9 0.7026351094245911 41 10 0.7023755311965942 41 11 0.6825000047683716 41 12 0.6825145483016968 41 13 0.6042466163635254 41 14 0.6430487036705017 41 15 0.6041827201843262 41 16 0.6628902554512024 41 17 0.54522705078125 41 18 0.702383816242218 41 19 0.7023317813873291 41 20 0.5258878469467163 41 21 0.6629899740219116 41 22 0.6630158424377441 41 23 0.6184441447257996 42 0 0.5445070266723633 42 1 0.6631927490234375 42 2 0.5839746594429016 42 3 0.7029545307159424 42 4 0.6036249399185181 42 5 0.6632580757141113 42 6 0.6829235553741455 42 7 0.6628705859184265 42 8 0.6827482581138611 42 9 0.5840606689453125 42 10 0.5640536546707153 42 11 0.6233620643615723 42 12 0.6233718395233154 42 13 0.6830699443817139 42 14 0.6632161736488342 42 15 0.7028592228889465 42 16 0.6632600426673889 42 17 0.6629627346992493 42 18 0.6432162523269653 42 19 0.6234870553016663 42 20 0.6828569173812866 42 21 0.7024537324905396 42 22 0.6236098408699036 42 23 0.6457833051681519 43 0 0.6038792133331299 43 1 0.662982165813446 43 2 0.6629082560539246 43 3 0.6036665439605713 43 4 0.6037797927856445 43 5 0.623568594455719 43 6 0.603446364402771 43 7 0.6432144641876221 43 8 0.7824839353561401 43 9 0.6038640737533569 43 10 0.6630749106407166 43 11 0.6432529091835022 43 12 0.6827659010887146 43 13 0.6432063579559326 43 14 0.6630494594573975 43 15 0.64329594373703 43 16 0.6235355734825134 43 17 0.6234860420227051 43 18 0.682815432548523 43 19 0.6236878633499146 43 20 0.6037987470626831 43 21 0.682945966720581 43 22 0.6827611327171326 43 23 0.6182771921157837 44 0 0.6038175821304321 44 1 0.742311954498291 44 2 0.7022193074226379 44 3 0.6432631015777588 44 4 0.7022355794906616 44 5 0.6040564179420471 44 6 0.5845098495483398 44 7 0.6039170622825623 44 8 0.6037532091140747 44 9 0.7026485800743103 44 10 0.6433156728744507 44 11 0.5250167846679688 44 12 0.6829155087471008 44 13 0.5837919116020203 44 14 0.6433960795402527 44 15 0.6035492420196533 44 16 0.6630601286888123 44 17 0.6630274057388306 44 18 0.603563129901886 44 19 0.6233254671096802 44 20 0.6632335782051086 44 21 0.6831690073013306 44 22 0.7625007033348083 44 23 0.6456429958343506 45 0 0.6234079003334045 45 1 0.6037489175796509 45 2 0.6433572769165039 45 3 0.8017926216125488 45 4 0.6629448533058167 45 5 0.6039292812347412 45 6 0.6632041931152344 45 7 0.7022113800048828 45 8 0.7218204736709595 45 9 0.6628320217132568 45 10 0.5847775936126709 45 11 0.623586893081665 45 12 0.7020450234413147 45 13 0.7409197688102722 45 14 0.604387104511261 45 15 0.7211979627609253 45 16 0.6433815360069275 45 17 0.6045181751251221 45 18 0.5849881768226624 45 19 0.5458157062530518 45 20 0.5450599789619446 45 21 0.564269483089447 45 22 0.6629728674888611 45 23 0.6735316514968872 46 0 0.5840503573417664 46 1 0.6233874559402466 46 2 0.7425189018249512 46 3 0.6432734727859497 46 4 0.6432859301567078 46 5 0.6433455944061279 46 6 0.6431961059570312 46 7 0.6630741357803345 46 8 0.6828051805496216 46 9 0.6234298348426819 46 10 0.6432244777679443 46 11 0.6825813055038452 46 12 0.5842838287353516 46 13 0.6235930323600769 46 14 0.5837635397911072 46 15 0.7426411509513855 46 16 0.6629602909088135 46 17 0.662871241569519 46 18 0.6235758066177368 46 19 0.7223563194274902 46 20 0.5449956059455872 46 21 0.7025839686393738 46 22 0.5645266175270081 46 23 0.6456915140151978 47 0 0.7223800420761108 47 1 0.6432640552520752 47 2 0.5841615796089172 47 3 0.5641729831695557 47 4 0.7227003574371338 47 5 0.6234514713287354 47 6 0.6234579086303711 47 7 0.7027156949043274 47 8 0.663018524646759 47 9 0.7024786472320557 47 10 0.722038984298706 47 11 0.6823997497558594 47 12 0.64337158203125 47 13 0.5846224427223206 47 14 0.5843645930290222 47 15 0.6235345602035522 47 16 0.7220204472541809 47 17 0.5450991988182068 47 18 0.6037095189094543 47 19 0.6632192730903625 47 20 0.6037029027938843 47 21 0.643264651298523 47 22 0.6631328463554382 47 23 0.6458355188369751 48 0 0.603600025177002 48 1 0.6632399559020996 48 2 0.7423489093780518 48 3 0.6234883666038513 48 4 0.603803277015686 48 5 0.6234838962554932 48 6 0.6828458905220032 48 7 0.6431981921195984 48 8 0.7421037554740906 48 9 0.6432614922523499 48 10 0.6235602498054504 48 11 0.6628668308258057 48 12 0.6628544330596924 48 13 0.6826381683349609 48 14 0.6039702892303467 48 15 0.6630273461341858 48 16 0.643280029296875 48 17 0.6234404444694519 48 18 0.5647169351577759 48 19 0.6630053520202637 48 20 0.6237260103225708 48 21 0.6432720422744751 48 22 0.6432974934577942 48 23 0.5908810496330261 49 0 0.7028915882110596 49 1 0.6037231087684631 49 2 0.6631491780281067 49 3 0.6233522295951843 49 4 0.623444676399231 49 5 0.6034817695617676 49 6 0.7228856086730957 49 7 0.6431588530540466 49 8 0.583836555480957 49 9 0.7225829362869263 49 10 0.7024571895599365 49 11 0.6629567742347717 49 12 0.6431175470352173 49 13 0.6039554476737976 49 14 0.6236836314201355 49 15 0.603657066822052 49 16 0.6631515622138977 49 17 0.6827135682106018 49 18 0.6827677488327026 49 19 0.6433534622192383 49 20 0.5842302441596985 49 21 0.6035856008529663 49 22 0.6035847067832947 49 23 0.7013729214668274 50 0 0.6628956198692322 50 1 0.6236217021942139 50 2 0.6235371828079224 50 3 0.623407244682312 50 4 0.6631922721862793 50 5 0.6233624219894409 50 6 0.6629335880279541 50 7 0.5837404727935791 50 8 0.7030171155929565 50 9 0.5438894033432007 50 10 0.7030901908874512 50 11 0.7228847742080688 50 12 0.6433830261230469 50 13 0.6630256175994873 50 14 0.6235201358795166 50 15 0.6829851269721985 50 16 0.6035059094429016 50 17 0.6829487681388855 50 18 0.5839247703552246 50 19 0.6432996392250061 50 20 0.7226059436798096 50 21 0.583885133266449 50 22 0.5837327241897583 50 23 0.7566474676132202 51 0 0.5839105248451233 51 1 0.603644847869873 51 2 0.6830171346664429 51 3 0.6432315707206726 51 4 0.7028694152832031 51 5 0.6434168815612793 51 6 0.5443171858787537 51 7 0.7427366375923157 51 8 0.5838898420333862 51 9 0.6432868838310242 51 10 0.5637764930725098 51 11 0.7630679607391357 51 12 0.6433491706848145 51 13 0.6630942225456238 51 14 0.6430766582489014 51 15 0.6034848093986511 51 16 0.7425395250320435 51 17 0.6235433220863342 51 18 0.5640198588371277 51 19 0.7624847888946533 51 20 0.7616430521011353 51 21 0.6432147026062012 51 22 0.5647371411323547 51 23 0.536094605922699 52 0 0.6234849691390991 52 1 0.6432667970657349 52 2 0.6036362648010254 52 3 0.6829256415367126 52 4 0.7624101638793945 52 5 0.6629206538200378 52 6 0.6629581451416016 52 7 0.5841906666755676 52 8 0.662952184677124 52 9 0.8013370633125305 52 10 0.5847458839416504 52 11 0.6627085208892822 52 12 0.6628919839859009 52 13 0.5842568278312683 52 14 0.604004442691803 52 15 0.6038715839385986 52 16 0.6828616261482239 52 17 0.6630246639251709 52 18 0.6234992146492004 52 19 0.6236399412155151 52 20 0.6431179046630859 52 21 0.5840946435928345 52 22 0.6432642936706543 52 23 0.6181138157844543 53 0 0.7226831912994385 53 1 0.6630285978317261 53 2 0.742143988609314 53 3 0.6040205359458923 53 4 0.5645473003387451 53 5 0.5641201734542847 53 6 0.7225737571716309 53 7 0.6828497052192688 53 8 0.6432837247848511 53 9 0.6235381364822388 53 10 0.6628789901733398 53 11 0.6037575006484985 53 12 0.7224841117858887 53 13 0.6628332138061523 53 14 0.6434051990509033 53 15 0.6431014537811279 53 16 0.5840900540351868 53 17 0.6236363649368286 53 18 0.7025203704833984 53 19 0.5841077566146851 53 20 0.6631513833999634 53 21 0.5641360282897949 53 22 0.6432528495788574 53 23 0.6458141803741455 54 0 0.524101197719574 54 1 0.6232233643531799 54 2 0.6831973195075989 54 3 0.643127977848053 54 4 0.6034228801727295 54 5 0.7033785581588745 54 6 0.7032428979873657 54 7 0.603481650352478 54 8 0.5834459066390991 54 9 0.6031633019447327 54 10 0.663362979888916 54 11 0.5630963444709778 54 12 0.6630938649177551 54 13 0.7235506176948547 54 14 0.7431628704071045 54 15 0.6631362438201904 54 16 0.663139820098877 54 17 0.6233999729156494 54 18 0.6630887985229492 54 19 0.6234291791915894 54 20 0.7028461694717407 54 21 0.6035835146903992 54 22 0.6632622480392456 54 23 0.6459469795227051 55 0 0.6432011127471924 55 1 0.6629540324211121 55 2 0.7820460200309753 55 3 0.7024217247962952 55 4 0.5846128463745117 55 5 0.6234632134437561 55 6 0.7420871257781982 55 7 0.5844329595565796 55 8 0.603844940662384 55 9 0.6826189160346985 55 10 0.6826687455177307 55 11 0.6431605815887451 55 12 0.6433168649673462 55 13 0.5647084712982178 55 14 0.6628147959709167 55 15 0.6827439069747925 55 16 0.643346905708313 55 17 0.5645118951797485 55 18 0.6234960556030273 55 19 0.5838934183120728 55 20 0.5837457776069641 55 21 0.6631737947463989 55 22 0.6630121469497681 55 23 0.6734899282455444 56 0 0.66312575340271 56 1 0.6431030035018921 56 2 0.6631560921669006 56 3 0.7026644945144653 56 4 0.6629712581634521 56 5 0.7221680879592896 56 6 0.6040079593658447 56 7 0.6039355993270874 56 8 0.6038258671760559 56 9 0.5640623569488525 56 10 0.6829618215560913 56 11 0.5440226793289185 56 12 0.6432586908340454 56 13 0.6630358099937439 56 14 0.6432833671569824 56 15 0.6831640005111694 56 16 0.6629421710968018 56 17 0.6432203650474548 56 18 0.6831434965133667 56 19 0.6432554125785828 56 20 0.6433737874031067 56 21 0.6234971284866333 56 22 0.6828956604003906 56 23 0.5906712412834167 57 0 0.6631191968917847 57 1 0.6830214262008667 57 2 0.7027126550674438 57 3 0.6630582809448242 57 4 0.7616952061653137 57 5 0.6040256023406982 57 6 0.6235883235931396 57 7 0.545001745223999 57 8 0.6036484837532043 57 9 0.6232755184173584 57 10 0.6232672333717346 57 11 0.6035640835762024 57 12 0.6033252477645874 57 13 0.643357515335083 57 14 0.7031652927398682 57 15 0.6632302403450012 57 16 0.6033183336257935 57 17 0.6233112215995789 57 18 0.6231698989868164 57 19 0.663180947303772 57 20 0.6032769680023193 57 21 0.6232531070709229 57 22 0.7633837461471558 57 23 0.6736336946487427 58 0 0.5836327075958252 58 1 0.5635756850242615 58 2 0.6230880618095398 58 3 0.6434150338172913 58 4 0.6631242036819458 58 5 0.5831151008605957 58 6 0.6230869889259338 58 7 0.6030116677284241 58 8 0.663282573223114 58 9 0.5627651214599609 58 10 0.5825842022895813 58 11 0.7042316794395447 58 12 0.7040072679519653 58 13 0.6633471250534058 58 14 0.7237708568572998 58 15 0.7233794331550598 58 16 0.7031333446502686 58 17 0.7029899954795837 58 18 0.5044436454772949 58 19 0.723051130771637 58 20 0.7428065538406372 58 21 0.5837646722793579 58 22 0.6631922721862793 58 23 0.6456779837608337 59 0 0.6036128997802734 59 1 0.7226972579956055 59 2 0.7421689033508301 59 3 0.7023854851722717 59 4 0.6236145496368408 59 5 0.5449017286300659 59 6 0.6431972980499268 59 7 0.6235135793685913 59 8 0.6234968304634094 59 9 0.6234095692634583 59 10 0.5440316200256348 59 11 0.6032662391662598 59 12 0.6631825566291809 59 13 0.6033575534820557 59 14 0.6633778214454651 59 15 0.5432209968566895 59 16 0.7036616206169128 59 17 0.7235292792320251 59 18 0.6431581974029541 59 19 0.6231856942176819 59 20 0.683208167552948 59 21 0.6631423234939575 59 22 0.7029953598976135 59 23 0.6734946966171265 60 0 0.6432065367698669 60 1 0.6828610301017761 60 2 0.643276572227478 60 3 0.6034777760505676 60 4 0.5837129950523376 60 5 0.6433153748512268 60 6 0.7426737546920776 60 7 0.6433272361755371 60 8 0.6035817265510559 60 9 0.6633191704750061 60 10 0.6828828454017639 60 11 0.5837816596031189 60 12 0.6631573438644409 60 13 0.5836969614028931 60 14 0.5833663940429688 60 15 0.7032623887062073 60 16 0.6432946920394897 60 17 0.6630216836929321 60 18 0.7229100465774536 60 19 0.5640596151351929 60 20 0.7031015753746033 60 21 0.6234509944915771 60 22 0.7424386739730835 60 23 0.535781741142273 61 0 0.6033778190612793 61 1 0.663107693195343 61 2 0.6034772396087646 61 3 0.7030739784240723 61 4 0.6035400032997131 61 5 0.6830830574035645 61 6 0.6631916165351868 61 7 0.7225637435913086 61 8 0.6035879254341125 61 9 0.5839260816574097 61 10 0.5637876987457275 61 11 0.6632200479507446 61 12 0.6832292675971985 61 13 0.6830736398696899 61 14 0.7625267505645752 61 15 0.7617822289466858 61 16 0.7019211649894714 61 17 0.5844319462776184 61 18 0.6236022710800171 61 19 0.58430415391922 61 20 0.5446810722351074 61 21 0.6431654095649719 61 22 0.5839239358901978 61 23 0.6735535264015198 62 0 0.6431429386138916 62 1 0.6235058307647705 62 2 0.643118679523468 62 3 0.5836930871009827 62 4 0.7430123090744019 62 5 0.6432592272758484 62 6 0.6431148052215576 62 7 0.5836441516876221 62 8 0.7427902221679688 62 9 0.7621099948883057 62 10 0.6236065626144409 62 11 0.6827437281608582 62 12 0.5645157098770142 62 13 0.6628991961479187 62 14 0.6433733701705933 62 15 0.6828711032867432 62 16 0.6039234399795532 62 17 0.5840624570846558 62 18 0.5839235782623291 62 19 0.6432211399078369 62 20 0.6232853531837463 62 21 0.6430909633636475 62 22 0.6431979537010193 62 23 0.7011814117431641 63 0 0.6235360503196716 63 1 0.6034393310546875 63 2 0.5634711384773254 63 3 0.6233056783676147 63 4 0.6632004380226135 63 5 0.6432527303695679 63 6 0.6432280540466309 63 7 0.623287558555603 63 8 0.6633803248405457 63 9 0.7833251357078552 63 10 0.6631753444671631 63 11 0.6829056739807129 63 12 0.5243989825248718 63 13 0.603297233581543 63 14 0.6231200695037842 63 15 0.6632593274116516 63 16 0.7030695676803589 63 17 0.7230323553085327 63 18 0.6432489156723022 63 19 0.6432315707206726 63 20 0.6432011127471924 63 21 0.6233223080635071 63 22 0.6432744860649109 63 23 0.673363447189331 64 0 0.643277108669281 64 1 0.524306058883667 64 2 0.6831489205360413 64 3 0.6433335542678833 64 4 0.643086314201355 64 5 0.663165271282196 64 6 0.6232755184173584 64 7 0.6232271194458008 64 8 0.6830365657806396 64 9 0.6033191084861755 64 10 0.683286190032959 64 11 0.6432574391365051 64 12 0.6631713509559631 64 13 0.6432409286499023 64 14 0.6831284761428833 64 15 0.6233617067337036 64 16 0.6233327984809875 64 17 0.683070719242096 64 18 0.6232178211212158 64 19 0.6232297420501709 64 20 0.6632572412490845 64 21 0.6431477069854736 64 22 0.6831350922584534 64 23 0.6735888719558716 65 0 0.5837918519973755 65 1 0.5836642384529114 65 2 0.5634294152259827 65 3 0.6633819937705994 65 4 0.6633450388908386 65 5 0.6633262634277344 65 6 0.6630735397338867 65 7 0.6632434725761414 65 8 0.6432862281799316 65 9 0.7428037524223328 65 10 0.6630735993385315 65 11 0.6431788206100464 65 12 0.6433249711990356 65 13 0.722509503364563 65 14 0.6627717018127441 65 15 0.662948727607727 65 16 0.5840760469436646 65 17 0.6629975438117981 65 18 0.5642224550247192 65 19 0.682978093624115 65 20 0.6434460282325745 65 21 0.6036326885223389 65 22 0.6629475355148315 65 23 0.6457524299621582 66 0 0.7226648926734924 66 1 0.5642846822738647 66 2 0.6433137655258179 66 3 0.6830548048019409 66 4 0.6827272772789001 66 5 0.5643483400344849 66 6 0.6631187796592712 66 7 0.6234199404716492 66 8 0.6828720569610596 66 9 0.6036099195480347 66 10 0.6629703640937805 66 11 0.7028486132621765 66 12 0.7222151160240173 66 13 0.6235794425010681 66 14 0.5840598344802856 66 15 0.6627021431922913 66 16 0.6235368251800537 66 17 0.6826878786087036 66 18 0.5248530507087708 66 19 0.6433115005493164 66 20 0.6630886793136597 66 21 0.6431634426116943 66 22 0.6035791039466858 66 23 0.729057252407074 67 0 0.6034385561943054 67 1 0.6035373210906982 67 2 0.7228655219078064 67 3 0.603611409664154 67 4 0.5837191939353943 67 5 0.6630956530570984 67 6 0.6631032228469849 67 7 0.6431063413619995 67 8 0.7425059676170349 67 9 0.6828711032867432 67 10 0.6433272957801819 67 11 0.6039701104164124 67 12 0.6235042810440063 67 13 0.6234362125396729 67 14 0.6828193664550781 67 15 0.6432791948318481 67 16 0.6431834697723389 67 17 0.5838320851325989 67 18 0.6830489039421082 67 19 0.6234076023101807 67 20 0.7623344659805298 67 21 0.5643734931945801 67 22 0.5642349720001221 67 23 0.7563067078590393 68 0 0.7619302868843079 68 1 0.6628766655921936 68 2 0.6235182881355286 68 3 0.6040332317352295 68 4 0.6236050128936768 68 5 0.682536780834198 68 6 0.5841962099075317 68 7 0.6433555483818054 68 8 0.7025271654129028 68 9 0.5447565317153931 68 10 0.663061261177063 68 11 0.662987470626831 68 12 0.7223237752914429 68 13 0.6432498097419739 68 14 0.6040196418762207 68 15 0.643117368221283 68 16 0.6039535403251648 68 17 0.6433236002922058 68 18 0.6234618425369263 68 19 0.5838199853897095 68 20 0.6232830882072449 68 21 0.6432396769523621 68 22 0.6829591989517212 68 23 0.7285715341567993 69 0 0.6234645843505859 69 1 0.7222966551780701 69 2 0.6827418208122253 69 3 0.7219171524047852 69 4 0.62359619140625 69 5 0.6237211227416992 69 6 0.6630421280860901 69 7 0.643430769443512 69 8 0.6432566046714783 69 9 0.6628310680389404 69 10 0.7608534693717957 69 11 0.5849193334579468 69 12 0.5847427248954773 69 13 0.6431583762168884 69 14 0.6825221180915833 69 15 0.6040759086608887 69 16 0.6236021518707275 69 17 0.5841747522354126 69 18 0.6629356145858765 69 19 0.6038174629211426 69 20 0.6433393955230713 69 21 0.623450756072998 69 22 0.6036533117294312 69 23 0.6734199523925781 70 0 0.7222650647163391 70 1 0.6236134767532349 70 2 0.6234713792800903 70 3 0.6037793159484863 70 4 0.6433662176132202 70 5 0.6630850434303284 70 6 0.6828614473342896 70 7 0.5841386914253235 70 8 0.6234976649284363 70 9 0.6431415677070618 70 10 0.6432263255119324 70 11 0.6630042791366577 70 12 0.6828798651695251 70 13 0.6234449744224548 70 14 0.72243732213974 70 15 0.6825740337371826 70 16 0.5841162800788879 70 17 0.8011270761489868 70 18 0.5452854633331299 70 19 0.6038564443588257 70 20 0.6037419438362122 70 21 0.682815670967102 70 22 0.6039631366729736 70 23 0.6183807253837585 71 0 0.5838474035263062 71 1 0.6233246326446533 71 2 0.5836734771728516 71 3 0.6233741641044617 71 4 0.723158597946167 71 5 0.6431928873062134 71 6 0.6033473014831543 71 7 0.683085560798645 71 8 0.7425945997238159 71 9 0.6432930827140808 71 10 0.6233901977539062 71 11 0.7224743962287903 71 12 0.6631799936294556 71 13 0.5841227769851685 71 14 0.5839528441429138 71 15 0.6629424691200256 71 16 0.5640010833740234 71 17 0.7429091930389404 71 18 0.6630028486251831 71 19 0.7224181890487671 71 20 0.5840321779251099 71 21 0.6431604623794556 71 22 0.5838432908058167 71 23 0.7010686993598938 72 0 0.6037932634353638 72 1 0.6434704065322876 72 2 0.6829454898834229 72 3 0.6828451156616211 72 4 0.6632125973701477 72 5 0.68254554271698 72 6 0.6038260459899902 72 7 0.6432551145553589 72 8 0.663053035736084 72 9 0.6039338707923889 72 10 0.6629505753517151 72 11 0.6629495620727539 72 12 0.6432313919067383 72 13 0.6431972980499268 72 14 0.623404324054718 72 15 0.7025189399719238 72 16 0.643202543258667 72 17 0.623586118221283 72 18 0.662992537021637 72 19 0.5643013715744019 72 20 0.6036750078201294 72 21 0.722249448299408 72 22 0.6630890369415283 72 23 0.5633416175842285 73 0 0.702672004699707 73 1 0.6631112098693848 73 2 0.603968620300293 73 3 0.5640817880630493 73 4 0.5637649297714233 73 5 0.6431477069854736 73 6 0.6631682515144348 73 7 0.6630265116691589 73 8 0.6034607887268066 73 9 0.6231081485748291 73 10 0.7031813859939575 73 11 0.7031642198562622 73 12 0.6035457253456116 73 13 0.6631762385368347 73 14 0.7228372097015381 73 15 0.5838236212730408 73 16 0.6035703420639038 73 17 0.7028608322143555 73 18 0.642998456954956 73 19 0.7624384760856628 73 20 0.6037608981132507 73 21 0.682637631893158 73 22 0.5841150283813477 73 23 0.6182209253311157 74 0 0.5638894438743591 74 1 0.6431854367256165 74 2 0.6034346222877502 74 3 0.6034085750579834 74 4 0.6830655932426453 74 5 0.5833717584609985 74 6 0.6832237243652344 74 7 0.6432228684425354 74 8 0.6630614995956421 74 9 0.6232909560203552 74 10 0.6831433176994324 74 11 0.6431124806404114 74 12 0.6631763577461243 74 13 0.6232736110687256 74 14 0.6632068157196045 74 15 0.6431025862693787 74 16 0.6234651803970337 74 17 0.7830127477645874 74 18 0.6433688402175903 74 19 0.6629611849784851 74 20 0.6828792095184326 74 21 0.6629042625427246 74 22 0.6236335635185242 74 23 0.5634409189224243 75 0 0.6433874368667603 75 1 0.6433877348899841 75 2 0.6434392929077148 75 3 0.5837483406066895 75 4 0.5834722518920898 75 5 0.6631580591201782 75 6 0.6632828116416931 75 7 0.6830031871795654 75 8 0.6233047246932983 75 9 0.6629642248153687 75 10 0.6033372282981873 75 11 0.6230934262275696 75 12 0.6233599185943604 75 13 0.623300313949585 75 14 0.6831018924713135 75 15 0.7231006622314453 75 16 0.6431432366371155 75 17 0.7027316093444824 75 18 0.6830097436904907 75 19 0.5640180110931396 75 20 0.6035008430480957 75 21 0.6432841420173645 75 22 0.6630551218986511 75 23 0.7290191650390625 76 0 0.6034866571426392 76 1 0.603429913520813 76 2 0.6827861070632935 76 3 0.6035939455032349 76 4 0.842045247554779 76 5 0.6628995537757874 76 6 0.682686448097229 76 7 0.6236281991004944 76 8 0.682410717010498 76 9 0.741494357585907 76 10 0.6043787598609924 76 11 0.6041246652603149 76 12 0.564759373664856 76 13 0.6039804816246033 76 14 0.6037176847457886 76 15 0.722472608089447 76 16 0.5448043942451477 76 17 0.7224225997924805 76 18 0.6235619783401489 76 19 0.6235411763191223 76 20 0.682810366153717 76 21 0.623565673828125 76 22 0.6630381345748901 76 23 0.5359984040260315 77 0 0.6432672739028931 77 1 0.7624792456626892 77 2 0.6235843896865845 77 3 0.643325924873352 77 4 0.6234953999519348 77 5 0.5838009119033813 77 6 0.7225797176361084 77 7 0.5048022270202637 77 8 0.5834947824478149 77 9 0.6430667638778687 77 10 0.6031363606452942 77 11 0.6432135701179504 77 12 0.6433106660842896 77 13 0.6631245613098145 77 14 0.6031386256217957 77 15 0.6633992195129395 77 16 0.643173336982727 77 17 0.6031239032745361 77 18 0.6832813620567322 77 19 0.7231779098510742 77 20 0.7630937099456787 77 21 0.6036167144775391 77 22 0.6631555557250977 77 23 0.6460047364234924 78 0 0.6034089922904968 78 1 0.583516001701355 78 2 0.6032861471176147 78 3 0.6230288743972778 78 4 0.7233707904815674 78 5 0.6432961225509644 78 6 0.6432769298553467 78 7 0.7230205535888672 78 8 0.6828004121780396 78 9 0.6231935024261475 78 10 0.6832310557365417 78 11 0.6827220320701599 78 12 0.7025054693222046 78 13 0.6432960033416748 78 14 0.5052862167358398 78 15 0.7027602195739746 78 16 0.623515784740448 78 17 0.623585045337677 78 18 0.6034378409385681 78 19 0.6631735563278198 78 20 0.6828720569610596 78 21 0.6433746218681335 78 22 0.6432052254676819 78 23 0.6183315515518188 79 0 0.6034486889839172 79 1 0.6431885361671448 79 2 0.6232753992080688 79 3 0.5834664702415466 79 4 0.583122968673706 79 5 0.6031093001365662 79 6 0.6835697889328003 79 7 0.5629386305809021 79 8 0.5626891851425171 79 9 0.5824546217918396 79 10 0.7447782754898071 79 11 0.6635122299194336 79 12 0.643231987953186 79 13 0.7439970374107361 79 14 0.6231157183647156 79 15 0.7637169361114502 79 16 0.6829814910888672 79 17 0.6033996343612671 79 18 0.6633058786392212 79 19 0.6430866122245789 79 20 0.6631820797920227 79 21 0.6033773422241211 79 22 0.7231005430221558 79 23 0.7011788487434387 80 0 0.6035243272781372 80 1 0.5637950301170349 80 2 0.5434660911560059 80 3 0.7234894037246704 80 4 0.6230505704879761 80 5 0.663182258605957 80 6 0.5433577299118042 80 7 0.6632931232452393 80 8 0.6430167555809021 80 9 0.7838143706321716 80 10 0.7029567956924438 80 11 0.5439912676811218 80 12 0.7831070423126221 80 13 0.6830199956893921 80 14 0.7420362234115601 80 15 0.5645585060119629 80 16 0.6630398035049438 80 17 0.6234760284423828 80 18 0.6431173086166382 80 19 0.6432269215583801 80 20 0.6432912945747375 80 21 0.5839410424232483 80 22 0.6035130620002747 80 23 0.7288191318511963 81 0 0.6233295202255249 81 1 0.7423219084739685 81 2 0.6827482581138611 81 3 0.5841083526611328 81 4 0.6630638837814331 81 5 0.6630489230155945 81 6 0.6432594060897827 81 7 0.6235715746879578 81 8 0.6631726026535034 81 9 0.6827347278594971 81 10 0.6039936542510986 81 11 0.7221723794937134 81 12 0.6431074738502502 81 13 0.6825006604194641 81 14 0.6040576100349426 81 15 0.5646832585334778 81 16 0.6432499885559082 81 17 0.643247663974762 81 18 0.7024358510971069 81 19 0.5250579714775085 81 20 0.682847797870636 81 21 0.5838149189949036 81 22 0.6431739330291748 81 23 0.6732702851295471 82 0 0.6430871486663818 82 1 0.6035764217376709 82 2 0.7427552938461304 82 3 0.6036638021469116 82 4 0.5640015006065369 82 5 0.5636287331581116 82 6 0.7631018161773682 82 7 0.8216053247451782 82 8 0.6826990246772766 82 9 0.5059664249420166 82 10 0.5837985873222351 82 11 0.6034846305847168 82 12 0.6631513833999634 82 13 0.6432781219482422 82 14 0.6234162449836731 82 15 0.6433896422386169 82 16 0.722594141960144 82 17 0.6827033758163452 82 18 0.6432824730873108 82 19 0.6829724907875061 82 20 0.6827859282493591 82 21 0.6628335118293762 82 22 0.6039338707923889 82 23 0.5085989832878113 83 0 0.5043320655822754 83 1 0.5832347869873047 83 2 0.7234818935394287 83 3 0.5631390810012817 83 4 0.6030771732330322 83 5 0.7637573480606079 83 6 0.5832909345626831 83 7 0.5831564664840698 83 8 0.6633123159408569 83 9 0.7034738063812256 83 10 0.6432994604110718 83 11 0.7233599424362183 83 12 0.6831416487693787 83 13 0.683293342590332 83 14 0.6034330725669861 83 15 0.6429203748703003 83 16 0.6631690859794617 83 17 0.6432942748069763 83 18 0.6232653260231018 83 19 0.7031053304672241 83 20 0.662900447845459 83 21 0.7424203753471375 83 22 0.5642178654670715 83 23 0.618307888507843 84 0 0.6629816293716431 84 1 0.603686511516571 84 2 0.6631137132644653 84 3 0.6432551145553589 84 4 0.6234530806541443 84 5 0.6631062030792236 84 6 0.6233856081962585 84 7 0.623291552066803 84 8 0.5636104941368103 84 9 0.6230452060699463 84 10 0.7033079862594604 84 11 0.6632401943206787 84 12 0.6033605933189392 84 13 0.7030438184738159 84 14 0.6033838391304016 84 15 0.6432954668998718 84 16 0.683038592338562 84 17 0.6433181762695312 84 18 0.6830005645751953 84 19 0.6034631729125977 84 20 0.6431451439857483 84 21 0.6431554555892944 84 22 0.683063805103302 84 23 0.7012269496917725 85 0 0.6433011293411255 85 1 0.682892918586731 85 2 0.6630232334136963 85 3 0.6234970092773438 85 4 0.6630399227142334 85 5 0.5838055610656738 85 6 0.563841700553894 85 7 0.723044216632843 85 8 0.7026396989822388 85 9 0.5443935990333557 85 10 0.7027273178100586 85 11 0.6431053280830383 85 12 0.7424534559249878 85 13 0.5447001457214355 85 14 0.7621850967407227 85 15 0.6630753874778748 85 16 0.623645544052124 85 17 0.662893533706665 85 18 0.6826923489570618 85 19 0.623412013053894 85 20 0.6039232611656189 85 21 0.5642780661582947 85 22 0.6630739569664001 85 23 0.5906378626823425 86 0 0.6430964469909668 86 1 0.5637626051902771 86 2 0.6233540773391724 86 3 0.6631165742874146 86 4 0.6830431222915649 86 5 0.5438028573989868 86 6 0.6431793570518494 86 7 0.5832794308662415 86 8 0.5428575277328491 86 9 0.6432474851608276 86 10 0.6026874780654907 86 11 0.6229965686798096 86 12 0.7847843766212463 86 13 0.6633733510971069 86 14 0.6632823944091797 86 15 0.5829894542694092 86 16 0.6836845874786377 86 17 0.7233852744102478 86 18 0.6632336378097534 86 19 0.7231515645980835 86 20 0.7226436138153076 86 21 0.6234033107757568 86 22 0.6035512089729309 86 23 0.7013927698135376 87 0 0.5838998556137085 87 1 0.6232386231422424 87 2 0.5834173560142517 87 3 0.6632943749427795 87 4 0.6632100343704224 87 5 0.6631796360015869 87 6 0.5834301710128784 87 7 0.6232693791389465 87 8 0.6833223700523376 87 9 0.7230252623558044 87 10 0.742405891418457 87 11 0.643138587474823 87 12 0.5442461967468262 87 13 0.722734808921814 87 14 0.5839169025421143 87 15 0.6830785274505615 87 16 0.5639971494674683 87 17 0.6232551336288452 87 18 0.643042802810669 87 19 0.623282790184021 87 20 0.7029556632041931 87 21 0.6234123110771179 87 22 0.7629097700119019 87 23 0.618169367313385 88 0 0.5839695930480957 88 1 0.6631371378898621 88 2 0.7625775933265686 88 3 0.7022668719291687 88 4 0.6037044525146484 88 5 0.7025182247161865 88 6 0.6235818266868591 88 7 0.6235851049423218 88 8 0.6628358364105225 88 9 0.6630265116691589 88 10 0.6825722455978394 88 11 0.6432957649230957 88 12 0.6236668825149536 88 13 0.6628664135932922 88 14 0.5845268964767456 88 15 0.5643195509910583 88 16 0.5837618708610535 88 17 0.6232112050056458 88 18 0.6631784439086914 88 19 0.6828815937042236 88 20 0.5837948322296143 88 21 0.7028652429580688 88 22 0.6431180238723755 88 23 0.645630419254303 89 0 0.682786762714386 89 1 0.7226619124412537 89 2 0.6824326515197754 89 3 0.5644360780715942 89 4 0.623581051826477 89 5 0.6038095951080322 89 6 0.5839014649391174 89 7 0.6431863903999329 89 8 0.6431724429130554 89 9 0.6232052445411682 89 10 0.6231707334518433 89 11 0.6231309175491333 89 12 0.6630868911743164 89 13 0.7232874631881714 89 14 0.6431117057800293 89 15 0.7225725650787354 89 16 0.5442442297935486 89 17 0.702765703201294 89 18 0.5440350770950317 89 19 0.8027700185775757 89 20 0.6234528422355652 89 21 0.7026736736297607 89 22 0.6237648725509644 89 23 0.5358734726905823 90 0 0.6235334873199463 90 1 0.6433196663856506 90 2 0.5833615064620972 90 3 0.6830476522445679 90 4 0.6232083439826965 90 5 0.6831421852111816 90 6 0.6631628274917603 90 7 0.6630141735076904 90 8 0.5837623476982117 90 9 0.6233103275299072 90 10 0.6631140112876892 90 11 0.6828182935714722 90 12 0.5237855315208435 90 13 0.6231685876846313 90 14 0.6833097338676453 90 15 0.7432463765144348 90 16 0.7427130937576294 90 17 0.6828511953353882 90 18 0.5840569734573364 90 19 0.5838457942008972 90 20 0.6828489303588867 90 21 0.6233885884284973 90 22 0.6829280257225037 90 23 0.5907724499702454 91 0 0.5239238739013672 91 1 0.743347704410553 91 2 0.6432914733886719 91 3 0.603285014629364 91 4 0.6630846261978149 91 5 0.6433356404304504 91 6 0.5834346413612366 91 7 0.723150908946991 91 8 0.7428768873214722 91 9 0.6827681064605713 91 10 0.6235195398330688 91 11 0.6431103944778442 91 12 0.6827728748321533 91 13 0.6232914328575134 91 14 0.6434172987937927 91 15 0.6827311515808105 91 16 0.6629879474639893 91 17 0.5839875936508179 91 18 0.5639250874519348 91 19 0.603548526763916 91 20 0.6431774497032166 91 21 0.6631189584732056 91 22 0.6433157920837402 91 23 0.6735807657241821 92 0 0.6632097959518433 92 1 0.7227146029472351 92 2 0.6432288289070129 92 3 0.7224485278129578 92 4 0.7023531794548035 92 5 0.6038610339164734 92 6 0.6826527714729309 92 7 0.5253637433052063 92 8 0.5838219523429871 92 9 0.6034942269325256 92 10 0.6431417465209961 92 11 0.7028746604919434 92 12 0.6828777194023132 92 13 0.6035481691360474 92 14 0.6431281566619873 92 15 0.6034091114997864 92 16 0.6629817485809326 92 17 0.5639477968215942 92 18 0.6631678342819214 92 19 0.6831285357475281 92 20 0.5637627243995667 92 21 0.663235604763031 92 22 0.5835797786712646 92 23 0.8125656247138977 93 0 0.643202006816864 93 1 0.583837628364563 93 2 0.5437372922897339 93 3 0.6832563281059265 93 4 0.703069806098938 93 5 0.6234695315361023 93 6 0.643186628818512 93 7 0.6430761814117432 93 8 0.6630482077598572 93 9 0.5237796902656555 93 10 0.6632148623466492 93 11 0.6631674766540527 93 12 0.6432106494903564 93 13 0.623275876045227 93 14 0.763375997543335 93 15 0.6033185124397278 93 16 0.7628822922706604 93 17 0.6826412677764893 93 18 0.6827786564826965 93 19 0.6236880421638489 93 20 0.6234350800514221 93 21 0.6234626770019531 93 22 0.6035182476043701 93 23 0.6734487414360046 94 0 0.6034489274024963 94 1 0.6431435346603394 94 2 0.6831182837486267 94 3 0.7225027680397034 94 4 0.7222744822502136 94 5 0.6628180146217346 94 6 0.6236293911933899 94 7 0.6236163377761841 94 8 0.6432797908782959 94 9 0.6040587425231934 94 10 0.6825586557388306 94 11 0.64335697889328 94 12 0.5647252202033997 94 13 0.6826640367507935 94 14 0.7024015188217163 94 15 0.6039963364601135 94 16 0.5840587615966797 94 17 0.6630741357803345 94 18 0.6233417391777039 94 19 0.6826778650283813 94 20 0.6035629510879517 94 21 0.5837982892990112 94 22 0.7026782631874084 94 23 0.6180973649024963 95 0 0.6432507038116455 95 1 0.5835354924201965 95 2 0.6432458758354187 95 3 0.6233153939247131 95 4 0.6829907298088074 95 5 0.6034549474716187 95 6 0.6035028100013733 95 7 0.6033007502555847 95 8 0.6631706357002258 95 9 0.6432484984397888 95 10 0.6629968881607056 95 11 0.7230461239814758 95 12 0.6432793140411377 95 13 0.6433378458023071 95 14 0.5835949778556824 95 15 0.6429418325424194 95 16 0.6631026864051819 95 17 0.7229056358337402 95 18 0.5637471675872803 95 19 0.6632493138313293 95 20 0.6034144163131714 95 21 0.7232415676116943 95 22 0.7029206156730652 95 23 0.6457856893539429 96 0 0.5440255403518677 96 1 0.6231690645217896 96 2 0.7231385707855225 96 3 0.6034873127937317 96 4 0.623272716999054 96 5 0.6032289266586304 96 6 0.5833268165588379 96 7 0.6432955861091614 96 8 0.663139820098877 96 9 0.6431457996368408 96 10 0.562950611114502 96 11 0.6431905031204224 96 12 0.5827932357788086 96 13 0.7242056727409363 96 14 0.6632599234580994 96 15 0.6834229230880737 96 16 0.6833696961402893 96 17 0.7631641030311584 96 18 0.7026908993721008 96 19 0.6234526038169861 96 20 0.603563666343689 96 21 0.6831380128860474 96 22 0.6827064752578735 96 23 0.6181612610816956 97 0 0.6431019306182861 97 1 0.6630289554595947 97 2 0.6631101369857788 97 3 0.6629524230957031 97 4 0.6630631685256958 97 5 0.6827210187911987 97 6 0.6233770847320557 97 7 0.5641769170761108 97 8 0.5839316248893738 97 9 0.7227342128753662 97 10 0.6433243751525879 97 11 0.5639314651489258 97 12 0.702905535697937 97 13 0.6629815697669983 97 14 0.5837674140930176 97 15 0.6630221009254456 97 16 0.6233948469161987 97 17 0.6232759952545166 97 18 0.6434025168418884 97 19 0.7028582096099854 97 20 0.7027926445007324 97 21 0.5837644934654236 97 22 0.6829530000686646 97 23 0.6182999014854431 98 0 0.7225446701049805 98 1 0.6234905123710632 98 2 0.66321861743927 98 3 0.6234978437423706 98 4 0.6432133913040161 98 5 0.722394585609436 98 6 0.5645356178283691 98 7 0.6827508211135864 98 8 0.6826172471046448 98 9 0.5644099712371826 98 10 0.5638497471809387 98 11 0.6431272029876709 98 12 0.6828582286834717 98 13 0.6233973503112793 98 14 0.6034864783287048 98 15 0.7032288908958435 98 16 0.5438748598098755 98 17 0.6631650328636169 98 18 0.64314204454422 98 19 0.5833728313446045 98 20 0.6631113886833191 98 21 0.6632822155952454 98 22 0.7029975652694702 98 23 0.7288326621055603 99 0 0.7025588154792786 99 1 0.6234968900680542 99 2 0.6829352378845215 99 3 0.6828265190124512 99 4 0.583983302116394 99 5 0.5444249510765076 99 6 0.7226544618606567 99 7 0.7025946974754333 99 8 0.6038890480995178 99 9 0.7223342657089233 99 10 0.6431484222412109 99 11 0.6233788728713989 99 12 0.5839719772338867 99 13 0.6432315111160278 99 14 0.6433089375495911 99 15 0.6036406755447388 99 16 0.5639127492904663 99 17 0.643213152885437 99 18 0.6432664394378662 99 19 0.6034228801727295 99 20 0.6830294728279114 99 21 0.6033776998519897 99 22 0.7429494261741638 99 23 0.7010647058486938 In [ ]: 1 ","date":"2022-02-22T00:00:00Z","permalink":"https://example.com/p/pytorch%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B08_%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86/","title":"pytorch基本模型08_加载数据集"},{"content":" 这是学习pytorch基本使用的记录（我记得看的是B站刘二老师的视频）\nimport torch import numpy as np y = np.array([1,0,0]) z = np.array([0.2,0.1,-0.1]) y_pred = np.exp(z) / np.exp(z).sum() loss = (-y * np.log(y_pred)).sum() print(loss) import torch y = torch.LongTensor([0]) z = torch.Tensor([[0.2,0.1,-0.1]]) criterion = torch.nn.CrossEntropyLoss() loss = criterion(z,y) print(loss) import torch criterion = torch.nn.CrossEntropyLoss() Y = torch.LongTensor([2,0,1]) Y_pred1 = torch.Tensor([[0.1,0.2,0.9], [1.1,0.1,0.2], [0.2,2.1,0.1]]) Y_pred2 = torch.Tensor(([[0.8,0.2,0.3], [0.2,0.3,0.5], [0.2,0.2,0.5]])) l1 = criterion(Y_pred1,Y) l2 = criterion(Y_pred2,Y) print(\u0026#34;Batch Loss1 = \u0026#34;,l1.data,\u0026#34;\\nBatch Loss2=\u0026#34;,l2.data) #transfrom是用来处理数据的，如改变维度之类的 import torch from torchvision import transforms from torchvision import datasets from torch.utils.data import DataLoader import torch.optim as optim batch_size = 64 transform = transforms.Compose([ #PIL Image to Tensor transforms.ToTensor(), #提供均值和方差 transforms.Normalize((0.1307,),(0.3081,)) ]) train_dataset = datasets.MNIST(root=\u0026#34;../dataset/minis\u0026#34;, train=True, download=True, transform=transform) train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size ) test_dataset = datasets.MNIST(root=\u0026#34;../dataset/minis\u0026#34;, train=False, download=True, transform=transform) test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size) class Net(torch.nn.Module): def __init__(self): super(Net,self).__init__() self.l1 = torch.nn.Linear(784,512) self.l2 = torch.nn.Linear(512,256) self.l3 = torch.nn.Linear(256,128) self.l4 = torch.nn.Linear(128,64) self.l5 = torch.nn.Linear(64,10) def forward(self,x): # -1其实就是自动获取mini_batch x = x.view(-1,784) x = torch.nn.ReLU(self.l1(x)) x = torch.nn.ReLU(self.l2(x)) x = torch.nn.ReLU(self.l3(x)) x = torch.nn.ReLU(self.l4(x)) #最后一层不做ReLU因为CrossEntryLoss中包含了Softmax return self.l5(x) model = Net() criterion = torch.nn.CrossEntropyLoss() optimizer = optim.SGD(model.parameters(),lr=0.01,momentum=0.5) def train(epoch): runing_loss = 0.0 for batch_idx,data in enumerate(train_loader,0): inputs,target = data optimizer.zero_grad() # 获得模型预测结果(64, 10) outputs = model(inputs) # 交叉熵代价函数outputs(64,10),target（64） loss = criterion(outputs,target) loss.backward() optimizer.step() runing_loss += loss.item() if batch_idx % 300 == 299: print(\u0026#34;[%d, %5d] loss: %.3f\u0026#34; % (epoch + 1,batch_idx + 1,runing_loss/300)) runing_loss = 0.0 def test(): correct = 0 total = 0 #测试集是用来测试 训练集训练的参数怎么样的，所以测试集是不需要反向求导的 with torch.no_grad(): for data in test_loader: images,labels =data outputs = model(images) _,predicted = torch.max(outputs.data,dim=1) total += label.size(0) correct += (predicted == labels).sum().item() print(\u0026#34;Accuracy on test set: %d %%\u0026#34; % (100 * correct / total)) if __name__ == \u0026#34;__main__\u0026#34;: for epoch in range(10): train(epoch) test() result:\n--------------------------------------------------------------------------- TypeError Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_8668/1354835192.py in \u0026lt;module\u0026gt; 1 if __name__ == \u0026#34;__main__\u0026#34;: 2 for epoch in range(10): ----\u0026gt; 3 train(epoch) 4 test() ~\\AppData\\Local\\Temp/ipykernel_8668/1954326027.py in train(epoch) 6 7 # 获得模型预测结果(64, 10) ----\u0026gt; 8 outputs = model(inputs) 9 # 交叉熵代价函数outputs(64,10),target（64） 10 loss = criterion(outputs,target) F:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py in _call_impl(self, *input, **kwargs) 1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1109 or _global_forward_hooks or _global_forward_pre_hooks): -\u0026gt; 1110 return forward_call(*input, **kwargs) 1111 # Do not call functions when jit is used 1112 full_backward_hooks, non_full_backward_hooks = [], [] ~\\AppData\\Local\\Temp/ipykernel_8668/2241425557.py in forward(self, x) 12 x = x.view(-1,784) 13 x = torch.nn.ReLU(self.l1(x)) ---\u0026gt; 14 x = torch.nn.ReLU(self.l2(x)) 15 x = torch.nn.ReLU(self.l3(x)) 16 x = torch.nn.ReLU(self.l4(x)) F:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py in _call_impl(self, *input, **kwargs) 1108 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks 1109 or _global_forward_hooks or _global_forward_pre_hooks): -\u0026gt; 1110 return forward_call(*input, **kwargs) 1111 # Do not call functions when jit is used 1112 full_backward_hooks, non_full_backward_hooks = [], [] F:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py in forward(self, input) 101 102 def forward(self, input: Tensor) -\u0026gt; Tensor: --\u0026gt; 103 return F.linear(input, self.weight, self.bias) 104 105 def extra_repr(self) -\u0026gt; str: TypeError: linear(): argument \u0026#39;input\u0026#39; (position 1) must be Tensor, not ReLU In [ ]: 1 ​ ","date":"2022-02-22T00:00:00Z","permalink":"https://example.com/p/pytorch%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B09_%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98/","title":"pytorch基本模型09_多分类问题"},{"content":" 这是学习pytorch基本使用的记录（我记得看的是B站刘二老师的视频）\n准备数据 设计计算单元 构建代价和优化函数 训练 import torch x_data = torch.Tensor([[1.0],[2.0],[3.0]]) y_data = torch.Tensor([[2.0],[4.0],[6.0]]) #计算单元 class LinerarModel(torch.nn.Module): def __init__(self): #super中传入类名 super(LinerarModel,self).__init__() self.linear = torch.nn.Linear(1,1)#weight and bias def forward(self,x): y_pred = self.linear(x) return y_pred #实例化 model = LinerarModel() #代价函数 实例化torch中的MSELoss criterion = torch.nn.MSELoss() #参数优化函数 实例化torch中的optim.SGD optimizer = torch.optim.SGD(model.parameters(),lr=0.01) print(model.parameters) result\n\u0026lt;bound method Module.parameters of LinerarModel( (linear): Linear(in_features=1, out_features=1, bias=True) )\u0026gt; code\nfor epoch in range(1000): y_pred = model(x_data)#forward #这里model就已经构建了一个计算图了，又因为事用loss去求偏导，所以下面用loss.backward #loss这里会自动拼接上去上面的那个计算图 loss = criterion(y_pred,y_data) print(epoch,loss) optimizer.zero_grad()#记得将导数归零 loss.backward()#用反向传播 optimizer.step()#更新参数 print(\u0026#34;w=\u0026#34;,model.linear.weight.item()) print(\u0026#34;b=\u0026#34;,model.linear.bias.item()) result:\n0 tensor(5.6337, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 1 tensor(4.4723, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 2 tensor(3.5542, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 3 tensor(2.8284, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 4 tensor(2.2545, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 5 tensor(1.8009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 6 tensor(1.4422, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 7 tensor(1.1586, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 8 tensor(0.9343, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 9 tensor(0.7570, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 10 tensor(0.6167, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 11 tensor(0.5057, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 12 tensor(0.4179, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 13 tensor(0.3484, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 14 tensor(0.2934, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 15 tensor(0.2498, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 16 tensor(0.2153, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 17 tensor(0.1879, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 18 tensor(0.1662, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 19 tensor(0.1489, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 20 tensor(0.1352, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 21 tensor(0.1242, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 22 tensor(0.1155, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 23 tensor(0.1085, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 24 tensor(0.1029, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 25 tensor(0.0984, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 26 tensor(0.0948, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 27 tensor(0.0918, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 28 tensor(0.0894, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 29 tensor(0.0874, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 30 tensor(0.0857, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 31 tensor(0.0843, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 32 tensor(0.0832, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 33 tensor(0.0822, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 34 tensor(0.0813, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 35 tensor(0.0805, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 36 tensor(0.0798, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 37 tensor(0.0792, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 38 tensor(0.0786, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 39 tensor(0.0781, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 40 tensor(0.0776, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 41 tensor(0.0771, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 42 tensor(0.0767, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 43 tensor(0.0763, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 44 tensor(0.0759, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 45 tensor(0.0755, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 46 tensor(0.0751, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 47 tensor(0.0747, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 48 tensor(0.0743, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 49 tensor(0.0739, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 50 tensor(0.0736, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 51 tensor(0.0732, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 52 tensor(0.0728, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 53 tensor(0.0725, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 54 tensor(0.0721, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 55 tensor(0.0718, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 56 tensor(0.0714, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 57 tensor(0.0711, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 58 tensor(0.0708, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 59 tensor(0.0704, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 60 tensor(0.0701, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 61 tensor(0.0697, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 62 tensor(0.0694, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 63 tensor(0.0691, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 64 tensor(0.0687, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 65 tensor(0.0684, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 66 tensor(0.0681, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 67 tensor(0.0677, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 68 tensor(0.0674, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 69 tensor(0.0671, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 70 tensor(0.0668, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 71 tensor(0.0665, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 72 tensor(0.0661, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 73 tensor(0.0658, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 74 tensor(0.0655, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 75 tensor(0.0652, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 76 tensor(0.0649, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 77 tensor(0.0646, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 78 tensor(0.0643, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 79 tensor(0.0639, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 80 tensor(0.0636, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 81 tensor(0.0633, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 82 tensor(0.0630, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 83 tensor(0.0627, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 84 tensor(0.0624, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 85 tensor(0.0621, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 86 tensor(0.0618, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 87 tensor(0.0615, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 88 tensor(0.0612, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 89 tensor(0.0609, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 90 tensor(0.0606, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 91 tensor(0.0604, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 92 tensor(0.0601, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 93 tensor(0.0598, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 94 tensor(0.0595, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 95 tensor(0.0592, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 96 tensor(0.0589, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 97 tensor(0.0586, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 98 tensor(0.0584, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 99 tensor(0.0581, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 100 tensor(0.0578, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 101 tensor(0.0575, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 102 tensor(0.0572, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 103 tensor(0.0570, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 104 tensor(0.0567, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 105 tensor(0.0564, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 106 tensor(0.0562, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 107 tensor(0.0559, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 108 tensor(0.0556, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 109 tensor(0.0553, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 110 tensor(0.0551, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 111 tensor(0.0548, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 112 tensor(0.0546, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 113 tensor(0.0543, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 114 tensor(0.0540, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 115 tensor(0.0538, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 116 tensor(0.0535, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 117 tensor(0.0533, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 118 tensor(0.0530, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 119 tensor(0.0527, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 120 tensor(0.0525, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 121 tensor(0.0522, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 122 tensor(0.0520, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 123 tensor(0.0517, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 124 tensor(0.0515, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 125 tensor(0.0512, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 126 tensor(0.0510, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 127 tensor(0.0508, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 128 tensor(0.0505, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 129 tensor(0.0503, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 130 tensor(0.0500, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 131 tensor(0.0498, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 132 tensor(0.0495, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 133 tensor(0.0493, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 134 tensor(0.0491, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 135 tensor(0.0488, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 136 tensor(0.0486, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 137 tensor(0.0484, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 138 tensor(0.0481, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 139 tensor(0.0479, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 140 tensor(0.0477, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 141 tensor(0.0474, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 142 tensor(0.0472, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 143 tensor(0.0470, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 144 tensor(0.0468, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 145 tensor(0.0465, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 146 tensor(0.0463, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 147 tensor(0.0461, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 148 tensor(0.0459, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 149 tensor(0.0457, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 150 tensor(0.0454, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 151 tensor(0.0452, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 152 tensor(0.0450, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 153 tensor(0.0448, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 154 tensor(0.0446, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 155 tensor(0.0444, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 156 tensor(0.0441, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 157 tensor(0.0439, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 158 tensor(0.0437, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 159 tensor(0.0435, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 160 tensor(0.0433, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 161 tensor(0.0431, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 162 tensor(0.0429, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 163 tensor(0.0427, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 164 tensor(0.0425, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 165 tensor(0.0423, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 166 tensor(0.0421, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 167 tensor(0.0419, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 168 tensor(0.0417, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 169 tensor(0.0415, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 170 tensor(0.0413, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 171 tensor(0.0411, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 172 tensor(0.0409, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 173 tensor(0.0407, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 174 tensor(0.0405, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 175 tensor(0.0403, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 176 tensor(0.0401, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 177 tensor(0.0399, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 178 tensor(0.0397, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 179 tensor(0.0395, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 180 tensor(0.0393, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 181 tensor(0.0391, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 182 tensor(0.0389, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 183 tensor(0.0388, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 184 tensor(0.0386, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 185 tensor(0.0384, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 186 tensor(0.0382, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 187 tensor(0.0380, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 188 tensor(0.0378, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 189 tensor(0.0377, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 190 tensor(0.0375, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 191 tensor(0.0373, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 192 tensor(0.0371, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 193 tensor(0.0369, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 194 tensor(0.0368, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 195 tensor(0.0366, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 196 tensor(0.0364, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 197 tensor(0.0362, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 198 tensor(0.0361, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 199 tensor(0.0359, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 200 tensor(0.0357, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 201 tensor(0.0355, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 202 tensor(0.0354, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 203 tensor(0.0352, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 204 tensor(0.0350, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 205 tensor(0.0349, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 206 tensor(0.0347, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 207 tensor(0.0345, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 208 tensor(0.0344, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 209 tensor(0.0342, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 210 tensor(0.0340, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 211 tensor(0.0339, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 212 tensor(0.0337, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 213 tensor(0.0335, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 214 tensor(0.0334, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 215 tensor(0.0332, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 216 tensor(0.0331, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 217 tensor(0.0329, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 218 tensor(0.0328, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 219 tensor(0.0326, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 220 tensor(0.0324, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 221 tensor(0.0323, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 222 tensor(0.0321, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 223 tensor(0.0320, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 224 tensor(0.0318, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 225 tensor(0.0317, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 226 tensor(0.0315, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 227 tensor(0.0314, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 228 tensor(0.0312, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 229 tensor(0.0311, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 230 tensor(0.0309, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 231 tensor(0.0308, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 232 tensor(0.0306, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 233 tensor(0.0305, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 234 tensor(0.0303, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 235 tensor(0.0302, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 236 tensor(0.0300, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 237 tensor(0.0299, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 238 tensor(0.0297, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 239 tensor(0.0296, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 240 tensor(0.0295, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 241 tensor(0.0293, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 242 tensor(0.0292, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 243 tensor(0.0290, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 244 tensor(0.0289, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 245 tensor(0.0288, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 246 tensor(0.0286, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 247 tensor(0.0285, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 248 tensor(0.0283, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 249 tensor(0.0282, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 250 tensor(0.0281, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 251 tensor(0.0279, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 252 tensor(0.0278, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 253 tensor(0.0277, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 254 tensor(0.0275, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 255 tensor(0.0274, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 256 tensor(0.0273, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 257 tensor(0.0271, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 258 tensor(0.0270, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 259 tensor(0.0269, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 260 tensor(0.0268, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 261 tensor(0.0266, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 262 tensor(0.0265, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 263 tensor(0.0264, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 264 tensor(0.0262, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 265 tensor(0.0261, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 266 tensor(0.0260, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 267 tensor(0.0259, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 268 tensor(0.0257, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 269 tensor(0.0256, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 270 tensor(0.0255, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 271 tensor(0.0254, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 272 tensor(0.0253, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 273 tensor(0.0251, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 274 tensor(0.0250, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 275 tensor(0.0249, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 276 tensor(0.0248, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 277 tensor(0.0247, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 278 tensor(0.0245, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 279 tensor(0.0244, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 280 tensor(0.0243, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 281 tensor(0.0242, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 282 tensor(0.0241, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 283 tensor(0.0240, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 284 tensor(0.0238, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 285 tensor(0.0237, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 286 tensor(0.0236, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 287 tensor(0.0235, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 288 tensor(0.0234, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 289 tensor(0.0233, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 290 tensor(0.0232, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 291 tensor(0.0230, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 292 tensor(0.0229, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 293 tensor(0.0228, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 294 tensor(0.0227, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 295 tensor(0.0226, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 296 tensor(0.0225, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 297 tensor(0.0224, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 298 tensor(0.0223, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 299 tensor(0.0222, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 300 tensor(0.0221, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 301 tensor(0.0220, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 302 tensor(0.0219, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 303 tensor(0.0218, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 304 tensor(0.0216, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 305 tensor(0.0215, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 306 tensor(0.0214, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 307 tensor(0.0213, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 308 tensor(0.0212, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 309 tensor(0.0211, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 310 tensor(0.0210, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 311 tensor(0.0209, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 312 tensor(0.0208, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 313 tensor(0.0207, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 314 tensor(0.0206, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 315 tensor(0.0205, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 316 tensor(0.0204, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 317 tensor(0.0203, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 318 tensor(0.0202, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 319 tensor(0.0201, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 320 tensor(0.0200, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 321 tensor(0.0199, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 322 tensor(0.0199, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 323 tensor(0.0198, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 324 tensor(0.0197, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 325 tensor(0.0196, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 326 tensor(0.0195, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 327 tensor(0.0194, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 328 tensor(0.0193, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 329 tensor(0.0192, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 330 tensor(0.0191, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 331 tensor(0.0190, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 332 tensor(0.0189, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 333 tensor(0.0188, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 334 tensor(0.0187, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 335 tensor(0.0186, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 336 tensor(0.0186, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 337 tensor(0.0185, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 338 tensor(0.0184, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 339 tensor(0.0183, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 340 tensor(0.0182, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 341 tensor(0.0181, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 342 tensor(0.0180, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 343 tensor(0.0179, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 344 tensor(0.0179, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 345 tensor(0.0178, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 346 tensor(0.0177, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 347 tensor(0.0176, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 348 tensor(0.0175, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 349 tensor(0.0174, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 350 tensor(0.0173, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 351 tensor(0.0173, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 352 tensor(0.0172, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 353 tensor(0.0171, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 354 tensor(0.0170, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 355 tensor(0.0169, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 356 tensor(0.0169, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 357 tensor(0.0168, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 358 tensor(0.0167, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 359 tensor(0.0166, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 360 tensor(0.0165, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 361 tensor(0.0165, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 362 tensor(0.0164, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 363 tensor(0.0163, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 364 tensor(0.0162, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 365 tensor(0.0161, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 366 tensor(0.0161, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 367 tensor(0.0160, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 368 tensor(0.0159, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 369 tensor(0.0158, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 370 tensor(0.0158, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 371 tensor(0.0157, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 372 tensor(0.0156, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 373 tensor(0.0155, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 374 tensor(0.0155, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 375 tensor(0.0154, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 376 tensor(0.0153, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 377 tensor(0.0152, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 378 tensor(0.0152, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 379 tensor(0.0151, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 380 tensor(0.0150, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 381 tensor(0.0149, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 382 tensor(0.0149, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 383 tensor(0.0148, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 384 tensor(0.0147, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 385 tensor(0.0147, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 386 tensor(0.0146, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 387 tensor(0.0145, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 388 tensor(0.0144, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 389 tensor(0.0144, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 390 tensor(0.0143, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 391 tensor(0.0142, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 392 tensor(0.0142, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 393 tensor(0.0141, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 394 tensor(0.0140, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 395 tensor(0.0140, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 396 tensor(0.0139, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 397 tensor(0.0138, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 398 tensor(0.0138, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 399 tensor(0.0137, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 400 tensor(0.0136, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 401 tensor(0.0136, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 402 tensor(0.0135, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 403 tensor(0.0134, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 404 tensor(0.0134, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 405 tensor(0.0133, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 406 tensor(0.0132, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 407 tensor(0.0132, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 408 tensor(0.0131, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 409 tensor(0.0131, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 410 tensor(0.0130, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 411 tensor(0.0129, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 412 tensor(0.0129, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 413 tensor(0.0128, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 414 tensor(0.0127, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 415 tensor(0.0127, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 416 tensor(0.0126, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 417 tensor(0.0126, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 418 tensor(0.0125, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 419 tensor(0.0124, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 420 tensor(0.0124, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 421 tensor(0.0123, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 422 tensor(0.0123, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 423 tensor(0.0122, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 424 tensor(0.0121, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 425 tensor(0.0121, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 426 tensor(0.0120, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 427 tensor(0.0120, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 428 tensor(0.0119, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 429 tensor(0.0119, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 430 tensor(0.0118, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 431 tensor(0.0117, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 432 tensor(0.0117, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 433 tensor(0.0116, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 434 tensor(0.0116, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 435 tensor(0.0115, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 436 tensor(0.0115, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 437 tensor(0.0114, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 438 tensor(0.0114, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 439 tensor(0.0113, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 440 tensor(0.0112, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 441 tensor(0.0112, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 442 tensor(0.0111, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 443 tensor(0.0111, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 444 tensor(0.0110, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 445 tensor(0.0110, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 446 tensor(0.0109, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 447 tensor(0.0109, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 448 tensor(0.0108, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 449 tensor(0.0108, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 450 tensor(0.0107, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 451 tensor(0.0107, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 452 tensor(0.0106, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 453 tensor(0.0106, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 454 tensor(0.0105, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 455 tensor(0.0105, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 456 tensor(0.0104, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 457 tensor(0.0104, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 458 tensor(0.0103, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 459 tensor(0.0103, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 460 tensor(0.0102, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 461 tensor(0.0102, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 462 tensor(0.0101, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 463 tensor(0.0101, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 464 tensor(0.0100, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 465 tensor(0.0100, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 466 tensor(0.0099, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 467 tensor(0.0099, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 468 tensor(0.0098, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 469 tensor(0.0098, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 470 tensor(0.0097, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 471 tensor(0.0097, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 472 tensor(0.0096, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 473 tensor(0.0096, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 474 tensor(0.0096, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 475 tensor(0.0095, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 476 tensor(0.0095, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 477 tensor(0.0094, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 478 tensor(0.0094, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 479 tensor(0.0093, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 480 tensor(0.0093, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 481 tensor(0.0092, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 482 tensor(0.0092, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 483 tensor(0.0091, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 484 tensor(0.0091, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 485 tensor(0.0091, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 486 tensor(0.0090, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 487 tensor(0.0090, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 488 tensor(0.0089, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 489 tensor(0.0089, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 490 tensor(0.0088, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 491 tensor(0.0088, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 492 tensor(0.0088, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 493 tensor(0.0087, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 494 tensor(0.0087, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 495 tensor(0.0086, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 496 tensor(0.0086, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 497 tensor(0.0085, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 498 tensor(0.0085, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 499 tensor(0.0085, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 500 tensor(0.0084, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 501 tensor(0.0084, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 502 tensor(0.0083, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 503 tensor(0.0083, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 504 tensor(0.0083, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 505 tensor(0.0082, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 506 tensor(0.0082, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 507 tensor(0.0081, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 508 tensor(0.0081, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 509 tensor(0.0081, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 510 tensor(0.0080, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 511 tensor(0.0080, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 512 tensor(0.0080, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 513 tensor(0.0079, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 514 tensor(0.0079, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 515 tensor(0.0078, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 516 tensor(0.0078, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 517 tensor(0.0078, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 518 tensor(0.0077, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 519 tensor(0.0077, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 520 tensor(0.0077, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 521 tensor(0.0076, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 522 tensor(0.0076, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 523 tensor(0.0075, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 524 tensor(0.0075, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 525 tensor(0.0075, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 526 tensor(0.0074, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 527 tensor(0.0074, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 528 tensor(0.0074, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 529 tensor(0.0073, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 530 tensor(0.0073, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 531 tensor(0.0073, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 532 tensor(0.0072, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 533 tensor(0.0072, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 534 tensor(0.0072, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 535 tensor(0.0071, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 536 tensor(0.0071, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 537 tensor(0.0071, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 538 tensor(0.0070, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 539 tensor(0.0070, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 540 tensor(0.0070, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 541 tensor(0.0069, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 542 tensor(0.0069, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 543 tensor(0.0069, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 544 tensor(0.0068, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 545 tensor(0.0068, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 546 tensor(0.0068, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 547 tensor(0.0067, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 548 tensor(0.0067, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 549 tensor(0.0067, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 550 tensor(0.0066, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 551 tensor(0.0066, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 552 tensor(0.0066, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 553 tensor(0.0065, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 554 tensor(0.0065, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 555 tensor(0.0065, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 556 tensor(0.0064, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 557 tensor(0.0064, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 558 tensor(0.0064, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 559 tensor(0.0063, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 560 tensor(0.0063, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 561 tensor(0.0063, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 562 tensor(0.0063, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 563 tensor(0.0062, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 564 tensor(0.0062, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 565 tensor(0.0062, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 566 tensor(0.0061, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 567 tensor(0.0061, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 568 tensor(0.0061, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 569 tensor(0.0060, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 570 tensor(0.0060, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 571 tensor(0.0060, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 572 tensor(0.0060, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 573 tensor(0.0059, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 574 tensor(0.0059, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 575 tensor(0.0059, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 576 tensor(0.0058, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 577 tensor(0.0058, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 578 tensor(0.0058, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 579 tensor(0.0058, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 580 tensor(0.0057, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 581 tensor(0.0057, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 582 tensor(0.0057, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 583 tensor(0.0057, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 584 tensor(0.0056, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 585 tensor(0.0056, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 586 tensor(0.0056, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 587 tensor(0.0055, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 588 tensor(0.0055, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 589 tensor(0.0055, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 590 tensor(0.0055, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 591 tensor(0.0054, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 592 tensor(0.0054, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 593 tensor(0.0054, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 594 tensor(0.0054, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 595 tensor(0.0053, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 596 tensor(0.0053, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 597 tensor(0.0053, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 598 tensor(0.0053, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 599 tensor(0.0052, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 600 tensor(0.0052, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 601 tensor(0.0052, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 602 tensor(0.0052, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 603 tensor(0.0051, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 604 tensor(0.0051, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 605 tensor(0.0051, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 606 tensor(0.0051, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 607 tensor(0.0050, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 608 tensor(0.0050, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 609 tensor(0.0050, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 610 tensor(0.0050, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 611 tensor(0.0049, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 612 tensor(0.0049, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 613 tensor(0.0049, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 614 tensor(0.0049, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 615 tensor(0.0048, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 616 tensor(0.0048, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 617 tensor(0.0048, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 618 tensor(0.0048, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 619 tensor(0.0048, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 620 tensor(0.0047, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 621 tensor(0.0047, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 622 tensor(0.0047, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 623 tensor(0.0047, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 624 tensor(0.0046, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 625 tensor(0.0046, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 626 tensor(0.0046, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 627 tensor(0.0046, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 628 tensor(0.0046, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 629 tensor(0.0045, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 630 tensor(0.0045, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 631 tensor(0.0045, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 632 tensor(0.0045, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 633 tensor(0.0044, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 634 tensor(0.0044, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 635 tensor(0.0044, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 636 tensor(0.0044, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 637 tensor(0.0044, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 638 tensor(0.0043, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 639 tensor(0.0043, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 640 tensor(0.0043, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 641 tensor(0.0043, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 642 tensor(0.0043, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 643 tensor(0.0042, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 644 tensor(0.0042, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 645 tensor(0.0042, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 646 tensor(0.0042, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 647 tensor(0.0042, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 648 tensor(0.0041, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 649 tensor(0.0041, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 650 tensor(0.0041, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 651 tensor(0.0041, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 652 tensor(0.0041, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 653 tensor(0.0040, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 654 tensor(0.0040, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 655 tensor(0.0040, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 656 tensor(0.0040, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 657 tensor(0.0040, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 658 tensor(0.0039, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 659 tensor(0.0039, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 660 tensor(0.0039, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 661 tensor(0.0039, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 662 tensor(0.0039, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 663 tensor(0.0038, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 664 tensor(0.0038, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 665 tensor(0.0038, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 666 tensor(0.0038, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 667 tensor(0.0038, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 668 tensor(0.0038, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 669 tensor(0.0037, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 670 tensor(0.0037, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 671 tensor(0.0037, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 672 tensor(0.0037, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 673 tensor(0.0037, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 674 tensor(0.0036, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 675 tensor(0.0036, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 676 tensor(0.0036, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 677 tensor(0.0036, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 678 tensor(0.0036, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 679 tensor(0.0036, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 680 tensor(0.0035, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 681 tensor(0.0035, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 682 tensor(0.0035, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 683 tensor(0.0035, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 684 tensor(0.0035, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 685 tensor(0.0035, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 686 tensor(0.0034, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 687 tensor(0.0034, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 688 tensor(0.0034, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 689 tensor(0.0034, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 690 tensor(0.0034, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 691 tensor(0.0034, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 692 tensor(0.0033, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 693 tensor(0.0033, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 694 tensor(0.0033, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 695 tensor(0.0033, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 696 tensor(0.0033, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 697 tensor(0.0033, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 698 tensor(0.0032, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 699 tensor(0.0032, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 700 tensor(0.0032, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 701 tensor(0.0032, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 702 tensor(0.0032, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 703 tensor(0.0032, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 704 tensor(0.0032, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 705 tensor(0.0031, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 706 tensor(0.0031, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 707 tensor(0.0031, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 708 tensor(0.0031, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 709 tensor(0.0031, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 710 tensor(0.0031, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 711 tensor(0.0031, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 712 tensor(0.0030, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 713 tensor(0.0030, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 714 tensor(0.0030, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 715 tensor(0.0030, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 716 tensor(0.0030, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 717 tensor(0.0030, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 718 tensor(0.0030, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 719 tensor(0.0029, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 720 tensor(0.0029, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 721 tensor(0.0029, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 722 tensor(0.0029, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 723 tensor(0.0029, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 724 tensor(0.0029, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 725 tensor(0.0029, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 726 tensor(0.0028, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 727 tensor(0.0028, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 728 tensor(0.0028, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 729 tensor(0.0028, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 730 tensor(0.0028, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 731 tensor(0.0028, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 732 tensor(0.0028, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 733 tensor(0.0027, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 734 tensor(0.0027, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 735 tensor(0.0027, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 736 tensor(0.0027, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 737 tensor(0.0027, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 738 tensor(0.0027, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 739 tensor(0.0027, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 740 tensor(0.0027, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 741 tensor(0.0026, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 742 tensor(0.0026, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 743 tensor(0.0026, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 744 tensor(0.0026, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 745 tensor(0.0026, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 746 tensor(0.0026, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 747 tensor(0.0026, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 748 tensor(0.0026, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 749 tensor(0.0025, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 750 tensor(0.0025, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 751 tensor(0.0025, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 752 tensor(0.0025, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 753 tensor(0.0025, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 754 tensor(0.0025, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 755 tensor(0.0025, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 756 tensor(0.0025, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 757 tensor(0.0024, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 758 tensor(0.0024, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 759 tensor(0.0024, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 760 tensor(0.0024, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 761 tensor(0.0024, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 762 tensor(0.0024, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 763 tensor(0.0024, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 764 tensor(0.0024, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 765 tensor(0.0024, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 766 tensor(0.0023, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 767 tensor(0.0023, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 768 tensor(0.0023, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 769 tensor(0.0023, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 770 tensor(0.0023, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 771 tensor(0.0023, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 772 tensor(0.0023, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 773 tensor(0.0023, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 774 tensor(0.0023, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 775 tensor(0.0022, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 776 tensor(0.0022, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 777 tensor(0.0022, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 778 tensor(0.0022, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 779 tensor(0.0022, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 780 tensor(0.0022, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 781 tensor(0.0022, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 782 tensor(0.0022, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 783 tensor(0.0022, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 784 tensor(0.0021, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 785 tensor(0.0021, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 786 tensor(0.0021, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 787 tensor(0.0021, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 788 tensor(0.0021, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 789 tensor(0.0021, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 790 tensor(0.0021, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 791 tensor(0.0021, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 792 tensor(0.0021, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 793 tensor(0.0021, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 794 tensor(0.0020, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 795 tensor(0.0020, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 796 tensor(0.0020, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 797 tensor(0.0020, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 798 tensor(0.0020, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 799 tensor(0.0020, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 800 tensor(0.0020, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 801 tensor(0.0020, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 802 tensor(0.0020, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 803 tensor(0.0020, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 804 tensor(0.0020, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 805 tensor(0.0019, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 806 tensor(0.0019, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 807 tensor(0.0019, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 808 tensor(0.0019, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 809 tensor(0.0019, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 810 tensor(0.0019, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 811 tensor(0.0019, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 812 tensor(0.0019, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 813 tensor(0.0019, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 814 tensor(0.0019, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 815 tensor(0.0018, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 816 tensor(0.0018, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 817 tensor(0.0018, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 818 tensor(0.0018, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 819 tensor(0.0018, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 820 tensor(0.0018, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 821 tensor(0.0018, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 822 tensor(0.0018, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 823 tensor(0.0018, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 824 tensor(0.0018, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 825 tensor(0.0018, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 826 tensor(0.0018, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 827 tensor(0.0017, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 828 tensor(0.0017, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 829 tensor(0.0017, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 830 tensor(0.0017, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 831 tensor(0.0017, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 832 tensor(0.0017, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 833 tensor(0.0017, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 834 tensor(0.0017, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 835 tensor(0.0017, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 836 tensor(0.0017, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 837 tensor(0.0017, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 838 tensor(0.0017, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 839 tensor(0.0016, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 840 tensor(0.0016, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 841 tensor(0.0016, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 842 tensor(0.0016, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 843 tensor(0.0016, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 844 tensor(0.0016, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 845 tensor(0.0016, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 846 tensor(0.0016, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 847 tensor(0.0016, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 848 tensor(0.0016, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 849 tensor(0.0016, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 850 tensor(0.0016, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 851 tensor(0.0016, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 852 tensor(0.0015, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 853 tensor(0.0015, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 854 tensor(0.0015, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 855 tensor(0.0015, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 856 tensor(0.0015, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 857 tensor(0.0015, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 858 tensor(0.0015, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 859 tensor(0.0015, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 860 tensor(0.0015, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 861 tensor(0.0015, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 862 tensor(0.0015, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 863 tensor(0.0015, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 864 tensor(0.0015, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 865 tensor(0.0015, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 866 tensor(0.0014, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 867 tensor(0.0014, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 868 tensor(0.0014, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 869 tensor(0.0014, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 870 tensor(0.0014, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 871 tensor(0.0014, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 872 tensor(0.0014, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 873 tensor(0.0014, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 874 tensor(0.0014, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 875 tensor(0.0014, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 876 tensor(0.0014, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 877 tensor(0.0014, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 878 tensor(0.0014, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 879 tensor(0.0014, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 880 tensor(0.0014, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 881 tensor(0.0013, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 882 tensor(0.0013, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 883 tensor(0.0013, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 884 tensor(0.0013, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 885 tensor(0.0013, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 886 tensor(0.0013, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 887 tensor(0.0013, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 888 tensor(0.0013, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 889 tensor(0.0013, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 890 tensor(0.0013, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 891 tensor(0.0013, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 892 tensor(0.0013, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 893 tensor(0.0013, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 894 tensor(0.0013, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 895 tensor(0.0013, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 896 tensor(0.0013, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 897 tensor(0.0012, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 898 tensor(0.0012, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 899 tensor(0.0012, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 900 tensor(0.0012, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 901 tensor(0.0012, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 902 tensor(0.0012, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 903 tensor(0.0012, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 904 tensor(0.0012, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 905 tensor(0.0012, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 906 tensor(0.0012, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 907 tensor(0.0012, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 908 tensor(0.0012, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 909 tensor(0.0012, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 910 tensor(0.0012, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 911 tensor(0.0012, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 912 tensor(0.0012, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 913 tensor(0.0012, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 914 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 915 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 916 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 917 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 918 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 919 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 920 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 921 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 922 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 923 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 924 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 925 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 926 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 927 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 928 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 929 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 930 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 931 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 932 tensor(0.0011, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 933 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 934 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 935 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 936 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 937 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 938 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 939 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 940 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 941 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 942 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 943 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 944 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 945 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 946 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 947 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 948 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 949 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 950 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 951 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 952 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 953 tensor(0.0010, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 954 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 955 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 956 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 957 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 958 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 959 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 960 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 961 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 962 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 963 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 964 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 965 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 966 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 967 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 968 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 969 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 970 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 971 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 972 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 973 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 974 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 975 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 976 tensor(0.0009, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 977 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 978 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 979 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 980 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 981 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 982 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 983 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 984 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 985 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 986 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 987 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 988 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 989 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 990 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 991 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 992 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 993 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 994 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 995 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 996 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 997 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 998 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) 999 tensor(0.0008, grad_fn=\u0026lt;MseLossBackward0\u0026gt;) w= 1.9679958820343018 b= 0.07275304943323135 code:\n#TEST x_test = torch.Tensor([[4.0]]) y_test = model(x_test) print(\u0026#34;y_pred:\u0026#34;,y_test.data) result:\ny_pred: tensor([[7.9447]]) code:\nimport torch x_data = torch.Tensor([[1.0],[2.0],[3.0]]) y_data = torch.Tensor([[2.0],[4.0],[6.0]]) class LinearModel2(torch.nn.Module): def __init__(self): super(LinearModel2,self).__init__() self.Linear = torch.nn.Linear(1,1) def forward(self,x): y_pred = self.Linear(x) return y_pred model2 = LinearModel2() criterion2 = torch.nn.MSELoss() optimizer2 = torch.optim.Adamax(model2.parameters()) for epoch in range(100): y_pred = model2(x_data) loss = criterion2(y_pred,y_data) print(epoch,loss.item()) optimizer2.zero_grad() loss.backward() optimizer2.step() print(\u0026#34;w=\u0026#34;,model2.Linear.weight.item()) print(\u0026#34;b=\u0026#34;,model2.Linear.bias.item()) x_test = torch.Tensor([[4.0]]) y_test = model2(x_test) print(\u0026#34;y_pred=\u0026#34;,y_test.data) import torch x_data = torch.Tensor([[1.0],[2.0],[3.0]]) y_data = torch.Tensor([[2.0],[4.0],[6.0]]) class LinearModel3(torch.nn.Module): def __init__(self): super(LinearModel3,self).__init__() self.linear3 = torch.nn.Linear(1,1) def forward(self,x): return self.linear3(x) model3 = LinearModel3() criterion3 = torch.nn.MSELoss() optimizer3 = torch.optim.Adagrad(model3.parameters(),lr=0.01) for epoch in range(100): y_pred = model3(x_data) loss = criterion3(y_pred,y_data) print(epoch,loss.item()) optimizer3.zero_grad() loss.backward() optimizer3.step() print(\u0026#34;w=\u0026#34;,model3.linear3.weight.item()) print(\u0026#34;b=\u0026#34;,model3.linear3.bias.item()) x_test = torch.Tensor([[4.0]]) y_test = model3(x_test) print(\u0026#34;y_pred=\u0026#34;,y_test.data) import torch x_data_1 = torch.Tensor([[1.0],[2.0],[3.0]]) y_data_1 = torch.Tensor([[2.0],[4.0],[6.0]]) class Linearmodel_1(torch.nn.Module): def __init__(self): super(Linearmodel_1,self).__init__() self.linear = torch.nn.Linear(1,1) def forward(self,x): return self.linear(x) model_1 = Linearmodel_1() criterion_1 = torch.nn.MSELoss() optimizer_1 = torch.optim.SGD(model_1.parameters(),lr=0.01) for epoch in range(1000): y_pred = model(x_data_1) loss = criterion_1(y_pred,y_data_1) print(epoch,loss.item()) optimizer_1.zero_grad() loss.backward() optimizer_1.step() print(\u0026#34;w=\u0026#34;,model_1.linear.weight.item()) print(\u0026#34;b=\u0026#34;,model_1.linear.bias.item()) x_test_1 = torch.Tensor([[4.0]]) y_test_1 = model_1(x_test) print(\u0026#34;pred=\u0026#34;,y_test_1) result:\n0 0.0007593086920678616 1 0.0007593086920678616 2 0.0007593086920678616 3 0.0007593086920678616 4 0.0007593086920678616 5 0.0007593086920678616 6 0.0007593086920678616 7 0.0007593086920678616 8 0.0007593086920678616 9 0.0007593086920678616 10 0.0007593086920678616 11 0.0007593086920678616 12 0.0007593086920678616 13 0.0007593086920678616 14 0.0007593086920678616 15 0.0007593086920678616 16 0.0007593086920678616 17 0.0007593086920678616 18 0.0007593086920678616 19 0.0007593086920678616 20 0.0007593086920678616 21 0.0007593086920678616 22 0.0007593086920678616 23 0.0007593086920678616 24 0.0007593086920678616 25 0.0007593086920678616 26 0.0007593086920678616 27 0.0007593086920678616 28 0.0007593086920678616 29 0.0007593086920678616 30 0.0007593086920678616 31 0.0007593086920678616 32 0.0007593086920678616 33 0.0007593086920678616 34 0.0007593086920678616 35 0.0007593086920678616 36 0.0007593086920678616 37 0.0007593086920678616 38 0.0007593086920678616 39 0.0007593086920678616 40 0.0007593086920678616 41 0.0007593086920678616 42 0.0007593086920678616 43 0.0007593086920678616 44 0.0007593086920678616 45 0.0007593086920678616 46 0.0007593086920678616 47 0.0007593086920678616 48 0.0007593086920678616 49 0.0007593086920678616 50 0.0007593086920678616 51 0.0007593086920678616 52 0.0007593086920678616 53 0.0007593086920678616 54 0.0007593086920678616 55 0.0007593086920678616 56 0.0007593086920678616 57 0.0007593086920678616 58 0.0007593086920678616 59 0.0007593086920678616 60 0.0007593086920678616 61 0.0007593086920678616 62 0.0007593086920678616 63 0.0007593086920678616 64 0.0007593086920678616 65 0.0007593086920678616 66 0.0007593086920678616 67 0.0007593086920678616 68 0.0007593086920678616 69 0.0007593086920678616 70 0.0007593086920678616 71 0.0007593086920678616 72 0.0007593086920678616 73 0.0007593086920678616 74 0.0007593086920678616 75 0.0007593086920678616 76 0.0007593086920678616 77 0.0007593086920678616 78 0.0007593086920678616 79 0.0007593086920678616 80 0.0007593086920678616 81 0.0007593086920678616 82 0.0007593086920678616 83 0.0007593086920678616 84 0.0007593086920678616 85 0.0007593086920678616 86 0.0007593086920678616 87 0.0007593086920678616 88 0.0007593086920678616 89 0.0007593086920678616 90 0.0007593086920678616 91 0.0007593086920678616 92 0.0007593086920678616 93 0.0007593086920678616 94 0.0007593086920678616 95 0.0007593086920678616 96 0.0007593086920678616 97 0.0007593086920678616 98 0.0007593086920678616 99 0.0007593086920678616 100 0.0007593086920678616 101 0.0007593086920678616 102 0.0007593086920678616 103 0.0007593086920678616 104 0.0007593086920678616 105 0.0007593086920678616 106 0.0007593086920678616 107 0.0007593086920678616 108 0.0007593086920678616 109 0.0007593086920678616 110 0.0007593086920678616 111 0.0007593086920678616 112 0.0007593086920678616 113 0.0007593086920678616 114 0.0007593086920678616 115 0.0007593086920678616 116 0.0007593086920678616 117 0.0007593086920678616 118 0.0007593086920678616 119 0.0007593086920678616 120 0.0007593086920678616 121 0.0007593086920678616 122 0.0007593086920678616 123 0.0007593086920678616 124 0.0007593086920678616 125 0.0007593086920678616 126 0.0007593086920678616 127 0.0007593086920678616 128 0.0007593086920678616 129 0.0007593086920678616 130 0.0007593086920678616 131 0.0007593086920678616 132 0.0007593086920678616 133 0.0007593086920678616 134 0.0007593086920678616 135 0.0007593086920678616 136 0.0007593086920678616 137 0.0007593086920678616 138 0.0007593086920678616 139 0.0007593086920678616 140 0.0007593086920678616 141 0.0007593086920678616 142 0.0007593086920678616 143 0.0007593086920678616 144 0.0007593086920678616 145 0.0007593086920678616 146 0.0007593086920678616 147 0.0007593086920678616 148 0.0007593086920678616 149 0.0007593086920678616 150 0.0007593086920678616 151 0.0007593086920678616 152 0.0007593086920678616 153 0.0007593086920678616 154 0.0007593086920678616 155 0.0007593086920678616 156 0.0007593086920678616 157 0.0007593086920678616 158 0.0007593086920678616 159 0.0007593086920678616 160 0.0007593086920678616 161 0.0007593086920678616 162 0.0007593086920678616 163 0.0007593086920678616 164 0.0007593086920678616 165 0.0007593086920678616 166 0.0007593086920678616 167 0.0007593086920678616 168 0.0007593086920678616 169 0.0007593086920678616 170 0.0007593086920678616 171 0.0007593086920678616 172 0.0007593086920678616 173 0.0007593086920678616 174 0.0007593086920678616 175 0.0007593086920678616 176 0.0007593086920678616 177 0.0007593086920678616 178 0.0007593086920678616 179 0.0007593086920678616 180 0.0007593086920678616 181 0.0007593086920678616 182 0.0007593086920678616 183 0.0007593086920678616 184 0.0007593086920678616 185 0.0007593086920678616 186 0.0007593086920678616 187 0.0007593086920678616 188 0.0007593086920678616 189 0.0007593086920678616 190 0.0007593086920678616 191 0.0007593086920678616 192 0.0007593086920678616 193 0.0007593086920678616 194 0.0007593086920678616 195 0.0007593086920678616 196 0.0007593086920678616 197 0.0007593086920678616 198 0.0007593086920678616 199 0.0007593086920678616 200 0.0007593086920678616 201 0.0007593086920678616 202 0.0007593086920678616 203 0.0007593086920678616 204 0.0007593086920678616 205 0.0007593086920678616 206 0.0007593086920678616 207 0.0007593086920678616 208 0.0007593086920678616 209 0.0007593086920678616 210 0.0007593086920678616 211 0.0007593086920678616 212 0.0007593086920678616 213 0.0007593086920678616 214 0.0007593086920678616 215 0.0007593086920678616 216 0.0007593086920678616 217 0.0007593086920678616 218 0.0007593086920678616 219 0.0007593086920678616 220 0.0007593086920678616 221 0.0007593086920678616 222 0.0007593086920678616 223 0.0007593086920678616 224 0.0007593086920678616 225 0.0007593086920678616 226 0.0007593086920678616 227 0.0007593086920678616 228 0.0007593086920678616 229 0.0007593086920678616 230 0.0007593086920678616 231 0.0007593086920678616 232 0.0007593086920678616 233 0.0007593086920678616 234 0.0007593086920678616 235 0.0007593086920678616 236 0.0007593086920678616 237 0.0007593086920678616 238 0.0007593086920678616 239 0.0007593086920678616 240 0.0007593086920678616 241 0.0007593086920678616 242 0.0007593086920678616 243 0.0007593086920678616 244 0.0007593086920678616 245 0.0007593086920678616 246 0.0007593086920678616 247 0.0007593086920678616 248 0.0007593086920678616 249 0.0007593086920678616 250 0.0007593086920678616 251 0.0007593086920678616 252 0.0007593086920678616 253 0.0007593086920678616 254 0.0007593086920678616 255 0.0007593086920678616 256 0.0007593086920678616 257 0.0007593086920678616 258 0.0007593086920678616 259 0.0007593086920678616 260 0.0007593086920678616 261 0.0007593086920678616 262 0.0007593086920678616 263 0.0007593086920678616 264 0.0007593086920678616 265 0.0007593086920678616 266 0.0007593086920678616 267 0.0007593086920678616 268 0.0007593086920678616 269 0.0007593086920678616 270 0.0007593086920678616 271 0.0007593086920678616 272 0.0007593086920678616 273 0.0007593086920678616 274 0.0007593086920678616 275 0.0007593086920678616 276 0.0007593086920678616 277 0.0007593086920678616 278 0.0007593086920678616 279 0.0007593086920678616 280 0.0007593086920678616 281 0.0007593086920678616 282 0.0007593086920678616 283 0.0007593086920678616 284 0.0007593086920678616 285 0.0007593086920678616 286 0.0007593086920678616 287 0.0007593086920678616 288 0.0007593086920678616 289 0.0007593086920678616 290 0.0007593086920678616 291 0.0007593086920678616 292 0.0007593086920678616 293 0.0007593086920678616 294 0.0007593086920678616 295 0.0007593086920678616 296 0.0007593086920678616 297 0.0007593086920678616 298 0.0007593086920678616 299 0.0007593086920678616 300 0.0007593086920678616 301 0.0007593086920678616 302 0.0007593086920678616 303 0.0007593086920678616 304 0.0007593086920678616 305 0.0007593086920678616 306 0.0007593086920678616 307 0.0007593086920678616 308 0.0007593086920678616 309 0.0007593086920678616 310 0.0007593086920678616 311 0.0007593086920678616 312 0.0007593086920678616 313 0.0007593086920678616 314 0.0007593086920678616 315 0.0007593086920678616 316 0.0007593086920678616 317 0.0007593086920678616 318 0.0007593086920678616 319 0.0007593086920678616 320 0.0007593086920678616 321 0.0007593086920678616 322 0.0007593086920678616 323 0.0007593086920678616 324 0.0007593086920678616 325 0.0007593086920678616 326 0.0007593086920678616 327 0.0007593086920678616 328 0.0007593086920678616 329 0.0007593086920678616 330 0.0007593086920678616 331 0.0007593086920678616 332 0.0007593086920678616 333 0.0007593086920678616 334 0.0007593086920678616 335 0.0007593086920678616 336 0.0007593086920678616 337 0.0007593086920678616 338 0.0007593086920678616 339 0.0007593086920678616 340 0.0007593086920678616 341 0.0007593086920678616 342 0.0007593086920678616 343 0.0007593086920678616 344 0.0007593086920678616 345 0.0007593086920678616 346 0.0007593086920678616 347 0.0007593086920678616 348 0.0007593086920678616 349 0.0007593086920678616 350 0.0007593086920678616 351 0.0007593086920678616 352 0.0007593086920678616 353 0.0007593086920678616 354 0.0007593086920678616 355 0.0007593086920678616 356 0.0007593086920678616 357 0.0007593086920678616 358 0.0007593086920678616 359 0.0007593086920678616 360 0.0007593086920678616 361 0.0007593086920678616 362 0.0007593086920678616 363 0.0007593086920678616 364 0.0007593086920678616 365 0.0007593086920678616 366 0.0007593086920678616 367 0.0007593086920678616 368 0.0007593086920678616 369 0.0007593086920678616 370 0.0007593086920678616 371 0.0007593086920678616 372 0.0007593086920678616 373 0.0007593086920678616 374 0.0007593086920678616 375 0.0007593086920678616 376 0.0007593086920678616 377 0.0007593086920678616 378 0.0007593086920678616 379 0.0007593086920678616 380 0.0007593086920678616 381 0.0007593086920678616 382 0.0007593086920678616 383 0.0007593086920678616 384 0.0007593086920678616 385 0.0007593086920678616 386 0.0007593086920678616 387 0.0007593086920678616 388 0.0007593086920678616 389 0.0007593086920678616 390 0.0007593086920678616 391 0.0007593086920678616 392 0.0007593086920678616 393 0.0007593086920678616 394 0.0007593086920678616 395 0.0007593086920678616 396 0.0007593086920678616 397 0.0007593086920678616 398 0.0007593086920678616 399 0.0007593086920678616 400 0.0007593086920678616 401 0.0007593086920678616 402 0.0007593086920678616 403 0.0007593086920678616 404 0.0007593086920678616 405 0.0007593086920678616 406 0.0007593086920678616 407 0.0007593086920678616 408 0.0007593086920678616 409 0.0007593086920678616 410 0.0007593086920678616 411 0.0007593086920678616 412 0.0007593086920678616 413 0.0007593086920678616 414 0.0007593086920678616 415 0.0007593086920678616 416 0.0007593086920678616 417 0.0007593086920678616 418 0.0007593086920678616 419 0.0007593086920678616 420 0.0007593086920678616 421 0.0007593086920678616 422 0.0007593086920678616 423 0.0007593086920678616 424 0.0007593086920678616 425 0.0007593086920678616 426 0.0007593086920678616 427 0.0007593086920678616 428 0.0007593086920678616 429 0.0007593086920678616 430 0.0007593086920678616 431 0.0007593086920678616 432 0.0007593086920678616 433 0.0007593086920678616 434 0.0007593086920678616 435 0.0007593086920678616 436 0.0007593086920678616 437 0.0007593086920678616 438 0.0007593086920678616 439 0.0007593086920678616 440 0.0007593086920678616 441 0.0007593086920678616 442 0.0007593086920678616 443 0.0007593086920678616 444 0.0007593086920678616 445 0.0007593086920678616 446 0.0007593086920678616 447 0.0007593086920678616 448 0.0007593086920678616 449 0.0007593086920678616 450 0.0007593086920678616 451 0.0007593086920678616 452 0.0007593086920678616 453 0.0007593086920678616 454 0.0007593086920678616 455 0.0007593086920678616 456 0.0007593086920678616 457 0.0007593086920678616 458 0.0007593086920678616 459 0.0007593086920678616 460 0.0007593086920678616 461 0.0007593086920678616 462 0.0007593086920678616 463 0.0007593086920678616 464 0.0007593086920678616 465 0.0007593086920678616 466 0.0007593086920678616 467 0.0007593086920678616 468 0.0007593086920678616 469 0.0007593086920678616 470 0.0007593086920678616 471 0.0007593086920678616 472 0.0007593086920678616 473 0.0007593086920678616 474 0.0007593086920678616 475 0.0007593086920678616 476 0.0007593086920678616 477 0.0007593086920678616 478 0.0007593086920678616 479 0.0007593086920678616 480 0.0007593086920678616 481 0.0007593086920678616 482 0.0007593086920678616 483 0.0007593086920678616 484 0.0007593086920678616 485 0.0007593086920678616 486 0.0007593086920678616 487 0.0007593086920678616 488 0.0007593086920678616 489 0.0007593086920678616 490 0.0007593086920678616 491 0.0007593086920678616 492 0.0007593086920678616 493 0.0007593086920678616 494 0.0007593086920678616 495 0.0007593086920678616 496 0.0007593086920678616 497 0.0007593086920678616 498 0.0007593086920678616 499 0.0007593086920678616 500 0.0007593086920678616 501 0.0007593086920678616 502 0.0007593086920678616 503 0.0007593086920678616 504 0.0007593086920678616 505 0.0007593086920678616 506 0.0007593086920678616 507 0.0007593086920678616 508 0.0007593086920678616 509 0.0007593086920678616 510 0.0007593086920678616 511 0.0007593086920678616 512 0.0007593086920678616 513 0.0007593086920678616 514 0.0007593086920678616 515 0.0007593086920678616 516 0.0007593086920678616 517 0.0007593086920678616 518 0.0007593086920678616 519 0.0007593086920678616 520 0.0007593086920678616 521 0.0007593086920678616 522 0.0007593086920678616 523 0.0007593086920678616 524 0.0007593086920678616 525 0.0007593086920678616 526 0.0007593086920678616 527 0.0007593086920678616 528 0.0007593086920678616 529 0.0007593086920678616 530 0.0007593086920678616 531 0.0007593086920678616 532 0.0007593086920678616 533 0.0007593086920678616 534 0.0007593086920678616 535 0.0007593086920678616 536 0.0007593086920678616 537 0.0007593086920678616 538 0.0007593086920678616 539 0.0007593086920678616 540 0.0007593086920678616 541 0.0007593086920678616 542 0.0007593086920678616 543 0.0007593086920678616 544 0.0007593086920678616 545 0.0007593086920678616 546 0.0007593086920678616 547 0.0007593086920678616 548 0.0007593086920678616 549 0.0007593086920678616 550 0.0007593086920678616 551 0.0007593086920678616 552 0.0007593086920678616 553 0.0007593086920678616 554 0.0007593086920678616 555 0.0007593086920678616 556 0.0007593086920678616 557 0.0007593086920678616 558 0.0007593086920678616 559 0.0007593086920678616 560 0.0007593086920678616 561 0.0007593086920678616 562 0.0007593086920678616 563 0.0007593086920678616 564 0.0007593086920678616 565 0.0007593086920678616 566 0.0007593086920678616 567 0.0007593086920678616 568 0.0007593086920678616 569 0.0007593086920678616 570 0.0007593086920678616 571 0.0007593086920678616 572 0.0007593086920678616 573 0.0007593086920678616 574 0.0007593086920678616 575 0.0007593086920678616 576 0.0007593086920678616 577 0.0007593086920678616 578 0.0007593086920678616 579 0.0007593086920678616 580 0.0007593086920678616 581 0.0007593086920678616 582 0.0007593086920678616 583 0.0007593086920678616 584 0.0007593086920678616 585 0.0007593086920678616 586 0.0007593086920678616 587 0.0007593086920678616 588 0.0007593086920678616 589 0.0007593086920678616 590 0.0007593086920678616 591 0.0007593086920678616 592 0.0007593086920678616 593 0.0007593086920678616 594 0.0007593086920678616 595 0.0007593086920678616 596 0.0007593086920678616 597 0.0007593086920678616 598 0.0007593086920678616 599 0.0007593086920678616 600 0.0007593086920678616 601 0.0007593086920678616 602 0.0007593086920678616 603 0.0007593086920678616 604 0.0007593086920678616 605 0.0007593086920678616 606 0.0007593086920678616 607 0.0007593086920678616 608 0.0007593086920678616 609 0.0007593086920678616 610 0.0007593086920678616 611 0.0007593086920678616 612 0.0007593086920678616 613 0.0007593086920678616 614 0.0007593086920678616 615 0.0007593086920678616 616 0.0007593086920678616 617 0.0007593086920678616 618 0.0007593086920678616 619 0.0007593086920678616 620 0.0007593086920678616 621 0.0007593086920678616 622 0.0007593086920678616 623 0.0007593086920678616 624 0.0007593086920678616 625 0.0007593086920678616 626 0.0007593086920678616 627 0.0007593086920678616 628 0.0007593086920678616 629 0.0007593086920678616 630 0.0007593086920678616 631 0.0007593086920678616 632 0.0007593086920678616 633 0.0007593086920678616 634 0.0007593086920678616 635 0.0007593086920678616 636 0.0007593086920678616 637 0.0007593086920678616 638 0.0007593086920678616 639 0.0007593086920678616 640 0.0007593086920678616 641 0.0007593086920678616 642 0.0007593086920678616 643 0.0007593086920678616 644 0.0007593086920678616 645 0.0007593086920678616 646 0.0007593086920678616 647 0.0007593086920678616 648 0.0007593086920678616 649 0.0007593086920678616 650 0.0007593086920678616 651 0.0007593086920678616 652 0.0007593086920678616 653 0.0007593086920678616 654 0.0007593086920678616 655 0.0007593086920678616 656 0.0007593086920678616 657 0.0007593086920678616 658 0.0007593086920678616 659 0.0007593086920678616 660 0.0007593086920678616 661 0.0007593086920678616 662 0.0007593086920678616 663 0.0007593086920678616 664 0.0007593086920678616 665 0.0007593086920678616 666 0.0007593086920678616 667 0.0007593086920678616 668 0.0007593086920678616 669 0.0007593086920678616 670 0.0007593086920678616 671 0.0007593086920678616 672 0.0007593086920678616 673 0.0007593086920678616 674 0.0007593086920678616 675 0.0007593086920678616 676 0.0007593086920678616 677 0.0007593086920678616 678 0.0007593086920678616 679 0.0007593086920678616 680 0.0007593086920678616 681 0.0007593086920678616 682 0.0007593086920678616 683 0.0007593086920678616 684 0.0007593086920678616 685 0.0007593086920678616 686 0.0007593086920678616 687 0.0007593086920678616 688 0.0007593086920678616 689 0.0007593086920678616 690 0.0007593086920678616 691 0.0007593086920678616 692 0.0007593086920678616 693 0.0007593086920678616 694 0.0007593086920678616 695 0.0007593086920678616 696 0.0007593086920678616 697 0.0007593086920678616 698 0.0007593086920678616 699 0.0007593086920678616 700 0.0007593086920678616 701 0.0007593086920678616 702 0.0007593086920678616 703 0.0007593086920678616 704 0.0007593086920678616 705 0.0007593086920678616 706 0.0007593086920678616 707 0.0007593086920678616 708 0.0007593086920678616 709 0.0007593086920678616 710 0.0007593086920678616 711 0.0007593086920678616 712 0.0007593086920678616 713 0.0007593086920678616 714 0.0007593086920678616 715 0.0007593086920678616 716 0.0007593086920678616 717 0.0007593086920678616 718 0.0007593086920678616 719 0.0007593086920678616 720 0.0007593086920678616 721 0.0007593086920678616 722 0.0007593086920678616 723 0.0007593086920678616 724 0.0007593086920678616 725 0.0007593086920678616 726 0.0007593086920678616 727 0.0007593086920678616 728 0.0007593086920678616 729 0.0007593086920678616 730 0.0007593086920678616 731 0.0007593086920678616 732 0.0007593086920678616 733 0.0007593086920678616 734 0.0007593086920678616 735 0.0007593086920678616 736 0.0007593086920678616 737 0.0007593086920678616 738 0.0007593086920678616 739 0.0007593086920678616 740 0.0007593086920678616 741 0.0007593086920678616 742 0.0007593086920678616 743 0.0007593086920678616 744 0.0007593086920678616 745 0.0007593086920678616 746 0.0007593086920678616 747 0.0007593086920678616 748 0.0007593086920678616 749 0.0007593086920678616 750 0.0007593086920678616 751 0.0007593086920678616 752 0.0007593086920678616 753 0.0007593086920678616 754 0.0007593086920678616 755 0.0007593086920678616 756 0.0007593086920678616 757 0.0007593086920678616 758 0.0007593086920678616 759 0.0007593086920678616 760 0.0007593086920678616 761 0.0007593086920678616 762 0.0007593086920678616 763 0.0007593086920678616 764 0.0007593086920678616 765 0.0007593086920678616 766 0.0007593086920678616 767 0.0007593086920678616 768 0.0007593086920678616 769 0.0007593086920678616 770 0.0007593086920678616 771 0.0007593086920678616 772 0.0007593086920678616 773 0.0007593086920678616 774 0.0007593086920678616 775 0.0007593086920678616 776 0.0007593086920678616 777 0.0007593086920678616 778 0.0007593086920678616 779 0.0007593086920678616 780 0.0007593086920678616 781 0.0007593086920678616 782 0.0007593086920678616 783 0.0007593086920678616 784 0.0007593086920678616 785 0.0007593086920678616 786 0.0007593086920678616 787 0.0007593086920678616 788 0.0007593086920678616 789 0.0007593086920678616 790 0.0007593086920678616 791 0.0007593086920678616 792 0.0007593086920678616 793 0.0007593086920678616 794 0.0007593086920678616 795 0.0007593086920678616 796 0.0007593086920678616 797 0.0007593086920678616 798 0.0007593086920678616 799 0.0007593086920678616 800 0.0007593086920678616 801 0.0007593086920678616 802 0.0007593086920678616 803 0.0007593086920678616 804 0.0007593086920678616 805 0.0007593086920678616 806 0.0007593086920678616 807 0.0007593086920678616 808 0.0007593086920678616 809 0.0007593086920678616 810 0.0007593086920678616 811 0.0007593086920678616 812 0.0007593086920678616 813 0.0007593086920678616 814 0.0007593086920678616 815 0.0007593086920678616 816 0.0007593086920678616 817 0.0007593086920678616 818 0.0007593086920678616 819 0.0007593086920678616 820 0.0007593086920678616 821 0.0007593086920678616 822 0.0007593086920678616 823 0.0007593086920678616 824 0.0007593086920678616 825 0.0007593086920678616 826 0.0007593086920678616 827 0.0007593086920678616 828 0.0007593086920678616 829 0.0007593086920678616 830 0.0007593086920678616 831 0.0007593086920678616 832 0.0007593086920678616 833 0.0007593086920678616 834 0.0007593086920678616 835 0.0007593086920678616 836 0.0007593086920678616 837 0.0007593086920678616 838 0.0007593086920678616 839 0.0007593086920678616 840 0.0007593086920678616 841 0.0007593086920678616 842 0.0007593086920678616 843 0.0007593086920678616 844 0.0007593086920678616 845 0.0007593086920678616 846 0.0007593086920678616 847 0.0007593086920678616 848 0.0007593086920678616 849 0.0007593086920678616 850 0.0007593086920678616 851 0.0007593086920678616 852 0.0007593086920678616 853 0.0007593086920678616 854 0.0007593086920678616 855 0.0007593086920678616 856 0.0007593086920678616 857 0.0007593086920678616 858 0.0007593086920678616 859 0.0007593086920678616 860 0.0007593086920678616 861 0.0007593086920678616 862 0.0007593086920678616 863 0.0007593086920678616 864 0.0007593086920678616 865 0.0007593086920678616 866 0.0007593086920678616 867 0.0007593086920678616 868 0.0007593086920678616 869 0.0007593086920678616 870 0.0007593086920678616 871 0.0007593086920678616 872 0.0007593086920678616 873 0.0007593086920678616 874 0.0007593086920678616 875 0.0007593086920678616 876 0.0007593086920678616 877 0.0007593086920678616 878 0.0007593086920678616 879 0.0007593086920678616 880 0.0007593086920678616 881 0.0007593086920678616 882 0.0007593086920678616 883 0.0007593086920678616 884 0.0007593086920678616 885 0.0007593086920678616 886 0.0007593086920678616 887 0.0007593086920678616 888 0.0007593086920678616 889 0.0007593086920678616 890 0.0007593086920678616 891 0.0007593086920678616 892 0.0007593086920678616 893 0.0007593086920678616 894 0.0007593086920678616 895 0.0007593086920678616 896 0.0007593086920678616 897 0.0007593086920678616 898 0.0007593086920678616 899 0.0007593086920678616 900 0.0007593086920678616 901 0.0007593086920678616 902 0.0007593086920678616 903 0.0007593086920678616 904 0.0007593086920678616 905 0.0007593086920678616 906 0.0007593086920678616 907 0.0007593086920678616 908 0.0007593086920678616 909 0.0007593086920678616 910 0.0007593086920678616 911 0.0007593086920678616 912 0.0007593086920678616 913 0.0007593086920678616 914 0.0007593086920678616 915 0.0007593086920678616 916 0.0007593086920678616 917 0.0007593086920678616 918 0.0007593086920678616 919 0.0007593086920678616 920 0.0007593086920678616 921 0.0007593086920678616 922 0.0007593086920678616 923 0.0007593086920678616 924 0.0007593086920678616 925 0.0007593086920678616 926 0.0007593086920678616 927 0.0007593086920678616 928 0.0007593086920678616 929 0.0007593086920678616 930 0.0007593086920678616 931 0.0007593086920678616 932 0.0007593086920678616 933 0.0007593086920678616 934 0.0007593086920678616 935 0.0007593086920678616 936 0.0007593086920678616 937 0.0007593086920678616 938 0.0007593086920678616 939 0.0007593086920678616 940 0.0007593086920678616 941 0.0007593086920678616 942 0.0007593086920678616 943 0.0007593086920678616 944 0.0007593086920678616 945 0.0007593086920678616 946 0.0007593086920678616 947 0.0007593086920678616 948 0.0007593086920678616 949 0.0007593086920678616 950 0.0007593086920678616 951 0.0007593086920678616 952 0.0007593086920678616 953 0.0007593086920678616 954 0.0007593086920678616 955 0.0007593086920678616 956 0.0007593086920678616 957 0.0007593086920678616 958 0.0007593086920678616 959 0.0007593086920678616 960 0.0007593086920678616 961 0.0007593086920678616 962 0.0007593086920678616 963 0.0007593086920678616 964 0.0007593086920678616 965 0.0007593086920678616 966 0.0007593086920678616 967 0.0007593086920678616 968 0.0007593086920678616 969 0.0007593086920678616 970 0.0007593086920678616 971 0.0007593086920678616 972 0.0007593086920678616 973 0.0007593086920678616 974 0.0007593086920678616 975 0.0007593086920678616 976 0.0007593086920678616 977 0.0007593086920678616 978 0.0007593086920678616 979 0.0007593086920678616 980 0.0007593086920678616 981 0.0007593086920678616 982 0.0007593086920678616 983 0.0007593086920678616 984 0.0007593086920678616 985 0.0007593086920678616 986 0.0007593086920678616 987 0.0007593086920678616 988 0.0007593086920678616 989 0.0007593086920678616 990 0.0007593086920678616 991 0.0007593086920678616 992 0.0007593086920678616 993 0.0007593086920678616 994 0.0007593086920678616 995 0.0007593086920678616 996 0.0007593086920678616 997 0.0007593086920678616 998 0.0007593086920678616 999 0.0007593086920678616 w= -0.022264838218688965 b= 0.8211911916732788 pred= tensor([[-2.4646]], grad_fn=\u0026lt;AddmmBackward0\u0026gt;) ","date":"2022-02-19T00:00:00Z","permalink":"https://example.com/p/pytorch%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B05%E7%94%A8pytorch%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","title":"pytorch基本模型05用PyTorch实现线性回归"},{"content":" 这是学习pytorch基本使用的记录（我记得看的是B站刘二老师的视频）\ncode\nimport torch x_data = torch.Tensor([[1.0],[2.0],[3.0]]) y_data = torch.Tensor([[0],[1],[0]]) class LogisticRegressionModel(torch.nn.Module): def __init__(self): super(LogisticRegressionModel,self).__init__() self.linear = torch.nn.Linear(1,1) def forward(self,x): #加了个激活函数 F.sigmoid() y_pred = torch.sigmoid(self.linear(x)) return y_pred model = LogisticRegressionModel() criterion = torch.nn.BCELoss() optimizer = torch.optim.SGD(model.parameters(),lr=0.01) for epoch in range(100): y_pred = model(x_data) loss = criterion(y_pred,y_data) print(epoch,loss.item()) optimizer.zero_grad() loss.backward() optimizer.step() print(\u0026#34;w=\u0026#34;,model.linear.weight.item()) print(\u0026#34;b=\u0026#34;,model.linear.bias.item()) result:\n0 0.6860817074775696 1 0.6860291957855225 2 0.6859772801399231 3 0.6859259605407715 4 0.6858751177787781 5 0.6858246922492981 6 0.6857749819755554 7 0.6857255101203918 8 0.6856767535209656 9 0.6856281757354736 10 0.6855802536010742 11 0.6855327486991882 12 0.6854856610298157 13 0.6854391098022461 14 0.6853928565979004 15 0.6853470802307129 16 0.6853017210960388 17 0.6852567195892334 18 0.6852121353149414 19 0.6851678490638733 20 0.6851240992546082 21 0.6850807070732117 22 0.6850376129150391 23 0.6849947571754456 24 0.6849524974822998 25 0.6849103569984436 26 0.6848687529563904 27 0.6848273277282715 28 0.684786319732666 29 0.6847454905509949 30 0.6847050786018372 31 0.6846650242805481 32 0.6846251487731934 33 0.6845856308937073 34 0.6845464706420898 35 0.684507429599762 36 0.6844688057899475 37 0.6844303607940674 38 0.6843922734260559 39 0.6843543648719788 40 0.6843166947364807 41 0.6842794418334961 42 0.6842422485351562 43 0.6842053532600403 44 0.6841687560081482 45 0.6841323375701904 46 0.6840961575508118 47 0.6840601563453674 48 0.6840245127677917 49 0.6839889883995056 50 0.683953583240509 51 0.6839186549186707 52 0.6838836669921875 53 0.6838489174842834 54 0.6838144659996033 55 0.6837801933288574 56 0.6837460398674011 57 0.6837120652198792 58 0.6836783289909363 59 0.6836447715759277 60 0.6836113929748535 61 0.6835780739784241 62 0.6835450530052185 63 0.6835122108459473 64 0.6834794878959656 65 0.6834468245506287 66 0.6834144592285156 67 0.6833822131156921 68 0.6833500266075134 69 0.6833180785179138 70 0.683286190032959 71 0.6832544803619385 72 0.6832229495048523 73 0.6831915974617004 74 0.6831602454185486 75 0.6831291317939758 76 0.6830981373786926 77 0.683067262172699 78 0.6830365061759949 79 0.6830058097839355 80 0.6829752922058105 81 0.6829448342323303 82 0.6829145550727844 83 0.6828843951225281 84 0.6828543543815613 85 0.6828243732452393 86 0.6827945709228516 87 0.6827647686004639 88 0.6827352046966553 89 0.6827056407928467 90 0.6826761364936829 91 0.6826467514038086 92 0.6826176047325134 93 0.6825883984565735 94 0.6825593113899231 95 0.682530403137207 96 0.6825015544891357 97 0.6824727058410645 98 0.6824440360069275 99 0.6824154257774353 w= -0.7670175433158875 b= 0.9575299620628357 code\nimport numpy as np import matplotlib.pyplot as plt x = np.linspace(0,10,200) x_t = torch.Tensor(x).view((200,1)) y_t = model(x_t) #data.numpy 可以将Tensor的数据转为numpy y = y_t.data.numpy() plt.plot(x,y) plt.plot([0,10],[0.5,0.5],c=\u0026#34;r\u0026#34;) plt.grid() plt.show() result: ","date":"2022-02-19T00:00:00Z","permalink":"https://example.com/p/pytorch%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B06._logistics%E5%9B%9E%E5%BD%92/","title":"pytorch基本模型06._logistics回归"},{"content":" 这是学习pytorch基本使用的记录（我记得看的是B站刘二老师的视频）\nTensor对象包含了data和grad import torch x_data = [1.0, 2.0, 3.0] y_data = [2.0, 4.0, 6.0] #记得用[]装起来 w = torch.Tensor([1.0]) w.requires_grad = True #参数requires_grad = True/False 是用来设置是否计算梯度（求导）,默认是True #定义一个线性的计算单元 def forward(x): return x*w #定义损失函数 def loss(x,y): y_pred = forward(x) return (y_pred - y) ** 2 #forward(4).item 注意这里是将w（tensor）中的值取出，由于它是tensor对象，所以用item获取 print(\u0026#34;predict (before training)\u0026#34;,4,forward(4).item) for epoch in range(100): for x,y in zip(x_data,y_data): #这里的loss就会自动构建一个计算图(正向)了 l = loss(x,y) #用tenor本身有的backward l.backward() #要这样取出w.grad.item()权重导数的值 print(\u0026#34;\\tgrad:\u0026#34;,x,y,w.grad.item()) #SGB w.data = w.data -0.01*w.grad.data #记得每次循环将此轮反向传播的梯度清零 要不然会累加 w.grad.data.zero_() # l.item()取出这次epoch的损失值 print(\u0026#34;progress:\u0026#34;,epoch,l.item()) print(\u0026#34;predict (after training)\u0026#34;,4,forward(4).item()) result:\npredict (before training) 4 \u0026lt;built-in method item of Tensor object at 0x00000253A7978400\u0026gt; grad: 1.0 2.0 -2.0 grad: 2.0 4.0 -7.840000152587891 grad: 3.0 6.0 -16.228801727294922 progress: 0 7.315943717956543 grad: 1.0 2.0 -1.478623867034912 grad: 2.0 4.0 -5.796205520629883 grad: 3.0 6.0 -11.998146057128906 progress: 1 3.9987640380859375 grad: 1.0 2.0 -1.0931644439697266 grad: 2.0 4.0 -4.285204887390137 grad: 3.0 6.0 -8.870372772216797 progress: 2 2.1856532096862793 grad: 1.0 2.0 -0.8081896305084229 grad: 2.0 4.0 -3.1681032180786133 grad: 3.0 6.0 -6.557973861694336 progress: 3 1.1946394443511963 grad: 1.0 2.0 -0.5975041389465332 grad: 2.0 4.0 -2.3422164916992188 grad: 3.0 6.0 -4.848389625549316 progress: 4 0.6529689431190491 grad: 1.0 2.0 -0.4417421817779541 grad: 2.0 4.0 -1.7316293716430664 grad: 3.0 6.0 -3.58447265625 progress: 5 0.35690122842788696 grad: 1.0 2.0 -0.3265852928161621 grad: 2.0 4.0 -1.2802143096923828 grad: 3.0 6.0 -2.650045394897461 progress: 6 0.195076122879982 grad: 1.0 2.0 -0.24144840240478516 grad: 2.0 4.0 -0.9464778900146484 grad: 3.0 6.0 -1.9592113494873047 progress: 7 0.10662525147199631 grad: 1.0 2.0 -0.17850565910339355 grad: 2.0 4.0 -0.699742317199707 grad: 3.0 6.0 -1.4484672546386719 progress: 8 0.0582793727517128 grad: 1.0 2.0 -0.1319713592529297 grad: 2.0 4.0 -0.5173273086547852 grad: 3.0 6.0 -1.070866584777832 progress: 9 0.03185431286692619 grad: 1.0 2.0 -0.09756779670715332 grad: 2.0 4.0 -0.3824653625488281 grad: 3.0 6.0 -0.7917022705078125 progress: 10 0.017410902306437492 grad: 1.0 2.0 -0.07213282585144043 grad: 2.0 4.0 -0.2827606201171875 grad: 3.0 6.0 -0.5853137969970703 progress: 11 0.009516451507806778 grad: 1.0 2.0 -0.053328514099121094 grad: 2.0 4.0 -0.2090473175048828 grad: 3.0 6.0 -0.43272972106933594 progress: 12 0.005201528314501047 grad: 1.0 2.0 -0.039426326751708984 grad: 2.0 4.0 -0.15455150604248047 grad: 3.0 6.0 -0.3199195861816406 progress: 13 0.0028430151287466288 grad: 1.0 2.0 -0.029148340225219727 grad: 2.0 4.0 -0.11426162719726562 grad: 3.0 6.0 -0.23652076721191406 progress: 14 0.0015539465239271522 grad: 1.0 2.0 -0.021549701690673828 grad: 2.0 4.0 -0.08447456359863281 grad: 3.0 6.0 -0.17486286163330078 progress: 15 0.0008493617060594261 grad: 1.0 2.0 -0.01593184471130371 grad: 2.0 4.0 -0.062453269958496094 grad: 3.0 6.0 -0.12927818298339844 progress: 16 0.00046424579340964556 grad: 1.0 2.0 -0.011778593063354492 grad: 2.0 4.0 -0.046172142028808594 grad: 3.0 6.0 -0.09557533264160156 progress: 17 0.0002537401160225272 grad: 1.0 2.0 -0.00870823860168457 grad: 2.0 4.0 -0.03413581848144531 grad: 3.0 6.0 -0.07066154479980469 progress: 18 0.00013869594840798527 grad: 1.0 2.0 -0.006437778472900391 grad: 2.0 4.0 -0.025236129760742188 grad: 3.0 6.0 -0.052239418029785156 progress: 19 7.580435340059921e-05 grad: 1.0 2.0 -0.004759550094604492 grad: 2.0 4.0 -0.018657684326171875 grad: 3.0 6.0 -0.038620948791503906 progress: 20 4.143271507928148e-05 grad: 1.0 2.0 -0.003518819808959961 grad: 2.0 4.0 -0.0137939453125 grad: 3.0 6.0 -0.028553009033203125 progress: 21 2.264650902361609e-05 grad: 1.0 2.0 -0.00260162353515625 grad: 2.0 4.0 -0.010198593139648438 grad: 3.0 6.0 -0.021108627319335938 progress: 22 1.2377059647405986e-05 grad: 1.0 2.0 -0.0019233226776123047 grad: 2.0 4.0 -0.0075397491455078125 grad: 3.0 6.0 -0.0156097412109375 progress: 23 6.768445018678904e-06 grad: 1.0 2.0 -0.0014221668243408203 grad: 2.0 4.0 -0.0055751800537109375 grad: 3.0 6.0 -0.011541366577148438 progress: 24 3.7000872907810844e-06 grad: 1.0 2.0 -0.0010514259338378906 grad: 2.0 4.0 -0.0041217803955078125 grad: 3.0 6.0 -0.008531570434570312 progress: 25 2.021880391112063e-06 grad: 1.0 2.0 -0.0007772445678710938 grad: 2.0 4.0 -0.0030469894409179688 grad: 3.0 6.0 -0.006305694580078125 progress: 26 1.1044940038118511e-06 grad: 1.0 2.0 -0.0005745887756347656 grad: 2.0 4.0 -0.0022525787353515625 grad: 3.0 6.0 -0.0046634674072265625 progress: 27 6.041091182851233e-07 grad: 1.0 2.0 -0.0004248619079589844 grad: 2.0 4.0 -0.0016651153564453125 grad: 3.0 6.0 -0.003444671630859375 progress: 28 3.296045179013163e-07 grad: 1.0 2.0 -0.0003139972686767578 grad: 2.0 4.0 -0.0012311935424804688 grad: 3.0 6.0 -0.0025491714477539062 progress: 29 1.805076408345485e-07 grad: 1.0 2.0 -0.00023221969604492188 grad: 2.0 4.0 -0.0009107589721679688 grad: 3.0 6.0 -0.0018854141235351562 progress: 30 9.874406714516226e-08 grad: 1.0 2.0 -0.00017189979553222656 grad: 2.0 4.0 -0.0006742477416992188 grad: 3.0 6.0 -0.00139617919921875 progress: 31 5.4147676564753056e-08 grad: 1.0 2.0 -0.0001270771026611328 grad: 2.0 4.0 -0.0004978179931640625 grad: 3.0 6.0 -0.00102996826171875 progress: 32 2.9467628337442875e-08 grad: 1.0 2.0 -9.393692016601562e-05 grad: 2.0 4.0 -0.0003681182861328125 grad: 3.0 6.0 -0.0007610321044921875 progress: 33 1.6088051779661328e-08 grad: 1.0 2.0 -6.937980651855469e-05 grad: 2.0 4.0 -0.00027179718017578125 grad: 3.0 6.0 -0.000560760498046875 progress: 34 8.734787115827203e-09 grad: 1.0 2.0 -5.125999450683594e-05 grad: 2.0 4.0 -0.00020122528076171875 grad: 3.0 6.0 -0.0004177093505859375 progress: 35 4.8466972657479346e-09 grad: 1.0 2.0 -3.790855407714844e-05 grad: 2.0 4.0 -0.000148773193359375 grad: 3.0 6.0 -0.000308990478515625 progress: 36 2.6520865503698587e-09 grad: 1.0 2.0 -2.8133392333984375e-05 grad: 2.0 4.0 -0.000110626220703125 grad: 3.0 6.0 -0.0002288818359375 progress: 37 1.4551915228366852e-09 grad: 1.0 2.0 -2.09808349609375e-05 grad: 2.0 4.0 -8.20159912109375e-05 grad: 3.0 6.0 -0.00016880035400390625 progress: 38 7.914877642178908e-10 grad: 1.0 2.0 -1.5497207641601562e-05 grad: 2.0 4.0 -6.103515625e-05 grad: 3.0 6.0 -0.000125885009765625 progress: 39 4.4019543565809727e-10 grad: 1.0 2.0 -1.1444091796875e-05 grad: 2.0 4.0 -4.482269287109375e-05 grad: 3.0 6.0 -9.1552734375e-05 progress: 40 2.3283064365386963e-10 grad: 1.0 2.0 -8.344650268554688e-06 grad: 2.0 4.0 -3.24249267578125e-05 grad: 3.0 6.0 -6.580352783203125e-05 progress: 41 1.2028067430946976e-10 grad: 1.0 2.0 -5.9604644775390625e-06 grad: 2.0 4.0 -2.288818359375e-05 grad: 3.0 6.0 -4.57763671875e-05 progress: 42 5.820766091346741e-11 grad: 1.0 2.0 -4.291534423828125e-06 grad: 2.0 4.0 -1.71661376953125e-05 grad: 3.0 6.0 -3.719329833984375e-05 progress: 43 3.842615114990622e-11 grad: 1.0 2.0 -3.337860107421875e-06 grad: 2.0 4.0 -1.33514404296875e-05 grad: 3.0 6.0 -2.86102294921875e-05 progress: 44 2.2737367544323206e-11 grad: 1.0 2.0 -2.6226043701171875e-06 grad: 2.0 4.0 -1.049041748046875e-05 grad: 3.0 6.0 -2.288818359375e-05 progress: 45 1.4551915228366852e-11 grad: 1.0 2.0 -1.9073486328125e-06 grad: 2.0 4.0 -7.62939453125e-06 grad: 3.0 6.0 -1.430511474609375e-05 progress: 46 5.6843418860808015e-12 grad: 1.0 2.0 -1.430511474609375e-06 grad: 2.0 4.0 -5.7220458984375e-06 grad: 3.0 6.0 -1.1444091796875e-05 progress: 47 3.637978807091713e-12 grad: 1.0 2.0 -1.1920928955078125e-06 grad: 2.0 4.0 -4.76837158203125e-06 grad: 3.0 6.0 -1.1444091796875e-05 progress: 48 3.637978807091713e-12 grad: 1.0 2.0 -9.5367431640625e-07 grad: 2.0 4.0 -3.814697265625e-06 grad: 3.0 6.0 -8.58306884765625e-06 progress: 49 2.0463630789890885e-12 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 50 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 51 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 52 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 53 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 54 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 55 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 56 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 57 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 58 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 59 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 60 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 61 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 62 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 63 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 64 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 65 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 66 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 67 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 68 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 69 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 70 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 71 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 72 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 73 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 74 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 75 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 76 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 77 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 78 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 79 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 80 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 81 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 82 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 83 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 84 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 85 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 86 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 87 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 88 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 89 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 90 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 91 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 92 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 93 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 94 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 95 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 96 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 97 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 98 9.094947017729282e-13 grad: 1.0 2.0 -7.152557373046875e-07 grad: 2.0 4.0 -2.86102294921875e-06 grad: 3.0 6.0 -5.7220458984375e-06 progress: 99 9.094947017729282e-13 predict (after training) 4 7.999998569488525 ","date":"2022-02-15T00:00:00Z","permalink":"https://example.com/p/pytorch%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B04_%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/","title":"pytorch基本模型04_反向传播"},{"content":" ","date":"2022-02-14T00:00:00Z","permalink":"https://example.com/p/%E5%88%86%E5%AD%90%E6%A8%A1%E6%8B%9F%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E8%B7%B5/","title":"《分子模拟的理论与实践》"},{"content":" 这是学习pytorch基本使用的记录（我记得看的是B站刘二老师的视频）\ncode\nimport torch torch.randn(*sizes, out=None) → Tensor\n等同于np.randn返回的是均值0，方差为1的正态分布 的张量\ncode\nx = torch.randn(1,10) x result: code:\nprev_h = torch.randn(1,20) prev_h result: code:\nW_h = torch.randn(20,20) W_h result: W_x = torch.randn(20,10) W_x 查看pytorch版本\nimport torch print(torch.__version__) ","date":"2022-02-13T00:00:00Z","permalink":"https://example.com/p/pytorch%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B01_%E7%89%88%E6%9C%ACdemo/","title":"pytorch基本模型01_版本demo"},{"content":" 这是学习pytorch基本使用的记录（我记得看的是B站刘二老师的视频）\nimport numpy as np import matplotlib.pyplot as plt x_data = [1.0,2.0,3.0] y_data = [2.0,4.0,6.0] #正向传播，预测y def forward(x): return x*w #计算损失 def loss(x,y): y_pred = forward(x) return (y_pred-y) * (y_pred-y) #权重更新存储 w_list = [] #mse均误差cunc mse_list = [] #产生不同的权重，用于计算 左闭右开 for w in np.arange(0.0,4.1,0.1): print(\u0026#34;w=\u0026#34;,w) l_sum = 0 #依次传入每个样本 for x_val,y_val in zip(x_data,y_data): y_pred_val = forward(x_val) #注意定义的loss中本身会计算产生一个正向传播的计算图 loss_val = loss(x_val,y_val) #mse这里是三个样本的损失之和 l_sum+=loss_val #t制表符会使得输出更好看 print(\u0026#34;\\t\u0026#34;,x_val,y_val,y_pred_val,loss_val) print(\u0026#34;MSE=\u0026#34;,l_sum/3) w_list.append(w) mse_list.append(l_sum/3) result\nw= 0.0 1.0 2.0 0.0 4.0 2.0 4.0 0.0 16.0 3.0 6.0 0.0 36.0 MSE= 18.666666666666668 w= 0.1 1.0 2.0 0.1 3.61 2.0 4.0 0.2 14.44 3.0 6.0 0.30000000000000004 32.49 MSE= 16.846666666666668 w= 0.2 1.0 2.0 0.2 3.24 2.0 4.0 0.4 12.96 3.0 6.0 0.6000000000000001 29.160000000000004 MSE= 15.120000000000003 w= 0.30000000000000004 1.0 2.0 0.30000000000000004 2.8899999999999997 2.0 4.0 0.6000000000000001 11.559999999999999 3.0 6.0 0.9000000000000001 26.009999999999998 MSE= 13.486666666666665 w= 0.4 1.0 2.0 0.4 2.5600000000000005 2.0 4.0 0.8 10.240000000000002 3.0 6.0 1.2000000000000002 23.04 MSE= 11.946666666666667 w= 0.5 1.0 2.0 0.5 2.25 2.0 4.0 1.0 9.0 3.0 6.0 1.5 20.25 MSE= 10.5 w= 0.6000000000000001 1.0 2.0 0.6000000000000001 1.9599999999999997 2.0 4.0 1.2000000000000002 7.839999999999999 3.0 6.0 1.8000000000000003 17.639999999999993 MSE= 9.146666666666663 w= 0.7000000000000001 1.0 2.0 0.7000000000000001 1.6899999999999995 2.0 4.0 1.4000000000000001 6.759999999999998 3.0 6.0 2.1 15.209999999999999 MSE= 7.886666666666666 w= 0.8 1.0 2.0 0.8 1.44 2.0 4.0 1.6 5.76 3.0 6.0 2.4000000000000004 12.959999999999997 MSE= 6.719999999999999 w= 0.9 1.0 2.0 0.9 1.2100000000000002 2.0 4.0 1.8 4.840000000000001 3.0 6.0 2.7 10.889999999999999 MSE= 5.646666666666666 w= 1.0 1.0 2.0 1.0 1.0 2.0 4.0 2.0 4.0 3.0 6.0 3.0 9.0 MSE= 4.666666666666667 w= 1.1 1.0 2.0 1.1 0.8099999999999998 2.0 4.0 2.2 3.2399999999999993 3.0 6.0 3.3000000000000003 7.289999999999998 MSE= 3.779999999999999 w= 1.2000000000000002 1.0 2.0 1.2000000000000002 0.6399999999999997 2.0 4.0 2.4000000000000004 2.5599999999999987 3.0 6.0 3.6000000000000005 5.759999999999997 MSE= 2.986666666666665 w= 1.3 1.0 2.0 1.3 0.48999999999999994 2.0 4.0 2.6 1.9599999999999997 3.0 6.0 3.9000000000000004 4.409999999999998 MSE= 2.2866666666666657 w= 1.4000000000000001 1.0 2.0 1.4000000000000001 0.3599999999999998 2.0 4.0 2.8000000000000003 1.4399999999999993 3.0 6.0 4.2 3.2399999999999993 MSE= 1.6799999999999995 w= 1.5 1.0 2.0 1.5 0.25 2.0 4.0 3.0 1.0 3.0 6.0 4.5 2.25 MSE= 1.1666666666666667 w= 1.6 1.0 2.0 1.6 0.15999999999999992 2.0 4.0 3.2 0.6399999999999997 3.0 6.0 4.800000000000001 1.4399999999999984 MSE= 0.746666666666666 w= 1.7000000000000002 1.0 2.0 1.7000000000000002 0.0899999999999999 2.0 4.0 3.4000000000000004 0.3599999999999996 3.0 6.0 5.1000000000000005 0.809999999999999 MSE= 0.4199999999999995 w= 1.8 1.0 2.0 1.8 0.03999999999999998 2.0 4.0 3.6 0.15999999999999992 3.0 6.0 5.4 0.3599999999999996 MSE= 0.1866666666666665 w= 1.9000000000000001 1.0 2.0 1.9000000000000001 0.009999999999999974 2.0 4.0 3.8000000000000003 0.0399999999999999 3.0 6.0 5.7 0.0899999999999999 MSE= 0.046666666666666586 w= 2.0 1.0 2.0 2.0 0.0 2.0 4.0 4.0 0.0 3.0 6.0 6.0 0.0 MSE= 0.0 w= 2.1 1.0 2.0 2.1 0.010000000000000018 2.0 4.0 4.2 0.04000000000000007 3.0 6.0 6.300000000000001 0.09000000000000043 MSE= 0.046666666666666835 w= 2.2 1.0 2.0 2.2 0.04000000000000007 2.0 4.0 4.4 0.16000000000000028 3.0 6.0 6.6000000000000005 0.36000000000000065 MSE= 0.18666666666666698 w= 2.3000000000000003 1.0 2.0 2.3000000000000003 0.09000000000000016 2.0 4.0 4.6000000000000005 0.36000000000000065 3.0 6.0 6.9 0.8100000000000006 MSE= 0.42000000000000054 w= 2.4000000000000004 1.0 2.0 2.4000000000000004 0.16000000000000028 2.0 4.0 4.800000000000001 0.6400000000000011 3.0 6.0 7.200000000000001 1.4400000000000026 MSE= 0.7466666666666679 w= 2.5 1.0 2.0 2.5 0.25 2.0 4.0 5.0 1.0 3.0 6.0 7.5 2.25 MSE= 1.1666666666666667 w= 2.6 1.0 2.0 2.6 0.3600000000000001 2.0 4.0 5.2 1.4400000000000004 3.0 6.0 7.800000000000001 3.2400000000000024 MSE= 1.6800000000000008 w= 2.7 1.0 2.0 2.7 0.49000000000000027 2.0 4.0 5.4 1.960000000000001 3.0 6.0 8.100000000000001 4.410000000000006 MSE= 2.2866666666666693 w= 2.8000000000000003 1.0 2.0 2.8000000000000003 0.6400000000000005 2.0 4.0 5.6000000000000005 2.560000000000002 3.0 6.0 8.4 5.760000000000002 MSE= 2.986666666666668 w= 2.9000000000000004 1.0 2.0 2.9000000000000004 0.8100000000000006 2.0 4.0 5.800000000000001 3.2400000000000024 3.0 6.0 8.700000000000001 7.290000000000005 MSE= 3.780000000000003 w= 3.0 1.0 2.0 3.0 1.0 2.0 4.0 6.0 4.0 3.0 6.0 9.0 9.0 MSE= 4.666666666666667 w= 3.1 1.0 2.0 3.1 1.2100000000000002 2.0 4.0 6.2 4.840000000000001 3.0 6.0 9.3 10.890000000000004 MSE= 5.646666666666668 w= 3.2 1.0 2.0 3.2 1.4400000000000004 2.0 4.0 6.4 5.760000000000002 3.0 6.0 9.600000000000001 12.96000000000001 MSE= 6.720000000000003 w= 3.3000000000000003 1.0 2.0 3.3000000000000003 1.6900000000000006 2.0 4.0 6.6000000000000005 6.7600000000000025 3.0 6.0 9.9 15.210000000000003 MSE= 7.886666666666668 w= 3.4000000000000004 1.0 2.0 3.4000000000000004 1.960000000000001 2.0 4.0 6.800000000000001 7.840000000000004 3.0 6.0 10.200000000000001 17.640000000000008 MSE= 9.14666666666667 w= 3.5 1.0 2.0 3.5 2.25 2.0 4.0 7.0 9.0 3.0 6.0 10.5 20.25 MSE= 10.5 w= 3.6 1.0 2.0 3.6 2.5600000000000005 2.0 4.0 7.2 10.240000000000002 3.0 6.0 10.8 23.040000000000006 MSE= 11.94666666666667 w= 3.7 1.0 2.0 3.7 2.8900000000000006 2.0 4.0 7.4 11.560000000000002 3.0 6.0 11.100000000000001 26.010000000000016 MSE= 13.486666666666673 w= 3.8000000000000003 1.0 2.0 3.8000000000000003 3.240000000000001 2.0 4.0 7.6000000000000005 12.960000000000004 3.0 6.0 11.4 29.160000000000004 MSE= 15.120000000000005 w= 3.9000000000000004 1.0 2.0 3.9000000000000004 3.610000000000001 2.0 4.0 7.800000000000001 14.440000000000005 3.0 6.0 11.700000000000001 32.49000000000001 MSE= 16.84666666666667 w= 4.0 1.0 2.0 4.0 4.0 2.0 4.0 8.0 16.0 3.0 6.0 12.0 36.0 MSE= 18.666666666666668 可视化看权重w和loss的关系 plt.figure(figsize=(10,8)) plt.plot(w_list,mse_list) plt.xlabel(\u0026#34;W\u0026#34;) plt.ylabel(\u0026#34;LOSS\u0026#34;) plt.show() result: ","date":"2022-02-13T00:00:00Z","permalink":"https://example.com/p/pytorch%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B02_%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/","title":"pytorch基本模型02_线性模型"},{"content":" 这是学习pytorch基本使用的记录（我记得看的是B站刘二老师的视频）\nimport numpy as np import matplotlib.pyplot as plt import matplotlib import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) import seaborn as sns plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = \u0026#34;SimHei\u0026#34; plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False x_data = [1.0,2.0,3.0] y_data = [2.0,4.0,6.0] #初始化个权重值 w = 1.0 #正向传播 def forward(x): return w*x #代价 def cost(xs,ys): cost = 0 for x,y in zip(xs,ys): y_pred = forward(x) cost += (y_pred-y)**2 return cost/len(xs) #梯度求导部分 def gradient(xs,ys): grad = 0 for x,y in zip(xs,ys): grad += 2*x*(x*w-y) return grad / len(xs) print(\u0026#34;Predict (before training)\u0026#34;,4,forward(4)) cost_list = [] for epoch in range(100): cost_val = cost(x_data,y_data) cost_list.append(cost_val) grad_val = gradient(x_data,y_data) w-= 0.01*grad_val print(\u0026#34;Epoch:\u0026#34;,epoch,\u0026#34;w:\u0026#34;,w,\u0026#34;loss=\u0026#34;,cost_val) print(\u0026#34;Predict (after training)\u0026#34;,4,forward(4)) result:\nPredict (before training) 4 7.999777758621207 Epoch: 0 w: 1.9999496252874736 loss= 1.4405775547323328e-08 Epoch: 1 w: 1.9999543269273095 loss= 1.1842187756547517e-08 Epoch: 2 w: 1.9999585897474272 loss= 9.73480465527282e-09 Epoch: 3 w: 1.999962454704334 loss= 8.002442084652284e-09 Epoch: 4 w: 1.9999659589319294 loss= 6.578363057677792e-09 Epoch: 5 w: 1.9999691360982828 loss= 5.407706805106763e-09 Epoch: 6 w: 1.9999720167291097 loss= 4.445375336336258e-09 Epoch: 7 w: 1.9999746285010596 loss= 3.654296098708337e-09 Epoch: 8 w: 1.9999769965076273 loss= 3.0039938062398064e-09 Epoch: 9 w: 1.9999791435002487 loss= 2.469416419574488e-09 Epoch: 10 w: 1.9999810901068922 loss= 2.0299700487455895e-09 Epoch: 11 w: 1.999982855030249 loss= 1.6687256009798603e-09 Epoch: 12 w: 1.9999844552274257 loss= 1.371766609548254e-09 Epoch: 13 w: 1.999985906072866 loss= 1.1276531204725252e-09 Epoch: 14 w: 1.9999872215060652 loss= 9.269809829497494e-10 Epoch: 15 w: 1.999988414165499 loss= 7.620195671191381e-10 Epoch: 16 w: 1.9999894955100526 loss= 6.264139517215796e-10 Epoch: 17 w: 1.9999904759291143 loss= 5.149401089318768e-10 Epoch: 18 w: 1.999991364842397 loss= 4.233036557694424e-10 Epoch: 19 w: 1.99999217079044 loss= 3.47974418527062e-10 Epoch: 20 w: 1.9999929015166655 loss= 2.8605043756363944e-10 Epoch: 21 w: 1.9999935640417768 loss= 2.351461730319416e-10 Epoch: 22 w: 1.999994164731211 loss= 1.9330060517327963e-10 Epoch: 23 w: 1.999994709356298 loss= 1.5890168858915366e-10 Epoch: 24 w: 1.9999952031497101 loss= 1.3062425031741918e-10 Epoch: 25 w: 1.9999956508557373 loss= 1.0737893928680942e-10 Epoch: 26 w: 1.9999960567758686 loss= 8.8270260486003e-11 Epoch: 27 w: 1.9999964248101207 loss= 7.256207723875048e-11 Epoch: 28 w: 1.9999967584945095 loss= 5.964925247279011e-11 Epoch: 29 w: 1.999997061035022 loss= 4.903433660737981e-11 Epoch: 30 w: 1.99999733533842 loss= 4.030840399694845e-11 Epoch: 31 w: 1.9999975840401674 loss= 3.313529956782591e-11 Epoch: 32 w: 1.9999978095297517 loss= 2.7238688929053905e-11 Epoch: 33 w: 1.9999980139736415 loss= 2.2391412909108122e-11 Epoch: 34 w: 1.9999981993361016 loss= 1.8406736584033667e-11 Epoch: 35 w: 1.9999983673980655 loss= 1.513115554895154e-11 Epoch: 36 w: 1.999998519774246 loss= 1.243848235763325e-11 Epoch: 37 w: 1.9999986579286497 loss= 1.0224985320645546e-11 Epoch: 38 w: 1.9999987831886423 loss= 8.405392376989174e-12 Epoch: 39 w: 1.9999988967577025 loss= 6.909606107972389e-12 Epoch: 40 w: 1.9999989997269836 loss= 5.680003312816431e-12 Epoch: 41 w: 1.9999990930857985 loss= 4.669215167726787e-12 Epoch: 42 w: 1.999999177731124 loss= 3.83830238824482e-12 Epoch: 43 w: 1.999999254476219 loss= 3.155255154808919e-12 Epoch: 44 w: 1.9999993240584386 loss= 2.593759969699999e-12 Epoch: 45 w: 1.9999993871463178 loss= 2.132185973933309e-12 Epoch: 46 w: 1.9999994443459947 loss= 1.7527516336648503e-12 Epoch: 47 w: 1.9999994962070353 loss= 1.440839742998853e-12 Epoch: 48 w: 1.999999543227712 loss= 1.1844343058800304e-12 Epoch: 49 w: 1.9999995858597923 loss= 9.736576414395159e-13 Epoch: 50 w: 1.9999996245128784 loss= 8.003898544090114e-13 Epoch: 51 w: 1.9999996595583431 loss= 6.579560331703111e-13 Epoch: 52 w: 1.9999996913328977 loss= 5.408691012517321e-13 Epoch: 53 w: 1.9999997201418271 loss= 4.4461844012431444e-13 Epoch: 54 w: 1.9999997462619232 loss= 3.654961188144078e-13 Epoch: 55 w: 1.9999997699441436 loss= 3.004540541517374e-13 Epoch: 56 w: 1.9999997914160235 loss= 2.469865863254769e-13 Epoch: 57 w: 1.9999998108838613 loss= 2.0303395115474217e-13 Epoch: 58 w: 1.999999828534701 loss= 1.66902931698312e-13 Epoch: 59 w: 1.9999998445381288 loss= 1.3720162774073653e-13 Epoch: 60 w: 1.9999998590479036 loss= 1.1278583585515511e-13 Epoch: 61 w: 1.9999998722034327 loss= 9.271496968522504e-14 Epoch: 62 w: 1.9999998841311122 loss= 7.621582562121307e-14 Epoch: 63 w: 1.9999998949455418 loss= 6.265279609927294e-14 Epoch: 64 w: 1.9999999047506245 loss= 5.150338287212101e-14 Epoch: 65 w: 1.9999999136405662 loss= 4.2338069819529135e-14 Epoch: 66 w: 1.99999992170078 loss= 3.4803775193755746e-14 Epoch: 67 w: 1.9999999290087072 loss= 2.8610250045883864e-14 Epoch: 68 w: 1.9999999356345612 loss= 2.35188970786036e-14 Epoch: 69 w: 1.9999999416420022 loss= 1.933357857240197e-14 Epoch: 70 w: 1.9999999470887486 loss= 1.589306090862069e-14 Epoch: 71 w: 1.999999952027132 loss= 1.3064802438933907e-14 Epoch: 72 w: 1.9999999565045998 loss= 1.07398482121278e-14 Epoch: 73 w: 1.9999999605641705 loss= 8.828632567944822e-15 Epoch: 74 w: 1.9999999642448478 loss= 7.257528338097003e-15 Epoch: 75 w: 1.9999999675819953 loss= 5.966010930476005e-15 Epoch: 76 w: 1.9999999706076756 loss= 4.904326170453415e-15 Epoch: 77 w: 1.9999999733509592 loss= 4.031574090228466e-15 Epoch: 78 w: 1.999999975838203 loss= 3.3141330976503296e-15 Epoch: 79 w: 1.999999978093304 loss= 2.724364694968737e-15 Epoch: 80 w: 1.999999980137929 loss= 2.2395488742462455e-15 Epoch: 81 w: 1.9999999819917222 loss= 1.841008718917265e-15 Epoch: 82 w: 1.9999999836724949 loss= 1.5133909910880908e-15 Epoch: 83 w: 1.9999999851963954 loss= 1.244074658107541e-15 Epoch: 84 w: 1.999999986578065 loss= 1.022684637796773e-15 Epoch: 85 w: 1.999999987830779 loss= 8.406922423772652e-16 Epoch: 86 w: 1.999999988966573 loss= 6.9108637602094e-16 Epoch: 87 w: 1.9999999899963594 loss= 5.681037357692236e-16 Epoch: 88 w: 1.9999999909300326 loss= 4.670065162242399e-16 Epoch: 89 w: 1.9999999917765627 loss= 3.839001026220015e-16 Epoch: 90 w: 1.9999999925440834 loss= 3.1558296571823256e-16 Epoch: 91 w: 1.999999993239969 loss= 2.594232242156015e-16 Epoch: 92 w: 1.9999999938709052 loss= 2.132574242690705e-16 Epoch: 93 w: 1.999999994442954 loss= 1.7530708297392726e-16 Epoch: 94 w: 1.9999999949616116 loss= 1.4411020615022906e-16 Epoch: 95 w: 1.9999999954318612 loss= 1.1846500119529846e-16 Epoch: 96 w: 1.9999999958582209 loss= 9.738349457896592e-17 Epoch: 97 w: 1.999999996244787 loss= 8.005355897435174e-17 Epoch: 98 w: 1.9999999965952735 loss= 6.580758284957657e-17 Epoch: 99 w: 1.999999996913048 loss= 5.409676005437524e-17 Predict (after training) 4 7.999999987652192 code:\nplt.figure(figsize=(10,8)) plt.plot(np.arange(0,100),cost_list,c=\u0026#34;red\u0026#34;,linewidth=5) plt.grid() plt.show() result: code:\nx_data = [1.0,2.0,3.0] y_data = [2.0,4.0,6.0] #初始化个权重值 w = 1.0 #正向传播 def forward(x): return w*x def loss(x,y): y_pred = forward(x) return (y_pred-y)**2 def gradient(x,y): return 2*x*(x*w-y) loss_lost = [] for epoch in range(100): for x,y in zip(x_data,y_data): grad = gradient(x,y) w-=0.01*grad #这里就算每次的epoch的 loss_lost.append(loss(x,y)) # print(\u0026#34;epoch\u0026#34;,epoch) plt.figure(figsize=(10,8)) plt.plot(np.arange(0,100),loss_lost,c=\u0026#34;red\u0026#34;,linewidth=5) plt.grid() plt.show() result ","date":"2022-02-13T00:00:00Z","permalink":"https://example.com/p/pytorch%E5%9F%BA%E6%9C%AC%E6%A8%A1%E5%9E%8B03_%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/","title":"pytorch基本模型03_梯度下降算法"},{"content":" 这里来学习一下Python中的_Underscore的用处 总的来说可以提高编码效率,教程原文：https://www.datacamp.com/community/tutorials/role-underscore-python\n存储作用 Python automatically stores the value of the last expression in the interpreter to a particular variable called \u0026ldquo;_.\u0026rdquo; You can also assign these value to another variable if you want.\n简单来说就是存储上一次未指定变量名字的结果，并且可以通过变量赋值 占位作用 Ignoring Values Underscore() is also used to ignore the values. If you don\u0026rsquo;t want to use specific values while unpacking, just assign that value to underscore().\nIgnoring means assigning the values to special variable underscore(). We\u0026rsquo;re assigning the values to underscore() given not using that in future code.\n简单来说就是，当你取指定的位置的值时可以通过_ 或者 *_省略中值的赋值，我觉得很要用\ncode:\na,_,b = (1,2,3) print(a,b) # only available in Python 3.x a,*_,b = (5,6,7,87,9) print(a,b) a,_,b,*_ = (5,6,7,87,9) print(a,b) result:\n1 3 5 9 5 7 循环中的使用 Use In Looping\nYou can use underscore(_) as a variable in looping. See the examples below to get an idea.\nloop中_代替i,此时_可以存储in的内容\ncode:\nfor _ in range(3): print(_) for _ in [\u0026#34;i\u0026#34;,\u0026#34;am\u0026#34;,\u0026#34;hahaah\u0026#34;]: print(_,end=\u0026#34; \u0026#34;) #顺便说下py的每一次print默认是以换行符结束的，可以使用end来修改 # # # while中好像不行 # _ = 6 # whlie _ \u0026lt;= 10: # print(_) # _+=1 result:\n0 1 2 i am hahaah 但是要区分这两个\ncode:\nfor i,j in [[1,2],[3,4]]: print(i,j) result:\n1 2 3 4 code:\nfor _,_ in [[1,2],[3,4]]: print(_,_) #当第一次循环时，_=1后又发生了_=2，因为_和_是同样的变量名所以肯定指向同一个值 result:\n2 2 4 4 补充个知识点： PYthon 有三个内置函数，可以将你传递给他们的十进制数字 转换为 八进制、十六进制、二进制…\nbin() —— 二进制、oct() —— 八进制、hex() —— 十六进制。 前缀： 二进制：0b （数字0）\n八进制：0o 或 0O （是数字0 和 字母小o 或 字母大O） 十进制：0D 或 0d （数字0） 十六进制：0x 或 0X（数字0） 或 使用后缀： H 表示！ 进制中的作用Separating Digits Of Numbers If you have a long digits number, you can separate the group of digits as you like for better understanding. 感觉上就是占位，代表0\ncode:\nmillion = 1_000_000 binary = 0b_0010 octa = 0o_64 hexa = 0x_23_ab print(million) print(binary) print(octa) print(hexa) result:\n1000000 2 52 9131 在函数命名中的作用，他这里有个import的错误值得注意 Underscore(_) can be used to name variables, functions and classes, etc..,\nSingle Pre Underscore:- _variable Signle Post Underscore:- variable_ Double Pre Underscores:- __variable Double Pre and Post Underscores:- variable single_pre_underscore_ _name Single Pre Underscore is used for internal use.\nMost of us don\u0026rsquo;t use it because of that reason.\n注意这里想讲的就是：函数的命名开头可以使用_,但是在导入这个函数时会报错，需要用另一种导入方式\ncode:\nclass T5: def __init__(self): self.name = \u0026#34;test5\u0026#34; self._num = 5 TT5 = T5() print(TT5.name) print(TT5._num) result:\ntest5 5 现在有一份文件py_test.py 内容：\ndef T5_name(): return \u0026#34;T5_test\u0026#34; def _private_func(): return 5 code:\n#这样导入是不行的 # from py_test.py import * 错误写法不要加上.py from py_test import * T5_name() _private_func()#在这里虽然可以，但是在交互界面可不行 result:\n5 code:\n#这样导入两种都可以显示。所以还是尽量用这个8 #但是from *的写法可以不用写文件名.func() 直接func就好了 看情况用哪个吧 import py_test py_test.T5_name() py_test._private_func() result:\n5 不能使用关键词来命名变量名，那可以在关键词后加上个_来命名 code:\nclass = 1 result:\nFile \u0026#34;\u0026lt;ipython-input-37-0251f39d4826\u0026gt;\u0026#34;, line 1 class = 1 ^ SyntaxError: invalid syntax code:\nclass_ = 1 print(class_) result:\n1 Double Pre Underscore __name Double Pre Underscores are used for the name mangling.\nDouble Pre Underscores tells the Python interpreter to rewrite the attribute name of subclasses to avoid naming conflicts.\nName Mangling:- interpreter of the Python alters the variable name in a way that it is challenging to clash when the class is inherited. code:\nclass Sample: def __init__(self): self.a = 1 self._b = 2 self.__c = 3#注意这个__双下划线与前面的单个下划线_b意义不一样 obj1 = Sample() dir(obj1)#返回了Sample所有的属性，可以看到前面写的a，_b是一模一样的名字 #但是__c前面加上了_和_Sample；注意下两个下划线__在这里的作用时Name Mangling名字重整，加改变变量名，前面加上_类名 result:\n[\u0026#39;_Sample__c\u0026#39;, \u0026#39;__class__\u0026#39;, \u0026#39;__delattr__\u0026#39;, \u0026#39;__dict__\u0026#39;, \u0026#39;__dir__\u0026#39;, \u0026#39;__doc__\u0026#39;, \u0026#39;__eq__\u0026#39;, \u0026#39;__format__\u0026#39;, \u0026#39;__ge__\u0026#39;, \u0026#39;__getattribute__\u0026#39;, \u0026#39;__gt__\u0026#39;, \u0026#39;__hash__\u0026#39;, \u0026#39;__init__\u0026#39;, \u0026#39;__init_subclass__\u0026#39;, \u0026#39;__le__\u0026#39;, \u0026#39;__lt__\u0026#39;, \u0026#39;__module__\u0026#39;, \u0026#39;__ne__\u0026#39;, \u0026#39;__new__\u0026#39;, \u0026#39;__reduce__\u0026#39;, \u0026#39;__reduce_ex__\u0026#39;, \u0026#39;__repr__\u0026#39;, \u0026#39;__setattr__\u0026#39;, \u0026#39;__sizeof__\u0026#39;, \u0026#39;__str__\u0026#39;, \u0026#39;__subclasshook__\u0026#39;, \u0026#39;__weakref__\u0026#39;, \u0026#39;_b\u0026#39;, \u0026#39;a\u0026#39;] code:\n#创建一个继承Sample的子类SecondClass class SecondClass(Sample): def __init__(self): super().__init__() self.a = \u0026#34;overridden\u0026#34; self._b = \u0026#34;overridden\u0026#34; self.__c = \u0026#34;overridden\u0026#34; obj2 = SecondClass() print(obj2.a) print(obj2._b) print(obj2.__c)#注意这里发生了名字重整 result:\noverridden overridden --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) \u0026lt;ipython-input-44-a9d737266262\u0026gt; in \u0026lt;module\u0026gt; 10 print(obj2.a) 11 print(obj2._b) ---\u0026gt; 12 print(obj2.__c)#注意这里发生了名字重整 AttributeError: \u0026#39;SecondClass\u0026#39; object has no attribute \u0026#39;__c\u0026#39; code:\n#创建一个继承Sample的子类SecondClass class SecondClass(Sample): def __init__(self): super().__init__() self.a = \u0026#34;overridden\u0026#34; self._b = \u0026#34;overridden\u0026#34; self.__c = \u0026#34;overridden\u0026#34; obj2 = SecondClass() print(obj2.a) print(obj2._b) print(obj2._SecondClass__c)#所以这样写才对,不用加上点. #同理前面的obj1也是这样取值 print(obj1.a) print(obj1._b) print(obj1._Sample__c) result:\noverridden overridden overridden 1 2 3 code:\n#另如果方法改写（即方法是__开头命名）的话，可以在类中下一个办法去return这个值 class SimpleClass: def __datacamp(self): return \u0026#34;datacamp\u0026#34; def call_datacamp(self): return self.__datacamp() obj = SimpleClass() print(obj.call_datacamp()) print(dir(obj)) print(obj.__datacamp()) ## 这里会报错，因为__方法也会发生改写 __datacamp 变成 _SimpleClass__datacamp # print(obj._SimpleClass__datacamp())#对于方法这样写是不行的 所以只能写个新方法去return这个值，在同一个类中时不考虑命名重整的发生先 result:\ndatacamp [\u0026#39;_SimpleClass__datacamp\u0026#39;, \u0026#39;__class__\u0026#39;, \u0026#39;__delattr__\u0026#39;, \u0026#39;__dict__\u0026#39;, \u0026#39;__dir__\u0026#39;, \u0026#39;__doc__\u0026#39;, \u0026#39;__eq__\u0026#39;, \u0026#39;__format__\u0026#39;, \u0026#39;__ge__\u0026#39;, \u0026#39;__getattribute__\u0026#39;, \u0026#39;__gt__\u0026#39;, \u0026#39;__hash__\u0026#39;, \u0026#39;__init__\u0026#39;, \u0026#39;__init_subclass__\u0026#39;, \u0026#39;__le__\u0026#39;, \u0026#39;__lt__\u0026#39;, \u0026#39;__module__\u0026#39;, \u0026#39;__ne__\u0026#39;, \u0026#39;__new__\u0026#39;, \u0026#39;__reduce__\u0026#39;, \u0026#39;__reduce_ex__\u0026#39;, \u0026#39;__repr__\u0026#39;, \u0026#39;__setattr__\u0026#39;, \u0026#39;__sizeof__\u0026#39;, \u0026#39;__str__\u0026#39;, \u0026#39;__subclasshook__\u0026#39;, \u0026#39;__weakref__\u0026#39;, \u0026#39;call_datacamp\u0026#39;] --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) \u0026lt;ipython-input-62-4df7a3343359\u0026gt; in \u0026lt;module\u0026gt; 12 print(dir(obj)) 13 # print(obj._SimpleClass__datacamp())#对于方法这样写是不行的 ---\u0026gt; 14 print(obj.__datacamp()) ## 这里会报错，因为__方法也会发生改写 __datacamp 变成 _SimpleClass__datacamp AttributeError: \u0026#39;SimpleClass\u0026#39; object has no attribute \u0026#39;__datacamp\u0026#39; code:\n#接下来体会下命令重整的另一个小点 _SimpleClass__name = \u0026#34;hahahah\u0026#34; #这个相当于__name重整后的赋值了 即在外部赋予重整名字后的变量值也会影响到那个类中的值 class SimpleClass: def return_name(self): return __name obj = SimpleClass() print(obj.return_name()) ##这样可以直接输出 result:\nhahahah 前面这两种总的来说就是：当类中的方法是以__(两个下划线)开头命名时，取值需要在写个方法去return这个方法，才能取出__方法的值\n如果是return中出现了__其他（没有定义的变量或者方法），那么可以再类的外面，通过重整命名来直接赋值;这里和前面的self.__c是差不多的理解\nDouble Pre And Post Underscores name\nIn Python, you will find different names which start and end with the double underscore. They are called as magic methods or dunder methods.\n魔术方法，这个要区别开仅有__开头的东西即前面讲的\ncode:\nclass Sample(): def __init__(self): self.__num__ = 7 obj = Sample() print(dir(obj)) obj.__num__ result:\n[\u0026#39;__class__\u0026#39;, \u0026#39;__delattr__\u0026#39;, \u0026#39;__dict__\u0026#39;, \u0026#39;__dir__\u0026#39;, \u0026#39;__doc__\u0026#39;, \u0026#39;__eq__\u0026#39;, \u0026#39;__format__\u0026#39;, \u0026#39;__ge__\u0026#39;, \u0026#39;__getattribute__\u0026#39;, \u0026#39;__gt__\u0026#39;, \u0026#39;__hash__\u0026#39;, \u0026#39;__init__\u0026#39;, \u0026#39;__init_subclass__\u0026#39;, \u0026#39;__le__\u0026#39;, \u0026#39;__lt__\u0026#39;, \u0026#39;__module__\u0026#39;, \u0026#39;__ne__\u0026#39;, \u0026#39;__new__\u0026#39;, \u0026#39;__num__\u0026#39;, \u0026#39;__reduce__\u0026#39;, \u0026#39;__reduce_ex__\u0026#39;, \u0026#39;__repr__\u0026#39;, \u0026#39;__setattr__\u0026#39;, \u0026#39;__sizeof__\u0026#39;, \u0026#39;__str__\u0026#39;, \u0026#39;__subclasshook__\u0026#39;, \u0026#39;__weakref__\u0026#39;] Out[66]: 7 ","date":"2022-02-11T00:00:00Z","permalink":"https://example.com/p/py_%E5%8D%A0%E4%BD%8D%E7%AC%A6%E8%BF%90%E7%94%A8/","title":"py_占位符运用"},{"content":"Native protein sequences are close to optimal for their structures\nlink:https://www.pnas.org/doi/abs/10.1073/pnas.97.19.10383\n","date":"2022-02-10T00:00:00Z","permalink":"https://example.com/p/native-protein-sequences-are-close-to-optimal-for-their-structures/","title":"Native protein sequences are close to optimal for their structures"},{"content":"The Backrub Motion: How Protein Backbone Shrugs When a Sidechain Dances\nlink:https://www.cell.com/fulltext/S0969-2126(06)00040-2\n","date":"2022-02-10T00:00:00Z","permalink":"https://example.com/p/the-backrub-motion-how-protein-backbone-shrugs-when-a-sidechain-dances/","title":"The Backrub Motion: How Protein Backbone Shrugs When a Sidechain Dances"},{"content":"make_moons() sklearn.datasets.make_moons(n_samples=100, shuffle=True, noise=None, random_state=None)\n制作月亮型数据\n重要参数：n_samples：设置样本数量、noise:设置噪声、random_state：设置随机参数（嘿嘿，无所谓，随便设），我们主要讲参数noise\nfrom sklearn.datasets import make_moons import matplotlib.pyplot as plt # plt.style.use(\u0026#34;seaborn-whitegrid\u0026#34;) a,b = make_moons(noise=0) plt.scatter(a[:,0],a[:,1],c=b) result: ![](picture/sklearn.datasets中的几个函数make_moons,%20make_circles(,make_classification.png)\n#将noise设置为0.1 a,b = make_moons(noise=0.1) plt.scatter(a[:,0],a[:,1],c=b) #发现这个noise设置的越大，那么噪声就越大 result: make_circles() sklearn.datasets.make_circles(n_samples=100, shuffle=True, noise=None, random_state=None, factor=0.8)\n重要参数：n_samples：设置样本数量、noise:设置噪声、factor：0 \u0026lt; double \u0026lt; 1 默认值0.8，内外圆之间的比例因子、random_state：设置随机参数（嘿嘿，无所谓，随便设），我们主要讲参数noise、factor\nfrom sklearn.datasets import make_circles #将moise设置为0，factor设置为0.1 a,b = make_circles(noise=0,factor=0.1) plt.scatter(a[:,0],a[:,1],c=b) result: code:\n#将noise设置为0.1，factor设置为0.5 a,b = make_circles(noise=0.1,factor=0.5) plt.scatter(a[:,0],a[:,1],c=b) #发现这个noise设置的越大，那么噪声就越大，factor设置的越大，两个环就越近 result: make_classfication sklearn.datasets.make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True,shift=0.0, scale=1.0, shuffle=True, random_state=None)\n功能：生成样本集，通常用于分类算法\n参数：\nn_features :特征个数= n_informative（） + n_redundant + n_repeated n_informative：多信息特征的个数 n_redundant：冗余信息，informative特征的随机线性组合 n_repeated ：重复信息，随机提取n_informative和n_redundant 特征 n_classes：分类类别 n_clusters_per_class ：某一个类别是由几个cluster构成的\n","date":"2022-02-06T00:00:00Z","permalink":"https://example.com/p/sklearn.datasets%E4%B8%AD%E7%9A%84%E5%87%A0%E4%B8%AA%E5%87%BD%E6%95%B0make_moonsmake_circlesmake_classification/","title":"sklearn.datasets中的几个函数make_moons,make_circles,make_classification"},{"content":" ","date":"2022-01-30T00:00:00Z","permalink":"https://example.com/p/gromacs%E4%B8%AD%E6%96%87%E6%89%8B%E5%86%8C/","title":"《GROMACS中文手册》"},{"content":" ","date":"2022-01-30T00:00:00Z","permalink":"https://example.com/p/gromacs%E6%95%99%E7%A8%8B_%E6%BC%8F%E6%96%97%E7%BD%91%E8%9C%98%E8%9B%9B%E6%AF%92%E7%B4%A0%E8%82%BD%E7%9A%84%E6%BA%B6%E5%89%82%E5%8C%96%E7%A0%94%E7%A9%B6_amber99sb-ildn%E5%8A%9B%E5%9C%BA_jerkwin/","title":"《GROMACS教程_漏斗网蜘蛛毒素肽的溶剂化研究_Amber99SB-ILDN力场_Jerkwin》"},{"content":" ","date":"2022-01-30T00:00:00Z","permalink":"https://example.com/p/gromacs%E6%95%99%E7%A8%8B_%E8%9B%8B%E7%99%BD%E8%B4%A8%E9%85%8D%E4%BD%93%E5%A4%8D%E5%90%88%E7%89%A9_jerkwin/","title":"《GROMACS教程_蛋白质配体复合物_Jerkwin》"},{"content":" ","date":"2022-01-30T00:00:00Z","permalink":"https://example.com/p/gromacs%E6%96%87%E4%BB%B6%E7%B1%BB%E5%9E%8B_jerkwin/","title":"《GROMACS文件类型_Jerkwin》"},{"content":" ","date":"2022-01-30T00:00:00Z","permalink":"https://example.com/p/gromacs%E6%95%99%E7%A8%8B_dppc%E8%86%9C%E4%B8%AD%E7%9A%84kalp_sub_15__sub__jerkwin/","title":"GROMACS教程_DPPC膜中的KALP_sub_15__sub__Jerkwin"},{"content":" ","date":"2022-01-30T00:00:00Z","permalink":"https://example.com/p/gromacs%E6%95%99%E7%A8%8B%E4%B9%8B%E6%B0%B4%E4%B8%AD%E7%9A%84%E6%BA%B6%E8%8F%8C%E9%85%B6_jerkwin/","title":"GROMACS教程之水中的溶菌酶_Jerkwin"},{"content":"想从 sklearn 包中导入模块 cross_validation，调用 cross_validation 里面别的函数，例如 交叉验证数据 使用到的 cross_val_score 函数，但是 from sklearn import cross_validation 运行报错 code:\nfrom sklearn import corss_validation result:\n--------------------------------------------------------------------------- ImportError Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_6408/3988079335.py in \u0026lt;module\u0026gt; ----\u0026gt; 1 from sklearn import corss_validation ImportError: cannot import name \u0026#39;corss_validation\u0026#39; from \u0026#39;sklearn\u0026#39; (F:\\anaconda3\\lib\\site-packages\\sklearn\\__init__.py) 这是因为 sklearn 0.21.1 版本的已经移除 cross_validation 模块 从 sklearn.model_selection 模块直接导入 cross_val_score 即\nfrom sklearn.model_selection import cross_val_score ","date":"2022-01-29T00:00:00Z","permalink":"https://example.com/p/cannot-import-name-cross_validation/","title":"cannot import name 'cross_validation' "},{"content":"code:\nfrom sklearn import cross_validation result:\n--------------------------------------------------------------------------- ImportError Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_19376/266941855.py in \u0026lt;module\u0026gt; ----\u0026gt; 1 from sklearn import cross_validation ImportError: cannot import name \u0026#39;cross_validation\u0026#39; from \u0026#39;sklearn\u0026#39; (F:\\anaconda3\\lib\\site-packages\\sklearn\\__init__.py) ‘cross_validation’ from ‘sklearn’”，后来百度才知道sklearn在0.18版本中，cross_validation被废弃了，原来在 cross_validation 里面的函数现在在 model_selection 里面，所以只要将cross_validation替换为model_selection就可以使用，数据信息都是一样的\nfrom sklearn.model_selection import cross_validate ","date":"2022-01-29T00:00:00Z","permalink":"https://example.com/p/ccannot-import-name-cross_validation-from-sklearn/","title":"ccannot import name ‘cross_validation’ from ‘sklearn’ "},{"content":"code:\nfrom sklearn.datasets.samples_generator import make_blobs result:\n--------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_14680/1800722232.py in \u0026lt;module\u0026gt; ----\u0026gt; 1 from sklearn.datasets.samples_generator import make_blobs ModuleNotFoundError: No module named \u0026#39;sklearn.datasets.samples_generator\u0026#39; 新版的sklearn中改为了这种用法：\nfrom sklearn.datasets import make_blobs ","date":"2022-01-29T00:00:00Z","permalink":"https://example.com/p/no-module-named-sklearn.datasets.samples_generator/","title":"No module named 'sklearn.datasets.samples_generator'’ "},{"content":"code:\nfrom sklearn.grid_search import GridSearchCV result:\nModuleNotFoundError Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_7712/1716585072.py in \u0026lt;module\u0026gt; ----\u0026gt; 1 from sklearn.grid_search import GridSearchCV ModuleNotFoundError: No module named \u0026#39;sklearn.grid_search\u0026#39; 检查Scikit-Learn的版本conda list scikit-learn如果高于等于0.20说明是grid_search模块已被弃用。\n改成这样了：\nfrom sklearn.model_selection import GridSearchCV ","date":"2022-01-29T00:00:00Z","permalink":"https://example.com/p/no-module-named-sklearn.grid_search/","title":"No module named 'sklearn.grid_search'"},{"content":"sklearn中提供了计算准确率的accurccy_score函数 from sklearn import metrics metrics.accuracy_score? 输入参数：\ny_true：真是标签。二分类和多分类情况下是一列，多标签情况下是标签的索引。\ny_pred：预测标签。二分类和多分类情况下是一列，多标签情况下是标签的索引。\nnormalize:bool, optional (default=True)，如果是false，正确分类的样本的数目(int)；如果为true，返回正确分类的样本的比例，必须严格匹配真实数据集中的label，才为1，否则为0。\nsample_weight：array-like of shape (n_samples,), default=None。Sample weights.\n输出：\n如果normalize == True,返回正确分类的样本的比例，否则返回正确分类的样本的数目(int)\n","date":"2022-01-28T00:00:00Z","permalink":"https://example.com/p/metrics.accuracy_score%E8%AE%A1%E7%AE%97%E5%88%86%E7%B1%BB%E7%9A%84%E5%87%86%E7%A1%AE%E7%8E%87/","title":"metrics.accuracy_score()计算分类的准确率"},{"content":"简介 python中，yield关键字的作用：1、将一个函数修改为生成器，利用生成器可以有效地节约系统资源，避免不必要的内存占用；2、用于定义上下文管理器；3、协程；4、配合from形成yield from用于消费子生成器并传递消息。\n常见的情况 生成器 生成器函数（generation function） 和 生成器（generation） 生成器函数是一种特殊的函数，它的函数内部含有yield表达式，调用它会返回一个特殊的迭代器，称生成器。\n具体情况 def foo(): print(\u0026#34;starting\u0026#34;) yield g = foo() g #\u0026lt;generator object foo at 0x000001B77CB6C350\u0026gt; #没有任何输出。这是因为有yield，函数并没有被执行。只是将foo()指向了g。 #函数2 def foo(): print(\u0026#34;starting\u0026#34;) yield 1 print(\u0026#34;ending\u0026#34;) g = foo() g #\u0026lt;generator object foo at 0x000001B77CB6C820\u0026gt; print(next(g)) #starting #1 print(next(g)) #ending #--------------------------------------------------------------------------- #StopIteration Traceback (most recent call last) #~\\AppData\\Local\\Temp/ipykernel_22044/2604396719.py in \u0026lt;module\u0026gt; #----\u0026gt; 1 print(next(g)) #StopIteration: 运行：输出了starting和1，并没有输出ending，这是因为next(g)只调用了一次，运行到了yield就返回了，print函数打印了返回值：1。这个时候函数停止了，等待下一次的next(g)调用。 迭代器是一个对象，这种对象每次只能调取一个数据元素。对迭代器不断调用 next() 方法（将迭代起变量放入next()中当参数），则可以依次获取下一个元素；当迭代器中没有元素时，调用 next() 方法会抛出 StopIteration（停止迭代） 异常。迭代器的 iter() 方法返回迭代器自身；因此迭代器也是可迭代的。 运行：接着上面的，第二个next(g)运行，报错是因为遍历结束了，无yield了。解决方法就是在最好加一个yield。\ndef foo(): print(\u0026#34;starting\u0026#34;) yield 1 print(\u0026#34;ending\u0026#34;) yield 2 g = foo() print(next(g)) print(next(g)) print(\u0026#34;end\u0026#34;) def func(): return 1 def gen(): yield 1 print(type(func)) # \u0026lt;class \u0026#39;function\u0026#39;\u0026gt; print(type(gen)) # \u0026lt;class \u0026#39;function\u0026#39;\u0026gt; print(type(func())) # \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; print(type(gen())) # \u0026lt;class \u0026#39;generator\u0026#39;\u0026gt; 个人理解 其实yield放在函数中起到的作用就是，将函数生成为一个生成器，每次调用才会运行yiled后面的语句（第一次调用运行第一个yield前的语句包含yield本身；第二次运行再从第一次运行的基础后面再运行到一个yield或者结束了） 其实就是可以将一个函数分段运行，每次运行再基于前一次的基础去运行，\npython中的yield是停止当前函数跳出函数，停止状态的函数等待被调用激活， yield是一个常用于python函数定义中的关键字，它的作用是返回一个可以用来迭代（for循环）的生成器，它的应用场景通常为一个需要返回一系列值的，含有循环的函数中 生成器函数是一种特殊的函数，它的函数内部含有yield表达式，调用它会返回一个特殊的迭代器，称生成器。 yield是生成器这是yield实现其功能所必须成为的样子。如函数1：\n在一个函数中，程序执行到yield语句的时候，程序暂停，返回yield后面表达式的值，在下一次调用的时候，从yield语句暂停的地方继续执行，如此循环，直到函数执行完。\nyield 的好处\n介绍了这么多定义和用法，那么到底为什么要用yield呢？它有什么样的好处呢？\n在很多时候，我们需要逐个去获取容器内的某些数据，而这种仅仅获取部分元素的情况，并不需要我们将容器内所有的元素都取出来。比如说一个容器内现有10000个元素，但我们只需要前5个元素，那么解决办法通常由如下两种：\n·获取容器内的所有元素，然后取出前 5 个；\n·从头开始，逐个迭代容器内的元素，迭代 5 个元素之后停止。\n显而易见，如果容器内的元素数量非常多（比如有 10 ** 8 个），或者容器内的元素体积非常大，那么后一种方案能节省巨大的时间、空间开销。\n现在假设，我们有一个函数，其产出（返回值）是一个列表。而若我们知道，调用者对该函数的返回值，只有逐个迭代这一种方式。那么，如果函数生产列表中的每一个元素都需要耗费非常多的时间，或者生成所有元素需要等待很长时间，则使用 yield 把函数变成一个生成器函数，每次只产生一个元素，就能节省很多开销了。\ncode:\ndef foo(): print(\u0026#34;starting\u0026#34;) yield g = foo() g #没有任何输出。这是因为有yield，函数并没有被执行。只是将foo()指向了g。 result:\n\u0026lt;generator object foo at 0x000001B77CB6C350\u0026gt; code:\n#函数2 def foo(): print(\u0026#34;starting\u0026#34;) yield 1 print(\u0026#34;ending\u0026#34;) g = foo() g result:\n\u0026lt;generator object foo at 0x000001B77CB6C820\u0026gt; code:\nprint(next(g)) result:\nstarting 1 运行：输出了starting和1，并没有输出ending，这是因为next(g)只调用了一次，运行到了yield就返回了，print函数打印了返回值：1。这个时候函数停止了，等待下一次的next(g)调用。\ncode:\n#函数3 def foo(): print(\u0026#34;starting\u0026#34;) yield 1 print(\u0026#34;ending\u0026#34;) g = foo() g result:\n\u0026lt;generator object foo at 0x000001B77CB6CA50\u0026gt; code:\nprint(next(g)) result:\nstarting 1 code:\nprint(next(g)) result:\nending --------------------------------------------------------------------------- StopIteration Traceback (most recent call last) ~\\AppData\\Local\\Temp/ipykernel_22044/2604396719.py in \u0026lt;module\u0026gt; ----\u0026gt; 1 print(next(g)) StopIteration: 迭代器是一个对象，这种对象每次只能调取一个数据元素。对迭代器不断调用 next() 方法（将迭代起变量放入next()中当参数），则可以依次获取下一个元素；当迭代器中没有元素时，调用 next() 方法会抛出 StopIteration（停止迭代） 异常。迭代器的 iter() 方法返回迭代器自身；因此迭代器也是可迭代的。\n运行：接着上面的，第二个next(g)运行，报错是因为遍历结束了，无yield了。解决方法就是在最好加一个yield。\ndef foo(): print(\u0026#34;starting\u0026#34;) yield 1 print(\u0026#34;ending\u0026#34;) yield 2 g = foo() print(next(g)) print(next(g)) print(\u0026#34;end\u0026#34;) result:\nstarting 1 ending 2 end yield的作用挺溜的，其丰富了函数的运行，让函数进入不同的运行阶段\n对，感觉上就是暂停了函数运行，等下次运行函数时再继续在前面运行的基础下往下运行\ncode:\ndef test(): a = 1 yield 2 生成器函数（generation function） 和 生成器（generation）\n生成器函数是一种特殊的函数，它的函数内部含有yield表达式，调用它会返回一个特殊的迭代器，称生成器。\ncode:\ndef func(): return 1 def gen(): yield 1 print(type(func)) # \u0026lt;class \u0026#39;function\u0026#39;\u0026gt; print(type(gen)) # \u0026lt;class \u0026#39;function\u0026#39;\u0026gt; print(type(func())) # \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; print(type(gen())) # \u0026lt;class \u0026#39;generator\u0026#39;\u0026gt; result:\n\u0026lt;class \u0026#39;function\u0026#39;\u0026gt; \u0026lt;class \u0026#39;function\u0026#39;\u0026gt; \u0026lt;class \u0026#39;int\u0026#39;\u0026gt; \u0026lt;class \u0026#39;generator\u0026#39;\u0026gt; code:\na = [1,2,3,4,5,6,7,8,9,10] result:\ndef test_1(llist): for i in llist: yield i code:\ntest_1(a) result:\n\u0026lt;generator object test_1 at 0x0000015B9F9EB200\u0026gt; code:\nnext(test_1(a)) result:\n1 code:\nnext(test_1(a)) result:\n1 ","date":"2022-01-25T00:00:00Z","permalink":"https://example.com/p/yield%E7%9A%84%E4%BD%9C%E7%94%A8/","title":"yield的作用"},{"content":" 此post用来存放一些ppt上的内容，仅方便学习用途\n这里就放了100个分析的命令（看手册）\n","date":"2022-01-24T00:00:00Z","permalink":"https://example.com/p/gmx10ppt_%E5%88%86%E6%9E%90%E6%A6%82%E8%AE%B2/","title":"GMX10ppt_分析概讲"},{"content":" 此post用来存放一些ppt上的内容，仅方便学习用途\n高斯这里的教程我跳过了。。\n","date":"2022-01-24T00:00:00Z","permalink":"https://example.com/p/gmx7ppt_%E7%AE%80%E5%8D%95%E5%BB%BA%E6%A8%A1/","title":"GMX7ppt_简单建模"},{"content":" 此post用来存放一些ppt上的内容，仅方便学习用途\n","date":"2022-01-24T00:00:00Z","permalink":"https://example.com/p/gmx8ppt_%E6%A0%87%E5%87%86%E6%AE%8B%E5%9F%BA%E7%94%B2%E5%9F%BA%E5%8C%96/","title":"GMX8ppt_标准残基甲基化"},{"content":" 此post用来存放一些ppt上的内容，仅方便学习用途\ntop删除了（每一部分属性中的都只保留残基的）\n第一行的那个删除了\nJSMOL\nTPPMKTOP ","date":"2022-01-24T00:00:00Z","permalink":"https://example.com/p/gmx9ppt_%E9%9D%9E%E6%A0%87%E5%87%86%E6%AE%8B%E5%9F%BA/","title":"GMX9ppt_非标准残基"},{"content":" 此post用来存放一些ppt上的内容，仅方便学习用途\nrtp文件 原子缺失 spdbv可以去补充缺失的原子\n残基突变\n真空中的模拟 （NVT平衡）\nxtc和gro可以看轨迹运动\n加溶剂\n","date":"2022-01-23T00:00:00Z","permalink":"https://example.com/p/gmx6ppt_%E8%9B%8B%E7%99%BD%E6%A8%A1%E6%8B%9F/","title":"GMX6ppt_蛋白模拟"},{"content":"collections的namedtuple使用 元组中的元素值是不能被更改的，由于元组不像字典那样可以为内部的元素命名，因此我们并不知道元组内的元素所表达的意义，在访问元组的时候也只能通过索引访问其中的元素。 于是Python标准库collections引入了namedtuple函数，它可以创建一个和元组类似但更为强大的类型——具名元组（namedtuple），也就是构造一个带字段名的元组\n基础语法 #导入 from collections import namedtuple #创建一个namedtuple对象 collections.namedtuple(typename=\u0026#34;\u0026#34;, field_names=[\u0026#34;\u0026#34;], *, verbose=False, rename=False, module=None) 参数说明 1.typename: 创建的元组名称，实际上是赋予对象类名，看后面的例子就知道了\n2.field_names:新创建的元组中的元素名称，可以类比与字典的key，这就是为什么可以通过键值去命名元组创建新或者取出已有元素的原因了 ；可以[\u0026ldquo;key1\u0026rdquo;,\u0026ldquo;key2\u0026rdquo;]或者空格隔开的形式\u0026quot;key1 key2\u0026quot;\n3.rename:为True时，不能不能包含有非Python标识符、Python中的关键字以及重复的name，如果有则会默认重命名成_index的形式，如\u0026quot;def abc\u0026quot;变成\u0026quot;_0 abc\u0026quot;\n简单的用法 1.创建好后可以通过实例化，直接传值\n2.可以通过index或者key访问value\n3.fields:可以访问指定namedtuple的所有键值名\n4.make：可以以list的方式赋值，与1略不同\n5.asdict:可以将namedtuple转为字典对象\n具体例子 code:\n#创建一个namedtuple对象,并实例化 tmp = namedtuple(\u0026#34;test\u0026#34;,[\u0026#34;id\u0026#34;,\u0026#34;name\u0026#34;,\u0026#34;word\u0026#34;]) #直接赋值 tmp1 = tmp(\u0026#34;0\u0026#34;,\u0026#34;yeyuhao\u0026#34;,\u0026#34;hahaha\u0026#34;) #make赋值 tmp2 = tmp._make([\u0026#34;1\u0026#34;,\u0026#34;miles\u0026#34;,\u0026#34;hehehe\u0026#34;]) print(tmp1) print(tmp2) print(\u0026#34;---------分割线-------------\u0026#34;) #index取值 print(tmp1[1]) #key取值 print(tmp1.name) #fileds返回所有key print(tmp1._fields)#不用加（） #转字典 print(tmp2._asdict()) result:\ntest(id=\u0026#39;0\u0026#39;, name=\u0026#39;yeyuhao\u0026#39;, word=\u0026#39;hahaha\u0026#39;) test(id=\u0026#39;1\u0026#39;, name=\u0026#39;miles\u0026#39;, word=\u0026#39;hehehe\u0026#39;) ---------分割线------------- yeyuhao yeyuhao (\u0026#39;id\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;word\u0026#39;) {\u0026#39;id\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;name\u0026#39;: \u0026#39;miles\u0026#39;, \u0026#39;word\u0026#39;: \u0026#39;hehehe\u0026#39;} ","date":"2022-01-22T00:00:00Z","permalink":"https://example.com/p/collections%E7%9A%84namedtuple%E5%91%BD%E5%90%8D%E5%85%83%E7%BB%84%E7%9F%A5%E8%AF%86/","title":"collections的namedtuple命名元组知识"},{"content":" 此post用来存放一些ppt上的内容，仅方便学习用途\n时间尺度与研究方法 注意下复杂性的问题. 空间尺度越小，研究尺度也就越小（就是说空间尺度越小呢，它在单位时间内跑的距离就越小）. 上面说是分子上的，现在我们关心的是蛋白上的，来说说蛋白运动的时间尺度问题\n氢键之类的那些跑到fs就可以看见了，但是蛋白那些至少要到ns才能看见\n相关概念 分子模拟 力场 成键相互作用 库仑力 VDW范德华力 L-J势能曲线 Buckingham势能曲线 1-4相互作用 粒子的运动 PBC 系综 初始速度 MD基本算法流程 VDM截断 库伦Ewald 预平衡 EM EM最小点 梯度下降-最陡下降 梯度下降-共轭梯度法 梯度下降 L-BFGC法 理论书籍推荐 问题部分： 这个就是pbc ","date":"2022-01-22T00:00:00Z","permalink":"https://example.com/p/gmx1ppt_%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5ppt%E6%95%B4%E7%90%86/","title":"GMX1ppt_基本概念ppt整理"},{"content":" 此post用来存放一些ppt上的内容，仅方便学习用途\n这里实际上开始讲的就是运行环境的配置\nPATH配置 xplorer2 msys2 可能要自己去编译配置下\nNotepad2 xplore中也要修改\nGMX2019 amber VMD spdbv4 Gview分子编辑 qtgrace APBS1.5 irfanview+pymol Jmol gunplot ","date":"2022-01-22T00:00:00Z","permalink":"https://example.com/p/gmx2ppt_%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE%E8%AF%B4%E6%98%8E/","title":"GMX2ppt_基本配置说明"},{"content":" 此post用来存放一些ppt上的内容，仅方便学习用途\n这些参数去手册上找（2019版本的）\n","date":"2022-01-22T00:00:00Z","permalink":"https://example.com/p/gmx3ppt_mdp%E6%96%87%E4%BB%B6/","title":"GMX3ppt_mdp文件"},{"content":" 此post用来存放一些ppt上的内容，仅方便学习用途\nalt + shfit 可以选中列\n文件选择框\n编码的问题？ 一般建议用这个字体~ ","date":"2022-01-22T00:00:00Z","permalink":"https://example.com/p/gmx4ppt_notepad%E4%BD%BF%E7%94%A8/","title":"GMX4ppt_Notepad使用"},{"content":" 此post用来存放一些ppt上的内容，仅方便学习用途\ngmx dump命令 其他的例子也是 尿素的问题 Qtgrade ","date":"2022-01-22T00:00:00Z","permalink":"https://example.com/p/gmx5ppt_gromacs%E5%85%A5%E9%97%A8/","title":"GMX5ppt_GROMACS入门"},{"content":"collection.Counter()的方法，可以将传入的对象的出现次数返回来，\nCounter() 以字典的形式返回，数：出现次数 Counter().items() 以元组的形式返回，按数在传入的顺序中的顺序 （数：出现次数） Counter().keys() 以list的整体返回，数 Counter().values() 以list的整体返回，出现次数 code:\nfrom collections import Counter list_test = [2,7,4,5,6,0,6,6,6,4] Counter(list_test) result:\nCounter({2: 1, 7: 1, 4: 2, 5: 1, 6: 4, 0: 1}) code:\nCounter(list_test).items() result:\ndict_items([(2, 1), (7, 1), (4, 2), (5, 1), (6, 4), (0, 1)]) code:\nCounter(list_test)[6] result:\n4 code:\nCounter(list_test).keys() result:\ndict_keys([2, 7, 4, 5, 6, 0]) code:\nCounter(list_test).values() result:\ndict_values([1, 1, 2, 1, 4, 1]) ","date":"2022-01-21T00:00:00Z","permalink":"https://example.com/p/collection.counter%E8%AE%A1%E6%95%B0%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"collection.Counter()计数的使用"},{"content":" 整理 下关于 argparse的用法，主要用于参数提示\n#coding=utf-8 import argparse from ast import Store, parse from cgi import test from email import parser from itertools import count from pydoc import describe from ssl import ALERT_DESCRIPTION_UNEXPECTED_MESSAGE from this import s from tokenize import group from turtle import Turtle import turtle from numpy import False_ 使用前要实例化一个ArgumentParser对象\nparser = argparse.ArgumentParser(description=\u0026#34;这是ArugementParser中的description\u0026#34;) parser.parse_args() 这上面当你运行\npython test.py -h 注意要加上-h参数，就是出现主要的decsription中的str\nparser = argparse.ArgumentParser() parser.add_argument(\u0026#34;test\u0026#34;) args = parser.parse_args() print(\u0026#34;输出的内容是%s\u0026#34; % args.test) 这个其实就是添加了cmd中一个位置，将获取到的值存入到test这个变量中 要取变量的值先通过parse_args()获取到args对象\n比如我运行\npython test.py hello 则会返回 输出的内容是hello\n但是运行 python test.py 则会提示缺乏argument参数，而且会提示那个参数命为test，即代码中写的， 那么用这个来可以提示用户缺乏了啥参数，即把test换成描述性语言\n运行 python test.py hello world 则会返回 多了个参数，因为代码中只定义了一个占位\n这里怎么为这个参数添加描述呢，就是说怎么让别人运行-h时能知道这个参数代表啥呢？\nadd_argument()中的help参数可以帮忙\nparser = argparse.ArgumentParser() parser.add_argument(\u0026#34;test\u0026#34;,help=\u0026#34;这是test的代表意义\u0026#34;) args = parser.parse_args() print(\u0026#34;输出的内容是%s\u0026#34; % args.test) 运行python test.py -h 时就可以展示这个信息了\nadd_argument()中的参数type可以指定参数类型,没有指定则会默认为str\n传入不符合的则会报错\nparser = argparse.ArgumentParser() parser.add_argument(\u0026#34;test\u0026#34;,typ e=int) args = parser.parse_args() print(args) 可选参数，这里觉得可以起到一个定位参数的作用\nadd_argument参数\u0026quot;-\u0026quot; / “\u0026ndash;”；在参数前加上前缀\u0026ndash;，即意味着这个参数是可选参数,和之前的”里面不加-是不一样的含义\n\u0026ndash; / - 表示参数可选，用户给不给值无所谓，而不加- / \u0026ndash; 则表示这个参数必选要用\nparser = argparse.ArgumentParser() #“”是必须的 parser.add_argument(\u0026#34;-t\u0026#34;) args = parser.parse_args() print(\u0026#34;%s\u0026#34; % args) 运行python test.py不会报错，只是print那里会用None替代\n运行python test.py -t test 完美！！\nadd_argument参数action=\u0026ldquo;store_true\u0026rdquo;\nparser = argparse.ArgumentParser() parser.add_argument(\u0026#34;-t\u0026#34;,action=\u0026#34;store_true\u0026#34;) args = parser.parse_args() print(\u0026#34;%s\u0026#34; % args) 运行python test.py -t 只要使用了-t 的值则会True,但是运行python test.py -t test 则会报错，也就是说定义了action=\u0026ldquo;store_true\u0026rdquo;，就不能从命令行传值,实现这一目的的方法是将action参数的值指定为 \u0026ldquo;store_true\u0026rdquo;\n通过这个特点，就可以实现使用某一个特定功能这个目的\nadd_argument中的“-”和“\u0026ndash;”指定参数是一样的，但是这里有个比较巧妙的地方是：\nparser = argparse.ArgumentParser(description=\u0026#34;这是主要描述\u0026#34;) parser.add_argument(\u0026#34;-t\u0026#34;,\u0026#34;--test\u0026#34;,help=\u0026#34;这是代码中test（-t）的意义\u0026#34;) parser.add_argument(\u0026#34;-d\u0026#34;,\u0026#34;--dest\u0026#34;,help=\u0026#34;这是代码中dest（-d）的意义\u0026#34;) args = parser.parse_args() 上面add_argument “-” ”\u0026ndash;“ 这样写的好处在于\u0026ndash;后可以直接指定意义，方便写代码，如下\nprint(args.test) #等同于args.t print(args.dest) add_argument中的参数choices可以为参数传入的值设定一个范围,用户传入的值不在范围内则报错,注意用户上传然后系统读取到的是str类型，所以如果choices是int类型的话，要指定type=int,否则会报错\nparser = argparse.ArgumentParser() parser.add_argument(\u0026#34;-t\u0026#34;,\u0026#34;--test\u0026#34;,type=int,choices=[1,2,3]) args = parser.parse_args() print(args.test) add_argument中的参数action=\u0026ldquo;count\u0026quot;可以计算出用户使用了这个参数的次数,但是指定了action=\u0026ldquo;count\u0026quot;的话用户就不能传值了，否则会报错\nparser = argparse.ArgumentParser() parser.add_argument(\u0026#34;-t\u0026#34;,\u0026#34;--test\u0026#34;,action=\u0026#34;count\u0026#34;,help=\u0026#34;这是test的help\u0026#34;) arg = parser.parse_args() print(arg.test) add_argument()中的参数default可以指定参数的默认值，没设定默认值时默认值为None\n设置冲突参数 就是说当一些参数只能选择一个使用时，可以用方法add_mutually_exclusive_group()\nparser = argparse.ArgumentParser() group = parser.add_mutually_exclusive_group() group.add_argument(\u0026#34;-t\u0026#34;) group.add_argument(\u0026#34;-d\u0026#34;) args = parser.parse_args() 下面是对ArgumentParser方法中参数的说明：\nprog中可以赋予运行程序出现的语句,默认为sys.argv[0]就是你脚本的名字,就是程序此时会变成你自定义的名字\nparser = argparse.ArgumentParser() parser = argparse.ArgumentParser(prog=\u0026#34;这是测试\u0026#34;) parser.print_help() usage可以修改cmd运行程序中第二个出现的字体, 如果要传递prog中的值，则需要写成 %(prog)s\nparser = argparse.ArgumentParser(prog=\u0026#34;这是测试\u0026#34;,usage=\u0026#34;%(prog)s [usage中的字]\u0026#34;) parser.print_help() description则可以添加主要描述，会换行\nparser = argparse.ArgumentParser(prog=\u0026#34;这是测试\u0026#34;,usage=\u0026#34;%(prog)s [usage中的字]\u0026#34;,description=\u0026#34;这是主要描述\u0026#34;) parser.print_help() epilog可以赋予结束语的描述\nparser = argparse.ArgumentParser( prog=\u0026#34;这是测试\u0026#34;,usage=\u0026#34;%(prog)s [usage中的字]\u0026#34;, description=\u0026#34;这是主要描述\u0026#34;, epilog=\u0026#34;这是结束语\u0026#34;) parser.print_help() parents这个没看懂有什么用\normatter_class 这个参数可以指定展示文字的格式\nprefix_chars可以修改运行程序时候前面的-,改成自己需要的样式，如，想要运行python test.py +test 123\nparser = argparse.ArgumentParser(prefix_chars=\u0026#34;+\u0026#34;) parser.add_argument(\u0026#34;+t\u0026#34;) parser.add_argument(\u0026#34;+d\u0026#34;) args = parser.parse_args() print(args.t,args.d) fromfile_prefix_char=‘@’ 从文件中读取命令参数 有一个文件为args.txt 注意文件中的参数和值要换行 在parse_args([\u0026quot;@args.txt\u0026rdquo;]) 类似这样传入文件命 命令行参数从文件中读取\nparser = argparse.ArgumentParser(fromfile_prefix_chars=\u0026#39;@\u0026#39;) parser.add_argument(\u0026#39;-f\u0026#39;) parser.parse_args([\u0026#39;-f\u0026#39;, \u0026#39;foo\u0026#39;, \u0026#39;@args.txt\u0026#39;]) exit_on_error=False 可以捕获用户传入的值不符合要求,而不会被argparsr中断程序\n错误类型为argparse.ArgumentError,只存在Python3.9.。。。\nparser = argparse.ArgumentParser(exit_on_errors=False) parser.add_argument(\u0026#34;-t\u0026#34;,\u0026#34;--t\u0026#34;,type=int) try: parser.parse_args(\u0026#34;--t a\u0026#34;.split()) except argparse.ArgumentError: print(\u0026#34;nonon!!\u0026#34;) add_argument方法中的参数说明\nname or flags其实就是赋予参数名\nparser = argparse.ArgumentParser() parser.add_argument(\u0026#34;-t\u0026#34;,\u0026#34;--t\u0026#34;)#一般会赋予短名和长名 action则是用来对参数的值进行互动的\n默认为action = \u0026ldquo;store\u0026rdquo; 即存储值 action = \u0026ldquo;store_const\u0026rdquo; 这是存储const的值；用户可以传值但是没有用，改变不了这个const的值 parser.add_argument(\u0026#34;--t\u0026#34;,action=\u0026#34;store_const\u0026#34;,const=20) print(parser.parse_args([\u0026#39;--t\u0026#39;])) action = “store_true” / action = “store_false”,可以理解为一种开关，只有用户使用了这个参数，才会返回Ture,否则会false parser.add_argument(\u0026#34;--t\u0026#34;,action=\u0026#34;store_true\u0026#34;) print(parser.parse_args().t) result:\npython test.py -t 是Ture python test.py 是False python test,py -t 123 报错，所以是不能传值的 action = “append”,这样写用户可以 python test.py -t 123 -t 123 传入多个t的值，且会被放到一个list中 parser.add_argument(\u0026#34;-t\u0026#34;,action=\u0026#34;append\u0026#34;) print(parser.parse_args().t) 这样写的话，可以运行python test.py -t 123 -t 456 但是只用456值会显示,就是说它会用最新的值取覆盖旧的值\nparser.add_argument(\u0026#34;-t\u0026#34;) print(parser.parse_args().t) action = \u0026ldquo;append_const\u0026rdquo;,这样会将const定义的的值传给定义的参数中，一般在多个参数需要在同一列表中存储常数时会有用 action = \u0026ldquo;count\u0026rdquo;,这个可以用来计算一个参数出现的次数 parser.add_argument(\u0026#34;-t\u0026#34;,action=\u0026#34;count\u0026#34;,default=0) print(parser.parse_args().t 运行python test.py -t -t 和 python test,py -tt 是一样的效果,python test.py -t 123 会报错，就是说定义了action=\u0026ldquo;count\u0026quot;的话用户是不能传值的\naction=\u0026ldquo;version\u0026rdquo;,help参数则是用来添加版信息,action定义了version，则后面要说明version的str parser = argparse.ArgumentParser(prog=\u0026#34;TEST\u0026#34;) parser.add_argument(\u0026#34;-t\u0026#34;,action=\u0026#34;version\u0026#34;,version=\u0026#34; %(prog)s 2.0\u0026#34;) args = parser.parse_args() print(\u0026#34;输出的内容是%s\u0026#34; % args.t) nargs,如果等于一个数字起到一个指定参数用户可以传入多少个值，然后程序会将这些值存在一个list中, 如果等于\u0026rdquo;*\u0026rdquo; \u0026ldquo;+\u0026rdquo; 则会将用户所有参数传入的值放在同一个list中,如果等于\u0026quot;?\u0026quot; 则运行用户直接提供一个文件，注意程序中记得将type = argparse.FileType(\u0026ldquo;w\u0026rdquo;)\nparser = argparse.ArgumentParser() parser.add_argument(\u0026#34;-t\u0026#34;,\u0026#34;--test\u0026#34;,nargs=3) print(parser.parse_args().test) 运行python test.py -t 123 456 789这样才可以 python test.py -t 123 456 和 python test.py -t 123 不行\nparser = argparse.ArgumentParser() parser.add_argument(\u0026#34;-t\u0026#34;,\u0026#34;--test\u0026#34;,nargs=\u0026#34;+\u0026#34;) parser.add_argument(\u0026#34;-dt\u0026#34;,\u0026#34;--dtest\u0026#34;,nargs=\u0026#34;+\u0026#34;) print(parser.parse_args()) python test.py -t 123 -dt 456 则会将 123 456 放在一个list中\ntype,可以定义用户传入数据的类型\nchoic,可以限制用户输入参数的值范围以及选项\nrequeired=True,可以将一个可选参数变为必选\nhelp,则是添加帮助信息,具体表现为用户运行python test.py -h 时显示的参数信息\nparser =argparse.ArgumentParser() parser.add_argument(\u0026#34;-t\u0026#34;,help=\u0026#34;123\u0026#34;) parser.add_argument(\u0026#34;-d\u0026#34;,help=\u0026#34;456\u0026#34;) #要有下面这句话 python test.py -h 才有用 parser.parse_args() parser = argparse.ArgumentParser(prog=\u0026#39;frobble\u0026#39;) parser.add_argument(\u0026#39;bar\u0026#39;, nargs=\u0026#39;?\u0026#39;, type=int, default=42, help=\u0026#39;the bar to %(prog)s (default: %(default)s)\u0026#39;) parser.print_help() 由这个可以看出如果要在方法内调用内部参数的值，则要写成 %(参数名)s\nmetavar,在显示帮助信息时，会将参数名字自动变成大学的，如-t -T,使用metavar 可以自定义-T名字\narser = argparse.ArgumentParser() parser.add_argument(\u0026#34;-t\u0026#34;,\u0026#34;--test\u0026#34;,metavar=\u0026#34;666\u0026#34;) parser.parse_args() usage: test.py [-h] [-t 666] optional arguments: -h, --help show this help message and exit -t 666, --test 666 dest,实则是修改属性名，系统会自动将\u0026ndash;test 或者 -test 生成这个dest的值 ,可以修改\nparser = argparse.ArgumentParser() parser.add_argument(\u0026#34;-t\u0026#34;,\u0026#34;--test\u0026#34;,dest=\u0026#34;dddtest\u0026#34;) parser.parse_args() usage: test.py [-h] [-t DDDTEST] optional arguments: -h, --help show this help message and exit -t DDDTEST, --test DDDTEST 参数和参数值在程序中的赋予 一般写成这样 [\u0026quot;\u0026ndash;test\u0026quot;,\u0026ldquo;value\u0026rdquo;] 对于长参数(\u0026ndash;开头)可以写成这样 [\u0026quot;\u0026ndash;test=value\u0026quot;] 对于短参数（-开头）可以写成这样 [\u0026quot;-tvalue\u0026quot;] 另外对于短参数的话，可以使用[\u0026quot;-xyzT\u0026quot;] 连接起来 x,y,z是参数，T是值，这里的结果是x=True,y=True,z=T\nparser = argparse.ArgumentParser() parser.add_argument(\u0026#34;-t\u0026#34;,action=\u0026#34;store_true\u0026#34;) parser.add_argument(\u0026#34;-d\u0026#34;) parser.parse_args([\u0026#34;-td1\u0026#34;]) 参数缩写,在程序内部是可以不写全参数名字的\nparser = argparse.ArgumentParser() parser.add_argument(\u0026#34;-test\u0026#34;) parser.add_argument(\u0026#34;-dtest\u0026#34;) a = parser.parse_args(\u0026#34;-tes 123\u0026#34;.split()) parser.parse_args(\u0026#34;-dtes 456\u0026#34;.split()) print(a) 程序内部赋值和用户赋值是不同的,就是说用户运行 python test.py -t 123 这是用户赋予-t参数的值123,而在程序内部则是 这样赋值的 parse_arg([\u0026quot;-t123\u0026quot;]) 等写法\n","date":"2022-01-20T00:00:00Z","permalink":"https://example.com/p/argparse%E5%8F%82%E6%95%B0%E4%B8%AA%E6%80%A7%E5%8C%96%E7%9A%84%E7%94%A8%E6%B3%95%E6%95%B4%E7%90%86/","title":"argparse参数个性化的用法整理"},{"content":"千万不要用行和列的思维去想axis，因为行和列是没有方向的，这样想会在遇到不同的例子时感到困惑。\n二维的理解 轴用来为超过一维的数组定义的属性，二维数据拥有两个轴：第0轴沿着行的垂直往下，第1轴沿着列的方向水平延伸。 注意看，官方对于0和1的解释是轴，也就是坐标轴。而坐标轴是有方向的，所以千万不要用行和列的思维去想axis，因为行和列是没有方向的，这样想会在遇到不同的例子时感到困惑。\n根据官方的说法，1表示横轴，方向从左到右；0表示纵轴，方向从上到下。当axis=1时，数组的变化是横向的，而体现出来的是列的增加或者减少。\n其实axis的重点在于方向，而不是行和列。具体到各种用法而言也是如此。当axis=1时，如果是求平均，那么是从左到右横向求平均；如果是拼接，那么也是左右横向拼接；如果是drop，那么也是横向发生变化，体现为列的减少。\n当考虑了方向，即axis=1为横向，axis=0为纵向，而不是行和列，那么所有的例子就都统一了。\ncode:\nimport numpy as np a = np.array([[1,2,3],[4,5,6]]) a result:\narray([[1, 2, 3], [4, 5, 6]]) code:\na.sum(axis=0) result:\narray([5, 7, 9]) code:\na.sum(axis=1) result:\narray([ 6, 15]) code:\na.sum(axis=-1) result:\narray([ 6, 15]) 高维的理解 这里解释一下三维，更高维也就都能理解了 地址：https://www.jianshu.com/p/93317c0dca6a\n什么意思呢？就是比如：\n当axis=0时，此时就时要找除了第一个下标，其他下标相同的放在一起，比如a000、a100、a200这个为一组，a001、a101、a201为一组\u0026hellip;. 最终为(4，5) 当axis=1时，此时就时要找除了第二个下标，其他下标相同的放在一起，比如a000、a010、a020、a030 这个为一组，a001、a011、a021、a031为一组\u0026hellip;.（3，5）\n当axis=-1（即为2时，解释下-1是什么，是找到最近的一个数，因为我们这里的下标就只有axxx，三位即0，1，2所以-1即为axis=2 a000,a001,a002,a003为一组。。。。 (3,4) image.png 总的来说就是 先分组处理，再根据要求合并\n","date":"2022-01-18T00:00:00Z","permalink":"https://example.com/p/python%E5%AF%B9axis%E7%9A%84%E7%90%86%E8%A7%A3/","title":"Python对axis的理解"},{"content":"pprint.pprint和 pprint.pformat 实现数据读写功能 pprint模块中使用的格式化可以按照一种格式正确的显示数据, 这种格式即可被解析器解析, 又很易读. 输出保存在一个单行内, 但如果有必要, 在分割多行数据时也可使用缩进表示.\nPython 的 pprint.pformat() 函数会以字符串形式，返回列表或字典中的内容。可以将其保存为一个 py 文件，以便将来读取使用\n具体例子 pprint的美化 code:\n#导入包 import pprint data_test = { \u0026#34;a\u0026#34;:{\u0026#34;1\u0026#34;:\u0026#34;dsad\u0026#34;,\u0026#34;2\u0026#34;:\u0026#34;hehehee\u0026#34;}, \u0026#34;b\u0026#34;:{\u0026#34;3\u0026#34;:\u0026#34;fdsff\u0026#34;,\u0026#34;4\u0026#34;:\u0026#34;rtdfd\u0026#34;,\u0026#39;f\u0026#39;: \u0026#39;F\u0026#39;, \u0026#39;g\u0026#39;: \u0026#39;G\u0026#39;, \u0026#39;h\u0026#39;: \u0026#39;H\u0026#39;}, \u0026#34;c\u0026#34;:{\u0026#39;e\u0026#39;: \u0026#39;E\u0026#39;, \u0026#39;f\u0026#39;: \u0026#39;F\u0026#39;, \u0026#39;g\u0026#39;: \u0026#39;G\u0026#39;, \u0026#39;h\u0026#39;: \u0026#39;H\u0026#39;,\u0026#39;i\u0026#39;: \u0026#39;I\u0026#39;, \u0026#39;j\u0026#39;: \u0026#39;J\u0026#39;, \u0026#39;k\u0026#39;: \u0026#39;K\u0026#39;, \u0026#39;l\u0026#39;: \u0026#39;L\u0026#39;} } print(data_test) print(\u0026#34;美化后\u0026#34;) #当元素超过大于等于8，才会真正展现pprint的美化用处 #具体来说就是每个行占据一个元素对象全部数据或者竖列展示 pprint.pprint(data_test) result:\n{\u0026#39;a\u0026#39;: {\u0026#39;1\u0026#39;: \u0026#39;dsad\u0026#39;, \u0026#39;2\u0026#39;: \u0026#39;hehehee\u0026#39;}, \u0026#39;b\u0026#39;: {\u0026#39;3\u0026#39;: \u0026#39;fdsff\u0026#39;, \u0026#39;4\u0026#39;: \u0026#39;rtdfd\u0026#39;, \u0026#39;f\u0026#39;: \u0026#39;F\u0026#39;, \u0026#39;g\u0026#39;: \u0026#39;G\u0026#39;, \u0026#39;h\u0026#39;: \u0026#39;H\u0026#39;}, \u0026#39;c\u0026#39;: {\u0026#39;e\u0026#39;: \u0026#39;E\u0026#39;, \u0026#39;f\u0026#39;: \u0026#39;F\u0026#39;, \u0026#39;g\u0026#39;: \u0026#39;G\u0026#39;, \u0026#39;h\u0026#39;: \u0026#39;H\u0026#39;, \u0026#39;i\u0026#39;: \u0026#39;I\u0026#39;, \u0026#39;j\u0026#39;: \u0026#39;J\u0026#39;, \u0026#39;k\u0026#39;: \u0026#39;K\u0026#39;, \u0026#39;l\u0026#39;: \u0026#39;L\u0026#39;}} 美化后 {\u0026#39;a\u0026#39;: {\u0026#39;1\u0026#39;: \u0026#39;dsad\u0026#39;, \u0026#39;2\u0026#39;: \u0026#39;hehehee\u0026#39;}, \u0026#39;b\u0026#39;: {\u0026#39;3\u0026#39;: \u0026#39;fdsff\u0026#39;, \u0026#39;4\u0026#39;: \u0026#39;rtdfd\u0026#39;, \u0026#39;f\u0026#39;: \u0026#39;F\u0026#39;, \u0026#39;g\u0026#39;: \u0026#39;G\u0026#39;, \u0026#39;h\u0026#39;: \u0026#39;H\u0026#39;}, \u0026#39;c\u0026#39;: {\u0026#39;e\u0026#39;: \u0026#39;E\u0026#39;, \u0026#39;f\u0026#39;: \u0026#39;F\u0026#39;, \u0026#39;g\u0026#39;: \u0026#39;G\u0026#39;, \u0026#39;h\u0026#39;: \u0026#39;H\u0026#39;, \u0026#39;i\u0026#39;: \u0026#39;I\u0026#39;, \u0026#39;j\u0026#39;: \u0026#39;J\u0026#39;, \u0026#39;k\u0026#39;: \u0026#39;K\u0026#39;, \u0026#39;l\u0026#39;: \u0026#39;L\u0026#39;}} ppformat保存作用 code\n#导入包 import pprint data_test = { \u0026#34;a\u0026#34;:{\u0026#34;1\u0026#34;:\u0026#34;dsad\u0026#34;,\u0026#34;2\u0026#34;:\u0026#34;hehehee\u0026#34;}, \u0026#34;b\u0026#34;:{\u0026#34;3\u0026#34;:\u0026#34;fdsff\u0026#34;,\u0026#34;4\u0026#34;:\u0026#34;rtdfd\u0026#34;,\u0026#39;f\u0026#39;: \u0026#39;F\u0026#39;, \u0026#39;g\u0026#39;: \u0026#39;G\u0026#39;, \u0026#39;h\u0026#39;: \u0026#39;H\u0026#39;}, \u0026#34;c\u0026#34;:{\u0026#39;e\u0026#39;: \u0026#39;E\u0026#39;, \u0026#39;f\u0026#39;: \u0026#39;F\u0026#39;, \u0026#39;g\u0026#39;: \u0026#39;G\u0026#39;, \u0026#39;h\u0026#39;: \u0026#39;H\u0026#39;,\u0026#39;i\u0026#39;: \u0026#39;I\u0026#39;, \u0026#39;j\u0026#39;: \u0026#39;J\u0026#39;, \u0026#39;k\u0026#39;: \u0026#39;K\u0026#39;, \u0026#39;l\u0026#39;: \u0026#39;L\u0026#39;} } # pprint.pformat(data_test) #py存起来 可以去看看生成的data_test.py 跟上面的pprint是一样的 file_data_test = open(\u0026#39;data_test.py\u0026#39;,\u0026#39;w\u0026#39;,encoding=\u0026#34;utf-8\u0026#34;) file_data_test.write(\u0026#39;pprint_pformattest = \u0026#39;+pprint.pformat(data_test)+\u0026#39;\\n\u0026#39;) file_data_test.close() #导入刚生成的data_test.py import data_test # 访问存储在data_set.py的数据 #实例化下导入的py中的对象 data = data_test.pprint_pformattest #不用加() 否则 \u0026#39;dict\u0026#39; object is not callable print(data[\u0026#34;c\u0026#34;]) result:\n{\u0026#39;e\u0026#39;: \u0026#39;E\u0026#39;, \u0026#39;f\u0026#39;: \u0026#39;F\u0026#39;, \u0026#39;g\u0026#39;: \u0026#39;G\u0026#39;, \u0026#39;h\u0026#39;: \u0026#39;H\u0026#39;, \u0026#39;i\u0026#39;: \u0026#39;I\u0026#39;, \u0026#39;j\u0026#39;: \u0026#39;J\u0026#39;, \u0026#39;k\u0026#39;: \u0026#39;K\u0026#39;, \u0026#39;l\u0026#39;: \u0026#39;L\u0026#39;} ","date":"2022-01-15T00:00:00Z","permalink":"https://example.com/p/pprint.pformat%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%86%99/","title":"pprint.pformat数据读写"},{"content":"将Python虚拟环境添加到Jupyter Notebook中\n将利用conda自定义的python虚拟环境添加到jupyter notebook中\n在activate env后运行(安装ipkernel)\nconda install ipkernel 添加到Jupyter\npython -m ipkernel install --name env名字 最后重启Jupyter即可\n","date":"2022-01-13T00:00:00Z","permalink":"https://example.com/p/env%E6%B7%BB%E5%8A%A0%E5%88%B0jupyter%E4%B8%AD/","title":"env添加到Jupyter中"},{"content":"Macromolecular Modeling with Rosetta\nlink:https://www.annualreviews.org/doi/abs/10.1146/annurev.biochem.77.062906.171838\n","date":"2022-01-12T00:00:00Z","permalink":"https://example.com/p/macromolecular-modeling-with-rosetta/","title":"Macromolecular Modeling with Rosetta"},{"content":"ROSETTALIGAND Docking with Full Ligand and Receptor Flexibility\nlink:https://www.sciencedirect.com/science/article/abs/pii/S0022283608014289\n","date":"2022-01-12T00:00:00Z","permalink":"https://example.com/p/rosettaligand-docking-with-full-ligand-and-receptor-flexibility/","title":"ROSETTALIGAND Docking with Full Ligand and Receptor Flexibility "},{"content":"原因在于grid_scores_在sklearn0.20版本中已被删除，取而代之的是cv_results_\n","date":"2022-01-11T00:00:00Z","permalink":"https://example.com/p/gridsearchcv-object-has-no-attribute-grid_scores_/","title":"'GridSearchCV' object has no attribute 'grid_scores_'"},{"content":"! ","date":"2022-01-10T00:00:00Z","permalink":"https://example.com/p/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E5%9F%BA%E4%BA%8Epython%E7%9A%84%E7%90%86%E8%AE%BA%E4%B8%8E%E5%AE%9E%E7%8E%B0%E9%AB%98%E6%B8%85%E4%B8%AD%E6%96%87%E7%89%88/","title":"《深度学习入门：基于Python的理论与实现》高清中文版"},{"content":"这里记录下anaconda常用的命名\n1.创建虚拟环境\nconda create -n 虚拟环境env的名字 python=X.X(可选)\n或者同时安装一些包\nconda create -n 虚拟环境env的名字 numpy pandas(等包名) python=X.X(可选)\n2.激活虚拟环境\nLinux: source activate env名字\nWindow: activate env名字\n3.退出虚拟环境\nLinux: source deactivate env名字\nWindow：deactivate env名字\n4.删除虚拟环境\n删除整个环境\nconda remove -n env名字 \u0026ndash;all\n删除环境中的某个包\nconda remove -n env名字 包名\n5.其他\n查看安装了哪些包\nconda list\n安装包\nconda install\n查看当前存在哪些env\nconda env list\n检查更新当前conda\nconda update conda\n","date":"2022-01-10T00:00:00Z","permalink":"https://example.com/p/anaconda%E5%88%9B%E5%BB%BAenv%E7%8E%AF%E5%A2%83/","title":"Anaconda创建env环境"},{"content":"#从Ipython.display 导入 Image from IPython.display import Image Image(\u0026#34;./47ce630c275ddfe89fa3c49ffaa767ce.jpg\u0026#34;) 可以使用个循环去封装，但是此时Image前记得加上display 即写成display(Image())\n","date":"2022-01-06T00:00:00Z","permalink":"https://example.com/p/%E4%BD%BF%E7%94%A8image%E5%9C%A8notebook%E4%B8%AD%E5%B1%95%E7%A4%BA%E5%9B%BE%E7%89%87/","title":"使用Image在notebook中展示图片"},{"content":"一般说来，你可以把句柄想象成一个对文本信息的“封装”。 相比普通文本信息，使用句柄至少有两个好处：\n对于以不同方式存储的信息，句柄提供了一个标准的处理方法。这些文本信息可能来自文件、内存中的一个字符串、命令行指令的输出或者来自于远程网站信息，但是句柄提供了一种通用的方式处理这些不同格式和来源的文本信息。 句柄可以依次读取文本信息，而不是一次读取所有信息。这点在处理超大文件时尤为有用，因为一次载入一个大文件可能会占去你所有的内存。 不论是从文件读取文本信息还是将文本信息写入文件，句柄都能胜任。在读取文件时，常用的函数有 read() 和 readline() , 前者可以通过句柄读取所有文本信息，而后者则每次读取一行；对于文本信息的写入，则通常使用 write() 函数。 句柄最常见的使用就是从文件读取信息，这可以通过Python内置函数 open 来完成。 下面示例中，我们打开一个指向文件 m_cold.fasta （可通过网址 http://biopython.org/DIST/docs/tutorial/examples/m_cold.fasta 获取）的句柄：\n\u0026gt;\u0026gt;\u0026gt; handle = open(\u0026#34;m_cold.fasta\u0026#34;, \u0026#34;r\u0026#34;) \u0026gt;\u0026gt;\u0026gt; handle.readline() \u0026#34;\u0026gt;gi|8332116|gb|BE037100.1|BE037100 MP14H09 MP Mesembryanthemum ...\\n\u0026#34; Biopython中句柄常用来向解析器（parsers）传递信息。比如说，自Biopython1.54版本后， Bio.SeqIO 和 Bio.AlignIO 模块中的主要函数都可以使用文件名来代替句柄使用：\nfrom Bio import SeqIO for record in SeqIO.parse(\u0026#34;m_cold.fasta\u0026#34;, \u0026#34;fasta\u0026#34;): print record.id, len(record) 在比较早的BioPython版本中，必须使用句柄：\nfrom Bio import SeqIO handle = open(\u0026#34;m_cold.fasta\u0026#34;, \u0026#34;r\u0026#34;) for record in SeqIO.parse(handle, \u0026#34;fasta\u0026#34;): print record.id, len(record) handle.close() 这种操作方式仍有其用武之地，比如在解析一个gzip压缩的FASTA文件中：\nimport gzip from Bio import SeqIO handle = gzip.open(\u0026#34;m_cold.fasta.gz\u0026#34;) for record in SeqIO.parse(handle, \u0026#34;fasta\u0026#34;): print record.id, len(record) handle.close() 从字符串创建句柄 一个比较有用的工具是将字符串中包含的文本信息传递给一个句柄。以下示例是使用Python标准文库 cStringIO 来展示如何实现的：\n\u0026gt;\u0026gt;\u0026gt; my_info = \u0026#39;A string\\n with multiple lines.\u0026#39; \u0026gt;\u0026gt;\u0026gt; print my_info A string with multiple lines. \u0026gt;\u0026gt;\u0026gt; from StringIO import StringIO \u0026gt;\u0026gt;\u0026gt; my_info_handle = StringIO(my_info) \u0026gt;\u0026gt;\u0026gt; first_line = my_info_handle.readline() \u0026gt;\u0026gt;\u0026gt; print first_line A string \u0026gt;\u0026gt;\u0026gt; second_line = my_info_handle.readline() \u0026gt;\u0026gt;\u0026gt; print second_line with multiple lines. ","date":"2022-01-06T00:00:00Z","permalink":"https://example.com/p/%E5%88%B0%E5%BA%95%E4%BB%80%E4%B9%88%E6%98%AF%E5%8F%A5%E6%9F%84/","title":"到底什么是句柄"},{"content":"理解蛋白质序列在机器学习模型的编码 link:https://academic.oup.com/bioinformatics/article/34/15/2642/4951834\n","date":"2022-01-03T00:00:00Z","permalink":"https://example.com/p/learned-protein-embeddings-for-machine-learning/","title":"Learned protein embeddings for machine learning"},{"content":" ","date":"2022-01-02T00:00:00Z","permalink":"https://example.com/p/if__name_____main___%E7%9A%84%E4%BD%9C%E7%94%A8/","title":"if__name__==___main___的作用"},{"content":"机器学习助力酶定向进化 link:http://www.cqvip.com/qk/92127x/202004/7102480996.html\n","date":"2021-12-29T00:00:00Z","permalink":"https://example.com/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%8A%A9%E5%8A%9B%E9%85%B6%E5%AE%9A%E5%90%91%E8%BF%9B%E5%8C%96/","title":"机器学习助力酶定向进化"},{"content":" ","date":"2021-12-27T00:00:00Z","permalink":"https://example.com/p/%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%B8%A6%E4%BD%A0%E5%8E%BB%E9%9D%A2%E8%AF%95/","title":"《百面机器学习_算法工程师带你去面试》"},{"content":" 这是为了熟悉并加强numpy而找来做的练习，来源：https://github.com/fengdu78/Data-Science-Notes/tree/master/2.numpy/numpy_exercises\n__author__ = \u0026#34;kyubyong. kbpark.linguist@gmail.com\u0026#34; import numpy as np np.__version__ \u0026#39;1.11.3\u0026#39; Order statistics Q1. Return the minimum value of x along the second axis.\nx = np.arange(4).reshape((2, 2)) print(\u0026#34;x=\\n\u0026#34;, x) print(\u0026#34;ans=\\n\u0026#34;, np.amin(x, 1)) x= [[0 1] [2 3]] ans= [0 2] Q2. Return the maximum value of x along the second axis. Reduce the second axis to the dimension with size one.\nx = np.arange(4).reshape((2, 2)) print(\u0026#34;x=\\n\u0026#34;, x) print(\u0026#34;ans=\\n\u0026#34;, np.amax(x, 1, keepdims=True)) x= [[0 1] [2 3]] ans= [[1] [3]] Q3. Calcuate the difference between the maximum and the minimum of x along the second axis.\nx = np.arange(10).reshape((2, 5)) print(\u0026#34;x=\\n\u0026#34;, x) out1 = np.ptp(x, 1) out2 = np.amax(x, 1) - np.amin(x, 1) assert np.allclose(out1, out2) print(\u0026#34;ans=\\n\u0026#34;, out1) x= [[0 1 2 3 4] [5 6 7 8 9]] ans= [4 4] Q4. Compute the 75th percentile of x along the second axis.\nx = np.arange(1, 11).reshape((2, 5)) print(\u0026#34;x=\\n\u0026#34;, x) print(\u0026#34;ans=\\n\u0026#34;, np.percentile(x, 75, 1)) x= [[ 1 2 3 4 5] [ 6 7 8 9 10]] ans= [ 4. 9.] Averages and variances Q5. Compute the median of flattened x.\nx = np.arange(1, 10).reshape((3, 3)) print(\u0026#34;x=\\n\u0026#34;, x) print(\u0026#34;ans=\\n\u0026#34;, np.median(x)) x= [[1 2 3] [4 5 6] [7 8 9]] ans= 5.0 Q6. Compute the weighted average of x.\nx = np.arange(5) weights = np.arange(1, 6) out1 = np.average(x, weights=weights) out2 = (x*(weights/weights.sum())).sum() assert np.allclose(out1, out2) print(out1) 2.66666666667 Q7. Compute the mean, standard deviation, and variance of x along the second axis.\nx = np.arange(5) print(\u0026#34;x=\\n\u0026#34;,x) out1 = np.mean(x) out2 = np.average(x) assert np.allclose(out1, out2) print(\u0026#34;mean=\\n\u0026#34;, out1) out3 = np.std(x) out4 = np.sqrt(np.mean((x - np.mean(x)) ** 2 )) assert np.allclose(out3, out4) print(\u0026#34;std=\\n\u0026#34;, out3) out5 = np.var(x) out6 = np.mean((x - np.mean(x)) ** 2 ) assert np.allclose(out5, out6) print(\u0026#34;variance=\\n\u0026#34;, out5) x= [0 1 2 3 4] mean= 2.0 std= 1.41421356237 variance= 2.0 Correlating Q8. Compute the covariance matrix of x and y.\nx = np.array([0, 1, 2]) y = np.array([2, 1, 0]) print(\u0026#34;ans=\\n\u0026#34;, np.cov(x, y)) ans= [[ 1. -1.] [-1. 1.]] Q9. In the above covariance matrix, what does the -1 mean?\nIt means x and y correlate perfectly in opposite directions.\nQ10. Compute Pearson product-moment correlation coefficients of x and y.\nx = np.array([0, 1, 3]) y = np.array([2, 4, 5]) print(\u0026#34;ans=\\n\u0026#34;, np.corrcoef(x, y)) ans= [[ 1. 0.92857143] [ 0.92857143 1. ]] Q11. Compute cross-correlation of x and y.\nx = np.array([0, 1, 3]) y = np.array([2, 4, 5]) print(\u0026#34;ans=\\n\u0026#34;, np.correlate(x, y)) ans= [19] Histograms Q12. Compute the histogram of x against the bins.\nx = np.array([0.5, 0.7, 1.0, 1.2, 1.3, 2.1]) bins = np.array([0, 1, 2, 3]) print(\u0026#34;ans=\\n\u0026#34;, np.histogram(x, bins)) import matplotlib.pyplot as plt %matplotlib inline plt.hist(x, bins=bins) plt.show() ans= (array([2, 3, 1], dtype=int64), array([0, 1, 2, 3])) Q13. Compute the 2d histogram of x and y.\nxedges = [0, 1, 2, 3] yedges = [0, 1, 2, 3, 4] x = np.array([0, 0.1, 0.2, 1., 1.1, 2., 2.1]) y = np.array([0, 0.1, 0.2, 1., 1.1, 2., 3.3]) H, xedges, yedges = np.histogram2d(x, y, bins=(xedges, yedges)) print(\u0026#34;ans=\\n\u0026#34;, H) plt.scatter(x, y) plt.grid() ans= [[ 3. 0. 0. 0.] [ 0. 2. 0. 0.] [ 0. 0. 1. 1.]] Q14. Count number of occurrences of 0 through 7 in x.\nx = np.array([0, 1, 1, 3, 2, 1, 7]) print(\u0026#34;ans=\\n\u0026#34;, np.bincount(x)) ans= [1 3 1 1 0 0 0 1] Q15. Return the indices of the bins to which each value in x belongs.\nx = np.array([0.2, 6.4, 3.0, 1.6]) bins = np.array([0.0, 1.0, 2.5, 4.0, 10.0]) print(\u0026#34;ans=\\n\u0026#34;, np.digitize(x, bins)) ans= [1 4 3 2] ","date":"2021-12-21T00:00:00Z","permalink":"https://example.com/p/numpy%E5%9F%BA%E7%A1%80%E7%BB%83%E4%B9%A013_statistics/","title":"numpy基础练习13_Statistics"},{"content":" 这是为了熟悉并加强numpy而找来做的练习，来源：https://github.com/fengdu78/Data-Science-Notes/tree/master/2.numpy/numpy_exercises\nimport numpy as np np.__version__ \u0026#39;1.11.2\u0026#39; author = \u0026#39;kyubyong. longinglove@nate.com\u0026#39; Sorting Q1. Sort x along the second axis.\nx = np.array([[1,4],[3,1]]) out = np.sort(x, axis=1) x.sort(axis=1) assert np.array_equal(out, x) print out [[1 4] [1 3]] Q2. Sort pairs of surnames and first names and return their indices. (first by surname, then by name).\nsurnames = (\u0026#39;Hertz\u0026#39;, \u0026#39;Galilei\u0026#39;, \u0026#39;Hertz\u0026#39;) first_names = (\u0026#39;Heinrich\u0026#39;, \u0026#39;Galileo\u0026#39;, \u0026#39;Gustav\u0026#39;) print np.lexsort((first_names, surnames)) [1 2 0] Q3. Get the indices that would sort x along the second axis.\nx = np.array([[1,4],[3,1]]) out = np.argsort(x, axis=1) print out [[0 1] [1 0]] Q4. Create an array such that its fifth element would be the same as the element of sorted x, and it divide other elements by their value.\nx = np.random.permutation(10) print \u0026#34;x =\u0026#34;, x print \u0026#34;\\nCheck the fifth element of this new array is 5, the first four elements are all smaller than 5, and 6th through the end are bigger than 5\\n\u0026#34;, out = np.partition(x, 5) x.partition(5) # in-place equivalent assert np.array_equal(x, out) print out x = [5 1 6 3 9 8 2 7 4 0] Check the fifth element of this new array is 5, the first four elements are all smaller than 5, and 6th through the end are bigger than 5 [2 0 4 3 1 5 8 7 6 9] Q5. Create the indices of an array such that its third element would be the same as the element of sorted x, and it divide other elements by their value.\nx = np.random.permutation(10) print \u0026#34;x =\u0026#34;, x partitioned = np.partition(x, 3) indices = np.argpartition(x, 3) print \u0026#34;partitioned =\u0026#34;, partitioned print \u0026#34;indices =\u0026#34;, partitioned assert np.array_equiv(x[indices], partitioned) x = [2 8 3 7 5 6 4 0 9 1] partitioned = [0 1 2 3 4 5 8 6 9 7] indices = [0 1 2 3 4 5 8 6 9 7] Searching Q6. Get the maximum and minimum values and their indices of x along the second axis.\nx = np.random.permutation(10).reshape(2, 5) print \u0026#34;x =\u0026#34;, x print \u0026#34;maximum values =\u0026#34;, np.max(x, 1) print \u0026#34;max indices =\u0026#34;, np.argmax(x, 1) print \u0026#34;minimum values =\u0026#34;, np.min(x, 1) print \u0026#34;min indices =\u0026#34;, np.argmin(x, 1) x = [[0 5 9 8 2] [3 7 4 1 6]] maximum values = [9 7] max indices = [2 1] minimum values = [0 1] min indices = [0 3] Q7. Get the maximum and minimum values and their indices of x along the second axis, ignoring NaNs.\nx = np.array([[np.nan, 4], [3, 2]]) print \u0026#34;maximum values ignoring NaNs =\u0026#34;, np.nanmax(x, 1) print \u0026#34;max indices =\u0026#34;, np.nanargmax(x, 1) print \u0026#34;minimum values ignoring NaNs =\u0026#34;, np.nanmin(x, 1) print \u0026#34;min indices =\u0026#34;, np.nanargmin(x, 1) maximum values ignoring NaNs = [ 4. 3.] max indices = [1 0] minimum values ignoring NaNs = [ 4. 2.] min indices = [1 1] Q8. Get the values and indices of the elements that are bigger than 2 in x.\nx = np.array([[1, 2, 3], [1, 3, 5]]) print \u0026#34;Values bigger than 2 =\u0026#34;, x[x\u0026gt;2] print \u0026#34;Their indices are \u0026#34;, np.nonzero(x \u0026gt; 2) assert np.array_equiv(x[x\u0026gt;2], x[np.nonzero(x \u0026gt; 2)]) assert np.array_equiv(x[x\u0026gt;2], np.extract(x \u0026gt; 2, x)) Values bigger than 2 = [3 3 5] Their indices are (array([0, 1, 1], dtype=int64), array([2, 1, 2], dtype=int64)) Q9. Get the indices of the elements that are bigger than 2 in the flattend x.\nx = np.array([[1, 2, 3], [1, 3, 5]]) print np.flatnonzero(x\u0026gt;2) assert np.array_equiv(np.flatnonzero(x), x.ravel().nonzero()) [2 4 5] Q10. Check the elements of x and return 0 if it is less than 0, otherwise the element itself.\nx = np.arange(-5, 4).reshape(3, 3) print np.where(x \u0026lt;0, 0, x) [[0 0 0] [0 0 0] [1 2 3]] Q11. Get the indices where elements of y should be inserted to x to maintain order.\nx = [1, 3, 5, 7, 9] y = [0, 4, 2, 6] np.searchsorted(x, y) array([0, 2, 1, 3], dtype=int64) Counting Q12. Get the number of nonzero elements in x.\nx = [[0,1,7,0,0],[3,0,0,2,19]] print np.count_nonzero(x) assert np.count_nonzero(x) == len(x[x!=0]) 5 ","date":"2021-12-20T00:00:00Z","permalink":"https://example.com/p/numpy%E5%9F%BA%E7%A1%80%E7%BB%83%E4%B9%A012_soring-searching-and-counting/","title":"numpy基础练习12_Soring, searching, and counting"},{"content":" 这是为了熟悉并加强numpy而找来做的练习，来源：https://github.com/fengdu78/Data-Science-Notes/tree/master/2.numpy/numpy_exercises\nimport numpy as np np.__version__ \u0026#39;1.11.2\u0026#39; __author__ = \u0026#39;kyubyong. longinglove@nate.com\u0026#39; Simple random data Q1. Create an array of shape (3, 2) and populate it with random samples from a uniform distribution over [0, 1).\nnp.random.rand(3, 2) # Or np.random.random((3,2)) array([[ 0.13879034, 0.71300174], [ 0.08121322, 0.00393554], [ 0.02349471, 0.56677474]]) Q2. Create an array of shape (1000, 1000) and populate it with random samples from a standard normal distribution. And verify that the mean and standard deviation is close enough to 0 and 1 repectively.\nout1 = np.random.randn(1000, 1000) out2 = np.random.standard_normal((1000, 1000)) out3 = np.random.normal(loc=0.0, scale=1.0, size=(1000, 1000)) assert np.allclose(np.mean(out1), np.mean(out2), atol=0.1) assert np.allclose(np.mean(out1), np.mean(out3), atol=0.1) assert np.allclose(np.std(out1), np.std(out2), atol=0.1) assert np.allclose(np.std(out1), np.std(out3), atol=0.1) print np.mean(out3) print np.std(out1) -0.00110028519551 0.999683483393 Q3. Create an array of shape (3, 2) and populate it with random integers ranging from 0 to 3 (inclusive) from a discrete uniform distribution.\nnp.random.randint(0, 4, (3, 2)) array([[1, 3], [3, 0], [0, 0]]) Q4. Extract 1 elements from x randomly such that each of them would be associated with probabilities .3, .5, .2. Then print the result 10 times.\nx = [b\u0026#39;3 out of 10\u0026#39;, b\u0026#39;5 out of 10\u0026#39;, b\u0026#39;2 out of 10\u0026#39;] for _ in range(10): print np.random.choice(x, p=[.3, .5, .2]) 2 out of 10 5 out of 10 3 out of 10 5 out of 10 5 out of 10 5 out of 10 2 out of 10 2 out of 10 5 out of 10 5 out of 10 Q5. Extract 3 different integers from 0 to 9 randomly with the same probabilities.\nnp.random.choice(10, 3, replace=False) array([5, 4, 0]) Permutations Q6. Shuffle numbers between 0 and 9 (inclusive).\nx = np.arange(10) np.random.shuffle(x) print x [2 3 8 4 5 1 0 6 9 7] # Or print np.random.permutation(10) [5 2 7 4 1 0 6 8 9 3] Random generator Q7. Assign number 10 to the seed of the random generator so that you can get the same value next time.\nnp.random.seed(10) ","date":"2021-12-19T00:00:00Z","permalink":"https://example.com/p/numpy%E5%9F%BA%E7%A1%80%E7%BB%83%E4%B9%A010_random-sampling/","title":"numpy基础练习10_Random Sampling"},{"content":" 这是为了熟悉并加强numpy而找来做的练习，来源：https://github.com/fengdu78/Data-Science-Notes/tree/master/2.numpy/numpy_exercises\nimport numpy as np np.__version__ \u0026#39;1.11.2\u0026#39; author = \u0026#39;kyubyong. longinglove@nate.com\u0026#39; Making proper sets Q1. Get unique elements and reconstruction indices from x. And reconstruct x.\nx = np.array([1, 2, 6, 4, 2, 3, 2]) out, indices = np.unique(x, return_inverse=True) print \u0026#34;unique elements =\u0026#34;, out print \u0026#34;reconstruction indices =\u0026#34;, indices print \u0026#34;reconstructed =\u0026#34;, out[indices] unique elements = [1 2 3 4 6] reconstruction indices = [0 1 4 3 1 2 1] reconstructed = [1 2 6 4 2 3 2] Boolean operations Q2. Create a boolean array of the same shape as x. If each element of x is present in y, the result will be True, otherwise False.\nx = np.array([0, 1, 2, 5, 0]) y = np.array([0, 1]) print np.in1d(x, y) [ True True False False True] Q3. Find the unique intersection of x and y.\nx = np.array([0, 1, 2, 5, 0]) y = np.array([0, 1, 4]) print np.intersect1d(x, y) [0 1] Q4. Find the unique elements of x that are not present in y.\nx = np.array([0, 1, 2, 5, 0]) y = np.array([0, 1, 4]) print np.setdiff1d(x, y) [2 5] Q5. Find the xor elements of x and y.\nx = np.array([0, 1, 2, 5, 0]) y = np.array([0, 1, 4]) out1 = np.setxor1d(x, y) out2 = np.sort(np.concatenate((np.setdiff1d(x, y), np.setdiff1d(y, x)))) assert np.allclose(out1, out2) print out1 [2 4 5] Q6. Find the union of x and y.\nx = np.array([0, 1, 2, 5, 0]) y = np.array([0, 1, 4]) out1 = np.union1d(x, y) out2 = np.sort(np.unique(np.concatenate((x, y)))) assert np.allclose(out1, out2) print np.union1d(x, y) [0 1 2 4 5] ","date":"2021-12-19T00:00:00Z","permalink":"https://example.com/p/numpy%E5%9F%BA%E7%A1%80%E7%BB%83%E4%B9%A011_set-routines/","title":"numpy基础练习11_Set routines"},{"content":" 这是为了熟悉并加强numpy而找来做的练习，来源：https://github.com/fengdu78/Data-Science-Notes/tree/master/2.numpy/numpy_exercises\nimport numpy as np np.__version__ \u0026#39;1.11.2\u0026#39; __author__ = \u0026#34;kyubyong. kbpark.linguist@gmail.com. https://github.com/kyubyong\u0026#34; Trigonometric functions Q1. Calculate sine, cosine, and tangent of x, element-wise.\nx = np.array([0., 1., 30, 90]) print \u0026#34;sine:\u0026#34;, np.sin(x) print \u0026#34;cosine:\u0026#34;, np.cos(x) print \u0026#34;tangent:\u0026#34;, np.tan(x) sine: [ 0. 0.84147098 -0.98803162 0.89399666] cosine: [ 1. 0.54030231 0.15425145 -0.44807362] tangent: [ 0. 1.55740772 -6.4053312 -1.99520041] Q2. Calculate inverse sine, inverse cosine, and inverse tangent of x, element-wise.\nx = np.array([-1., 0, 1.]) print \u0026#34;inverse sine:\u0026#34;, np.arcsin(x2) print \u0026#34;inverse cosine:\u0026#34;, np.arccos(x2) print \u0026#34;inverse tangent:\u0026#34;, np.arctan(x2) inverse sine: [ 1.57079633 0. 1.57079633] inverse cosine: [ 0. 1.57079633 0. ] inverse tangent: [ 0.78539816 0. 0.78539816] Q3. Convert angles from radians to degrees.\nx = np.array([-np.pi, -np.pi/2, np.pi/2, np.pi]) out1 = np.degrees(x) out2 = np.rad2deg(x) assert np.array_equiv(out1, out2) print out1 [-180. -90. 90. 180.] Q4. Convert angles from degrees to radians.\nx = np.array([-180., -90., 90., 180.]) out1 = np.radians(x) out2 = np.deg2rad(x) assert np.array_equiv(out1, out2) print out1 [-3.14159265 -1.57079633 1.57079633 3.14159265] Hyperbolic functions Q5. Calculate hyperbolic sine, hyperbolic cosine, and hyperbolic tangent of x, element-wise.\nx = np.array([-1., 0, 1.]) print np.sinh(x) print np.cosh(x) print np.tanh(x) [-1.17520119 0. 1.17520119] [ 1.54308063 1. 1.54308063] [-0.76159416 0. 0.76159416] Rounding Q6. Predict the results of these, paying attention to the difference among the family functions.\nx = np.array([2.1, 1.5, 2.5, 2.9, -2.1, -2.5, -2.9]) out1 = np.around(x) out2 = np.floor(x) out3 = np.ceil(x) out4 = np.trunc(x) out5 = [round(elem) for elem in x] print out1 print out2 print out3 print out4 print out5 [ 2. 2. 2. 3. -2. -2. -3.] [ 2. 1. 2. 2. -3. -3. -3.] [ 3. 2. 3. 3. -2. -2. -2.] [ 2. 1. 2. 2. -2. -2. -2.] [2.0, 2.0, 3.0, 3.0, -2.0, -3.0, -3.0] Q7. Implement out5 in the above question using numpy.\nprint np.floor(np.abs(x) + 0.5) * np.sign(x) # Read http://numpy-discussion.10968.n7.nabble.com/why-numpy-round-get-a-different-result-from-python-round-function-td19098.html [ 2. 2. 3. 3. -2. -3. -3.] Sums, products, differences Q8. Predict the results of these.\nx = np.array( [[1, 2, 3, 4], [5, 6, 7, 8]]) outs = [np.sum(x), np.sum(x, axis=0), np.sum(x, axis=1, keepdims=True), \u0026#34;\u0026#34;, np.prod(x), np.prod(x, axis=0), np.prod(x, axis=1, keepdims=True), \u0026#34;\u0026#34;, np.cumsum(x), np.cumsum(x, axis=0), np.cumsum(x, axis=1), \u0026#34;\u0026#34;, np.cumprod(x), np.cumprod(x, axis=0), np.cumprod(x, axis=1), \u0026#34;\u0026#34;, np.min(x), np.min(x, axis=0), np.min(x, axis=1, keepdims=True), \u0026#34;\u0026#34;, np.max(x), np.max(x, axis=0), np.max(x, axis=1, keepdims=True), \u0026#34;\u0026#34;, np.mean(x), np.mean(x, axis=0), np.mean(x, axis=1, keepdims=True)] for out in outs: if out == \u0026#34;\u0026#34;: print else: print(\u0026#34;-\u0026gt;\u0026#34;, out) (\u0026#39;-\u0026gt;\u0026#39;, 36) (\u0026#39;-\u0026gt;\u0026#39;, array([ 6, 8, 10, 12])) (\u0026#39;-\u0026gt;\u0026#39;, array([[10], [26]])) (\u0026#39;-\u0026gt;\u0026#39;, 40320) (\u0026#39;-\u0026gt;\u0026#39;, array([ 5, 12, 21, 32])) (\u0026#39;-\u0026gt;\u0026#39;, array([[ 24], [1680]])) (\u0026#39;-\u0026gt;\u0026#39;, array([ 1, 3, 6, 10, 15, 21, 28, 36])) (\u0026#39;-\u0026gt;\u0026#39;, array([[ 1, 2, 3, 4], [ 6, 8, 10, 12]])) (\u0026#39;-\u0026gt;\u0026#39;, array([[ 1, 3, 6, 10], [ 5, 11, 18, 26]])) (\u0026#39;-\u0026gt;\u0026#39;, array([ 1, 2, 6, 24, 120, 720, 5040, 40320])) (\u0026#39;-\u0026gt;\u0026#39;, array([[ 1, 2, 3, 4], [ 5, 12, 21, 32]])) (\u0026#39;-\u0026gt;\u0026#39;, array([[ 1, 2, 6, 24], [ 5, 30, 210, 1680]])) (\u0026#39;-\u0026gt;\u0026#39;, 1) (\u0026#39;-\u0026gt;\u0026#39;, array([1, 2, 3, 4])) (\u0026#39;-\u0026gt;\u0026#39;, array([[1], [5]])) (\u0026#39;-\u0026gt;\u0026#39;, 8) (\u0026#39;-\u0026gt;\u0026#39;, array([5, 6, 7, 8])) (\u0026#39;-\u0026gt;\u0026#39;, array([[4], [8]])) (\u0026#39;-\u0026gt;\u0026#39;, 4.5) (\u0026#39;-\u0026gt;\u0026#39;, array([ 3., 4., 5., 6.])) (\u0026#39;-\u0026gt;\u0026#39;, array([[ 2.5], [ 6.5]])) /usr/local/lib/python2.7/dist-packages/ipykernel/__main__.py:34: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison Q9. Calculate the difference between neighboring elements, element-wise.\nx = np.array([1, 2, 4, 7, 0]) print np.diff(x) [ 1 2 3 -7] Q10. Calculate the difference between neighboring elements, element-wise, and\nprepend [0, 0] and append[100] to it.\nx = np.array([1, 2, 4, 7, 0]) out1 = np.ediff1d(x, to_begin=[0, 0], to_end=[100]) out2 = np.insert(np.append(np.diff(x), 100), 0, [0, 0]) assert np.array_equiv(out1, out2) print out2 [ 0 0 1 2 3 -7 100] Q11. Return the cross product of x and y.\nx = np.array([1, 2, 3]) y = np.array([4, 5, 6]) print np.cross(x, y) [-3 6 -3] Exponents and logarithms Q12. Compute $e^x$, element-wise.\nx = np.array([1., 2., 3.], np.float32) out = np.exp(x) print out [ 2.71828175 7.38905621 20.08553696] Q13. Calculate exp(x) - 1 for all elements in x.\nx = np.array([1., 2., 3.], np.float32) out1 = np.expm1(x) out2 = np.exp(x) - 1. assert np.allclose(out1, out2) print out1 [ 1.71828175 6.38905621 19.08553696] Q14. Calculate $2^p$ for all p in x.\nx = np.array([1., 2., 3.], np.float32) out1 = np.exp2(x) out2 = 2 ** x assert np.allclose(out1, out2) print out1 [ 2. 4. 8.] Q15. Compute natural, base 10, and base 2 logarithms of x element-wise.\nx = np.array([1, np.e, np.e**2]) print \u0026#34;natural log =\u0026#34;, np.log(x) print \u0026#34;common log =\u0026#34;, np.log10(x) print \u0026#34;base 2 log =\u0026#34;, np.log2(x) natural log = [ 0. 1. 2.] common log = [ 0. 0.43429448 0.86858896] base 2 log = [ 0. 1.44269504 2.88539008] Q16. Compute the natural logarithm of one plus each element in x in floating-point accuracy.\nx = np.array([1e-99, 1e-100]) print np.log1p(x) # Compare it with np.log(1 +x) [ 1.00000000e-099 1.00000000e-100] Floating point routines Q17. Return element-wise True where signbit is set.\nx = np.array([-3, -2, -1, 0, 1, 2, 3]) out1 = np.signbit(x) out2 = x \u0026lt; 0 assert np.array_equiv(out1, out2) print out1 [ True True True False False False False] Q18. Change the sign of x to that of y, element-wise.\nx = np.array([-1, 0, 1]) y = -1.1 print np.copysign(x, y) [-1. -0. -1.] Arithmetic operations Q19. Add x and y element-wise.\nx = np.array([1, 2, 3]) y = np.array([-1, -2, -3]) out1 = np.add(x, y) out2 = x + y assert np.array_equal(out1, out2) print out1 [0 0 0] Q20. Subtract y from x element-wise.\nx = np.array([3, 4, 5]) y = np.array(3) out1 = np.subtract(x, y) out2 = x - y assert np.array_equal(out1, out2) print out1 [0 1 2] Q21. Multiply x by y element-wise.\nx = np.array([3, 4, 5]) y = np.array([1, 0, -1]) out1 = np.multiply(x, y) out2 = x * y assert np.array_equal(out1, out2) print out1 [ 3 0 -5] Q22. Divide x by y element-wise in two different ways.\nx = np.array([3., 4., 5.]) y = np.array([1., 2., 3.]) out1 = np.true_divide(x, y) out2 = x / y assert np.array_equal(out1, out2) print out1 out3 = np.floor_divide(x, y) out4 = x // y assert np.array_equal(out3, out4) print out3 # Note that in Python 2 and 3, the handling of `divide` differs. # See https://docs.scipy.org/doc/numpy/reference/generated/numpy.divide.html#numpy.divide [ 3. 2. 1.66666667] [ 3. 2. 1.] Q23. Compute numerical negative value of x, element-wise.\nx = np.array([1, -1]) out1 = np.negative(x) out2 = -x assert np.array_equal(out1, out2) print out1 [-1 1] Q24. Compute the reciprocal of x, element-wise.\nx = np.array([1., 2., .2]) out1 = np.reciprocal(x) out2 = 1/x assert np.array_equal(out1, out2) print out1 [ 1. 0.5 5. ] Q25. Compute $x^y$, element-wise.\nx = np.array([[1, 2], [3, 4]]) y = np.array([[1, 2], [1, 2]]) out = np.power(x, y) print out [[ 1 4] [ 3 16]] Q26. Compute the remainder of x / y element-wise in two different ways.\nx = np.array([-3, -2, -1, 1, 2, 3]) y = 2 out1 = np.mod(x, y) out2 = x % y assert np.array_equal(out1, out2) print out1 out3 = np.fmod(x, y) print out3 [1 0 1 1 0 1] [-1 0 -1 1 0 1] Miscellaneous Q27. If an element of x is smaller than 3, replace it with 3.\nAnd if an element of x is bigger than 7, replace it with 7.\nx = np.arange(10) out1 = np.clip(x, 3, 7) out2 = np.copy(x) out2[out2 \u0026lt; 3] = 3 out2[out2 \u0026gt; 7] = 7 assert np.array_equiv(out1, out2) print out1 [3 3 3 3 4 5 6 7 7 7] Q28. Compute the square of x, element-wise.\nx = np.array([1, 2, -1]) out1 = np.square(x) out2 = x * x assert np.array_equal(out1, out2) print out1 [1 4 1] Q29. Compute square root of x element-wise.\nx = np.array([1., 4., 9.]) out = np.sqrt(x) print out [ 1. 2. 3.] Q30. Compute the absolute value of x.\nx = np.array([[1, -1], [3, -3]]) out = np.abs(x) print out [[1 1] [3 3]] Q31. Compute an element-wise indication of the sign of x, element-wise.\nx = np.array([1, 3, 0, -1, -3]) out1 = np.sign(x) out2 = np.copy(x) out2[out2 \u0026gt; 0] = 1 out2[out2 \u0026lt; 0] = -1 assert np.array_equal(out1, out2) print out1 [ 1 1 0 -1 -1] ","date":"2021-12-19T00:00:00Z","permalink":"https://example.com/p/numpy%E5%9F%BA%E7%A1%80%E7%BB%83%E4%B9%A09_mathematical-functions/","title":"numpy基础练习9_Mathematical functions"},{"content":" 这是为了熟悉并加强numpy而找来做的练习，来源：https://github.com/fengdu78/Data-Science-Notes/tree/master/2.numpy/numpy_exercises\nfrom __future__ import print_function import numpy as np import matplotlib.pyplot as plt %matplotlib inline from datetime import date date.today() datetime.date(2017, 11, 2) author = \u0026#34;kyubyong. https://github.com/Kyubyong/numpy_exercises\u0026#34; np.__version__ \u0026#39;1.13.1\u0026#39; Complex Numbers Q1. Return the angle of a in radian.\na = 1+1j output = np.angle(a, deg=False) print(output) 0.785398163397 Q2. Return the real part and imaginary part of a.\na = np.array([1+2j, 3+4j, 5+6j]) real = a.real imag = a.imag print(\u0026#34;real part=\u0026#34;, real) print(\u0026#34;imaginary part=\u0026#34;, imag) real part= [ 1. 3. 5.] imaginary part= [ 2. 4. 6.] Q3. Replace the real part of a with 9, the imaginary part with [5, 7, 9].\na = np.array([1+2j, 3+4j, 5+6j]) a.real = 9 a.imag = [5, 7, 9] print(a) [ 9.+5.j 9.+7.j 9.+9.j] Q4. Return the complex conjugate of a.\na = 1+2j output = np.conjugate(a) print(output) (1-2j) Discrete Fourier Transform Q5. Compuete the one-dimensional DFT of a.\na = np.exp(2j * np.pi * np.arange(8)) output = np.fft.fft(a) print(output) [ 8.00000000e+00 -6.85802208e-15j 2.36524713e-15 +9.79717439e-16j 9.79717439e-16 +9.79717439e-16j 4.05812251e-16 +9.79717439e-16j 0.00000000e+00 +9.79717439e-16j -4.05812251e-16 +9.79717439e-16j -9.79717439e-16 +9.79717439e-16j -2.36524713e-15 +9.79717439e-16j] Q6. Compute the one-dimensional inverse DFT of the output in the above question.\nprint(\u0026#34;a=\u0026#34;, a) inversed = np.fft.ifft(output) print(\u0026#34;inversed=\u0026#34;, a) a= [ 1. +0.00000000e+00j 1. -2.44929360e-16j 1. -4.89858720e-16j 1. -7.34788079e-16j 1. -9.79717439e-16j 1. -1.22464680e-15j 1. -1.46957616e-15j 1. -1.71450552e-15j] inversed= [ 1. +0.00000000e+00j 1. -2.44929360e-16j 1. -4.89858720e-16j 1. -7.34788079e-16j 1. -9.79717439e-16j 1. -1.22464680e-15j 1. -1.46957616e-15j 1. -1.71450552e-15j] Q7. Compute the one-dimensional discrete Fourier Transform for real input a.\na = [0, 1, 0, 0] output = np.fft.rfft(a) print(output) assert output.size==len(a)//2+1 if len(a)%2==0 else (len(a)+1)//2 # cf. output2 = np.fft.fft(a) print(output2) [ 1.+0.j 0.-1.j -1.+0.j] [ 1.+0.j 0.-1.j -1.+0.j 0.+1.j] Q8. Compute the one-dimensional inverse DFT of the output in the above question.\ninversed = np.fft.ifft(output) print(\u0026#34;inversed=\u0026#34;, a) inversed= [0, 1, 0, 0] Q9. Return the DFT sample frequencies of a.\nsignal = np.array([-2, 8, 6, 4, 1, 0, 3, 5], dtype=np.float32) fourier = np.fft.fft(signal) n = signal.size freq = np.fft.fftfreq(n, d=1) print(freq) [ 0. 0.125 0.25 0.375 -0.5 -0.375 -0.25 -0.125] Window Functions fig = plt.figure(figsize=(19, 10)) # Hamming window window = np.hamming(51) plt.plot(np.bartlett(51), label=\u0026#34;Bartlett window\u0026#34;) plt.plot(np.blackman(51), label=\u0026#34;Blackman window\u0026#34;) plt.plot(np.hamming(51), label=\u0026#34;Hamming window\u0026#34;) plt.plot(np.hanning(51), label=\u0026#34;Hanning window\u0026#34;) plt.plot(np.kaiser(51, 14), label=\u0026#34;Kaiser window\u0026#34;) plt.xlabel(\u0026#34;sample\u0026#34;) plt.ylabel(\u0026#34;amplitude\u0026#34;) plt.legend() plt.grid() plt.show() ","date":"2021-12-18T00:00:00Z","permalink":"https://example.com/p/numpy%E5%9F%BA%E7%A1%80%E7%BB%83%E4%B9%A07_number/","title":"numpy基础练习7_number"},{"content":" 这是为了熟悉并加强numpy而找来做的练习，来源：https://github.com/fengdu78/Data-Science-Notes/tree/master/2.numpy/numpy_exercises\nimport numpy as np np.__version__ \u0026#39;1.11.2\u0026#39; Truth value testing Q1. Let x be an arbitrary array. Return True if none of the elements of x is zero. Remind that 0 evaluates to False in python.\nx = np.array([1,2,3]) print np.all(x) x = np.array([1,0,3]) print np.all(x) True False Q2. Let x be an arbitrary array. Return True if any of the elements of x is non-zero.\nx = np.array([1,0,0]) print np.any(x) x = np.array([0,0,0]) print np.any(x) True False Array contents Q3. Predict the result of the following code.\nx = np.array([1, 0, np.nan, np.inf]) #print np.isfinite(x) Q4. Predict the result of the following code.\nx = np.array([1, 0, np.nan, np.inf]) #print np.isinf(x) Q5. Predict the result of the following code.\nx = np.array([1, 0, np.nan, np.inf]) #print np.isnan(x) Array type testing Q6. Predict the result of the following code.\nx = np.array([1+1j, 1+0j, 4.5, 3, 2, 2j]) #print np.iscomplex(x) Q7. Predict the result of the following code.\nx = np.array([1+1j, 1+0j, 4.5, 3, 2, 2j]) #print np.isreal(x) Q8. Predict the result of the following code.\n#print np.isscalar(3) #print np.isscalar([3]) #print np.isscalar(True) Logical operations Q9. Predict the result of the following code.\n#print np.logical_and([True, False], [False, False]) #print np.logical_or([True, False, True], [True, False, False]) #print np.logical_xor([True, False, True], [True, False, False]) #print np.logical_not([True, False, 0, 1]) Comparison Q10. Predict the result of the following code.\n#print np.allclose([3], [2.999999]) #print np.array_equal([3], [2.999999]) Q11. Write numpy comparison functions such that they return the results as you see.\nx = np.array([4, 5]) y = np.array([2, 5]) print np.greater(x, y) print np.greater_equal(x, y) print np.less(x, y) print np.less_equal(x, y) [ True False] [ True True] [False False] [False True] Q12. Predict the result of the following code.\n#print np.equal([1, 2], [1, 2.000001]) #print np.isclose([1, 2], [1, 2.000001]) ","date":"2021-12-18T00:00:00Z","permalink":"https://example.com/p/numpy%E5%9F%BA%E7%A1%80%E7%BB%83%E4%B9%A08_logic-functions/","title":"numpy基础练习8_Logic functions"},{"content":"code:\ntest = [1,2,3,4] test_go = [6,6,6] test.append(test_go) test result:\n[1, 2, 3, 4, [6, 6, 6]] code:\nlen(test) result:\n5 可以看出append(a)，会将a作为一个大整体传入\ncode:\ntest = [1,2,3,4] test_go = [6,6,6] test.extend(test_go) test result:\n[1, 2, 3, 4, 6, 6, 6] code:\nlen(test) result:\n7 但是，可以看出extend(a)，会将a中的一个一个元素去取出融合到要extend的对象中\n","date":"2021-12-18T00:00:00Z","permalink":"https://example.com/p/python%E4%B8%AD%E7%9A%84append%E5%92%8Cexpend/","title":"python中的append和expend"},{"content":" 这是为了熟悉并加强numpy而找来做的练习，来源：https://github.com/fengdu78/Data-Science-Notes/tree/master/2.numpy/numpy_exercises\nimport numpy as np np.__version__ \u0026#39;1.11.2\u0026#39; Matrix and vector products Q1. Predict the results of the following code.\nx = [1,2] y = [[4, 1], [2, 2]] print np.dot(x, y) print np.dot(y, x) print np.matmul(x, y) print np.inner(x, y) print np.inner(y, x) [8 5] [6 6] [8 5] [6 6] [6 6] Q2. Predict the results of the following code.\nx = [[1, 0], [0, 1]] y = [[4, 1], [2, 2], [1, 1]] print np.dot(y, x) print np.matmul(y, x) [[4 1] [2 2] [1 1]] [[4 1] [2 2] [1 1]] Q3. Predict the results of the following code.\nx = np.array([[1, 4], [5, 6]]) y = np.array([[4, 1], [2, 2]]) print np.vdot(x, y) print np.vdot(y, x) print np.dot(x.flatten(), y.flatten()) print np.inner(x.flatten(), y.flatten()) print (x*y).sum() 30 30 30 30 30 Q4. Predict the results of the following code.\nx = np.array([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;], dtype=object) y = np.array([1, 2]) print np.inner(x, y) print np.inner(y, x) print np.outer(x, y) print np.outer(y, x) abb abb [[\u0026#39;a\u0026#39; \u0026#39;aa\u0026#39;] [\u0026#39;b\u0026#39; \u0026#39;bb\u0026#39;]] [[\u0026#39;a\u0026#39; \u0026#39;b\u0026#39;] [\u0026#39;aa\u0026#39; \u0026#39;bb\u0026#39;]] Decompositions Q5. Get the lower-trianglular L in the Cholesky decomposition of x and verify it.\nx = np.array([[4, 12, -16], [12, 37, -43], [-16, -43, 98]], dtype=np.int32) L = np.linalg.cholesky(x) print L assert np.array_equal(np.dot(L, L.T.conjugate()), x) [[ 2. 0. 0.] [ 6. 1. 0.] [-8. 5. 3.]] Q6. Compute the qr factorization of x and verify it.\nx = np.array([[12, -51, 4], [6, 167, -68], [-4, 24, -41]], dtype=np.float32) q, r = np.linalg.qr(x) print \u0026#34;q=\\n\u0026#34;, q, \u0026#34;\\nr=\\n\u0026#34;, r assert np.allclose(np.dot(q, r), x) q= [[-0.85714287 0.39428571 0.33142856] [-0.42857143 -0.90285712 -0.03428571] [ 0.2857143 -0.17142858 0.94285715]] r= [[ -14. -21. 14.] [ 0. -175. 70.] [ 0. 0. -35.]] Q7. Factor x by Singular Value Decomposition and verify it.\nx = np.array([[1, 0, 0, 0, 2], [0, 0, 3, 0, 0], [0, 0, 0, 0, 0], [0, 2, 0, 0, 0]], dtype=np.float32) U, s, V = np.linalg.svd(x, full_matrices=False) print \u0026#34;U=\\n\u0026#34;, U, \u0026#34;\\ns=\\n\u0026#34;, s, \u0026#34;\\nV=\\n\u0026#34;, v assert np.allclose(np.dot(U, np.dot(np.diag(s), V)), x) U= [[ 0. 1. 0. 0.] [ 1. 0. 0. 0.] [ 0. 0. 0. -1.] [ 0. 0. 1. 0.]] s= [ 3. 2.23606801 2. 0. ] V= [[ 1. 0. 0.] [ 0. 1. 0.] [ 0. 0. 1.]] Matrix eigenvalues Q8. Compute the eigenvalues and right eigenvectors of x. (Name them eigenvals and eigenvecs, respectively)\nx = np.diag((1, 2, 3)) eigenvals = np.linalg.eig(x)[0] eigenvals_ = np.linalg.eigvals(x) assert np.array_equal(eigenvals, eigenvals_) print \u0026#34;eigenvalues are\\n\u0026#34;, eigenvals eigenvecs = np.linalg.eig(x)[1] print \u0026#34;eigenvectors are\\n\u0026#34;, eigenvecs eigenvalues are [ 1. 2. 3.] eigenvectors are [[ 1. 0. 0.] [ 0. 1. 0.] [ 0. 0. 1.]] Q9. Predict the results of the following code.\nprint np.array_equal(np.dot(x, eigenvecs), eigenvals * eigenvecs) True Norms and other numbers Q10. Calculate the Frobenius norm and the condition number of x.\nx = np.arange(1, 10).reshape((3, 3)) print np.linalg.norm(x, \u0026#39;fro\u0026#39;) print np.linalg.cond(x, \u0026#39;fro\u0026#39;) 16.8819430161 4.56177073661e+17 Q11. Calculate the determinant of x.\nx = np.arange(1, 5).reshape((2, 2)) out1 = np.linalg.det(x) out2 = x[0, 0] * x[1, 1] - x[0, 1] * x[1, 0] assert np.allclose(out1, out2) print out1 -2.0 Q12. Calculate the rank of x.\nx = np.eye(4) out1 = np.linalg.matrix_rank(x) out2 = np.linalg.svd(x)[1].size assert out1 == out2 print out1 4 Q13. Compute the sign and natural logarithm of the determinant of x.\nx = np.arange(1, 5).reshape((2, 2)) sign, logdet = np.linalg.slogdet(x) det = np.linalg.det(x) assert sign == np.sign(det) assert logdet == np.log(np.abs(det)) print sign, logdet -1.0 0.69314718056 Q14. Return the sum along the diagonal of x.\nx = np.eye(4) out1 = np.trace(x) out2 = x.diagonal().sum() assert out1 == out2 print out1 4.0 Solving equations and inverting matrices Q15. Compute the inverse of x.\nx = np.array([[1., 2.], [3., 4.]]) out1 = np.linalg.inv(x) assert np.allclose(np.dot(x, out1), np.eye(2)) print out1 [[-2. 1. ] [ 1.5 -0.5]] ","date":"2021-12-17T00:00:00Z","permalink":"https://example.com/p/numpy%E5%9F%BA%E7%A1%80%E7%BB%83%E4%B9%A06_linear-algebra/","title":"numpy基础练习6_Linear algebra"},{"content":" 这是为了熟悉并加强numpy而找来做的练习，来源：https://github.com/fengdu78/Data-Science-Notes/tree/master/2.numpy/numpy_exercises\nfrom __future__ import print_function import numpy as np author = \u0026#34;kyubyong. https://github.com/Kyubyong/numpy_exercises\u0026#34; np.__version__ \u0026#39;1.12.0\u0026#39; from datetime import date print(date.today()) 2017-04-01 NumPy binary files (NPY, NPZ) Q1. Save x into temp.npy and load it.\nx = np.arange(10) np.save(\u0026#39;temp.npy\u0026#39;, x) # Actually you can omit the extension. If so, it will be added automatically. # Check if there exists the \u0026#39;temp.npy\u0026#39; file. import os if os.path.exists(\u0026#39;temp.npy\u0026#39;): x2 = np.load(\u0026#39;temp.npy\u0026#39;) print(np.array_equal(x, x2)) True Q2. Save x and y into a single file \u0026rsquo;temp.npz\u0026rsquo; and load it.\nx = np.arange(10) y = np.arange(11, 20) np.savez(\u0026#39;temp.npz\u0026#39;, x=x, y=y) # np.savez_compressed(\u0026#39;temp.npz\u0026#39;, x=x, y=y) # If you want to save x and y into a single file in compressed .npz format. with np.load(\u0026#39;temp.npz\u0026#39;) as data: x2 = data[\u0026#39;x\u0026#39;] y2 = data[\u0026#39;y\u0026#39;] print(np.array_equal(x, x2)) print(np.array_equal(y, y2)) True True Text files Q3. Save x to \u0026rsquo;temp.txt\u0026rsquo; in string format and load it.\nx = np.arange(10).reshape(2, 5) header = \u0026#39;num1 num2 num3 num4 num5\u0026#39; np.savetxt(\u0026#39;temp.txt\u0026#39;, x, fmt=\u0026#34;%d\u0026#34;, header=header) np.loadtxt(\u0026#39;temp.txt\u0026#39;) array([[ 0., 1., 2., 3., 4.], [ 5., 6., 7., 8., 9.]]) Q4. Save x, y, and z to \u0026rsquo;temp.txt\u0026rsquo; in string format line by line, then load it.\nx = np.arange(10) y = np.arange(11, 21) z = np.arange(22, 32) np.savetxt(\u0026#39;temp.txt\u0026#39;, (x, y, z), fmt=\u0026#39;%d\u0026#39;) np.loadtxt(\u0026#39;temp.txt\u0026#39;) array([[ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], [ 11., 12., 13., 14., 15., 16., 17., 18., 19., 20.], [ 22., 23., 24., 25., 26., 27., 28., 29., 30., 31.]]) Q5. Convert x into bytes, and load it as array.\nx = np.array([1, 2, 3, 4]) x_bytes = x.tostring() # Don\u0026#39;t be misled by the function name. What it really does is it returns bytes. x2 = np.fromstring(x_bytes, dtype=x.dtype) # returns a 1-D array even if x is not. print(np.array_equal(x, x2)) True Q6. Convert a into an ndarray and then convert it into a list again.\na = [[1, 2], [3, 4]] x = np.array(a) a2 = x.tolist() print(a == a2) True String formatting Q7. Convert x to a string, and revert it.\nx = np.arange(10).reshape(2,5) x_str = np.array_str(x) print(x_str, \u0026#34;\\n\u0026#34;, type(x_str)) x_str = x_str.replace(\u0026#34;[\u0026#34;, \u0026#34;\u0026#34;) # [] must be stripped x_str = x_str.replace(\u0026#34;]\u0026#34;, \u0026#34;\u0026#34;) x2 = np.fromstring(x_str, dtype=x.dtype, sep=\u0026#34; \u0026#34;).reshape(x.shape) assert np.array_equal(x, x2) [[0 1 2 3 4] [5 6 7 8 9]] \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; Text formatting options Q8. Print x such that all elements are displayed with precision=1, no suppress.\nx = np.random.uniform(size=[10,100]) np.set_printoptions(precision=1, threshold=np.nan, suppress=True) print(x) [[ 0.5 0. 0.8 0.2 0.3 0.2 0.2 1. 0.4 0.8 0.6 0.2 0.5 0.1 0.4 0.1 0.9 0.6 0.1 0.5 0.8 0.8 0.8 0. 0.6 0.8 0.4 0.3 0.8 0.2 0.7 0.7 0.2 1. 0.8 0.1 0.2 0.1 0.3 0.1 0.5 0.9 0.6 0.9 0.6 0.5 0.8 0.3 0.3 0.5 0.1 0.6 0.1 0.3 0.6 0.2 0.4 0.8 0.6 0.4 0.2 0.6 0. 0.3 0.8 0.5 0.7 0.9 0.8 0.6 0.9 0.8 0.4 0.4 0.7 0.8 0. 0.1 0.5 0.4 0.7 1. 0.1 0.2 0.6 0.3 0.9 0.1 0.6 0.4 0.3 0.8 0.3 0.6 0.6 0.3 1. 0.2 0.9 0.2] [ 0.9 0.2 0.4 0.9 0.5 0.6 0.1 0.7 0. 0. 0.1 0.8 0.8 1. 0.2 0.8 0.3 0.2 1. 0.6 1. 0.3 0.4 0.4 0.7 0.5 0.4 0.8 0.5 0.9 0.3 0.5 0.7 0.4 0.2 0.3 0.9 0. 0.6 0.8 0.3 0.5 0.2 0.3 0. 0.6 0.5 0.2 0.5 0.8 0.2 0.8 0. 0.9 0. 0.7 0.1 0.4 0.2 0.5 0.6 0.2 0.6 0.1 0.1 0. 0.5 0.9 0.4 0.5 0.8 0.5 0.1 0.7 0. 1. 0.5 0.4 0.2 0. 1. 0.4 0.1 0.7 0.7 0.4 0.8 0.4 0.6 0.6 0.5 0.8 0.8 0.2 0.2 0.3 0.2 0.5 0.9 0.5] [ 0.3 0.6 0.4 0.5 0.5 0. 0.7 0.1 0. 0.9 0.5 0.7 0.6 0.3 0.9 0.5 0.1 0.4 0.1 0.9 0.8 0.6 0.8 0.8 0.1 0.4 0.9 0.1 1. 0.7 0.4 0.3 0.8 0.3 0.8 0.8 0.2 0.7 0.2 0.8 0.3 0.9 0.1 0.9 0.2 0.8 0.9 0.6 0.1 0.3 0.4 1. 0.1 0.7 0.3 0.9 0.3 0.5 0.9 0. 0.6 0. 0.8 0.1 0.9 0. 0.8 0.6 0.5 0.5 0.2 1. 0.4 0. 0.2 0. 0.9 0.9 0.8 0.2 0.7 0.3 0.2 0.1 1. 0.4 0.5 0.4 0.8 0.8 0.8 0.7 0.6 0.4 0.7 0.6 0.5 0.8 0.7 0.6] [ 0.2 0.6 0.9 0.7 0.1 0.1 1. 0.5 0.8 0.3 1. 0.4 0.1 0.5 0.6 0.8 0.8 0.8 0.1 1. 0.8 0. 0.7 0.6 0.8 0.2 0.5 0.9 0.4 0.8 0.7 0.2 0.8 0.6 0.9 0.6 0.9 0.8 0.9 1. 0.6 0.6 0.7 0.1 0.5 0.3 0. 0.8 0. 0.5 0.8 0.3 0.8 0.7 0.1 0.5 0.2 0.1 0.7 0. 0. 0.6 0. 0.8 0.7 0.1 0.4 0.1 0.2 0.1 0.9 0.6 0.9 0.3 0.4 0.9 0.2 0.6 0.8 0.9 0.6 0.8 0.5 0.1 0.6 1. 0. 0.7 0.7 0.4 0.1 0.9 0.4 0.1 0.7 0.6 0.3 0.9 0.3 0.5] [ 0.9 0.3 0.1 0.1 0.2 0.4 0.3 0.5 0.2 0. 0.5 0.4 0.5 0.3 0.6 1. 0.1 0.7 0.6 0.2 0.3 0.3 0.1 0.5 0.6 0. 0.6 0.7 0.6 0.4 0.2 0.6 0.1 0.9 0.9 0.1 0.9 0.1 0.6 0.6 0. 0.1 0.6 0.4 0.3 0.1 0.9 0.8 0.1 0.2 0.8 0.4 0.7 0.8 0.6 0.4 0.9 0.3 0.6 0.7 0.4 0.8 0.3 0. 0. 0.9 0.3 0.3 0.8 0.5 0.8 1. 0.2 0.6 0.6 0.2 0.2 0.2 0.4 0.6 0.6 0.4 0.4 0.8 0.2 0.5 0.7 0.7 0.1 0.9 0.5 0.6 0.3 0.3 0.6 0.8 0.6 0.8 0.4 0.3] [ 0.3 1. 0.6 0.9 0.6 1. 0.7 0.9 0.4 0.3 0.9 0.9 0.3 0.8 0.3 0.6 0.7 0.3 0.1 0.1 0.4 0.3 0.6 0.5 0.1 0.6 0.1 0.5 0.9 0.5 0.5 0.6 0.4 0.4 0.3 1. 0.6 0.6 0.3 0.1 0.4 0.7 0.7 0.1 0.5 0.1 0.3 0.1 0.6 0.7 0. 0.1 0.2 0.4 0.1 0.4 0.7 0.3 0.2 0.9 0.5 0. 0.4 0.9 1. 0.4 0. 0.2 0.3 0.9 0.3 0. 0.8 0.9 0.8 0.6 0.4 0.5 0. 0.9 0.6 0.6 0.1 0.6 0.9 0.1 0.8 0.6 0.6 0.5 0.7 1. 0.5 0.3 0.3 0.4 0.6 0.6 1. 0.2] [ 0.7 0.7 0.9 0.2 0.6 0.3 0.9 0.2 0.9 0.8 0.5 0.3 0.9 0.5 1. 0.6 0.9 0.5 0.5 0.1 0.8 0.3 0.9 0.5 0.7 1. 0.6 0.7 0.1 0.7 0.9 0.4 0.8 0.9 0.4 1. 0.1 1. 0.5 0.1 0.4 0.7 1. 0.4 0.3 0.2 0.2 0.6 0.6 0.3 0.7 0.5 0.7 0.1 0.3 0.5 1. 0.8 0.4 0.8 0.8 0.7 0.1 0.2 0.4 0.3 0.4 0.3 0.5 0.4 0.6 0.3 0.1 0.7 0.8 0.6 0.6 0.2 0.7 0.9 0.9 0.7 0.3 0.9 0.4 0.6 0. 0.4 0.4 0.2 0.8 0.3 0.1 0.2 0.6 0.5 0.9 0.8 0.9 0.7] [ 0.8 0.7 0.7 0.6 0.9 0.1 0.4 0.9 1. 0.3 0. 0.2 0.1 0.5 0.8 0.1 0.7 0.7 0.6 1. 0.7 1. 0.4 0.6 0.2 0.4 0.4 0.6 0. 0.1 1. 0.5 0.1 0.2 0.8 0.2 0.1 0.4 0.7 0.5 0.4 1. 0.5 0.5 0.4 0.8 0.2 0.1 0.7 0.2 0.1 0.4 0.3 0.6 0.9 0.9 0.9 0.9 0.1 0.1 0. 1. 0. 0.1 0.4 0.6 1. 0.4 0.9 0.3 0.2 0.7 0. 0.3 0.2 0.7 0.4 0.3 0.9 0.3 0. 0.5 0.2 0.3 0.1 0.2 0. 0.1 0.6 0.9 0.2 0.5 0.8 0.7 0. 0.4 0.8 0.8 0.5 0.2] [ 0.2 0.3 0. 0.1 0.8 0.4 0.1 0.2 0. 0.7 0. 1. 0.6 0.7 0.3 0.3 0.7 0.9 0.3 0.7 0.1 0.1 0.5 0.6 0.3 0.8 0.7 0.1 0.6 0.6 0.3 0.2 0.3 0.3 1. 0.1 0.1 0.2 0.4 0.4 0.6 0.5 0.7 0.7 0.2 0. 0.8 0.3 0.9 0.1 0.1 0.4 0.4 0.5 0.3 0.9 0.6 0.9 0.3 0.5 0. 0.4 0.8 1. 0.3 0.5 0.7 0.5 0.8 0.7 0.6 0.3 0.1 0.2 0.5 1. 0.9 0.5 0.6 0.6 0.2 0.8 0.6 0. 0.5 0.6 0.8 0.5 0.8 0.8 0.9 0.7 0.9 0.5 0.2 1. 1. 0.1 0.3 0.3] [ 0. 0.3 0.4 0.7 0.2 0.9 0.2 0.3 0.6 0.8 0.4 0.7 0.3 0.5 0.6 0.3 0.7 0. 0.1 0.1 0.9 0. 0.7 0.7 0.1 0.6 0.6 0. 0.3 0.5 0.9 0.3 0.1 0.3 0.1 0.9 0.6 0.3 0.3 0.4 0.4 0.2 0.3 0.1 0.5 0.3 0.8 0. 0.8 0.6 0.2 0.7 0.4 0.8 0.2 0.9 1. 1. 0.7 0.9 0.1 0.2 0. 0.5 0.8 0.7 0.6 0.7 0.7 0.5 0.9 0.2 0.2 0.1 0.2 0.1 0.7 1. 0.6 0.3 0.9 1. 0.3 0.3 0.7 0.9 0.5 0.8 0.9 0.7 0.2 0.7 0.3 0.1 0.9 0.2 0.5 0.6 0.3 0.4]] Base-n representations Q9. Convert 12 into a binary number in string format.\nout1 = np.binary_repr(12) out2 = np.base_repr(12, base=2) assert out1 == out2 # But out1 is better because it\u0026#39;s much faster. print(out1) 1100 Q10. Convert 12 into a hexadecimal number in string format.\nnp.base_repr(1100, base=16) \u0026#39;44C\u0026#39; ","date":"2021-12-16T00:00:00Z","permalink":"https://example.com/p/numpy%E5%9F%BA%E7%A1%80%E7%BB%83%E4%B9%A05_input-and-output/","title":"numpy基础练习5_Input and Output"},{"content":" 这是为了熟悉并加强numpy而找来做的练习，来源：https://github.com/fengdu78/Data-Science-Notes/tree/master/2.numpy/numpy_exercises\nimport numpy as np np.__version__ \u0026#39;1.11.2\u0026#39; Q1. Search for docstrings of the numpy functions on linear algebra.\nnp.lookfor(\u0026#39;linear algebra\u0026#39;) Search results for \u0026#39;linear algebra\u0026#39; ----------------------------------- numpy.linalg.solve Solve a linear matrix equation, or system of linear scalar equations. numpy.poly Find the coefficients of a polynomial with the given sequence of roots. numpy.restoredot Restore `dot`, `vdot`, and `innerproduct` to the default non-BLAS numpy.linalg.eig Compute the eigenvalues and right eigenvectors of a square array. numpy.linalg.cond Compute the condition number of a matrix. numpy.linalg.eigh Return the eigenvalues and eigenvectors of a Hermitian or symmetric matrix. numpy.linalg.pinv Compute the (Moore-Penrose) pseudo-inverse of a matrix. numpy.linalg.LinAlgError Generic Python-exception-derived object raised by linalg functions. Q2. Get help information for numpy dot function.\nnp.info(np.dot) dot(a, b, out=None) Dot product of two arrays. For 2-D arrays it is equivalent to matrix multiplication, and for 1-D arrays to inner product of vectors (without complex conjugation). For N dimensions it is a sum product over the last axis of `a` and the second-to-last of `b`:: dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m]) Parameters ---------- a : array_like First argument. b : array_like Second argument. out : ndarray, optional Output argument. This must have the exact kind that would be returned if it was not used. In particular, it must have the right type, must be C-contiguous, and its dtype must be the dtype that would be returned for `dot(a,b)`. This is a performance feature. Therefore, if these conditions are not met, an exception is raised, instead of attempting to be flexible. Returns ------- output : ndarray Returns the dot product of `a` and `b`. If `a` and `b` are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. If `out` is given, then it is returned. Raises ------ ValueError If the last dimension of `a` is not the same size as the second-to-last dimension of `b`. See Also -------- vdot : Complex-conjugating dot product. tensordot : Sum products over arbitrary axes. einsum : Einstein summation convention. matmul : \u0026#39;@\u0026#39; operator as method with out parameter. Examples -------- \u0026gt;\u0026gt;\u0026gt; np.dot(3, 4) 12 Neither argument is complex-conjugated: \u0026gt;\u0026gt;\u0026gt; np.dot([2j, 3j], [2j, 3j]) (-13+0j) For 2-D arrays it is the matrix product: \u0026gt;\u0026gt;\u0026gt; a = [[1, 0], [0, 1]] \u0026gt;\u0026gt;\u0026gt; b = [[4, 1], [2, 2]] \u0026gt;\u0026gt;\u0026gt; np.dot(a, b) array([[4, 1], [2, 2]]) \u0026gt;\u0026gt;\u0026gt; a = np.arange(3*4*5*6).reshape((3,4,5,6)) \u0026gt;\u0026gt;\u0026gt; b = np.arange(3*4*5*6)[::-1].reshape((5,4,6,3)) \u0026gt;\u0026gt;\u0026gt; np.dot(a, b)[2,3,2,1,2,2] 499128 \u0026gt;\u0026gt;\u0026gt; sum(a[2,3,2,:] * b[1,2,:,2]) 499128 ","date":"2021-12-15T00:00:00Z","permalink":"https://example.com/p/numpy%E5%9F%BA%E7%A1%80%E7%BB%83%E4%B9%A04_info/","title":"numpy基础练习4_info"},{"content":" 这是为了熟悉并加强numpy而找来做的练习，来源：https://github.com/fengdu78/Data-Science-Notes/tree/master/2.numpy/numpy_exercises\nString operations from __future__ import print_function import numpy as np author = \u0026#34;kyubyong. https://github.com/Kyubyong/numpy_exercises\u0026#34; np.__version__ \u0026#39;1.11.3\u0026#39; Q1. Concatenate x1 and x2.\nx1 = np.array([\u0026#39;Hello\u0026#39;, \u0026#39;Say\u0026#39;], dtype=np.str) x2 = np.array([\u0026#39; world\u0026#39;, \u0026#39; something\u0026#39;], dtype=np.str) out = np.char.add(x1, x2) print(out) [\u0026#39;Hello world\u0026#39; \u0026#39;Say something\u0026#39;] Q2. Repeat x three time element-wise.\nx = np.array([\u0026#39;Hello \u0026#39;, \u0026#39;Say \u0026#39;], dtype=np.str) out = np.char.multiply(x, 3) print(out) [\u0026#39;Hello Hello Hello \u0026#39; \u0026#39;Say Say Say \u0026#39;] Q3-1. Capitalize the first letter of x element-wise.\nQ3-2. Lowercase x element-wise.\nQ3-3. Uppercase x element-wise.\nQ3-4. Swapcase x element-wise.\nQ3-5. Title-case x element-wise.\nx = np.array([\u0026#39;heLLo woRLd\u0026#39;, \u0026#39;Say sOmething\u0026#39;], dtype=np.str) capitalized = np.char.capitalize(x) lowered = np.char.lower(x) uppered = np.char.upper(x) swapcased = np.char.swapcase(x) titlecased = np.char.title(x) print(\u0026#34;capitalized =\u0026#34;, capitalized) print(\u0026#34;lowered =\u0026#34;, lowered) print(\u0026#34;uppered =\u0026#34;, uppered) print(\u0026#34;swapcased =\u0026#34;, swapcased) print(\u0026#34;titlecased =\u0026#34;, titlecased) capitalized = [\u0026#39;Hello world\u0026#39; \u0026#39;Say something\u0026#39;] lowered = [\u0026#39;hello world\u0026#39; \u0026#39;say something\u0026#39;] uppered = [\u0026#39;HELLO WORLD\u0026#39; \u0026#39;SAY SOMETHING\u0026#39;] swapcased = [\u0026#39;HEllO WOrlD\u0026#39; \u0026#39;sAY SoMETHING\u0026#39;] titlecased = [\u0026#39;Hello World\u0026#39; \u0026#39;Say Something\u0026#39;] Q4. Make the length of each element 20 and the string centered / left-justified / right-justified with paddings of _.\nx = np.array([\u0026#39;hello world\u0026#39;, \u0026#39;say something\u0026#39;], dtype=np.str) centered = np.char.center(x, 20, fillchar=\u0026#39;_\u0026#39;) left = np.char.ljust(x, 20, fillchar=\u0026#39;_\u0026#39;) right = np.char.rjust(x, 20, fillchar=\u0026#39;_\u0026#39;) print(\u0026#34;centered =\u0026#34;, centered) print(\u0026#34;left =\u0026#34;, left) print(\u0026#34;right =\u0026#34;, right) centered = [\u0026#39;____hello world_____\u0026#39; \u0026#39;___say something____\u0026#39;] left = [\u0026#39;hello world_________\u0026#39; \u0026#39;say something_______\u0026#39;] right = [\u0026#39;_________hello world\u0026#39; \u0026#39;_______say something\u0026#39;] Q5. Encode x in cp500 and decode it again.\nx = np.array([\u0026#39;hello world\u0026#39;, \u0026#39;say something\u0026#39;], dtype=np.str) encoded = np.char.encode(x, \u0026#39;cp500\u0026#39;) decoded = np.char.decode(encoded,\u0026#39;cp500\u0026#39;) print(\u0026#34;encoded =\u0026#34;, encoded) print(\u0026#34;decoded =\u0026#34;, decoded) encoded = [b\u0026#39;\\x88\\x85\\x93\\x93\\x96@\\xa6\\x96\\x99\\x93\\x84\u0026#39; b\u0026#39;\\xa2\\x81\\xa8@\\xa2\\x96\\x94\\x85\\xa3\\x88\\x89\\x95\\x87\u0026#39;] decoded = [\u0026#39;hello world\u0026#39; \u0026#39;say something\u0026#39;] Q6. Insert a space between characters of x.\nx = np.array([\u0026#39;hello world\u0026#39;, \u0026#39;say something\u0026#39;], dtype=np.str) out = np.char.join(\u0026#34; \u0026#34;, x) print(out) [\u0026#39;h e l l o w o r l d\u0026#39; \u0026#39;s a y s o m e t h i n g\u0026#39;] Q7-1. Remove the leading and trailing whitespaces of x element-wise.\nQ7-2. Remove the leading whitespaces of x element-wise.\nQ7-3. Remove the trailing whitespaces of x element-wise.\nx = np.array([\u0026#39; hello world \u0026#39;, \u0026#39;\\tsay something\\n\u0026#39;], dtype=np.str) stripped = np.char.strip(x) lstripped = np.char.lstrip(x) rstripped = np.char.rstrip(x) print(\u0026#34;stripped =\u0026#34;, stripped) print(\u0026#34;lstripped =\u0026#34;, lstripped) print(\u0026#34;rstripped =\u0026#34;, rstripped) stripped = [\u0026#39;hello world\u0026#39; \u0026#39;say something\u0026#39;] lstripped = [\u0026#39;hello world \u0026#39; \u0026#39;say something\\n\u0026#39;] rstripped = [\u0026#39; hello world\u0026#39; \u0026#39;\\tsay something\u0026#39;] Q8. Split the element of x with spaces.\nx = np.array([\u0026#39;Hello my name is John\u0026#39;], dtype=np.str) out = np.char.split(x) print(out) [[\u0026#39;Hello\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;John\u0026#39;]] Q9. Split the element of x to multiple lines.\nx = np.array([\u0026#39;Hello\\nmy name is John\u0026#39;], dtype=np.str) out = np.char.splitlines(x) print(out) [[\u0026#39;Hello\u0026#39;, \u0026#39;my name is John\u0026#39;]] Q10. Make x a numeric string of 4 digits with zeros on its left.\nx = np.array([\u0026#39;34\u0026#39;], dtype=np.str) out = np.char.zfill(x, 4) print(out) [\u0026#39;0034\u0026#39;] Q11. Replace \u0026ldquo;John\u0026rdquo; with \u0026ldquo;Jim\u0026rdquo; in x.\nx = np.array([\u0026#39;Hello nmy name is John\u0026#39;], dtype=np.str) out = np.char.replace(x, \u0026#34;John\u0026#34;, \u0026#34;Jim\u0026#34;) print(out) [\u0026#39;Hello nmy name is Jim\u0026#39;] Comparison Q12. Return x1 == x2, element-wise.\nx1 = np.array([\u0026#39;Hello\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;John\u0026#39;], dtype=np.str) x2 = np.array([\u0026#39;Hello\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;Jim\u0026#39;], dtype=np.str) out = np.char.equal(x1, x2) print(out) [ True True True True False] Q13. Return x1 != x2, element-wise.\nx1 = np.array([\u0026#39;Hello\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;John\u0026#39;], dtype=np.str) x2 = np.array([\u0026#39;Hello\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;Jim\u0026#39;], dtype=np.str) out = np.char.not_equal(x1, x2) print(out) [False False False False True] String information Q14. Count the number of \u0026ldquo;l\u0026rdquo; in x, element-wise.\nx = np.array([\u0026#39;Hello\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;Lily\u0026#39;], dtype=np.str) out = np.char.count(x, \u0026#34;l\u0026#34;) print(out) [2 0 0 0 1] Q15. Count the lowest index of \u0026ldquo;l\u0026rdquo; in x, element-wise.\nx = np.array([\u0026#39;Hello\u0026#39;, \u0026#39;my\u0026#39;, \u0026#39;name\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;Lily\u0026#39;], dtype=np.str) out = np.char.find(x, \u0026#34;l\u0026#34;) print(out) # compare # print(np.char.index(x, \u0026#34;l\u0026#34;)) # =\u0026gt; This raises an error! [ 2 -1 -1 -1 2] Q16-1. Check if each element of x is composed of digits only.\nQ16-2. Check if each element of x is composed of lower case letters only.\nQ16-3. Check if each element of x is composed of upper case letters only.\nx = np.array([\u0026#39;Hello\u0026#39;, \u0026#39;I\u0026#39;, \u0026#39;am\u0026#39;, \u0026#39;20\u0026#39;, \u0026#39;years\u0026#39;, \u0026#39;old\u0026#39;], dtype=np.str) out1 = np.char.isdigit(x) out2 = np.char.islower(x) out3 = np.char.isupper(x) print(\u0026#34;Digits only =\u0026#34;, out1) print(\u0026#34;Lower cases only =\u0026#34;, out2) print(\u0026#34;Upper cases only =\u0026#34;, out3) Digits only = [False False False True False False] Lower cases only = [False False True False True True] Upper cases only = [False True False False False False] Q17. Check if each element of x starts with \u0026ldquo;hi\u0026rdquo;.\nx = np.array([\u0026#39;he\u0026#39;, \u0026#39;his\u0026#39;, \u0026#39;him\u0026#39;, \u0026#39;his\u0026#39;], dtype=np.str) out = np.char.startswith(x, \u0026#34;hi\u0026#34;) print(out) [False True True True] ","date":"2021-12-14T00:00:00Z","permalink":"https://example.com/p/numpy%E5%9F%BA%E7%A1%80%E7%BB%83%E4%B9%A03_string/","title":"numpy基础练习3_String"},{"content":" 这是为了熟悉并加强numpy而找来做的练习，来源：https://github.com/fengdu78/Data-Science-Notes/tree/master/2.numpy/numpy_exercises\nimport numpy as np np.__version__ \u0026#39;1.11.2\u0026#39; Q1. Let x be a ndarray [10, 10, 3] with all elements set to one. Reshape x so that the size of the second dimension equals 150.\nx = np.ones([10, 10, 3]) out = np.reshape(x, [-1, 150]) print out assert np.allclose(out, np.ones([10, 10, 3]).reshape([-1, 150])) [[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]] Q2. Let x be array [[1, 2, 3], [4, 5, 6]]. Convert it to [1 4 2 5 3 6].\nx = np.array([[1, 2, 3], [4, 5, 6]]) out1 = np.ravel(x, order=\u0026#39;F\u0026#39;) out2 = x.flatten(order=\u0026#34;F\u0026#34;) assert np.allclose(out1, out2) print out1 [1 4 2 5 3 6] Q3. Let x be array [[1, 2, 3], [4, 5, 6]]. Get the 5th element.\nx = np.array([[1, 2, 3], [4, 5, 6]]) out1 = x.flat[4] out2 = np.ravel(x)[4] assert np.allclose(out1, out2) print out1 5 Q4. Let x be an arbitrary 3-D array of shape (3, 4, 5). Permute the dimensions of x such that the new shape will be (4,3,5).\nx = np.zeros((3, 4, 5)) out1 = np.swapaxes(x, 1, 0) out2 = x.transpose([1, 0, 2]) assert out1.shape == out2.shape print out1.shape (4L, 3L, 5L) Q5. Let x be an arbitrary 2-D array of shape (3, 4). Permute the dimensions of x such that the new shape will be (4,3).\nx = np.zeros((3, 4)) out1 = np.swapaxes(x, 1, 0) out2 = x.transpose() out3 = x.T assert out1.shape == out2.shape == out3.shape print out1.shape (4L, 3L) Let x be an arbitrary 2-D array of shape (3, 4). Insert a nex axis such that the new shape will be (3, 1, 4).\nx = np.zeros((3, 4)) print np.expand_dims(x, axis=1).shape (3L, 1L, 4L) Q6. Let x be an arbitrary 3-D array of shape (3, 4, 1). Remove a single-dimensional entries such that the new shape will be (3, 4).\nx = np.zeros((3, 4, 1)) print np.squeeze(x).shape (3L, 4L) Q7. Lex x be an array\n[[ 1,2,3], [ 4,5, 6]] and y be an array [[ 7, 8, 9] [10,11,12]] Concatenate x and y so that a new array looks like [[1, 2, 3, 7, 8, 9], [4, 5, 6, 10, 11, 12]].\nx = np.array([[1, 2, 3], [4, 5, 6]]) y = np.array([[7, 8, 9], [10, 11, 12]]) out1 = np.concatenate((x, y), 1) out2 = np.hstack((x, y)) assert np.allclose(out1, out2) print out2 [[ 1 2 3 7 8 9] [ 4 5 6 10 11 12]] Q8. Lex x be an array [[ 1, 2, 3], [ 4, 5, 6]] and y be an array [[ 7, 8, 9] [10,11, 12]] Concatenate x and y so that a new array looks like [[1,2,3], [4,5,6], [7,8,9], [10,11,12]]]\nx = np.array([[1, 2, 3], [4, 5, 6]]) y = np.array([[7, 8, 9], [10, 11, 12]]) out1 = np.concatenate((x, y), 0) out2 = np.vstack((x, y)) assert np.allclose(out1, out2) print out2 [[ 1 2 3] [ 4 5 6] [ 7 8 9] [10 11 12]] Q8. Let x be an array [1,2,3] and y be [4,5, 6]. Convert it to[[1, 4], [2, 5], [3, 6]].\nx = np.array((1,2,3)) y = np.array((4,5,6)) out1 = np.column_stack((x, y)) out2 = np.squeeze(np.dstack((x, y))) out3 = np.vstack((x, y)).T assert np.allclose(out1, out2) assert np.allclose(out2, out3) print out1 array([[1, 4], [2, 5], [3, 6]]) Q9. Let x be an array [[[[1],[2],[3]]] and y be [[[[4], [5], [6]]]].\nx = np.array([[1],[2],[3]]) y = np.array([[4],[5],[6]]) out = np.dstack((x, y)) print out [[[1 4]] [[2 5]] [[3 6]]] Q10. Let x be an array [1, 2, 3, \u0026hellip;, 9]. Split x into 3 arrays, each of which has 4, 2, and 3 elements in the original order.\nx = np.arange(1, 10) print np.split(x, [4, 6]) [array([1, 2, 3, 4]), array([5, 6]), array([7, 8, 9])] Q11. Let x be an array\n[[[ 0., 1., 2., 3.], [ 4., 5., 6., 7.]], [[ 8., 9., 10., 11.], [ 12., 13., 14., 15.]]] Split it into two such that the first array looks like\n[[[ 0., 1., 2.], [ 4., 5., 6.]], [[ 8., 9., 10.], [ 12., 13., 14.]]] and the second one look like:\n[[[ 3.], [ 7.]], [[ 11.], [ 15.]]] x = np.arange(16).reshape(2, 2, 4) out1 = np.split(x, [3],axis=2) out2 = np.dsplit(x, [3]) assert np.allclose(out1[0], out2[0]) assert np.allclose(out1[1], out2[1]) print out1 [array([[[ 0, 1, 2], [ 4, 5, 6]], [[ 8, 9, 10], [12, 13, 14]]]), array([[[ 3], [ 7]], [[11], [15]]])] Q12. Let x be an array\n[[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [ 12., 13., 14., 15.]]. Split it into two arrays along the second axis.\nx = np.arange(16).reshape((4, 4)) out1 = np.hsplit(x, 2) out2 = np.split(x, 2, 1) assert np.allclose(out1[0], out2[0]) assert np.allclose(out1[1], out2[1]) print out1 [array([[ 0, 1], [ 4, 5], [ 8, 9], [12, 13]]), array([[ 2, 3], [ 6, 7], [10, 11], [14, 15]])] Q13. Let x be an array\n[[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [ 12., 13., 14., 15.]]. Split it into two arrays along the first axis.\nx = np.arange(16).reshape((4, 4)) out1 = np.vsplit(x, 2) out2 = np.split(x, 2, 0) assert np.allclose(out1[0], out2[0]) assert np.allclose(out1[1], out2[1]) print out1 [array([[0, 1, 2, 3], [4, 5, 6, 7]]), array([[ 8, 9, 10, 11], [12, 13, 14, 15]])] Q14. Let x be an array [0, 1, 2]. Convert it to\n[[0, 1, 2, 0, 1, 2], [0, 1, 2, 0, 1, 2]] x = np.array([0, 1, 2]) out1 = np.tile(x, [2, 2]) out2 = np.resize(x, [2, 6]) assert np.allclose(out1, out2) print out1 [[0 1 2 0 1 2] [0 1 2 0 1 2]] Q15. Let x be an array [0, 1, 2]. Convert it to [0, 0, 1, 1, 2, 2].\nx = np.array([0, 1, 2]) print np.repeat(x, 2) [0 0 1 1 2 2] Q16. Let x be an array [0, 0, 0, 1, 2, 3, 0, 2, 1, 0]. remove the leading the trailing zeros.\nx = np.array((0, 0, 0, 1, 2, 3, 0, 2, 1, 0)) out = np.trim_zeros(x) print out [1 2 3 0 2 1] Q17. Let x be an array [2, 2, 1, 5, 4, 5, 1, 2, 3]. Get two arrays of unique elements and their counts.\nx = np.array([2, 2, 1, 5, 4, 5, 1, 2, 3]) u, indices = np.unique(x, return_counts=True) print u, indices [1 2 3 4 5] [2 3 1 1 2] Q18. Lex x be an array\n[[1,2], [3,4]] Flip x along the second axis.\nx = np.array([[1,2], [3,4]]) out1 = np.fliplr(x) out2 = x[:, ::-1] assert np.allclose(out1, out2) print out1 [[2 1] [4 3]] Q19. Lex x be an array\n[[1,2], [3,4]] Flip x along the first axis.\nx = np.array([[1,2], [3,4]]) out1 = np.flipud(x) out2 = x[::-1, :] assert np.allclose(out1, out2) print out1 [[3 4] [1 2]] Q20. Lex x be an array\n[[1,2], [3,4]] Rotate x 90 degrees counter-clockwise.\nx = np.array([[1,2], [3,4]]) out = np.rot90(x) print out [[2 4] [1 3]] Q21 Lex x be an array\n[[1,2,3,4] [5,6,7,8]] Shift elements one step to right along the second axis.\nx = np.arange(1, 9).reshape([2, 4]) print np.roll(x, 1, axis=1) [[4 1 2 3] [8 5 6 7]] ","date":"2021-12-13T00:00:00Z","permalink":"https://example.com/p/numpy%E5%9F%BA%E7%A1%80%E7%BB%83%E4%B9%A02_array-manipulation-routines/","title":"numpy基础练习2_Array manipulation routines"},{"content":" 这是为了熟悉并加强numpy而找来做的练习，来源：https://github.com/fengdu78/Data-Science-Notes/tree/master/2.numpy/numpy_exercises\nOnes and zeros import numpy as np Create a new array of 2*2 integers, without initializing entries.\nnp.empty([2,2], int) array([[0, 0], [0, 0]]) Let X = np.array([1,2,3], [4,5,6], np.int32).\nCreate a new array with the same shape and type as X.\nX = np.array([[1,2,3], [4,5,6]], np.int32) np.empty_like(X) array([[1, 2, 3], [4, 5, 6]]) Create a 3-D array with ones on the diagonal and zeros elsewhere.\nnp.eye(3) array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]) np.identity(3) array([[ 1., 0., 0.], [ 0., 1., 0.], [ 0., 0., 1.]]) Create a new array of 3*2 float numbers, filled with ones.\nnp.ones([3,2], float) array([[ 1., 1.], [ 1., 1.], [ 1., 1.]]) Let x = np.arange(4, dtype=np.int64). Create an array of ones with the same shape and type as X.\nx = np.arange(4, dtype=np.int64) np.ones_like(x) array([1, 1, 1, 1], dtype=int64) Create a new array of 3*2 float numbers, filled with zeros.\nnp.zeros((3,2), float) array([[ 0., 0.], [ 0., 0.], [ 0., 0.]]) Let x = np.arange(4, dtype=np.int64). Create an array of zeros with the same shape and type as X.\nx = np.arange(4, dtype=np.int64) np.zeros_like(x) array([0, 0, 0, 0], dtype=int64) Create a new array of 2*5 uints, filled with 6.\nnp.full((2, 5), 6, dtype=np.uint) array([[6, 6, 6, 6, 6], [6, 6, 6, 6, 6]], dtype=uint32) np.ones([2, 5], dtype=np.uint) * 6 array([[6, 6, 6, 6, 6], [6, 6, 6, 6, 6]], dtype=uint32) Let x = np.arange(4, dtype=np.int64). Create an array of 6\u0026rsquo;s with the same shape and type as X.\nx = np.arange(4, dtype=np.int64) np.full_like(x, 6) array([6, 6, 6, 6], dtype=int64) np.ones_like(x) * 6 array([6, 6, 6, 6], dtype=int64) From existing data Create an array of [1, 2, 3].\nnp.array([1, 2, 3]) array([1, 2, 3]) Let x = [1, 2]. Convert it into an array.\nx = [1,2] np.asarray(x) array([1, 2]) Let X = np.array([[1, 2], [3, 4]]). Convert it into a matrix.\nX = np.array([[1, 2], [3, 4]]) np.asmatrix(X) matrix([[1, 2], [3, 4]]) Let x = [1, 2]. Conver it into an array of float.\nx = [1, 2] np.asfarray(x) array([ 1., 2.]) np.asarray(x, float) array([ 1., 2.]) Let x = np.array([30]). Convert it into scalar of its single element, i.e. 30.\nx = np.array([30]) np.asscalar(x) 30 x[0] 30 Let x = np.array([1, 2, 3]). Create a array copy of x, which has a different id from x.\nx = np.array([1, 2, 3]) y = np.copy(x) print id(x), x print id(y), y 70140352 [1 2 3] 70140752 [1 2 3] Numerical ranges Create an array of 2, 4, 6, 8, \u0026hellip;, 100.\nnp.arange(2, 101, 2) array([ 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100]) Create a 1-D array of 50 evenly spaced elements between 3. and 10., inclusive.\nnp.linspace(3., 10, 50) array([ 3. , 3.14285714, 3.28571429, 3.42857143, 3.57142857, 3.71428571, 3.85714286, 4. , 4.14285714, 4.28571429, 4.42857143, 4.57142857, 4.71428571, 4.85714286, 5. , 5.14285714, 5.28571429, 5.42857143, 5.57142857, 5.71428571, 5.85714286, 6. , 6.14285714, 6.28571429, 6.42857143, 6.57142857, 6.71428571, 6.85714286, 7. , 7.14285714, 7.28571429, 7.42857143, 7.57142857, 7.71428571, 7.85714286, 8. , 8.14285714, 8.28571429, 8.42857143, 8.57142857, 8.71428571, 8.85714286, 9. , 9.14285714, 9.28571429, 9.42857143, 9.57142857, 9.71428571, 9.85714286, 10. ]) Create a 1-D array of 50 element spaced evenly on a log scale between 3. and 10., exclusive.\nnp.logspace(3., 10., 50, endpoint=False) array([ 1.00000000e+03, 1.38038426e+03, 1.90546072e+03, 2.63026799e+03, 3.63078055e+03, 5.01187234e+03, 6.91830971e+03, 9.54992586e+03, 1.31825674e+04, 1.81970086e+04, 2.51188643e+04, 3.46736850e+04, 4.78630092e+04, 6.60693448e+04, 9.12010839e+04, 1.25892541e+05, 1.73780083e+05, 2.39883292e+05, 3.31131121e+05, 4.57088190e+05, 6.30957344e+05, 8.70963590e+05, 1.20226443e+06, 1.65958691e+06, 2.29086765e+06, 3.16227766e+06, 4.36515832e+06, 6.02559586e+06, 8.31763771e+06, 1.14815362e+07, 1.58489319e+07, 2.18776162e+07, 3.01995172e+07, 4.16869383e+07, 5.75439937e+07, 7.94328235e+07, 1.09647820e+08, 1.51356125e+08, 2.08929613e+08, 2.88403150e+08, 3.98107171e+08, 5.49540874e+08, 7.58577575e+08, 1.04712855e+09, 1.44543977e+09, 1.99526231e+09, 2.75422870e+09, 3.80189396e+09, 5.24807460e+09, 7.24435960e+09]) Building matrices Let X = np.array([[ 0, 1, 2, 3],\n[ 4, 5, 6, 7],\n[ 8, 9, 10, 11]]).\nGet the diagonal of X, that is, [0, 5, 10].\nX = np.array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) np.diag(X) array([ 0, 5, 10]) X.diagonal() array([ 0, 5, 10]) Create a 2-D array whose diagonal equals [1, 2, 3, 4] and 0\u0026rsquo;s elsewhere.\nnp.diagflat([1, 2, 3, 4]) array([[1, 0, 0, 0], [0, 2, 0, 0], [0, 0, 3, 0], [0, 0, 0, 4]]) Create an array which looks like below.\narray([[ 0., 0., 0., 0., 0.], [ 1., 0., 0., 0., 0.], [ 1., 1., 0., 0., 0.]]) np.tri(3, 5, -1) array([[ 0., 0., 0., 0., 0.], [ 1., 0., 0., 0., 0.], [ 1., 1., 0., 0., 0.]]) Create an array which looks like below.\narray([[ 0, 0, 0], [ 4, 0, 0], [ 7, 8, 0], [10, 11, 12]]) np.tril(np.arange(1, 13).reshape(4, 3), -1) array([[ 0, 0, 0], [ 4, 0, 0], [ 7, 8, 0], [10, 11, 12]]) Create an array which looks like below.\narray([[ 1, 2, 3], [ 4, 5, 6], [ 0, 8, 9], [ 0, 0, 12]]) np.triu(np.arange(1, 13).reshape(4, 3), -1) array([[ 1, 2, 3], [ 4, 5, 6], [ 0, 8, 9], [ 0, 0, 12]]) ","date":"2021-12-12T00:00:00Z","permalink":"https://example.com/p/numpy%E5%9F%BA%E7%A1%80%E7%BB%83%E4%B9%A01_array-creation-routines/","title":"numpy基础练习1_Array creation routines"},{"content":" 这是为了熟悉并加强numpy而找来做的练习，来源：https://github.com/rougier/numpy-100\n100 numpy exercises with hint This is a collection of exercises that have been collected in the numpy mailing list, on stack overflow and in the numpy documentation. The goal of this collection is to offer a quick reference for both old and new users but also to provide a set of exercises for those who teach.\nIf you find an error or think you\u0026rsquo;ve a better way to solve some of them, feel free to open an issue at https://github.com/rougier/numpy-100\nImport the numpy package under the name np (★☆☆) (hint: import … as …)\nimport numpy as np Print the numpy version and the configuration (★☆☆) (hint: np.version, np.show_config)\nprint(np.__version__)#版本 print(np.show_config())#配置信息 1.20.1 blas_mkl_info: libraries = [\u0026#39;mkl_rt\u0026#39;] library_dirs = [\u0026#39;F:/anaconda3\\\\Library\\\\lib\u0026#39;] define_macros = [(\u0026#39;SCIPY_MKL_H\u0026#39;, None), (\u0026#39;HAVE_CBLAS\u0026#39;, None)] include_dirs = [\u0026#39;F:/anaconda3\\\\Library\\\\include\u0026#39;] blas_opt_info: libraries = [\u0026#39;mkl_rt\u0026#39;] library_dirs = [\u0026#39;F:/anaconda3\\\\Library\\\\lib\u0026#39;] define_macros = [(\u0026#39;SCIPY_MKL_H\u0026#39;, None), (\u0026#39;HAVE_CBLAS\u0026#39;, None)] include_dirs = [\u0026#39;F:/anaconda3\\\\Library\\\\include\u0026#39;] lapack_mkl_info: libraries = [\u0026#39;mkl_rt\u0026#39;] library_dirs = [\u0026#39;F:/anaconda3\\\\Library\\\\lib\u0026#39;] define_macros = [(\u0026#39;SCIPY_MKL_H\u0026#39;, None), (\u0026#39;HAVE_CBLAS\u0026#39;, None)] include_dirs = [\u0026#39;F:/anaconda3\\\\Library\\\\include\u0026#39;] lapack_opt_info: libraries = [\u0026#39;mkl_rt\u0026#39;] library_dirs = [\u0026#39;F:/anaconda3\\\\Library\\\\lib\u0026#39;] define_macros = [(\u0026#39;SCIPY_MKL_H\u0026#39;, None), (\u0026#39;HAVE_CBLAS\u0026#39;, None)] include_dirs = [\u0026#39;F:/anaconda3\\\\Library\\\\include\u0026#39;] None Create a null vector of size 10 (★☆☆) (hint: np.zeros)\n#np的方法zeros(a) 可以创建一个长度为a的一维全零向量 z = np.zeros(10) print(z) [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] How to find the memory size of any array (★☆☆) (hint: size, itemsize)\n#size是指数组中有多少个元素 #itemsize是指每一个元素占据了多少空间 #二者相乘就是这个数组整体所占的磁盘空间 print(z.size) print(z.itemsize) print(z.size * z.itemsize) 10 8 80 How to get the documentation of the numpy add function from the command line? (★☆☆) (hint: np.info)\n#info(a) 可以这样查找a函数的方法说明 np.info(np.sum) sum(a, axis=None, dtype=None, out=None, keepdims=\u0026lt;no value\u0026gt;, initial=\u0026lt;no value\u0026gt;, where=\u0026lt;no value\u0026gt;) Sum of array elements over a given axis. Parameters ---------- a : array_like Elements to sum. axis : None or int or tuple of ints, optional Axis or axes along which a sum is performed. The default, axis=None, will sum all of the elements of the input array. If axis is negative it counts from the last to the first axis. .. versionadded:: 1.7.0 If axis is a tuple of ints, a sum is performed on all of the axes specified in the tuple instead of a single axis or all the axes as before. dtype : dtype, optional The type of the returned array and of the accumulator in which the elements are summed. The dtype of `a` is used by default unless `a` has an integer dtype of less precision than the default platform integer. In that case, if `a` is signed then the platform integer is used while if `a` is unsigned then an unsigned integer of the same precision as the platform integer is used. out : ndarray, optional Alternative output array in which to place the result. It must have the same shape as the expected output, but the type of the output values will be cast if necessary. keepdims : bool, optional If this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array. If the default value is passed, then `keepdims` will not be passed through to the `sum` method of sub-classes of `ndarray`, however any non-default value will be. If the sub-class\u0026#39; method does not implement `keepdims` any exceptions will be raised. initial : scalar, optional Starting value for the sum. See `~numpy.ufunc.reduce` for details. .. versionadded:: 1.15.0 where : array_like of bool, optional Elements to include in the sum. See `~numpy.ufunc.reduce` for details. .. versionadded:: 1.17.0 Returns ------- sum_along_axis : ndarray An array with the same shape as `a`, with the specified axis removed. If `a` is a 0-d array, or if `axis` is None, a scalar is returned. If an output array is specified, a reference to `out` is returned. See Also -------- ndarray.sum : Equivalent method. add.reduce : Equivalent functionality of `add`. cumsum : Cumulative sum of array elements. trapz : Integration of array values using the composite trapezoidal rule. mean, average Notes ----- Arithmetic is modular when using integer types, and no error is raised on overflow. The sum of an empty array is the neutral element 0: \u0026gt;\u0026gt;\u0026gt; np.sum([]) 0.0 For floating point numbers the numerical precision of sum (and ``np.add.reduce``) is in general limited by directly adding each number individually to the result causing rounding errors in every step. However, often numpy will use a numerically better approach (partial pairwise summation) leading to improved precision in many use-cases. This improved precision is always provided when no ``axis`` is given. When ``axis`` is given, it will depend on which axis is summed. Technically, to provide the best speed possible, the improved precision is only used when the summation is along the fast axis in memory. Note that the exact precision may vary depending on other parameters. In contrast to NumPy, Python\u0026#39;s ``math.fsum`` function uses a slower but more precise approach to summation. Especially when summing a large number of lower precision floating point numbers, such as ``float32``, numerical errors can become significant. In such cases it can be advisable to use `dtype=\u0026#34;float64\u0026#34;` to use a higher precision for the output. Examples -------- \u0026gt;\u0026gt;\u0026gt; np.sum([0.5, 1.5]) 2.0 \u0026gt;\u0026gt;\u0026gt; np.sum([0.5, 0.7, 0.2, 1.5], dtype=np.int32) 1 \u0026gt;\u0026gt;\u0026gt; np.sum([[0, 1], [0, 5]]) 6 \u0026gt;\u0026gt;\u0026gt; np.sum([[0, 1], [0, 5]], axis=0) array([0, 6]) \u0026gt;\u0026gt;\u0026gt; np.sum([[0, 1], [0, 5]], axis=1) array([1, 5]) \u0026gt;\u0026gt;\u0026gt; np.sum([[0, 1], [np.nan, 5]], where=[False, True], axis=1) array([1., 5.]) If the accumulator is too small, overflow occurs: \u0026gt;\u0026gt;\u0026gt; np.ones(128, dtype=np.int8).sum(dtype=np.int8) -128 You can also start the sum with a value other than zero: \u0026gt;\u0026gt;\u0026gt; np.sum([10], initial=5) 15 Create a null vector of size 10 but the fifth value which is 1 (★☆☆) (hint: array[4])\nz[4]=1 z array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]) Create a vector with values ranging from 10 to 49 (★☆☆) (hint: np.arange)\n#np.arange() 的用法和规则类似range，这里就懒得talk了 只不过这里生成的是一个一维数组array对象 z = np.arange(10,50) z array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]) Reverse a vector (first element becomes last) (★☆☆) (hint: array[::-1])\nz[::-1] array([49, 48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10]) Create a 3x3 matrix with values ranging from 0 to 8 (★☆☆) (hint: reshape)\n#reshape(a,b) 可以使得数组改变为a*b的二维数组 #当然可以先通过arrange生成一个一维数组去reshape z = np.arange(0,9).reshape(3,3) z array([[0, 1, 2], [3, 4, 5], [6, 7, 8]]) Find indices of non-zero elements from [1,2,0,0,4,0] (★☆☆) (hint: np.nonzero)\n#nonzero(a) 传入一个a数组，返回非零元素的索引 np.nonzero([1,2,3,4,0]) (array([0, 1, 2, 3], dtype=int64),) Create a 3x3 identity matrix (★☆☆) (hint: np.eye)\n#eye(a) 可以生成一个边长为a的即a*a的单位矩阵 np.eye(3) array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) Create a 3x3x3 array with random values (★☆☆) (hint: np.random.random)\n#注意传入多维的参数时，记得加上（）表示素组的维度 np.random.random((3,3,3)) array([[[0.57935336, 0.34393613, 0.61811468], [0.46222058, 0.99523385, 0.14160091], [0.94043471, 0.79746322, 0.37884551]], [[0.60833341, 0.02653931, 0.11820293], [0.72195183, 0.58368156, 0.46602534], [0.66055443, 0.63090632, 0.91181164]], [[0.30114151, 0.18621385, 0.95552573], [0.62492293, 0.56531296, 0.53331325], [0.72005836, 0.12660282, 0.48243832]]]) Create a 10x10 array with random values and find the minimum and maximum values (★☆☆) (hint: min, max)\nz = np.random.random((10,10)) print(np.max(z)) #也可以这样写 print(z.max()) 因为z本身时np数组了 print(np.min(z)) 0.9642191956147255 0.02893560516616378 Create a random vector of size 30 and find the mean value (★☆☆) (hint: mean)\nz.mean() 0.5204702710616022 Create a 2d array with 1 on the border and 0 inside (★☆☆) (hint: array[1:-1, 1:-1])\n#这里是要我们去创造一个二维向量，边界值为1，非边界值为0 #传统方法：可以先创建一个1的二维向量，再改变值 z = np.ones((10,10))#不要少个()整体 #传入行，列 z[1:-1,1:-1] = 0 #切片操作 z array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.], [1., 0., 0., 0., 0., 0., 0., 0., 0., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]) How to add a border (filled with 0\u0026rsquo;s) around an existing array? (★☆☆) (hint: np.pad)\nz = np.ones((5,5)) print(z) #np本身有一个填充函数pad，可以优于传统方法去切片操作 #pad(array,pad_width=int,model=\u0026#34;\u0026#34;,constant_values=int) #需要传入一个array数组，指定pad_width填充宽度，model指定填充的方式，因为这里填充的是0，常量，所以为constant,constant_values指定填充的值 #理解下填充宽度：如这里z是个5*5的数组，那么填充宽度为2 的意思是，在这个5*5的基础上，上下各加两行，左右各加两列，就变成了9*9了 # np.info(np.pad) z = np.pad(z,pad_width=2,mode=\u0026#34;constant\u0026#34;,constant_values=0) print(z) [[1. 1. 1. 1. 1.] [1. 1. 1. 1. 1.] [1. 1. 1. 1. 1.] [1. 1. 1. 1. 1.] [1. 1. 1. 1. 1.]] [[0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 1. 1. 1. 1. 1. 0. 0.] [0. 0. 1. 1. 1. 1. 1. 0. 0.] [0. 0. 1. 1. 1. 1. 1. 0. 0.] [0. 0. 1. 1. 1. 1. 1. 0. 0.] [0. 0. 1. 1. 1. 1. 1. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0. 0.]] What is the result of the following expression? (★☆☆) (hint: NaN = not a number, inf = infinity)\n0 * np.nan np.nan == np.nan np.inf \u0026gt; np.nan np.nan - np.nan np.nan in set([np.nan]) 0.3 == 3 * 0.1 print(0 * np.nan, np.nan == np.nan, np.inf \u0026gt; np.nan, np.nan - np.nan, np.nan in set([np.nan]), 0.3 == 3 * 0.1) # 想表达什么呢？ # 就是说： # 1.nan相加减法乘除任何东西都是nan # 2.nan和nan是不相等的，但是可以用来判断是否存在nan # 3.浮点数的精度问题了，右边实际上还有很多0看不见，但是可以通过近似判断两个浮点数是否相等,用到math的isclose函数，传入两个需要判断的值 import math #本质上就是控制误差范围，在误差范围中成立 math.isclose(0.3,3 * 0.1) nan False False nan True False True Create a 5x5 matrix with values 1,2,3,4 just below the diagonal (★☆☆) (hint: np.diag)\n#diagonal对角线 #np.diag() :如果传入的是一个一维数据array，它会创造一个以这个一维数据为对角线的二维矩阵； #np.diag() :如果传入的是一个已经存在的矩阵，那么它会返回这个矩阵的对角线 #参数k=int ：可以指定修改（返回）对角线 k=-1向左下45°移 k=+1向右上45°移 # np.info(np.diag) z = np.diag(np.arange(4)+1) print(np.diag(np.arange(4)+1)) print(np.diag(np.arange(4)+1,k=-1)) print(np.diag(z)) print(np.diag(z,k=-1)) [[1 0 0 0] [0 2 0 0] [0 0 3 0] [0 0 0 4]] [[0 0 0 0 0] [1 0 0 0 0] [0 2 0 0 0] [0 0 3 0 0] [0 0 0 4 0]] [1 2 3 4] [0 0 0] Create a 8x8 matrix and fill it with a checkerboard pattern (★☆☆) (hint: array[::2])\n#checkerboard pattern棋盘格就是010101010101.....交替 z = np.zeros((8,8),dtype=int)#顺便说下因为np指定都是float，所以可以通过dtype=? 指定数据类型，把小数点去掉 #可以用步长行和列改值 z[::2,::2]=1 print(z) z[1::2,1::2]=1 print(z) [[1 0 1 0 1 0 1 0] [0 0 0 0 0 0 0 0] [1 0 1 0 1 0 1 0] [0 0 0 0 0 0 0 0] [1 0 1 0 1 0 1 0] [0 0 0 0 0 0 0 0] [1 0 1 0 1 0 1 0] [0 0 0 0 0 0 0 0]] [[1 0 1 0 1 0 1 0] [0 1 0 1 0 1 0 1] [1 0 1 0 1 0 1 0] [0 1 0 1 0 1 0 1] [1 0 1 0 1 0 1 0] [0 1 0 1 0 1 0 1] [1 0 1 0 1 0 1 0] [0 1 0 1 0 1 0 1]] Consider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element? (hint: np.unravel_index)\n#查找某个矩阵中指定第几个元素的位置(坐标) #可以使用np.unravel_index(indices, shape) 参数indices为第几个元素，shape为一个元组，即indices在哪里查找 #返回的是坐标 # np.info(np.unravel_index) np.unravel_index(100,(6,7,8)) (1, 5, 4) Create a checkerboard 8x8 matrix using the tile function (★☆☆) (hint: np.tile)\n#平铺 ---我理解为一种复制矩阵的方法，起到扩增的作用 #np.tile(array,(x,y)) 传入参数array需要被复制的矩阵 (x,y)往x轴展开平铺x次，y轴展开平铺y次 array自己本身算一次 #这里再来理解下平铺：在x轴是指行复制，y轴是指列复制 #如下面的x复制4次是指1,0和0,1复制，一次为两行分别为1，0/0，1,进行4次 # np.info(np.tile) np.tile([[1,0],[0,1]],(4,4)) array([[1, 0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 1, 0, 1, 0, 1], [1, 0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 1, 0, 1, 0, 1], [1, 0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 1, 0, 1, 0, 1], [1, 0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 1, 0, 1, 0, 1]]) Normalize a 5x5 random matrix (★☆☆) (hint: (x - mean) / std)\nz = np.random.random((5,5)) print(z) z = (z-z.mean())/z.std() print(z) [[0.19225158 0.83180823 0.69629812 0.21497037 0.00577836] [0.65630251 0.02206297 0.89210034 0.28508354 0.27579095] [0.97269437 0.63731918 0.75236911 0.75242934 0.79723611] [0.89510738 0.21178359 0.98463058 0.29512421 0.82291717] [0.66843253 0.74183572 0.89547524 0.61346713 0.93663821]] [[-1.34016774 0.75165483 0.30843669 -1.26586052 -1.95007287] [ 0.17762149 -1.89681017 0.94885455 -1.03653868 -1.06693232] [ 1.21245654 0.11553198 0.49183021 0.49202721 0.63857844] [ 0.95868979 -1.27628365 1.25149676 -1.00369826 0.72257449] [ 0.21729563 0.45737825 0.95989295 0.03751818 1.09452625]] Create a custom dtype that describes a color as four unsigned bytes (RGBA) (★☆☆) (hint: np.dtype)\n#自定义自己的dtype数据类型 #在dtype([(),(),...]) 首先要[]包起来，再每一个类型用()划分且在()中指定一些类型 #这里的ubyte是unsigned bytes color = np.dtype([(\u0026#34;R\u0026#34;,np.ubyte), (\u0026#34;G\u0026#34;,np.ubyte), (\u0026#34;B\u0026#34;,np.ubyte), (\u0026#34;A\u0026#34;,np.ubyte)]) color dtype([(\u0026#39;R\u0026#39;, \u0026#39;u1\u0026#39;), (\u0026#39;G\u0026#39;, \u0026#39;u1\u0026#39;), (\u0026#39;B\u0026#39;, \u0026#39;u1\u0026#39;), (\u0026#39;A\u0026#39;, \u0026#39;u1\u0026#39;)]) Multiply a 5x3 matrix by a 3x2 matrix (real matrix product) (★☆☆) (hint: np.dot | @)\n#矩阵乘法 可以通过dot或者@ # np.dot(np.ones((5,3),dtype=int),np.ones((3,2),dtype=int)) np.ones((5,3),dtype=int)@np.ones((3,2),dtype=int) array([[3, 3], [3, 3], [3, 3], [3, 3], [3, 3]]) Given a 1D array, negate all elements which are between 3 and 8, in place. (★☆☆) (hint: \u0026gt;, \u0026lt;=)\n#好像一直忽略了一点，就是不管是py的range还是np的arange返回的都是指定范围内的int z = np.arange(11) #array[]中不仅仅可以用索引，还可以写入一些小语法 但是此时and只能用\u0026amp; #这里就相当于一种遍历了，对z中的元素遍历，满足条件的进行后面的处理 z[(z\u0026gt;=3) \u0026amp; (z\u0026lt;=8)] *= -1 z array([ 0, 1, 2, -3, -4, -5, -6, -7, -8, 9, 10]) What is the output of the following script? (★☆☆) (hint: np.sum)\n# Author: Jake VanderPlas print(sum(range(5),-1)) from numpy import * print(sum(range(5),-1)) # print(sum([[1,2],[2,3]],1)) import numpy as np # print(np.sum([[1,2],[2,3]],1)) #奇怪这里理论上有一点点区别 #py本身的sum是全部数相加即后面哪个-1也作为一个数求和 #而np的sum后面的哪个1是指axis=1即列求和，0表示行求和，-1表示所有和 print(sum(range(5),-1)) print(sum(range(5),-1)) 10 10 How to round away from zero a float array ? (★☆☆) (hint: np.uniform, np.copysign, np.ceil, np.abs)\nz = np.random.uniform(-10,10,10)#random.uniform是返回指定范围内的带小数的数 print(z) #取整 向上取整ceil 向下取整floor #向上取整是指取大于这个数的最近整数 向下则是取小于这个数的最近整数 print(np.ceil(z)) print(np.floor(z)) [-1.98559208 -0.2699722 -2.14877507 8.23634325 -0.10097698 -1.23497223 -5.85249328 9.59521161 9.66366041 -1.78747611] [-1. -0. -2. 9. -0. -1. -5. 10. 10. -1.] [-2. -1. -3. 8. -1. -2. -6. 9. 9. -2.] How to find common values between two arrays? (★☆☆) (hint: np.intersect1d)\nz1 = np.random.randint(0,10,10) z2 = np.random.randint(0,10,10) print(z1,z2) #数组取交集 np.intersect?d ?要指定数组是几唯的 print(np.intersect1d(z1,z2)) [6 0 1 9 1 0 4 3 8 4] [0 4 6 5 0 4 3 9 5 3] [0 3 4 6 9] How to ignore all numpy warnings (not recommended)? (★☆☆) (hint: np.seterr, np.errstate)\n#忽略报错 np有提供seterr() 往括号中指定要忽略的类型如divide以及是ignore还是raise(正常抛出错误) # divide : {\u0026#39;ignore\u0026#39;, \u0026#39;warn\u0026#39;, \u0026#39;raise\u0026#39;, \u0026#39;call\u0026#39;, \u0026#39;print\u0026#39;, \u0026#39;log\u0026#39;}, optional # Treatment for division by zero. # over : {\u0026#39;ignore\u0026#39;, \u0026#39;warn\u0026#39;, \u0026#39;raise\u0026#39;, \u0026#39;call\u0026#39;, \u0026#39;print\u0026#39;, \u0026#39;log\u0026#39;}, optional # Treatment for floating-point overflow. # under : {\u0026#39;ignore\u0026#39;, \u0026#39;warn\u0026#39;, \u0026#39;raise\u0026#39;, \u0026#39;call\u0026#39;, \u0026#39;print\u0026#39;, \u0026#39;log\u0026#39;}, optional # Treatment for floating-point underflow. # invalid : {\u0026#39;ignore\u0026#39;, \u0026#39;warn\u0026#39;, \u0026#39;raise\u0026#39;, \u0026#39;call\u0026#39;, \u0026#39;print\u0026#39;, \u0026#39;log\u0026#39;}, optional # Treatment for invalid floating-point operation. # np.info(np.seterr) np.seterr(divide=\u0026#34;ignore\u0026#34;) np.array(1) / 0 #inf # np.seterr(divide=\u0026#34;warn\u0026#34;) inf #另外可以用上文文管理器with 但此时只能用np.errstate 因为seterr不具有收尾功能 #就是说这样只会忽略with中的语句的错误，而外部的语句仍然会正常抛出错误 #seterr是全局的 with np.errstate(divide=\u0026#34;ignore\u0026#34;): np.array(1) / 0 # np.array(1) / 0 Is the following expressions true? (★☆☆) (hint: imaginary number)\nnp.sqrt(-1) == np.emath.sqrt(-1) # np.sqrt(-1)#这个是计算不了负数的根号的即不饿能计算虚数 np.emath.sqrt(-1) #这个可以计算虚数 i的平方等于-1 1j How to get the dates of yesterday, today and tomorrow? (★☆☆) (hint: np.datetime64, np.timedelta64)\nprint(np.datetime64(\u0026#34;today\u0026#34;))#可以返回当下的日期 #如果想要加减的话 在后面调用np.timedelta64(200,\u0026#39;D\u0026#39;) 传入数值，并且声明是D/M/Y 比如这里传入200，声明是200天 print(np.datetime64(\u0026#34;today\u0026#34;) - np.timedelta64(200,\u0026#39;D\u0026#39;) ) 2021-07-20 2021-01-01 How to get all the dates corresponding to the month of July 2016? (★★☆) (hint: np.arange(dtype=datetime64[\u0026lsquo;D\u0026rsquo;]))\n#arange()还可以产生日期的组合，只不过dtype记得指定datetime64[D] #日期指定范围输入的格式：xxxx-xx-xx 月和日要两位数 # np.arange(\u0026#34;2016-7\u0026#34;,\u0026#34;2016-8\u0026#34;,dtype=\u0026#39;datetime64[D]\u0026#39;) Error parsing datetime string \u0026#34;2016-7\u0026#34; at position 5 np.arange(\u0026#34;2016-07\u0026#34;,\u0026#34;2016-08\u0026#34;,dtype=\u0026#39;datetime64[D]\u0026#39;) array([\u0026#39;2016-07-01\u0026#39;, \u0026#39;2016-07-02\u0026#39;, \u0026#39;2016-07-03\u0026#39;, \u0026#39;2016-07-04\u0026#39;, \u0026#39;2016-07-05\u0026#39;, \u0026#39;2016-07-06\u0026#39;, \u0026#39;2016-07-07\u0026#39;, \u0026#39;2016-07-08\u0026#39;, \u0026#39;2016-07-09\u0026#39;, \u0026#39;2016-07-10\u0026#39;, \u0026#39;2016-07-11\u0026#39;, \u0026#39;2016-07-12\u0026#39;, \u0026#39;2016-07-13\u0026#39;, \u0026#39;2016-07-14\u0026#39;, \u0026#39;2016-07-15\u0026#39;, \u0026#39;2016-07-16\u0026#39;, \u0026#39;2016-07-17\u0026#39;, \u0026#39;2016-07-18\u0026#39;, \u0026#39;2016-07-19\u0026#39;, \u0026#39;2016-07-20\u0026#39;, \u0026#39;2016-07-21\u0026#39;, \u0026#39;2016-07-22\u0026#39;, \u0026#39;2016-07-23\u0026#39;, \u0026#39;2016-07-24\u0026#39;, \u0026#39;2016-07-25\u0026#39;, \u0026#39;2016-07-26\u0026#39;, \u0026#39;2016-07-27\u0026#39;, \u0026#39;2016-07-28\u0026#39;, \u0026#39;2016-07-29\u0026#39;, \u0026#39;2016-07-30\u0026#39;, \u0026#39;2016-07-31\u0026#39;], dtype=\u0026#39;datetime64[D]\u0026#39;) How to compute ((A+B)*(-A/2)) in place (without copy)? (★★☆) (hint: np.add(out=), np.negative(out=), np.multiply(out=), np.divide(out=))\n#因为np有广播机制所以本身加减乘除都是按位运算的，即每一个元素都会进行 A = np.ones(5) B = np.ones(5)*2 print((A+B)*(-A/2)) #np本身也有对应的方法实现加减乘除 #np.add(a,b) a+b #np.negative(a) -a #np,multiply(a,b) a*b #np.divide(a,b) a/b print(np.multiply((np.add(A,B)),(np.divide(np.negative(A),2)))) [-1.5 -1.5 -1.5 -1.5 -1.5] [-1.5 -1.5 -1.5 -1.5 -1.5] Extract the integer part of a random array using 5 different methods (★★☆) (hint: %, np.floor, np.ceil, astype, np.trunc)\nz = np.random.uniform(0,10,5) #取整的五种方法 print(z) print(np.floor(z))#floor向下取整 print(np.ceil(z)-1)#ceil向上取整 print(z.astype(int))#astype直接指定为int类型 不是四舍五入！ print(z - z%1)#直接减去小数部分 print(np.trunc(z))#trunc截断方法，作用和floor向下取整一样 [2.63114571 1.48724554 5.40849786 9.79204129 6.72451636] [2. 1. 5. 9. 6.] [2. 1. 5. 9. 6.] [2 1 5 9 6] [2. 1. 5. 9. 6.] [2. 1. 5. 9. 6.] Create a 5x5 matrix with row values ranging from 0 to 4 (★★☆) (hint: np.arange)\nz = np.zeros((5,5))#初始化一个5*5的零矩阵 #运用np的广播机制 z += np.arange(5) #np.arange(5) 生成一个[0,1,2,3,4] 再和np.zeros((5,5)) 相加 #这里的广播就体现在它会将[0,1,2,3,4] 也变成一个5，5的矩阵再去相加 print(z) [[0. 1. 2. 3. 4.] [0. 1. 2. 3. 4.] [0. 1. 2. 3. 4.] [0. 1. 2. 3. 4.] [0. 1. 2. 3. 4.]] Consider a generator function that generates 10 integers and use it to build an array (★☆☆) (hint: np.fromiter)\ndef generator(): for i in range(10): yield i # np.info(np.fromiter) #fromiter(iterable, dtype=?, count=int) 这是可以用来传入一个可迭代的对象，如果是方法的循环也算； #dtype可以指定取出的元素的类型，count指定从iterable中取出多少各元素，-1表示全部取出 print(np.fromiter(generator(),dtype=int,count=-1)) [0 1 2 3 4 5 6 7 8 9] Create a vector of size 10 with values ranging from 0 to 1, both excluded (★★☆) (hint: np.linspace)\n#linspace(a,b,c)是一个等分作用从[a,b]的范围分成c份的点 返回等分点 #如果想要的范围是(a,b) 可以加上参数endpoint=Fales 此时是[a,b) 但是a还是包含，那么可以直接（a,b,c+1）[1:] print(np.linspace(0,1,10)) print(np.linspace(0,1,11,endpoint=False)[1:]) [0. 0.11111111 0.22222222 0.33333333 0.44444444 0.55555556 0.66666667 0.77777778 0.88888889 1. ] [0.09090909 0.18181818 0.27272727 0.36363636 0.45454545 0.54545455 0.63636364 0.72727273 0.81818182 0.90909091] Create a random vector of size 10 and sort it (★★☆) (hint: sort)\nsorted(np.random.random(10)) [0.006388517198304577, 0.18206305792041022, 0.40837486171268167, 0.5852149211992692, 0.6296565954134207, 0.6830471902983081, 0.6842648144269139, 0.7541891760309777, 0.901382646210188, 0.9247366706406132] How to sum a small array faster than np.sum? (★★☆) (hint: np.add.reduce)\n#对于小矩阵来说，np.add.reduce的求和比np.sum要快得多 print(np.add.reduce(np.arange(100))) 4950 Consider two random array A and B, check if they are equal (★★☆) (hint: np.allclose, np.array_equal)\n#array([])可以用来自己创造矩阵写在[]这里面 x = np.array([0.3,0.3]) y = np.array([0.3,3*0.1]) #np.equal(x,y) 可以用来判断矩阵x和y是相等，也是按位运算的即一个个对应的元素相比较 print(np.equal(x,y)) #如果想直接判断array相不相等则使用np.qrray_equal print(np.array_equal(x,y)) #同样np也有近似判断 allclose print(np.allclose(x,y)) [ True False] False True Make an array immutable (read-only) (★★☆) (hint: flags.writeable)\n#flags.writeable = False 可以修改array的是否可修改，如果为False则不可修改，修改会报错 x = np.array([0.3,0.3]) x.flags.writeable = False x[0]=1 --------------------------------------------------------------------------- ValueError Traceback (most recent call last) \u0026lt;ipython-input-34-b54c501d3157\u0026gt; in \u0026lt;module\u0026gt; 2 x = np.array([0.3,0.3]) 3 x.flags.writeable = False ----\u0026gt; 4 x[0]=1 ValueError: assignment destination is read-only Consider a random 10x2 matrix representing cartesian coordinates, convert them to polar coordinates (★★☆) (hint: np.sqrt, np.arctan2)\n#cartesian coordinates 直角坐标 polar coordinates 极坐标 a = np.random.randint(0,10,(10,2)) x,y = a[:,0],a[:,1] R = np.sqrt(x**2+y**2)#注意这里得到的是(10,1)的矩阵 theta = np.arctan2(y,x)#夹角 (10,1)的matrix #zip(a,b) zip的作用是将a,b组合拼接成一个矩阵，这里就是变成了(10,2)的matrix dict_polar = { i[0]:i[1] for i in zip(R,theta)} dict_polar {6.4031242374328485: 0.8960553845713439, 10.295630140987: 1.0636978224025597, 8.54400374531753: 1.2120256565243244, 10.816653826391969: 0.5880026035475675, 3.605551275463989: 0.5880026035475675, 10.0: 0.9272952180016122, 2.0: 0.0, 10.63014581273465: 0.7188299996216245} Create random vector of size 10 and replace the maximum value by 0 (★★☆) (hint: argmax)\nz = np.random.randint(0,10,10) print(z) #mp.argmax(a)可以返回a矩阵中最大值的索引 z[z.argmax()] = 0 print(z) [4 9 3 9 5 2 7 0 9 0] [4 0 3 9 5 2 7 0 9 0] Create a structured array with x and y coordinates covering the [0,1]x[0,1] area (★★☆) (hint: np.meshgrid)\nz = np.zeros((5,5),[(\u0026#34;x\u0026#34;,float),(\u0026#34;y\u0026#34;,float)])#先创造一个布局矩阵 print(z) #meshgrid(x,y)生成一个以x为x轴，y为y轴的坐标系 注意传入的x和y要弄好刻度，这里用的是切线段linespace print(np.meshgrid(np.linspace(0,1,5),np.linspace(0,1,5))) z[\u0026#34;x\u0026#34;],z[\u0026#34;y\u0026#34;] = np.meshgrid(np.linspace(0,1,5),np.linspace(0,1,5))#好好理解下这里 #就是说meshgrid生成了x为[0. , 0.25, 0.5 , 0.75, 1.] y为[0. , 0.25, 0.5 , 0.75, 1.] print(z) [[(0., 0.) (0., 0.) (0., 0.) (0., 0.) (0., 0.)] [(0., 0.) (0., 0.) (0., 0.) (0., 0.) (0., 0.)] [(0., 0.) (0., 0.) (0., 0.) (0., 0.) (0., 0.)] [(0., 0.) (0., 0.) (0., 0.) (0., 0.) (0., 0.)] [(0., 0.) (0., 0.) (0., 0.) (0., 0.) (0., 0.)]] [array([[0. , 0.25, 0.5 , 0.75, 1. ], [0. , 0.25, 0.5 , 0.75, 1. ], [0. , 0.25, 0.5 , 0.75, 1. ], [0. , 0.25, 0.5 , 0.75, 1. ], [0. , 0.25, 0.5 , 0.75, 1. ]]), array([[0. , 0. , 0. , 0. , 0. ], [0.25, 0.25, 0.25, 0.25, 0.25], [0.5 , 0.5 , 0.5 , 0.5 , 0.5 ], [0.75, 0.75, 0.75, 0.75, 0.75], [1. , 1. , 1. , 1. , 1. ]])] [[(0. , 0. ) (0.25, 0. ) (0.5 , 0. ) (0.75, 0. ) (1. , 0. )] [(0. , 0.25) (0.25, 0.25) (0.5 , 0.25) (0.75, 0.25) (1. , 0.25)] [(0. , 0.5 ) (0.25, 0.5 ) (0.5 , 0.5 ) (0.75, 0.5 ) (1. , 0.5 )] [(0. , 0.75) (0.25, 0.75) (0.5 , 0.75) (0.75, 0.75) (1. , 0.75)] [(0. , 1. ) (0.25, 1. ) (0.5 , 1. ) (0.75, 1. ) (1. , 1. )]] 怎么一回事呢\n首先一开始生成的z是这样的一个(0,0)全构成的5,5的矩阵，然后通过meshgrid生成了另一个矩阵，但是这个矩阵指定了它前五行是x的，后五行是y值，最后我们将其赋予给z，就是一一对应赋予（1-1，2-2，3-3\u0026hellip;..) 具体来说就是meshgrid种的两个1分别取出第一个元素，去构成z种的第一行的x和y； 。。。后面以此类推\n直观来说就是x轴向上平移的过程：横坐标不变，纵坐标增大的所有点展示打印处理\nGiven two arrays, X and Y, construct the Cauchy matrix C (Cij =1/(xi - yj)) (hint: np.subtract.outer)\nx = np.arange(5) y = x+1 print(x) print(y) #subtract.outer(x,y) 可以起到逐一运算xi - yj的作用：就是说那X的第一元素0，去与y的的每一个元素做运算得到新的一行 # 0-1得到 -1 ；0-2得到-2....0-5；在1-1，1-2，1-3...... print(np.subtract.outer(x,y)) [0 1 2 3 4] [1 2 3 4 5] [[-1 -2 -3 -4 -5] [ 0 -1 -2 -3 -4] [ 1 0 -1 -2 -3] [ 2 1 0 -1 -2] [ 3 2 1 0 -1]] Print the minimum and maximum representable value for each numpy scalar type (★★☆) (hint: np.iinfo, np.finfo, eps)\n#可以查看每种类型数据字长的最大小值 #整数 print(np.iinfo(np.int8)) print(np.iinfo(np.int8).max) #浮点数 print(np.finfo(np.float16)) Machine parameters for int8 --------------------------------------------------------------- min = -128 max = 127 --------------------------------------------------------------- 127 Machine parameters for float16 --------------------------------------------------------------- precision = 3 resolution = 1.00040e-03 machep = -10 eps = 9.76562e-04 negep = -11 epsneg = 4.88281e-04 minexp = -14 tiny = 6.10352e-05 maxexp = 16 max = 6.55040e+04 nexp = 5 min = -max --------------------------------------------------------------- How to print all the values of an array? (★★☆) (hint: np.set_printoptions)\n#调整打印 就是有时候会有省略号 #可以通过np.set_printoptions(threshold=int) 设置一个阈值，维度相乘(元素总数）超过这个阈值则用上省略号，没超过则全部打印 #threshold=inf可以满足任何都打印 z = np.zeros((50,50)) a = np.zeros((6,6)) np.set_printoptions(threshold=60) print(z) print(a) [[0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] ... [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.] [0. 0. 0. ... 0. 0. 0.]] [[0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0.]] How to find the closest value (to a given scalar) in a vector? (★★☆) (hint: argmin)\nx = np.arange(100) scalar = np.random.uniform(0,100) i = (np.abs(x-scalar)).argmin()#x-scalar是会广播的，就是说x中的每个元素都会减去scalar print(scalar) print(x[i]) 58.78598295032318 59 Create a structured array representing a position (x,y) and a color (r,g,b) (★★☆) (hint: dtype)\n#道理同Q23 Consider a random vector with shape (100,2) representing coordinates, find point by point distances (★★☆) (hint: np.atleast_2d, T, np.sqrt)\n#计算每个点之间的距离 注意理解下这里的空间 x = np.random.randint(0,10,(10,2)) print(x) #可以给x和y加上一个维度变成二维，原来是一维的， #为什么要变成二维呢？因为每个点和另一个点的所有距离放在一起：如第一个点，与剩下的点的距离 #使用atleast_2d 这样原来的一维就变成了二维中的一行了 X,Y = np.atleast_2d(x[:,0],x[:,1]) print(X) print(X.T) #正常矩阵的相加减满足 ： (a,b) (b,c) R = np.sqrt((X-X.T)**2+(Y-Y.T)**2) #X-X.T 这里是X的第一个元素减去X.T的每一个元素得到新的一列，以此类推第二个元素 print(R) [[4 9] [7 1] [0 7] [4 1] [1 1] [5 1] [2 1] [2 2] [6 2] [7 4]] [[4 7 0 4 1 5 2 2 6 7]] [[4] [7] [0] [4] [1] [5] [2] [2] [6] [7]] [[0. 8.54400375 4.47213595 ... 7.28010989 7.28010989 5.83095189] [8.54400375 0. 9.21954446 ... 5.09901951 1.41421356 3. ] [4.47213595 9.21954446 0. ... 5.38516481 7.81024968 7.61577311] ... [7.28010989 5.09901951 5.38516481 ... 0. 4. 5.38516481] [7.28010989 1.41421356 7.81024968 ... 4. 0. 2.23606798] [5.83095189 3. 7.61577311 ... 5.38516481 2.23606798 0. ]] How to convert a float (32 bits) array into an integer (32 bits) in place? (hint: astype(copy=False))\n#astype(?)可以改变数据类型 R.astype(np.int0) #并不影响原来的空间 # print(R) array([[0, 8, 4, ..., 7, 7, 5], [8, 0, 9, ..., 5, 1, 3], [4, 9, 0, ..., 5, 7, 7], ..., [7, 5, 5, ..., 0, 4, 5], [7, 1, 7, ..., 4, 0, 2], [5, 3, 7, ..., 5, 2, 0]], dtype=int64) How to read the following file? (★★☆) (hint: np.genfromtxt)\n1, 2, 3, 4, 5 6, , , 7, 8 , , 9,10,11 #读取文件 #这里用IO模拟个文件 from io import StringIO s = StringIO(\u0026#34;\u0026#34;\u0026#34;1, 2, 3, 4, 5 6, , , 7, 8 , , 9,10,11 \u0026#34;\u0026#34;\u0026#34;)#这个s就相当于文件了，内容就是\u0026#34;\u0026#34;\u0026#34;内容\u0026#34;\u0026#34;\u0026#34; #np.genfromtxt(s,delimiter=\u0026#34;,\u0026#34;,dtype=np.int64) genfromtxt可以从读取文本中的内容生成矩阵 #传入文件a，因为a中的内容是以逗号分隔的，所以delimiter=\u0026#34;,\u0026#34;,顺便将类型转为了int使用dtype np.genfromtxt(s,delimiter=\u0026#34;,\u0026#34;,dtype=int) #-1代表空，就没有取到值 array([[ 1, 2, 3, 4, 5], [ 6, -1, -1, 7, 8], [-1, -1, 9, 10, 11]]) What is the equivalent of enumerate for numpy arrays? (★★☆) (hint: np.ndenumerate, np.ndindex)\ns = np.random.randint(0,10,(2,2)) print(s) #py中可以用enumerate枚举，np中可以用ndenumerate # [print(i,j) for x,j in np.ndenumerate(s)] 这样打咩X for x,j in np.ndenumerate(s): print(x,j) [[4 6] [3 3]] (0, 0) 4 (0, 1) 6 (1, 0) 3 (1, 1) 3 How to randomly place p elements in a 2D array? (★★☆) (hint: np.put, np.random.choice)\n#np.put(矩阵，位置，值) put可以在传入的矩阵中，修改指定位置的值 #注意这里传入的位置是：比如原来的矩阵是二维的且有100个元素，如果想要修改第80个元素的值，则位置输入80； #就是说这里的位置是针对传入的矩阵第几个元素，即相当于传入的矩阵变成了一维 x = np.zeros((5,5)) print(x) np.put(x,np.random.choice(range(25),5),1) #指向原空间 print(x) [[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]] [[0. 1. 0. 0. 1.] [0. 0. 0. 0. 0.] [0. 1. 0. 0. 1.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]] Subtract the mean of each row of a matrix (★★☆) (hint: mean(axis=,keepdims=))\na = np.random.randint(0,10,(3,5)) print(a) print(a.mean())#返回的是全部数的均值 print(a.mean(axis=0))#axis=0返回的是每一列的均值；axis=1返回的每一行的均值 但是这样返回的是一维的 print(a.mean(axis=0,keepdims=True))#可以用参数keepdim来保持原有的维度 #有什么用呢：在这个例子里你发现加了keepdim的仍然是二维的，跟a的数据维度是保持一致的，这样就可以运用到广播机制了 #这里的广播机制就体现在：8减去所在列的均值8得到新的值，7也是；对于5来说则减去5.。。。以此类推 a - a.mean(axis=0,keepdims=True) [[8 5 4 7 6] [7 4 6 4 4] [9 6 8 9 1]] 5.866666666666666 [8. 5. 6. 6.66666667 3.66666667] [[8. 5. 6. 6.66666667 3.66666667]] array([[ 0. , 0. , -2. , 0.33333333, 2.33333333], [-1. , -1. , 0. , -2.66666667, 0.33333333], [ 1. , 1. , 2. , 2.33333333, -2.66666667]]) How to sort an array by the nth column? (★★☆) (hint: argsort)\nprint(a) #argsort()可以按照指定的行或者列排序 返回的是索引 b = a[:,1].argsort()#这里按照第二列排序 print(b) #返回[1, 0, 2] 说明第2(1)行的第二列的数是最小的，最大的为第3(2)行 print(a[b]) [[8 5 4 7 6] [7 4 6 4 4] [9 6 8 9 1]] [1 0 2] [[7 4 6 4 4] [8 5 4 7 6] [9 6 8 9 1]] How to tell if a given 2D array has null columns? (★★☆) (hint: any, ~)\na = np.random.randint(0,10,(3,3)) print(a) #~array.any() 可以判断是否存在空 print((~a.any(axis=0))) [[2 3 4] [4 4 4] [8 2 7]] [False False False] Find the nearest value from a given value in an array (★★☆) (hint: np.abs, argmin, flat)\na = np.random.randint(0,10,(6)) print(a) n=5 print(a[np.argmin(abs(a-n))]) [4 7 2 7 7 8] 4 Considering two arrays with shape (1,3) and (3,1), how to compute their sum using an iterator? (★★☆) (hint: np.nditer)\nx = np.arange(3).reshape(1,3) y = np.arange(3).reshape(3,1) #满足 (a,b) (b,c) print(x) print(y) #np.nditer([a,b]) 作用和zip(a,b)一样都是将a,b拼接成一个新的矩阵 # Iterator global flags must be a list or tuple of strings 所以记得在nditer中加上[] temp = [] for a,b in np.nditer([x,y]): temp.append(a+b) #a+b:0+0,0+1,0+2,1+0。。。。。就是每个元素相加得到新的值 注意不是全部加起来还要求和 # \u0026#39;list\u0026#39; object has no attribute \u0026#39;reshape\u0026#39; 将list变成array print(np.array(temp).reshape(3,3)) [[0 1 2]] [[0] [1] [2]] [[0 1 2] [1 2 3] [2 3 4]] ","date":"2021-12-11T00:00:00Z","permalink":"https://example.com/p/100%E9%81%93numpy%E7%BB%83%E4%B9%A0/","title":"100道numpy练习"},{"content":"原生XGBoost需要先把数据集按输入特征部分，输出部分分开，然后放到一个DMatrix数据结构里面，这个DMatrix我们不需要关心里面的细节，使用我们的训练集X和y初始化即可。\nimport pandas as pd import numpy as np import matplotlib import matplotlib.pyplot as plt matplotlib.style.use(\u0026#34;ggplot\u0026#34;) plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = \u0026#34;SimHei\u0026#34; plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15,5) import xgboost as xgb from sklearn.model_selection import GridSearchCV from sklearn.model_selection import train_test_split # from sklearn.datasets.samples_generator import make_classification from sklearn.datasets import make_classification # X为样本特征，y为样本类别输出， 共10000个样本，每个样本20个特征，输出有2个类别，没有冗余特征，每个类别一个簇 X,y = make_classification(n_samples=10000,n_features=20,n_classes=2, n_clusters_per_class=1,n_redundant=0,flip_y=0.1) #flip_y 随机分配的样本的比例，增大会加大噪声，加大分类难度 X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=22) dtrain = xgb.DMatrix(X_train,y_train) dtest = xgb.DMatrix(X_test,y_test) 上面的代码中，我们随机初始化了一个二分类的数据集，然后分成了训练集和验证集。使用训练集和验证集分别初始化了一个DMatrix，有了DMatrix，就可以做训练和预测了。简单的示例代码如下：\n# param = {\u0026#39;max_depth\u0026#39;:5, \u0026#39;eta\u0026#39;:0.5, \u0026#39;verbosity\u0026#39;:1, \u0026#39;objective\u0026#39;:\u0026#39;binary:logistic\u0026#39;} param = {\u0026#34;max_depth\u0026#34;:5,\u0026#34;eta\u0026#34;:0.5,\u0026#34;verbosity\u0026#34;:1,\u0026#34;eval_metric\u0026#34;:\u0026#34;logloss\u0026#34;} raw_model = xgb.train(param,dtrain,num_boost_round=22) dtrain.get_label() result:\narray([0., 0., 1., ..., 1., 0., 1.], dtype=float32) from sklearn.metrics import accuracy_score pred_train_raw = raw_model.predict(dtrain) for i in range (len(pred_train_raw)): if pred_train_raw[i] \u0026gt; 0.5: pred_train_raw[i] = 1 else: pred_train_raw[i] = 0 #dtrain.get_label() 获得每个样本的类别，返回一个list整体对象 print(accuracy_score(dtrain.get_label(),pred_train_raw)) result:\n0.9549333333333333 再看看验证集的表现：\npred_test_raw = raw_model.predict(dtest) for i in range(len(pred_test_raw)): if pred_test_raw[i] \u0026gt; 0.5: pred_test_raw[i] = 1 else: pred_test_raw[i] = 0 print(accuracy_score(dtest.get_label(),pred_test_raw)) result:\n0.9432 验证集的准确率我这里的输出是0.9432，已经很高了。\n不过对于我这样用惯sklearn风格API的，还是不太喜欢原生Python API接口，既然有sklearn的wrapper，那么就尽量使用sklearn风格的接口吧。\n使用sklearn风格接口，使用原生参数\n其实就是使用XGBClassifier/XGBRegressor的**kwargs参数，把上面原生参数的params集合放进去，代码如下：\nparam = {\u0026#34;max_depth\u0026#34;:5,\u0026#34;eta\u0026#34;:0.5,\u0026#34;verbosity\u0026#34;:1,\u0026#34;eval_metric\u0026#34;:\u0026#34;logloss\u0026#34;} sklearn_model_raw = xgb.XGBClassifier(**param) sklearn_model_raw.fit(X_train,y_train,early_stopping_rounds=10,eval_metric=\u0026#34;error\u0026#34;, eval_set=[(X_test,y_test)]) result: 使用sklearn风格的接口，却使用原始的参数名定义，感觉还是有点怪，所以我一般还是习惯使用另一种风格接口，sklearn风格的参数命名\n使用sklearn风格接口，使用sklearn风格参数\n使用sklearn风格的接口，并使用sklearn风格的参数，是我推荐的方式，主要是这样做和GBDT之类的sklearn库使用起来没有什么两样了，也可以使用sklearn的网格搜索。\n不过这样做的话，参数定义命名和2.1与2.2节就有些不同了。具体的参数意义我们后面讲，我们看看分类的算法初始化，训练与调用的简单过程：\nsklearn_model_new = xgb.XGBClassifier(max_depth=5,learning_rate=0.5,verbosity=1,eval_metric=\u0026#34;logloss\u0026#34;, random_state=22) 可以看到，参数定义直接放在了XGBClassifier的类参数里，和sklearn类似。大家可以看到之前两节我们定义的步长eta，这里变成了另一个名字learning_rate。 在初始化后，训练和预测的方法就和2.2节没有区别了。\nsklearn_model_new.fit(X_train,y_train,early_stopping_rounds=10,eval_metric=\u0026#34;error\u0026#34;, eval_set=[(X_test,y_test)]) result: XGBoost可以和sklearn的网格搜索类GridSeachCV结合使用来调参，使用时和普通sklearn分类回归算法没有区别。具体的流程的一个示例如下：\ngsCV = GridSearchCV(sklearn_model_new, {\u0026#34;max_depth\u0026#34;:[4,5,6], \u0026#34;n_estimators\u0026#34;:[5,10,20]}) gsCV.fit(X_train,y_train) result: print(gsCV.best_score_) print(gsCV.best_params_) result:\n0.9408 {\u0026#39;max_depth\u0026#39;: 4, \u0026#39;n_estimators\u0026#39;: 5} 接着尝试在上面搜索的基础上调learning_rate ：\nsklearn_model_new2 = xgb.XGBClassifier(max_depth=4,n_estimator=10,learning_rate=0.5,verbosity=1,eval_metric=\u0026#34;logloss\u0026#34;, random_state=22) gsCV2 = GridSearchCV(sklearn_model_new2, {\u0026#34;learning_rate\u0026#34;:[0.3,0.5,0.7]}) gsCV2.fit(X_train,y_train) result: code:\nprint(gsCV2.best_score_) print(gsCV2.best_params_) result:\n0.9393333333333335 {\u0026#39;learning_rate\u0026#39;: 0.3} 当然实际情况这里需要继续调参，这里假设我们已经调参完毕，我们尝试用验证集看看效果：\nsklearn_model_new2 = xgb.XGBClassifier(max_depth=4,n_estimator=10,learning_rate=0.3,verbosity=1,eval_metric=\u0026#34;logloss\u0026#34;, random_state=22) sklearn_model_new2.fit(X_train,y_train,early_stopping_rounds=10,eval_metric=\u0026#34;error\u0026#34;, eval_set=[(X_test,y_test)]) result: validation_0-error:0.05480,也就是准确率了\n","date":"2021-12-10T00:00:00Z","permalink":"https://example.com/p/%E5%88%98%E5%BB%BA%E5%B9%B3%E8%80%81%E5%B8%88pinard%E5%8D%9A%E5%AE%A2%E7%9A%84xgboost%E7%B1%BB%E5%BA%93%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/","title":"刘建平老师Pinard博客的XGBoost类库代码学习记录"},{"content":"import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib # matplotlib.style.use(\u0026#34;ggplot\u0026#34;) matplotlib.line_width = 5000 matplotlib.max_columns = 60 plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = \u0026#34;SimHei\u0026#34; plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) from sklearn.ensemble import AdaBoostClassifier from sklearn.tree import DecisionTreeClassifier #用make_gaussian_quantiles生成分组多维正态分布的数据 from sklearn.datasets import make_gaussian_quantiles 接着我们生成一些随机数据来做二元分类\n#生成一些随机数据按位数分为两类，500个样本，2个样本特征，协方差系数为2 X1, y1 = make_gaussian_quantiles(cov=2.0,n_samples=500,n_features=2, n_classes=2,random_state=23) #生成的两个样本特征均值都为3 X2, y2 = make_gaussian_quantiles(cov=1.5,n_samples=400,n_features=2,n_classes=2, random_state=23,mean=(3,3)) X1[:5],y1[:5],X2[:5],y2[:5] result: #合并两组数据 #记得用一个()装 X = np.concatenate((X1,X2)) y = np.concatenate((y1,y2)) X[:5],y[:5] result: 我们通过可视化看看我们的分类数据，它有两个特征，两个输出类别，用颜色区别\n#这里的参数c 是color它会根据赋值产生颜色 plt.scatter(X[:,0],X[:,1],marker=\u0026#34;o\u0026#34;,c=y) result: 可以看到数据有些混杂，我们现在用基于决策树的Adaboost来做分类拟合。\nbdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2,min_samples_leaf=5, min_samples_split=20),algorithm=\u0026#34;SAMME\u0026#34;, n_estimators=200,learning_rate=0.8) bdt.fit(X,y) result: 这里我们选择了SAMME算法，最多200个弱分类器，步长0.8，在实际运用中你可能需要通过交叉验证调参而选择最好的参数。拟合完了后，我们用网格图来看看它拟合的区域。\n#这个可以当作固定写法了 x_min,x_max = X[:,0].min() - 1,X[:,0].max() + 1 y_min,y_max = X[:,1].min() - 1,X[:,1].max() + 1 xx,yy = np.meshgrid(np.arange(x_min,x_max,0.02), np.arange(y_min,y_max,0.02)) Z = bdt.predict(np.c_[xx.ravel(),yy.ravel()]) Z = Z.reshape(xx.shape) cs = plt.contourf(xx,yy,Z,camp=plt.cm.Paired) plt.scatter(X[:,0],X[:,1],marker = \u0026#34;o\u0026#34;,c=y) plt.show() result: 从图中可以看出，Adaboost的拟合效果还是不错的，现在我们看看拟合分数：\nbdt.score(X,y) result:\n0.9866666666666667 也就是说拟合训练集数据的分数还不错。当然分数高并不一定好，因为可能过拟合。\n现在我们将最大弱分离器个数从200增加到300。再来看看拟合分数。\nbdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2,min_samples_split=20, min_samples_leaf=5),algorithm=\u0026#34;SAMME\u0026#34;, n_estimators=300,learning_rate=0.8) bdt.fit(X,y) bdt.score(X,y) result:\n0.9866666666666667 这印证了我们前面讲的，弱分离器个数越多，则拟合程度越好，当然也越容易过拟合。\n现在我们降低步长，将步长从上面的0.8减少到0.5，再来看看拟合分数。\nbdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2,min_samples_split=20, min_samples_leaf=5),algorithm=\u0026#34;SAMME\u0026#34;, n_estimators=300,learning_rate=0.5) bdt.fit(X,y) bdt.score(X,y) result:\n0.9777777777777777 可见在同样的弱分类器的个数情况下，如果减少步长，拟合效果会下降\n因为步长的作用起到一个平衡，平衡误差和复杂度 最后我们看看当弱分类器个数为700，步长为0.7时候的情况：\nbdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2,min_samples_split=20, min_samples_leaf=5),algorithm=\u0026#34;SAMME\u0026#34;, n_estimators=700,learning_rate=0.7) bdt.fit(X,y) bdt.score(X,y) result:\n0.99 ","date":"2021-12-05T00:00:00Z","permalink":"https://example.com/p/%E5%88%98%E5%BB%BA%E5%B9%B3%E8%80%81%E5%B8%88pinard%E5%8D%9A%E5%AE%A2%E7%9A%84adaboostclassifier%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/","title":"刘建平老师Pinard博客的AdaBoostClassifier代码学习记录"},{"content":"import pandas as pd import numpy as np import matplotlib.pyplot as plt import matplotlib matplotlib.style.use(\u0026#34;ggplot\u0026#34;) plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = \u0026#34;SimHei\u0026#34; plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False from sklearn.ensemble import GradientBoostingClassifier # from sklearn import cross_validation, metrics cross_validation 换成了 cross_val_score from sklearn.model_selection import GridSearchCV,cross_val_score from sklearn import metrics 接着，我们把解压的数据用下面的代码载入，顺便看看数据的类别分布。\ntrain = pd.read_csv(\u0026#34;./train_modified.csv\u0026#34;) train result: code:\ntarget = \u0026#34;Disbursed\u0026#34; IDcol = \u0026#34;ID\u0026#34; train[\u0026#34;Disbursed\u0026#34;].value_counts() # 可以看到类别输出如下，也就是类别0的占大多数。 result: 现在我们得到我们的训练集。最后一列Disbursed是分类输出。前面的所有列（不考虑ID列）都是样本特征 code:\nx_columns = [x for x in train.columns if x not in [target,IDcol]] X = train[x_columns] y = train[\u0026#34;Disbursed\u0026#34;] X.head() result:\n不管任何参数，都用默认的，我们拟合下数据看看：\ngbm0 = GradientBoostingClassifier(random_state=22) gbm0.fit(X,y) y_pred = gbm0.predict(X) y_predprob = gbm0.predict_proba(X)[:,1] print(\u0026#34;accuracy:%.4g\u0026#34;% metrics.accuracy_score(y.values,y_pred)) print(\u0026#34;AUC Score(train):%f\u0026#34; % metrics.roc_auc_score(y,y_predprob)) # 输出如下，可见拟合还可以，我们下面看看怎么通过调参提高模型的泛化能力。 result:\naccuracy:0.9852 AUC Score(train):0.900531 首先我们从步长(learning rate)和迭代次数(n_estimators)入手。一般来说,开始选择一个较小的步长来网格搜索最好的迭代次数。这里，我们将步长初始值设置为0.1。对于迭代次数进行网格搜索如下：\nparam_test1 = {\u0026#34;n_estimators\u0026#34;:range(20,81,10)} gsearch1 = GridSearchCV(estimator=GradientBoostingClassifier(learning_rate=0.1,min_samples_split=300, min_samples_leaf=20,max_depth=8, max_features=\u0026#34;sqrt\u0026#34;,subsample=0.8, random_state=10), param_grid=param_test1,scoring=\u0026#34;roc_auc\u0026#34;,cv=5) gsearch1.fit(X,y) gsearch1.cv_results_,gsearch1.best_params_,gsearch1.best_score_ # 输出如下，可见最好的迭代次数是40。 result: 后面的就是关于其他的参数调参了，跟adaboost的调参过程差不多 https://www.cnblogs.com/pinard/p/6143927.html\n","date":"2021-12-05T00:00:00Z","permalink":"https://example.com/p/%E5%88%98%E5%BB%BA%E5%B9%B3%E8%80%81%E5%B8%88pinard%E5%8D%9A%E5%AE%A2%E7%9A%84sklearngbdt%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/","title":"刘建平老师Pinard博客的sklearnGBDT代码学习记录"},{"content":"# 导包 import numpy as np import pandas as pd import matplotlib.pyplot as plt plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = [\u0026#34;SimHei\u0026#34;] plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) #从集成中导入RF from sklearn.ensemble import RandomForestClassifier #导入网格调参 # from sklearn.grid_search import GridSearchCV 旧版的sklearn from sklearn.model_selection import GridSearchCV # from sklearn import cross_validation,metrics 旧版写法 from sklearn.model_selection import cross_validate from sklearn import metrics #数据 train = pd.read_csv(\u0026#34;./train_modified.csv\u0026#34;) train result: code:\ntarget = \u0026#34;Disbursed\u0026#34;#Disbursed的值就是二元分类的输出 IDcol = \u0026#34;ID\u0026#34; train[\u0026#34;Disbursed\u0026#34;].value_counts()#查看类别的数量 result:\n0 19680 1 320 Name: Disbursed, dtype: int64 可以看到类别输出如上，也就是类别0的占大多数。\n#提取样本特征和类别输出 x_columns = [x for x in train.columns if x not in [target,IDcol]] # x_columns#获取了所有非输出和ID的特征 X = train[x_columns] Y = train[\u0026#34;Disbursed\u0026#34;] X result: code:\nY result: 不管任何参数，都用默认的，我们拟合下数据看看：\nRF0 = RandomForestClassifier(oob_score=True,random_state=10)#random_state保证每次运行代码的结果相同 RF0.fit(X,Y) print(RF0.oob_score_) Y_predprob = RF0.predict_proba(X)[:,1] print(\u0026#34;AUC Score (Train): %f\u0026#34; % metrics.roc_auc_score(Y,Y_predprob)) result:\n0.98315 AUC Score (Train): 0.999994 输出可见袋外分数已经很高，而且AUC分数也很高。相对于GBDT的默认参数输出，RF的默认参数拟合效果对本例要好一些。\n我们首先对n_estimators（弱学习器）进行网格搜索：\nparam_test1 = {\u0026#34;n_estimators\u0026#34;:range(10,71,10)} gsearch1 = GridSearchCV(estimator= RandomForestClassifier(min_samples_split=100, min_samples_leaf=20, max_depth=8, max_features=\u0026#34;sqrt\u0026#34;, random_state=10), param_grid=param_test1,scoring=\u0026#34;roc_auc\u0026#34;,cv=15) gsearch1.fit(X,Y) # gsearch1.grid_score_,gsearch1.best_params_,gsearch1.best_score_ #grid_score_被cv_results_取代 gsearch1.cv_results_,gsearch1.best_params_,gsearch1.best_score_ result: 这样我们得到了最佳的弱学习器迭代次数，接着我们对决策树最大深度max_depth和内部节点再划分所需最小样本数min_samples_split进行网格搜索。\n#其实调用网格搜索的话，哪个param_grid就是我们想要确定的东西 dict包起来 param_test2 = {\u0026#34;max_depth\u0026#34;:range(3,14,2),\u0026#34;min_samples_split\u0026#34;:range(50,201,20)} gsearch2 = GridSearchCV(estimator=RandomForestClassifier(n_estimators=70, min_samples_leaf=20, max_features=\u0026#34;sqrt\u0026#34;, oob_score=True, random_state=10), param_grid=param_test2,scoring=\u0026#34;roc_auc\u0026#34;,cv=5) gsearch2.fit(X,Y) #cv_results_ 返回所有训练的结果 #best_params_ 返回训练最好参数 #best_score_ 返回最佳的分数 gsearch2.cv_results_,gsearch2.best_params_,gsearch2.best_score_ result: 我们看看我们现在模型的袋外分数：\nRF1 = RandomForestClassifier(n_estimators=70,max_depth=13,min_samples_split=110, min_samples_leaf=20, max_features=\u0026#34;sqrt\u0026#34;, oob_score=True, random_state=10) RF1.fit(X,Y) print(RF1.oob_score_) result:\n0.984 可见此时我们的袋外分数有一定的提高。也就是时候模型的泛化能力增强了。\n对于内部节点再划分所需最小样本数min_samples_split，我们暂时不能一起定下来，因为这个还和决策树其他的参数存在关联。下面我们再对内部节点再划分所需最小样本数min_samples_split和叶子节点最少样本数min_samples_leaf一起调参。\nparam_test3 = {\u0026#34;min_samples_split\u0026#34;:range(80,150,20),\u0026#34;min_samples_leaf\u0026#34;:range(10,60,10)} gsearch3 = GridSearchCV(estimator=RandomForestClassifier(n_estimators=70, max_depth=13, max_features=\u0026#34;sqrt\u0026#34;, oob_score=True, random_state=10), param_grid=param_test3,scoring=\u0026#34;roc_auc\u0026#34;,cv=5) gsearch3.fit(X,Y) gsearch3.cv_results_,gsearch3.best_params_,gsearch3.best_score_ result: 最后我们再对最大特征数max_features做调参:\nparam_test4 = {\u0026#34;max_features\u0026#34;:range(3,11,2)} gsearch4 = GridSearchCV(estimator=RandomForestClassifier(n_estimators=70,max_depth=13,min_samples_leaf=20, min_samples_split=120,oob_score=True,random_state=10), param_grid=param_test4,scoring=\u0026#34;roc_auc\u0026#34;,cv=5) gsearch4.fit(X,Y) gsearch4.cv_results_,gsearch4.best_params_,gsearch4.best_score_ result: 用我们搜索到的最佳参数，我们再看看最终的模型拟合：\nRF2 = RandomForestClassifier(n_estimators=70,max_depth=13,min_samples_leaf=20, min_samples_split=120,max_features=7,oob_score=True) RF2.fit(X,Y) print(RF2.oob_score_) result:\n0.984 可见此时模型的袋外分数基本没有提高，主要原因是0.984已经是一个很高的袋外分数了，如果想进一步需要提高模型的泛化能力，我们需要更多的数据。\n","date":"2021-12-05T00:00:00Z","permalink":"https://example.com/p/%E5%88%98%E5%BB%BA%E5%B9%B3%E8%80%81%E5%B8%88pinard%E5%8D%9A%E5%AE%A2%E7%9A%84%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/","title":"刘建平老师Pinard博客的随机森林代码学习记录"},{"content":" 这里来记录下make_classification的参数详情 import numpy as np import pandas as pd import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) import matplotlib.pyplot as plt from sklearn.datasets import make_classification from sklearn.model_selection import train_test_split X,y = make_classification(n_samples=1000,#1000个样本 n_features=2,#两个特征，方便画图 n_informative=2,#信息特征(有用特征) n_redundant=0,#冗余特征，它是信息特征的线性组合 n_repeated=0,#重复特征 n_classes=2,#分类特征 random_state=None, n_clusters_per_class=2,#每个类别两簇 shuffle=True, class_sep=1,#将每个簇分隔开来，较大的值将使分类任务更加容易 shift = 10, scale = 3, flip_y = 0)#无噪声 #训练集与测试集分割函数 x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=22) data = np.concatenate((X,y.reshape(1000,1)),axis=1) x0 = [] x1 = [] y0 = [] y1 = [] for d in data: if d[2]==0: x0.append(d[0]) y0.append(d[1]) else: x1.append(d[0]) y1.append(d[1]) plt.scatter(x0,y0) plt.scatter(x1,y1) plt.show() result: ","date":"2021-11-27T00:00:00Z","permalink":"https://example.com/p/%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E5%88%9B%E5%BB%BAmake_classification%E7%9A%84%E5%8F%82%E6%95%B0%E8%AF%A6%E6%83%85/","title":"数据集的创建make_classification的参数详情"},{"content":"numpy生成 import numpy as np np.random.rand(2,2,2) result: np.random.randn(3,2) result: #只需要在randn上每个生成的值x上做变换σx+μ即可 2*np.random.randn(3,2) + 1 result: np.random.randint(3,6,[2,3,4]) result: np.random.random_integers(3,6,[2,3,4]) result: np.random.random_sample([2,2]) result: #如果是其他区间[a,b),可以加以转换(b - a) * random_sample([size]) + a (5-2)*np.random.random_sample([3]) + 2 result: 回归模型随机数据 这里我们使用make_regression生成回归模型数据。几个关键参数有n_samples（生成样本数）， n_features（样本特征数），noise（样本随机噪音）和coef（是否返回回归系数）。例子代码如下：\nimport matplotlib.pyplot as plt from sklearn.datasets import make_regression #X为样本特征，y为样本输出， coef为回归系数，共1000个样本，每个样本1个特征 X,y,coef = make_regression(n_samples=1000,n_features=1,noise=10,coef=True) plt.scatter(X,y,color=\u0026#34;black\u0026#34;) #看来coef是不包含bias print(coef) plt.plot(X,X*coef,color=\u0026#34;blue\u0026#34;,linewidth=3) plt.xticks(()) plt.yticks(()) plt.show() result: 分类模型随机数据 这里我们用make_classification生成三元分类模型数据。几个关键参数有n_samples（生成样本数）， n_features（样本特征数）， n_redundant（冗余特征数）和n_classes（输出的类别数），例子代码如下\nfrom sklearn.datasets import make_classification # X1为样本特征，Y1为样本类别输出， 共400个样本，每个样本2个特征，输出有3个类别，没有冗余特征，每个类别一个簇 X1,Y1 = make_classification(n_samples=400,n_classes=3,n_clusters_per_class=1,n_features=2,n_redundant=0) plt.scatter(X1[:,0],X1[:,1],marker=\u0026#34;o\u0026#34;,c=Y1) plt.xticks(()) plt.yticks(()) plt.show() result: 聚类模型随机数据 这里我们用make_blobs生成聚类模型数据。几个关键参数有n_samples（生成样本数）， n_features（样本特征数），centers(簇中心的个数或者自定义的簇中心)和cluster_std（簇数据方差，代表簇的聚合程度）。 例子如下：\nfrom sklearn.datasets import make_blobs # X为样本特征，Y为样本簇类别， 共1000个样本，每个样本2个特征，共3个簇，簇中心在[-1,-1], [1,1], [2,2]， 簇方差分别为[0.4, 0.5, 0.2] X,y = make_blobs(n_samples=1000,n_features=2,centers=[[-1,-1],[1,1],[2,2]],cluster_std=[0.4,0.5,0.2]) plt.scatter(X[:,0],X[:,1],marker=\u0026#34;o\u0026#34;,c=y) plt.xticks(()) plt.yticks(()) plt.show() result: 分组正态分布混合数据 我们用make_gaussian_quantiles生成分组多维正态分布的数据。几个关键参数有n_samples（生成样本数）， n_features（正态分布的维数），mean（特征均值）， cov（样本协方差的系数）， n_classes（数据在正态分布中按分位数分配的组数）。 例子如下：\nfrom sklearn.datasets import make_gaussian_quantiles #生成2维正态分布，生成的数据按分位数分成3组，1000个样本,2个样本特征均值为1和2，协方差系数为2 X1,Y1 = make_gaussian_quantiles(n_samples=1000,n_features=2,n_classes=3,mean=[1,2],cov=2) plt.scatter(X1[:,0],X1[:,1],marker=\u0026#34;o\u0026#34;,c=Y1) plt.xticks(()) plt.yticks(()) plt.show() result: ","date":"2021-11-27T00:00:00Z","permalink":"https://example.com/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E7%9A%84%E9%9A%8F%E6%9C%BA%E6%95%B0%E6%8D%AE%E7%94%9F%E6%88%90/","title":"机器学习算法的随机数据生成"},{"content":"机器学习练习8 集成学习 课程完整代码：https://github.com/fengdu78/WZU-machine-learning-course\n代码修改并注释：黄海广，haiguang2000@wzu.edu.cn\nimport warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) import pandas as pd from sklearn.model_selection import train_test_split 生成数据 生成12000行的数据，训练集和测试集按照3:1划分\nfrom sklearn.datasets import make_hastie_10_2 data, target = make_hastie_10_2() X_train, X_test, y_train, y_test = train_test_split(data, target, random_state=123) X_train.shape, X_test.shape result:\n((9000, 10), (3000, 10)) 模型对比 对比六大模型，都使用默认参数，因为数据是\nfrom sklearn.linear_model import LogisticRegression from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import AdaBoostClassifier from sklearn.ensemble import GradientBoostingClassifier from xgboost import XGBClassifier from lightgbm import LGBMClassifier from sklearn.model_selection import cross_val_score import time clf1 = LogisticRegression() clf2 = RandomForestClassifier() clf3 = AdaBoostClassifier() clf4 = GradientBoostingClassifier() clf5 = XGBClassifier() clf6 = LGBMClassifier() for clf, label in zip([clf1, clf2, clf3, clf4, clf5, clf6], [ \u0026#39;Logistic Regression\u0026#39;, \u0026#39;Random Forest\u0026#39;, \u0026#39;AdaBoost\u0026#39;, \u0026#39;GBDT\u0026#39;, \u0026#39;XGBoost\u0026#39;, \u0026#39;LightGBM\u0026#39; ]): start = time.time() scores = cross_val_score(clf, X_train, y_train, scoring=\u0026#39;accuracy\u0026#39;, cv=5) end = time.time() running_time = end - start print(\u0026#34;Accuracy: %0.8f (+/- %0.2f),耗时%0.2f秒。模型名称[%s]\u0026#34; % (scores.mean(), scores.std(), running_time, label)) result: 对比了六大模型，可以看出，逻辑回归速度最快，但准确率最低。 而LightGBM，速度快，而且准确率最高，所以，现在处理结构化数据的时候，大部分都是用LightGBM算法。\nXGBoost的使用 XGBoost的使用 import xgboost as xgb #记录程序运行时间 import time start_time = time.time() #xgb矩阵赋值 xgb_train = xgb.DMatrix(X_train, y_train) xgb_test = xgb.DMatrix(X_test, label=y_test) ##参数 params = { \u0026#39;booster\u0026#39;: \u0026#39;gbtree\u0026#39;, # \u0026#39;silent\u0026#39;: 1, #设置成1则没有运行信息输出，最好是设置为0. #\u0026#39;nthread\u0026#39;:7,# cpu 线程数 默认最大 \u0026#39;eta\u0026#39;: 0.007, # 如同学习率 \u0026#39;min_child_weight\u0026#39;: 3, # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言 #，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。 #这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。 \u0026#39;max_depth\u0026#39;: 6, # 构建树的深度，越大越容易过拟合 \u0026#39;gamma\u0026#39;: 0.1, # 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。 \u0026#39;subsample\u0026#39;: 0.7, # 随机采样训练样本 \u0026#39;colsample_bytree\u0026#39;: 0.7, # 生成树时进行的列采样 \u0026#39;lambda\u0026#39;: 2, # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。 #\u0026#39;alpha\u0026#39;:0, # L1 正则项参数 #\u0026#39;scale_pos_weight\u0026#39;:1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。 #\u0026#39;objective\u0026#39;: \u0026#39;multi:softmax\u0026#39;, #多分类的问题 #\u0026#39;num_class\u0026#39;:10, # 类别数，多分类与 multisoftmax 并用 \u0026#39;seed\u0026#39;: 1000, #随机种子 #\u0026#39;eval_metric\u0026#39;: \u0026#39;auc\u0026#39; } plst = list(params.items()) num_rounds = 500 # 迭代次数 watchlist = [(xgb_train, \u0026#39;train\u0026#39;), (xgb_test, \u0026#39;val\u0026#39;)] #训练模型并保存 # early_stopping_rounds 当设置的迭代次数较大时，early_stopping_rounds 可在一定的迭代次数内准确率没有提升就停止训练 model = xgb.train( plst, xgb_train, num_rounds, watchlist, early_stopping_rounds=100, ) #model.save_model(\u0026#39;./model/xgb.model\u0026#39;) # 用于存储训练出的模型 print(\u0026#34;best best_ntree_limit\u0026#34;, model.best_ntree_limit) y_pred = model.predict(xgb_test, ntree_limit=model.best_ntree_limit) print(\u0026#39;error=%f\u0026#39; % (sum(1 for i in range(len(y_pred)) if int(y_pred[i] \u0026gt; 0.5) != y_test[i]) / float(len(y_pred)))) # 输出运行时长 cost_time = time.time() - start_time print(\u0026#34;xgboost success!\u0026#34;, \u0026#39;\\n\u0026#39;, \u0026#34;cost time:\u0026#34;, cost_time, \u0026#34;(s)......\u0026#34;) result: 使用scikit-learn接口 会改变的函数名是：\neta -\u0026gt; learning_rate\nlambda -\u0026gt; reg_lambda\nalpha -\u0026gt; reg_alpha\nfrom sklearn.model_selection import train_test_split from sklearn import metrics from xgboost import XGBClassifier clf = XGBClassifier( # silent=0, #设置成1则没有运行信息输出，最好是设置为0.是否在运行升级时打印消息。 #nthread=4,# cpu 线程数 默认最大 learning_rate=0.3, # 如同学习率 min_child_weight=1, # 这个参数默认是 1，是每个叶子里面 h 的和至少是多少，对正负样本不均衡时的 0-1 分类而言 #，假设 h 在 0.01 附近，min_child_weight 为 1 意味着叶子节点中最少需要包含 100 个样本。 #这个参数非常影响结果，控制叶子节点中二阶导的和的最小值，该参数值越小，越容易 overfitting。 max_depth=6, # 构建树的深度，越大越容易过拟合 gamma=0, # 树的叶子节点上作进一步分区所需的最小损失减少,越大越保守，一般0.1、0.2这样子。 subsample=1, # 随机采样训练样本 训练实例的子采样比 max_delta_step=0, #最大增量步长，我们允许每个树的权重估计。 colsample_bytree=1, # 生成树时进行的列采样 reg_lambda=1, # 控制模型复杂度的权重值的L2正则化项参数，参数越大，模型越不容易过拟合。 #reg_alpha=0, # L1 正则项参数 #scale_pos_weight=1, #如果取值大于0的话，在类别样本不平衡的情况下有助于快速收敛。平衡正负权重 #objective= \u0026#39;multi:softmax\u0026#39;, #多分类的问题 指定学习任务和相应的学习目标 #num_class=10, # 类别数，多分类与 multisoftmax 并用 n_estimators=100, #树的个数 seed=1000 #随机种子 #eval_metric= \u0026#39;auc\u0026#39; ) clf.fit(X_train, y_train) y_true, y_pred = y_test, clf.predict(X_test) print(\u0026#34;Accuracy : %.4g\u0026#34; % metrics.accuracy_score(y_true, y_pred)) result: LIghtGBM的使用 原生接口 import lightgbm as lgb from sklearn.metrics import mean_squared_error # 加载你的数据 # print(\u0026#39;Load data...\u0026#39;) # df_train = pd.read_csv(\u0026#39;../regression/regression.train\u0026#39;, header=None, sep=\u0026#39;\\t\u0026#39;) # df_test = pd.read_csv(\u0026#39;../regression/regression.test\u0026#39;, header=None, sep=\u0026#39;\\t\u0026#39;) # # y_train = df_train[0].values # y_test = df_test[0].values # X_train = df_train.drop(0, axis=1).values # X_test = df_test.drop(0, axis=1).values # 创建成lgb特征的数据集格式 lgb_train = lgb.Dataset(X_train, y_train) # 将数据保存到LightGBM二进制文件将使加载更快 lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train) # 创建验证数据 # 将参数写成字典下形式 params = { \u0026#39;task\u0026#39;: \u0026#39;train\u0026#39;, \u0026#39;boosting_type\u0026#39;: \u0026#39;gbdt\u0026#39;, # 设置提升类型 \u0026#39;objective\u0026#39;: \u0026#39;regression\u0026#39;, # 目标函数 \u0026#39;metric\u0026#39;: {\u0026#39;l2\u0026#39;, \u0026#39;auc\u0026#39;}, # 评估函数 \u0026#39;num_leaves\u0026#39;: 31, # 叶子节点数 \u0026#39;learning_rate\u0026#39;: 0.05, # 学习速率 \u0026#39;feature_fraction\u0026#39;: 0.9, # 建树的特征选择比例 \u0026#39;bagging_fraction\u0026#39;: 0.8, # 建树的样本采样比例 \u0026#39;bagging_freq\u0026#39;: 5, # k 意味着每 k 次迭代执行bagging \u0026#39;verbose\u0026#39;: 1 # \u0026lt;0 显示致命的, =0 显示错误 (警告), \u0026gt;0 显示信息 } print(\u0026#39;Start training...\u0026#39;) # 训练 cv and train gbm = lgb.train(params, lgb_train, num_boost_round=500, valid_sets=lgb_eval, early_stopping_rounds=5) # 训练数据需要参数列表和数据集 print(\u0026#39;Save model...\u0026#39;) gbm.save_model(\u0026#39;model.txt\u0026#39;) # 训练后保存模型到文件 print(\u0026#39;Start predicting...\u0026#39;) # 预测数据集 y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration ) #如果在训练期间启用了早期停止，可以通过best_iteration方式从最佳迭代中获得预测 # 评估模型 print(\u0026#39;error=%f\u0026#39; % (sum(1 for i in range(len(y_pred)) if int(y_pred[i] \u0026gt; 0.5) != y_test[i]) / float(len(y_pred)))) result: scikit-learn接口 from sklearn import metrics from lightgbm import LGBMClassifier clf = LGBMClassifier( boosting_type=\u0026#39;gbdt\u0026#39;, # 提升树的类型 gbdt,dart,goss,rf num_leaves=31, #树的最大叶子数，对比xgboost一般为2^(max_depth) max_depth=-1, #最大树的深度 learning_rate=0.1, #学习率 n_estimators=100, # 拟合的树的棵树，相当于训练轮数 subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, # 最小分割增益 min_child_weight=0.001, # 分支结点的最小权重 min_child_samples=20, subsample=1.0, # 训练样本采样率 行 subsample_freq=0, # 子样本频率 colsample_bytree=1.0, # 训练特征采样率 列 reg_alpha=0.0, # L1正则化系数 reg_lambda=0.0, # L2正则化系数 random_state=None, n_jobs=-1, silent=True, ) clf.fit(X_train, y_train, eval_metric=\u0026#39;auc\u0026#39;) #设置验证集合 verbose=False不打印过程 clf.fit(X_train, y_train) y_true, y_pred = y_test, clf.predict(X_test) print(\u0026#34;Accuracy : %.4g\u0026#34; % metrics.accuracy_score(y_true, y_pred)) result:\nAccuracy : 0.927 参考 1.https://xgboost.readthedocs.io/\n2.https://lightgbm.readthedocs.io/\n3.https://blog.csdn.net/q383700092/article/details/53763328?locationNum=9\u0026amp;fps=1\n","date":"2021-11-19T00:00:00Z","permalink":"https://example.com/p/wzu_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/","title":"WZU_集成学习算法代码学习记录"},{"content":" ","date":"2021-11-07T00:00:00Z","permalink":"https://example.com/p/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95_%E6%9D%8E%E8%88%AA/","title":"《统计学习方法_李航》"},{"content":" 写在前面的话：这个笔记不过多深入知识，因为做入门有，大概知道什么时候该用哪个，以及现在有什么技术\nLinux {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.6345844269466316in\u0026rdquo;}\n有一定的基础所以不做笔记了\nPerl 可以去看其他的课程\n","date":"2021-11-04T00:00:00Z","permalink":"https://example.com/p/%E5%B1%B1%E4%B8%9C%E5%A4%A7%E7%94%9F%E4%BF%A1_linux_perl_note/","title":"山东大生信_Linux_Perl_note"},{"content":" 写在前面的话：这个笔记不过多深入知识，因为做入门有，大概知道什么时候该用哪个，以及现在有什么技术\n教程地址：https://www.bilibili.com/video/BV13t411372E?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click\n写在前面的话：这个笔记不过多深入知识，因为做入门有，大概知道什么时候该用哪个，以及现在有什么技术\n生物数据库 PubMed的使用（原核生物） PubMed 是拥有超过两百六十万生物医学文献的数据库。这些文献来源于 MEDLINE， 也就是生物医学文献数据库、生命科学领域学术杂志以及在线的专业书籍。他们大部分提供 全文链接。注意，提供的是链接，你有没有权限通过这个链接打开或者下载全文另当别论。 不管怎么说，看上去还不错。PubMed 主页（http://www.ncbi.nlm.nih.gov/pubmed）上有个搜 索条。不管三七二十一，先把家说的 dUTPase 敲到搜索条里，点搜索。找到了五百多条文 献。每个文献有题目，作者，刊物，出版时间等等。如果列出的这些信息还不够，或者无法 满足你的要求，你从页面上方设置每个文献是要显示内容、总结、摘要，还是其他。还可以 控制每页显示几个文献，以及按照你期望的顺序进行排序\n如果找到的文献太多，一时看不完，可以把他们保存到本地。只要选中你要保存的文献， 然后通过发送按钮，选择文件，再选择保存的内容以及顺序，最后点创建文件。这样你选中 文章的信息就以纯文本文件的形式保存到本地电脑上了。\nPubmed 提供文 献的摘要和全文链接。这里有两个全文链接。其中一个链接的图标上有 free 字样。Jim 很幸 运，这篇文章是 open access 的，也就是免费阅读的。两个链接，第一个是期刊提供的全文 链接，第二个是 PubMed 中心提供的全文链接。点其中一个链接，就可以在线浏览文献全文 了，或者下载全文的 PDF 文件到本地。至此，JIM 总算找到了些许有用的信息\n回到 PUBMED 搜索结果页面，在显示内容格式这个下拉菜单里，除了总结，摘要，还 有个叫 MEDLINE 的项目。你可以把它简单的理解为数据库中文献记录的内部结构。每条 文献都是以这样的内部结构存储在数据库中的。一篇文献的所以信息被分割成小节，每个小 节都有自己的索引名，比如 TI 代表题目，AB 代表摘要，AU 代表作者等等。这些由几个字 母组成的索引名是规定好的。\n了解了 MEDLINE 结构，我们就可以在搜索条中通过引入索引名，来按照不同的规则 搜索。比如搜索 Down 这个词。我们在 Down 的后面加上空格，中括号 AU（Down [AU]）， 就会返回所有作者名里有 Down 这个词的文献。如果加上[TI]，则返回题目中有 Down 的 文献。中括号 AD 是搜索发表单位。如果什么限制都没有，只写 Down 的话是在任意地方搜 索。我们看到，引入不同索引名后，搜索到的文献数量是不相同的\n现在，我们的 Jim 已经学会了 PubMed 的基本使用。他打算趁暑假回北京期间找个相关 的专家拜访一下。于是他在 PubMed 上搜索，题目和摘要里有 dUTPase 的文献，而且文献的 发表单位是北京的（dUTPase [TIAB] Beijing [AD]）。搜索后，找到了这四篇文章。 其中第二篇文章是研究 dUTPase 晶体结构的，Jim 很感兴趣。点进去看一下，找到最后一位 的作者。我们说，通常课题的主要负责人会以通讯作者的身份出现在作者列队的最后。那么， 这个研究团队的主要负责人就是这位 Su XD，Su 教授。他的单位是北京大学。之后的任务 就可以交给度娘来完成了。度娘很快就找到了苏教授的地址和电话，而且连照片都有。Jim 通过这些信息，成功的拜访了北京大学生命科学学院的苏晓东教授\n除了搜索条，我们还可以利用高级搜索工具更精确的查找。高级搜索可以无限的添加条 件。比如发表日期从哪天到哪天，文章类型是 research article 还是 review，语言是英语发表 的还是其他语言发表的等等。我们可以查找比如 2000 年至今发表的，题目有 dUTPase 关键 词的，英语的，review 文章。一共找到五篇，相信 Jim 同学看完这 5 篇文章之后，一定会对 dUPTase 有一个很好的了解\n关于 PubMed 还有几点要说明的。在搜索时，可以使用引号。引号里的词会被当作一个 整体来看待，而不会被拆开。这个小技巧同样适用于度娘等网络搜索引擎。也可以使用逻辑词 AND OR NOT。比如你可以规定，题目里有 dUPTase，并且题目里有 bacteria，但是作者 里不要有 Smith（dUTPase [TI] AND bacteria [TI] NOT Smith [AU]）。此外，如 果知名知姓，可以利用正确的名字缩写来提高搜索的准确度。还有非常关键的一点，每篇文 章都有自己唯一的 PubMed ID（PMID）。通过这个号，可以直接找到某一篇文章。最后，不 得不说的是，有的时候 PubMed 也帮不了你。比如，搜索 1995 年以前的文献中排名十位以 后的作者是白费力气。搜索 1976 年以前的文献是没有摘要的。搜索 1965 年以前的文献，就 别想了。关于 PubMed 就给大家介绍到这里\n一级核酸数据库：GenBank原核生物核酸序列 看一级核酸数据库，他主要包括三大核酸数据库和基因组数据库。三大核 酸数据库包括 NCBI 的 Genbank，EMBL 的 ENA 和 DDBJ，它们共同构成国际核酸序列数据 库。三大核酸数据库，美国一个，欧洲一个，亚洲一个。美国的 Genbank 由美国国家生物 技术信息中心 NCBI 开发并负责维护。NCBI 隶属于美国国立卫生研究院 NIH。欧洲核苷酸 序列数据集 ENA 由欧洲分子生物学研究室 EMBL 开发并负责维护。亚洲的核酸数据库 DDBJ 由位于日本静冈的日本国立遗传学研究所 NIG 开发并负责维护。Genbank，EMBL 与 DDBJ 共同构成国际核酸序列数据库合作联盟 INSDC。通过 INSDC，三大核酸数据库的信息每日 相互交换，更新汇总。这使得他们几乎在任何时候都享有相同的数据\n我们以 NCBI 的 Genbank 为例，教你如何解读一级核酸数据库。我们将分别浏览一个原 核生物的基因和一个真核生物的基因。为此，请先跟我复习一下原核生物与真核生物基因的 不同之处。原核生物基因组小，真核生物基因组大。原核生物基因密度高，1000 个碱基里 就有 1 个基因，而真核生物基因密度低，比如人，要 10 万个碱基才有 1 个基因。与此对应， 原核生物编码区含量高，而真核生物低。此外，原核生物的基因是呈线性分布的，而真核生 物的基因是非线性的，因为翻译蛋白质的外显子被内含子分隔开来。也就是真核生物的 mRNA 要经历剪切的过程，剪切后的成熟 mRNA 才能进行翻译。这是原核生物和真核生物 基因的最大区别，即，原核生物没有内含子，真核生物有内含子。这个巨大的区别，将导致 两种基因在数据库中不同的存储及注释方式\n我们首先来看一条原核生物的 DNA 序列，它是编码大肠杆菌 dUTPase 的基因，在 Genbank 里的数据库编号是 X01714。从 NCBI 的主页（http://www.ncbi.nlm.nih.gov/）选择 Genbank 数据库。Nucleotide 数据库就是 Genbank 数据库，然后在搜索条中直接写入这条序 列对应的数据库编号 X01714，点击\u0026quot;搜索\u0026quot;。结果返回编号为 X01714 的序列在 Genbank 中 详细记录。从这条记录的标题我们得知，dUTPase 是脱氧尿苷焦磷酸酶，编码他的基因叫 dut 基因，所属物种是大肠杆菌。下面是关于这个基因的详细注释，我们逐条浏览一下：\nLOCUS 这一行里包括基因座的名字，核酸序列长度，分子的类别，拓扑类型，原核生 物的基因拓扑类型都是线性的，最后是更新日期\nDEFINITION 是这条序列的简短定义，也就是前面看到的标题\nACCESSION 就是在搜索条中输入的那个数据库编号，也叫做检索号，每条记录的检索 号在数据库中是唯一且不变的。即使数据提交者改变了数据内容，Accession 也不会变。 你会发现，这条记录里，Accession 和 Locus 是一样的。这是因为这个基因在录入数据 库之前并没有起名字，因此录入数据的时候便将检索号作为了基因的名字。但是有些基因， 在录入数据库之前已经有了自己的名字，那么这些基因所对应的 Accession 和 Locus 就 不一样了。你可以这样理解，Locus 是一个同学的真实姓名，而 Accession 是这个同学 的学号。同一个人在不同的学校里会有不同的学号，而名字只有一个。基因也是一样，同一 个基因在不同的数据库中会有不同的检索号，而基因的名字只有一个\nVersion 版本号和 Locus，Accession 长得差不多。版本号的格式是\u0026quot;检索号点上 一个数字\u0026quot;。版本号于 1999 年 2 月由三大数据库采纳使用。主要用于识别数据库中一条单一 的特定核苷酸序列。在数据库中，如果某条序列发生了改变，即使是单碱基的改变，它的版 本号都将增加，而它的 Accession 也就是检索号保持不变。比如，版本号由 U12345.1 变 为 U12345.2，而检索号依然是 U12345。版本号后面还有个 GI 号。GI 号与前面的版本号系 统是平行运行的。当一条序列改变后，它将被赋予一个新的 GI 号，同时它的版本号将增加\nKEYWORDS 提供能够大致描述该条目的几个关键词，可用于数据库搜索。\nSOURCE，基因序列所属物种的俗名。他下面还有一个子条目，ORGANISM，是对所属 物种更详细的定义，包括他的科学分类\nREFERENCE 是基因序列来源的科学文献。有时一条基因序列的不同片段可能来源于不 同的文献，那样的话，就会有很多个 REFERENCE 条目出现。REFERENCE 的子条目包括文 献的作者、题目和刊物。刊物下面还包括 PubMed ID 作为其子条目\nCOMMENT 是自由撰写的内容，比如致谢，或者是无法归入前面几项的内容\nFEATURES 是非常重要的注释内容，它描述了核酸序列中各个已确定的片段区域，包含 很多子条目，比如来源，启动子，核糖体结合位点等等\npromoter 列出了启动子的位置。细菌有两个启动子区，-35 区和-10 区。-35 区位于第 286 个碱基到第 291 个碱基 ，-10 区位于第 310 个碱基到第 316 个碱基\nmisc_feature 列出了一些杂项，比如，这条说明了从第 322 个碱基到第 324 个碱基 是一个推测的，但无实验证实的转录起始位置\nRBS 是核糖体结合位点的位置\nCDS，Coding Segment，编码区。对于原核生物来讲，CDS 记录了一个开放阅读框，从 第 343 个碱基开始的起始密码子 ATG 到第 798 个碱基结束的结束密码子 TAA。除了位置信 息，还包括翻译产物的诸多信息。翻译产物蛋白的名字是 dUTPase，这个编码区编码该蛋白 的第 1 到第 151 个氨基酸。翻译的起始位置和翻译所使用的密码本，以及计算机使用翻译密 码本根据核酸序列翻译出的蛋白质序列。需要强调的是，这不是生物自然翻译的，而是计算 机翻译的。事实上，蛋白质数据库中的大多数蛋白质序列都是根据核酸序列由计算机根据翻译密码本自动翻译出来的。中间部分是翻译出的蛋白在各种蛋白质数据库中对应的检索号。 通过这些检索号可以轻松的链接到其他数据库 （此外，X01714 这条核酸序列还包含第二个\u0026quot;潜在的\u0026quot;基因，也就是计算机预测出来的 基因。它编码的蛋白目前的数据库里没有详细记录，是个未知的蛋白。像这样，一条核酸序 列包含多个基因的情况在 Genbank 里是很常见的）\nORIGIN 作为最后一个条目记录的是核酸序列，并以双斜线作为整条记录的结束符。至 此整条记录就浏览完了\n有时你可能会想要保存这条序列，但是直接从这里拷贝，序列里既有空格，又有数字， 不是纯序列，手动删除这些又很麻烦。这时，你可以在这条记录的标题下面找到一个叫做 FASTA 的链接。点击他，你会获得 FASTA 格式的核酸序列。FASTA 格式是最常用的序列书 写格式，他由两部分组成，第一部分就是第一行，以大于号开始。大于号后面接序列的名称 或注释。第二部分就是第二行以后的纯序列部分，这部分只能写序列，不能有其他内容，比 如空格，注释，行号之类的都不能在序列部分出现。早期的 FASTA 格式要求序列部分每行 60 个字母。但这个规定早已被打破，每行 80，或每行 100，都可以\n标题下方，除了 FASTA 链接，还有一个图形化链接，点击可以看到 Features 里的注 释信息以图形的形式更直观的展示出来。可以看到这条序列包含的两个基因，他们的启动子 的位置，核糖体结合位点的位置等。其中一条基因是编码 dUTPase 的 dut 基因，另一个是编 码未知蛋白的潜在的通过计算预测出的基因\n如果想要保存这条记录，最好的方法是像保存 PubMed 文献列表那样，点击发送链接， 然后选择以纯文本文件的形式保存整条记录到本地电脑上\n一级核酸数据库：GenBank真核生物核酸序列 真核生物的基因与原核生物不同，是非线性排列的，也 就是基因里有外显子和内含子。因此真核生物核酸序列的数据库记录要要比原核生物复杂。 有时需要几条记录拼凑在一起才能描述出一个完整的基因。我们先来看看编码人 dUTPase 的成熟 mRNA 序列。成熟 mRNA 是已经剪切掉内含子，只剩外显子的序列，所以这条成熟 mRNA 序列和我们之前看到的原核生物的 DNA 序列从拓扑结构上看是几乎一样的，都是线 性的。输入这条成熟 mRNA 序列的检索号 U90223，搜索\n打开数据库记录，基本的注释内容和原核生物的差不多，这里只挑两点特别的地方说一 下。大家看到 KEYWORDS 后面只有一个点。这个点提示我们，数据库并不是完美的，所有 数据库都存在数据不完整的问题。再有，JOURNAL 后面我们看到是写的是未正式发表。但 事实上，这篇文章早在 1997 年就已经发表在 JBC 上了。因此，忠言逆耳：别指望 Genbank 或任何一个数据库能够百分百做到数据无误且实时更新\nFeatures 里的注释内容与原核生物的数据库记录相似，CDS 指出了从 63 到 821 是一 段编码区，在这段编码区里基因是连续的，因为是经过剪切后的成熟 mRNA，它将被翻译 成线粒体型 dUTPase 蛋白。下面/translation 里给出的是计算机翻译出的该蛋白的序列\n在 Features 里还有两个新的条目之前没有见到过。sig_peptide 和 mat_peptide。 sig_peptide，也就是 signal peptide，指出了编码信号肽的碱基的位置。信号肽决定了蛋 白质的亚细胞定位，也就是蛋白质工作的地方。mat_peptide，也就是 mature peptide，指 出了编码成熟肽链的碱基的位置。他从信号肽后面开始，到编码区结尾提前三个碱基结束。 编码区一直到第 821 号碱基，而编码成熟蛋白的最后一个碱基是第 818 号碱基，这中间差了 3 个碱基，那最后的这三个碱基干嘛去了呢？编码区的最后三个碱基是终止密码子，不翻译。 这条真核生物序列的 Genbank 注释看起来和原核生物的差不多，这是因为我们很小心的挑 了一条成熟 mRNA 的序列\n基因组里的 DNA 序列，是非线性分布的基因序列。我们仍然浏览编码人的 dUTPase 的 dut 基因序列。输入检索号 AF018430，搜索\n这个检索号下的序列标题是\u0026quot;人 dut 基因的第三号外显子\u0026quot;。人的 dut 基因肯定包含多 个外显子，而当前的这条 DNA 序列里只包含了一个外显子。其他的外显子在别的数据库记 录里。从 SEGMENT 处可以看到，人的 dut 基因序列被分成了 4 个片段，并且分别存储在 4 条数据库记录中。也就是说，只有把四个片段全部凑在一起，才能拼凑出完整的基因。当前 这条数据库记录是所有四个片段里的第二个。这个片段里只包含一个外显子，是第三号外显 子。需要注意的是，一个片段可以只包含一个外显子，也可以包含不止一个外显子。另外， 这个例子告诉我们，LOCUS 和 ACCESSION 是可以不相同的\n从 FEATURES 里可以找到这个序列片段在染色体上的具体位置。是在 15 号染色体的长 臂上，位置在 15 到 21.1 条带之间\nGENE 这部分指出了拼出完整基因所需的所有四个片段的检索号，以及具体的位置。也 就是 AF018429 这条序列的 1 到 1735 号碱基，连上当前这条序列的 1 到 1177 号碱基，连上 AF018431 这条序列的 1 到 45 号碱基，连上 AF018432 这条序列的 658 到 732 号碱基和 884 到 954 号碱基，以及 1391 到 1447 号碱基。后面给出了基因的名字，dut\nmRNA 给出了拼凑出上面基因的各个片段中外显子的位置。也就是说，GENE 里的片段 拼在一起是完整的基因，mRNA 里的片段拼在一起就等于完成了剪切的过程，相当于成熟 mRNA。值得注意的是，剪接后形成的 mRNA 有两种，其中一种比另一种在前端多了一个 外显子。多的这一段将被翻译成定位线粒体的信号肽，从而翻译出线粒体型的蛋白质。而另 一种没有信号肽的将形成细胞核型蛋白质\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.9825699912510937in\u0026rdquo;}\n上表清晰的列出了四个片段中所有外显子的位置。能够清楚的看到，线粒体型的比细胞 核型的多了一个翻译信号肽的外显子，其他的翻译成熟肽段的外显子都是一样的。虽然信号 肽最终也会被切掉，但是由此产生了两种亚细胞定位的蛋白质。有信号肽的会到线粒体中去， 没有的将留在细胞核里\n最后 exon 条目明确的告诉我们，当前这条序列里 560 到 651 号碱基是 dut 基因的第三 个外显子。至此，大家应该看得出来，解读真核生物的 DNA 序列要比原核生物复杂得多。 但是，只要你熟知基因的结构和 Genbank 的存储方式，这本天书不难看懂\n基因组数据库 Ensemble 在查看人的整个基因组之前，需要先搞清楚几件事。人的基因组有 33 亿个碱基分布在 23 个染色体上。我们现在已经获得了人的全基因组序列。起初拿到手的就是 33 亿个字母， 下一步面临的巨大挑战就是给它们添加注释，也就是做一个详细的 FEATURES 表。全世界 每时每刻关于人类基因及其功能都有新的发现。研究基因的方法五花八门，层出不穷，不可 能全部学会，只能是用到哪学到哪\n我们从 Ensembl 数据库（http://www.ensembl.org）查看人的基因组。Ensembl 是由欧洲 生物信息学研究所 EBI 和英国桑格研究院合作开发的。它收入了各种动物的基因组，特别 是那些离我们人类近的脊椎动物的基因组。这些基因组的注释都是通过配套开发的软件自动 添加的。Ensembl 主页左下角有人，老鼠，斑马鱼这三个点击率最高的基因组的快速链接。 其中，人的基因组有两个。右边是 2009 年获得的基因组信息，左边是 2013 年重新测序获得 的基因组信息。我们看右边这个最新的\n点击进入之后，我们点这个查看染色体，就可以看到人的所有染色体的图例。不知到大 家还记不记得，之前看到的某些信息似乎和 15 号染色体有点儿什么关系！没错，前面一直 研究的那个编码 dUPTase 的 dut 基因就在 15 号染色体上。点一下 15 号染色体，在弹出窗口 中选择染色体概要（chromosome summary）。这时我们会得到 15 号染色体的一个一览图。里 面包括编码蛋白的基因、非编码基因、假基因分别在染色体上不同区段内的含量，以及 GC 百分比（红线），和卫星 DNA 百分比（黑线）。染色体统计表给出了 15 号染色体的长度， 以及各种类型的基因的个数\n从 Genbank 我们了解到，dut 基因的第三号外显子位于 15 号染色体的长臂条带 21.1 附 近。所以我们进一步进入这个条带看一下。点击条带 21.1，选择区间链接。这时，这个区间 内所有的基因就都被显示在一张图上。如果眼力好的话，可以从这个图谱上直接找到 dut 基 因，并以他为中心放大。如果找不到，也可以通过搜索条输入基因的名字进行查找\n在以 dut 基因为中心显示的放大图谱中，点击 dut 或者对应的区域，在弹出的概况窗口 中选择 Ensemble 数据库的检索号。之后就会出现dut基因在 Ensemble 数据库中的详细记录\n微生物宏基因组数据库JCVI http://www.jcvi.org/\n微生物宏基因组数据库是非常有用的一级核酸数据库资源。说到微生物宏基因组学，不 得不介绍的是美国基因组研究所 TIGR 和克莱格反特研究所 JCVI。美国基因组研究所致力 于微生物基因组的研究，也有部分植物基因组项目。它是克莱格·凡特研究所的一部分。自 1995 年成立之初的两个基因组，至今已拥有超过 700 个基因组，而且还将更多。TIGR 是 NCBI 基因组资源的有力补充，因为它不仅拥有已完成测序的基因组，还有那些测序中的基 因组信息。在植物基因组项目中可以找到拟南芥、玉米、苜蓿和柳树的基因组信息。在微生 物与环境基因组目中，特别值得关注的是\u0026quot;人类微生物组计划\u0026quot;，HMP\nHMP 由美国 NIH 发起，由 4 个四个测序中心共同完成，其中一个就是克莱格凡特学院。 \u0026ldquo;人类微生物组计划\u0026quot;堪比\u0026quot;人类基因组计划\u0026rdquo;。我们目前认知的微生物不到 1%，生活在我 们肠道中的微生物细胞，是人体细胞的 10 倍。这些微生物基因组之和是人类基因组的 100 倍。微生物影响并超越我们的生老病死，有一天人死了，但身体中的微生物却还活着。除了 近年来少量的有关糖尿病等与肠道微生物的研究外，我们完全不清楚肠道微生物，呼吸道微 生物，还有体表微生物等在人体内做了什么，他们的喜怒哀乐与我们的生老病死有什么关系。 所以世界上诸多科学家都呼吁完成微生物组的研究计划。HMP 就是其中之一。目前，HMP 主要包括了人类鼻腔、口腔、皮肤、胃肠道和泌尿生殖道的宏基因组样本数据和分析流程\n从 JCVI 主页（http://www.jcvi.org/）的统计表中我们可以看到不同器官中有多少微生物 基因组已被测序并被注释。点击下方的统计链接。可以得到 HMP 中已研究的所有微生物基 因组。这些微生物在人体中存在的位置，测序及注释是已完成还是在进行中。已完成的基因 组后面会有三个链接\nWGS 是全基因组鸟枪法测序项目数据库记录的链接\nSRA 是高通量测序数据库记录的链接。这两个链接里记录的是测序的信息。相比之下对 大家更为有意义的是\nANNOTATION 链接里的内容，他列出了某个基因组在 Genbank 中所有注释的链接。 比如微生物 Acinetobacter radioresistens SK82 的基因组共分成 82 条序列记录在 Genbank 数据 库中。通过前面章节的学习，解读这些序列并不困难\n二级核酸数据库 二级核酸数据库包括的内容非常多。其中 NCBI 下属的三个数据库经常会用到。他们是 RefSeq 数据库，dbEST 数据库和 Gene 数据库。RefSeq 数据库，也叫参考序列数据库，是通 过自动及人工精选出的非冗余数据库，包括基因组序列、转录序列和蛋白质序列。凡是叫 ref 什么的数据库都是非冗余数据库，就是已经帮你把重复的内容去除掉了。dbEST 数据库， 也就是表达序列标签数据库，存储的是不同物种的表达序列标签。Gene 数据库以基因为记 录对象为用户提供基因序列注释和检索服务，收录了来自 5300 多个物种的 430 万条基因记录\n此外，非编码 RNA 数据库，提供非编码 RNA 的序列和功能信息。非编码 RNA 不编码 蛋白质但在细胞中起调节作用。目前该数据库包含来源于 99 种细菌，古细菌和真核生物的 3 万多条序列。microRNA 数据库主要存放已发表的 microRNA 序列和注释。这个数据库可 以分析 microRNA 在基因组中的定位和挖掘 microRNA 序列间的关系\n{width=\u0026ldquo;5.430555555555555in\u0026rdquo; height=\u0026ldquo;2.2916666666666665in\u0026rdquo;}\n一级蛋白质序列数据库：蛋白质序列数据库UniProtKB {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.90625in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.049759405074366in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.521139545056868in\u0026rdquo;}\ninteraction {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.125106080489939in\u0026rdquo;}\nstructure {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.9633880139982502in\u0026rdquo;}\nFamily \u0026amp; Domains {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.799250874890639in\u0026rdquo;}\nSeq {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.96869750656168in\u0026rdquo;}\nCross-references {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.9376826334208226in\u0026rdquo;}\nPublication {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.113636264216973in\u0026rdquo;}\nEntry information {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.8024682852143483in\u0026rdquo;}\nMiscellaneous {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.044243219597551in\u0026rdquo;}\nSimillar proteins {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.033374890638671in\u0026rdquo;}\nText纯文本数据库记录 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.949798775153106in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.309232283464567in\u0026rdquo;}\n一级蛋白质数据库 蛋白质结构数据库PDB 蛋白质的基础结构概念 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4897779965004374in\u0026rdquo;}\nPDB库的概况 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.485168416447944in\u0026rdquo;}\n在PubMed上查找的资料\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.354765966754156in\u0026rdquo;}\nPDB库的使用 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.182671697287839in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.186359361329834in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.13661854768154in\u0026rdquo;}\nPDB文件的介绍 HEADER : 分子类别 ， 日期， PDB ID\nTITLE : 结构的标题，一般就是相关文献的标题\nCOMPD： 对各个分子的描述\n\u0026lt;!-- --\u0026gt; 如这里是由三条链构成的三聚体结构 \u0026lt;!-- --\u0026gt; SOURCE : 结构中包括的每一个分子的实验来源 （生物学 / 化学）\nKEYWDS： 用于数据库搜索的关键词\nEXPDTA: 测定结构所采用的实验方法\n\u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1316940069991253in\u0026rdquo;} \u0026lt;!-- --\u0026gt; AUTHOR: 结构测定者\nREVDAT: 历史上曾对该数据库记录进行过的修改\nJRNL：发表这个结构的文献\nREMARK: 无法归入其他部分的注释\nDBREF: 该蛋白质在蛋白质序列数据库看看里的检索号等信息\nSEQRES: 氨基酸序列\nMODRES:对标准残基上的修饰\n\u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.598545494313211in\u0026rdquo;} \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.450038276465442in\u0026rdquo;} \u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; LINK: 残基间的化学键，比如肽键，氢键，二硫键 \u0026lt;!-- --\u0026gt; 如 107号上的N 与 106号上的C 脱去一分子的水形成1.32A \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5027252843394576in\u0026rdquo;} \u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; 3D坐标部分 \u0026lt;!-- --\u0026gt; 每一行是一个原子，如这是A链上的20号氨基酸PRO上的N原子，\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.6044969378827645in\u0026rdquo;}\n\u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;6.395739282589676in\u0026rdquo;} \u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.050152012248469in\u0026rdquo;} \u0026lt;!-- --\u0026gt; PDB在线3D查看 JSMOL {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.514746281714786in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3333333333333335in\u0026rdquo;}\n二级蛋白质数据库 Pfam简述 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3306846019247596in\u0026rdquo;}\npfam上的SEARCH可以帮我们查找某条序列上有哪些结构域\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.252445319335083in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.5911154855643046in\u0026rdquo;}\nSummary可以获得这个结构域的功能注释以及结构信息\nDomin organisation 可以看出在当前有多少蛋白质拥有TIR结构域以及结构域之间的关系Sturctures : 会列出当前TIR结构域的蛋白质结构以及在UNIport中的链接\nCATH {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.225036089238845in\u0026rdquo;}\nCATH分类代码\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.581871172353456in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.195364173228347in\u0026rdquo;}\nCATH的使用 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.249998906386701in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.985738188976378in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.630951443569554in\u0026rdquo;}\n3H6X是三条链的\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.715883639545057in\u0026rdquo;}\n另外一个有趣的图\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.42660542432196in\u0026rdquo;}\nSCOP2 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.298932633420822in\u0026rdquo;}\nSCOP2的使用 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.632825896762904in\u0026rdquo;}\n四层分类\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.670062335958005in\u0026rdquo;}\n专用数据库 KEGG 首先看个图（生物代谢图）\n每一个点就是一个化合物，每一条线就是一个生化反应\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5799781277340332in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;0.7091765091863517in\u0026rdquo;}\n正式开始KEGG的介绍\n以代谢通路来讲解\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.208489720034995in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.919989063867017in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.53876968503937in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.893409886264217in\u0026rdquo;}\n放大看看\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.316514654418198in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.3274660979877515in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.547490157480315in\u0026rdquo;}\n接下来看下Organismal Systems\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.6585640857392825in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.440069991251094in\u0026rdquo;}\n这是人的各种Toll样受体信号传导图\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.378748906386702in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.241024715660543in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.016052055993001in\u0026rdquo;}\nOMIM 人类遗传病数据库 简介\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.003987314085739in\u0026rdquo;}\n例子\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.1526695100612425in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.201524496937883in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.979549431321085in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.9959448818897636in\u0026rdquo;}\n可以查看这个基因的全部信息\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.5044149168853895in\u0026rdquo;}\n序列比较 基础概念 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3103608923884513in\u0026rdquo;}\n序列相似性 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.227069116360455in\u0026rdquo;}\n相似的序列说明可能来自同一祖先而且可能具有相似的结构和功能\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3227121609798775in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3592727471566053in\u0026rdquo;}\n例子的一致度为：50%；\n替换计分矩阵 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4233748906386703in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.468004155730534in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2975951443569556in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.382060367454068in\u0026rdquo;}\nPAM后面的数体现的是序列差异度，而BLOSUM后面的数字体现的是相似性\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.6440846456692912in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0868766404199475in\u0026rdquo;}\n其他两种蛋白质序列比对的替换计分矩阵\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.353906386701662in\u0026rdquo;}\n那现在来解决下前面遗留的相似度问题：\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4432042869641295in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.406619641294838in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5194849081364827in\u0026rdquo;}\n（2+1）:代表两对相同的，一对相似的\n那么问题来了，两个序列的长度不相同怎么办呢？\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5037270341207347in\u0026rdquo;}\n先学习下两个序列的比较方法\n序列两两比较的方法 打点法 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.021797900262467in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.142396106736658in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.716813210848644in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3695647419072614in\u0026rdquo;}\n打点法在线软件 Dotlet [http://myhits.isb-sib.ch/cgi-bin/dotlet]\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3341786964129483in\u0026rdquo;}\nInput中复制序列进去\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0793339895013125in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.550419947506562in\u0026rdquo;}\n如果选其他的， 比如选择 15，那就是一次比较 15 个字母，也就是看 15 个字母长度的序列整体的相似度如 何来确定打不打点。注意这里不是比较完前 15 个字母，然后再从第 16 个字母开始比较后面 的 15 个字母，而是第 1 次比较第 1 到第 15 个字母，然后再比较第 2 到第 16 个字母，再是 第 3 到第 17 个字母，依次类推；\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3186100174978126in\u0026rdquo;}\n注意默认的颜色方案是在越相 似的地方打的点的颜色越浅，越不相似的地方颜色越深；\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.076650262467192in\u0026rdquo;}\n这里我们可以通过调整灰度条，来屏蔽大多数低分值的点，让他们统统变成黑色背景， 并且强化高分值的点，让他们以纯白色突出显示出来\n接下来打点两条独立的序列\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.522202537182852in\u0026rdquo;}\n这时我们看到除去主对角线外，还有很多条对角线（图 4）。说明序列中存在串联重复 序列。前面我们讲过，半个矩阵范围内，数数包括主对角线在内，有多少条等距平行线，就 说明重复了多少次，最短的平行线就是一个重复单元\n接下来使用串联重复序列\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.3725973315835525in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5668208661417324in\u0026rdquo;}\n序列比对法 但是用打点法只能让你大致了解两条序列是否相似，无 法定量的描述。如果想要精确地知道两条序列到底有多相似，就需要使用序列比对法 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.8823195538057744in\u0026rdquo;}\n就是通过插入空位，让上下两 行中尽可能多的一致的和相似的字符对在一起。这不是随便摆摆看看就能完成的，需要使用 专门的序列比对算法\n双序列全局比对以及算法 有替换积分矩阵以确定不同字母间的相似度得分，以及空位罚分,空位 罚分就是当字母对空位的时候应该得几分。我们还是希望一致或相似的字母尽可能的对在一 起，字母对空位的情况和不相似的字母对在一起的情况一样，都不是我们希望的，还是少出 现为好，所以通常字母对空位会得到一个负分，这个负分就叫做空位罚分。这里我们让空位 罚分，也就是 gap 分值为-5 分\n不过要注意，得分矩阵 p 和 q 的前面各留一个空列和一个空行，也就是第 0 列和第 0 行\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.985601487314086in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.919752843394576in\u0026rdquo;}\n我们再回过头来看一下第一行和第一列 （图 6）。其实，第一行的每一个值都是从左边的格加 gap 来的。所以我们给它们补上向左 的箭头。第一列的每一个值都是从上边的格加 gap 来的。所以我们给它们补上向上的箭头。 至此，所有的箭头和数值就都填好了\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.7441163604549432in\u0026rdquo;}\n追溯箭头是从右下角到左上角，但 是写全局比对是从左上角开始，如果是斜箭头则是字符对字符，如果是水平箭头或垂直箭头 则是字符对空位，箭头指着的序列为空位。我们看第一个是斜箭头，字母对字母，就是 A 对 A，第二个是水平箭头，字母对空位，箭头指着的序列是空位，也就是 C 对空位。然后 斜箭头 G 对 A，斜箭头 T 对 T，斜箭头 C 对 C，一直写到右下角，全局比对就出现了\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.999339457567804in\u0026rdquo;}\n双序列局部比对 得分越高，越相似，这个例子告诉我 们，对于像这样一长一短的两条序列，比较局部比比较全长更有意义。这就是为什么除了全 局比对，还有局部比对\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.294534120734908in\u0026rdquo;}\n局部比对的计算公式在全局比对的基础上增加了第四个元素\u0026quot;0\u0026quot;\n从 s(1,1)开始要选择四个值中的最大值。除了上面格 s(0,1)+gap=0+-5=-5，左边 格 s(1,0)+gap=0+-5=-5，斜上格 s(0,0)+w(1,1)=0+-3=-3，还有一个 0。max(-5, -5,-3,0)=0。并且这个 0 既不是从上面格，也不是从左边格，以及斜上格三个方向来的， 而是来自于公式里增加的\u0026quot;0\u0026quot;，所以不用画箭头\n与全局比对不同，局部比对的得分不是在右 下角，而是在整个矩阵中找最大值。这个最大值才是局部比对的最终得分，他可能出现在任，何一个位置。这次箭头追溯也不是从右下角到左上角，而是从刚刚找到的最大值开始追溯到 没有箭头为止。追溯箭头终止的位置也可以是得分矩阵中的任何一个位置。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4164982502187224in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.602577646544182in\u0026rdquo;}\n一致度和相似性 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.381994750656168in\u0026rdquo;}\n在线双序列比对工具 EMBL http://www.ebi.ac.uk/Tools/psa 首先看下全局alignment，输入值非常简单，把要比较的两条 蛋白质序列贴在输入框里或者上传。可以使用示例文件 global.fasta 里面的两条序列。如果想 要进一步设置比对的参数，可以点 More options。从这里可以选择使用哪种替换记分矩阵。 按照之前讲过的原则，选择 PAM 矩阵或 BLOSUM 矩阵。如果实在不知道选哪个矩阵，就 闭着眼睛选 BLOSUME62 吧！下拉菜单里默认选的就是 BLOSUM62。除了选择替换记分矩 阵，这里还可以设置空位罚分（gap open），也就是 gap 的分值。这里实际上是让你选空位对字母的情况 罚几分，所以显示的是正数，但在计算的过程中还是按照负数处理\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5439063867016625in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5545942694663166in\u0026rdquo;}\n结果：\n在结果页面里，上部是比 对使用的参数，以及得出的序列比对的长度、一致度、相似度、空位比例和比对得分。下部 就是序列比对。在序列比对里，左边是两条序列的名字，因为输入的是 FASTA 格式的序列， 所以程序自动识别出了序列的名字。右边的序列比对分 3 行。上下两行是序列，里面插入了 许多的空位。中间这行标记出了哪些位置上下两个字母是完全相同的，用竖线表示。上下两 个字母相似，用双点表示。上下不相似，用单点表示。字母对空位的情况，用空格表示。这 样，我们只要数数比对结果里竖线的个数（40 个），再除以比对的长度（196 个），就可计算 出一致度。再用竖线的个数加上双点的个数（40+29=69 个），除以比对长度（196 个），就 是相似度。整个比对里一共插入了 65 个空位，占整个比对长度的 33%。序列两边的数是这 一行中的字母在序列中的位置数，而不是这一行的长度。比如第二行是 seq1 的第 49 个字母 到第 97 个字母，是 seq2 的第 27 个字母到第 75 个字母\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.9740212160979875in\u0026rdquo;}\ngap EMBL 比对工具将 gap 分为两种，一种叫\u0026quot;gap 开头（GAP OPEN）\u0026quot;，另一种叫\u0026quot;gap 延长（GAP EXTEND）\u0026quot;（图 1）。gap 开头就是连续的一串 gap 里面打头的那一个，可以当 它是队长。gap 延长就是剩下的那些 gap，也就是队长后面跟着的小兵\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.141512467191601in\u0026rdquo;}\n这一串里，第一个 gap 是 gap 开头，后面的都是 gap 延长。单独的一个 gap 按 gap 开头算。gap 开头和 gap 延长 可以分别定义不同的罚分。默认情况下，gap 开头罚分多，gap 延长罚分少。全局比对的例 子里我们就是用这种搭配组合方案做出的比对。这次我们反过来试试，让 gap 开头罚分少， 让 gap 延长罚分多。比如 gap 开头选罚 1 分，gap 延长选罚 5 分，其他参数不变，再作一次 看看结果发生了什么变化\n当 gap 开头小，gap 延长大的时候，做出来的比对里面，gap 很分散，极少有连续长串 的 gap 出现（图 2-A）。开头的一串 gap 是个例外，因为 seq2 太短， seq1 的这一段只能跟 gap 相对。其他部分的 gap 都是分散出现的。这和我们第一次做出来的比对结果是截然不同的（图 2-B）。在第一次做的结果里，也就是 gap 开头大，gap 延长小的时候，gap 很集中，有很多成 长串出现的 gap。大家可以想想其中的奥妙。当 gap 开头大，gap 延长小的时候，说明在连 续的字母里插入一个 gap 打开一个缺口要付出很大的代价，因为 gap 开头罚分大。但是这个 缺口一旦打开了，也就是一旦有了第一个 gap，后面再接更多的 gap 就容易了，因为 gap 延 长罚分小。所以这种情况下，gap 都集中连成长串出现\n而反过来，当 gap 开头小，gap 延长大的时候，说明在连续的字母里插入一个 gap 打开 一个缺口很容易，并不需要付出太大代价，因为 gap 开头罚分小。但是想在第一个 gap 后面 再接一个 gap 就难了，因为 gap 延长罚分大。所以这种情况下很难有长串的 gap 出现，gap 每延长一个都要付出巨大代价。因此在第二次我们做的结果里都是分散的 gap。 除了开头一段因两条序列长短不同而不得已出现的长串 gap 外，没有其他的长串 gap 了\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.8938046806649167in\u0026rdquo;}\n这就是说，通过调整 gap 开头和 gap 延长，我们可以把序列比对做成我们期待的样子。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1637390638670166in\u0026rdquo;}\n举两个例子，看看应该怎样调整 gap 最合理:\n第一个例子，你知道要比对的两条序列很相似， 是同源序列，所以它们的结构和功能也应该都差不多。其中一条序列的结构已知，另一条未 知。你想把它们很好的比对在一起，用其中已知结构的序列做模板，来预测另一个序列的结 构。这时候我们期待得到的是 gap 分散的比对结果还是 gap 集中的呢？另一例子，你知道要 比对的两条序列绝大部分区域都很相似，但是其中一条序列的一个功能区在另一条序列中是 缺失的。你想要通过序列比对把这个功能区找出来。这时候我们要怎么设置 gap 开头和 gap 延长呢？这两个例子告诉我们，在实际应用中，需要根据不同的情况选取不同的 gap 罚分， 以满足不同的生物学意义。如果你对结果没有什么预期，那就请保持默认的参数。\n除此之外，结尾的 gap 也可以划分出不同的种类并赋予不同的罚分，如果把 END GAP PENALTY 选成 true，就可以设置结尾的 gap 罚分了。结尾 gap 不太常用，特别是在做亲缘 关系较近的序列比对时，是否设置结尾 gap，比对结果差别不大。\nEMBL 局部双序列比对 EMBL 的局部双序列比对工具可以选择经典的 Smith-Waterman 算法\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4712434383202098in\u0026rdquo;}\n从比对结果可以看出，只有中间黑色的相似的部分出现在比对结果中了，两头 红色的不相似的部分被忽略掉了。也就是只返回了局部最相似，得分最高的片段的比对结果。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.6701870078740155in\u0026rdquo;}\n用这两条序列再做一次全局比对，从两次的比对结果可以更清楚的看出，全局 比对里前面和后面对得不好的部分在局部比对里就都被忽略了\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4096686351706036in\u0026rdquo;}\n除了一长一短两条序列适合做局部比对，有的时候两条差不多长的序列也可以做局部比 对，以找出它们最相似的局部片段。为了让相似的部分突出出来，我们把 gap 都调大，gap 开头调到 10，gap 延长调到 5，提交。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.272180664916885in\u0026rdquo;}\n比对结果中，只有黑色的相似的部分出现在最终的比对结果中了，两头红色的不相似的 部分全部被忽略了\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.649401793525809in\u0026rdquo;}\n如果给这两条序列做全局比对的话，会发现，绝大部分位置对得都很差，只有中间这一 段对的还不错，所以，有时候两条序列并不同源，它们只是有一个功能相似的区域， 这时用局部比对我们就能很快找到这一区域在两条序列中的位置。但是如果做全局比对的话， 结果就不如局部比对明显了。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.247603893263342in\u0026rdquo;}\n其他在线序列比对工具 可以做双序列比对的工具很多（表 1）。不同网站都有自己的比对工具，所使用的算法 也不尽相同，但是它们的核心算法都是讲过的 Nidelmann-Wunsch 和 Smith-Waterman 算法\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.09249343832021in\u0026rdquo;}\nBLAST http://www.ncbi.nlm.nih.gov/\n简介 们已经学会如何做双序列比对，那可不可以拿一条序列和数据库中的每条序列逐一进 行双序列比对，通过这种方法来找相似呢？这确实是一个办法。这样我们只要根据比对后得 出的相似度排序，就可以找到最相似的那条序列了。但是，这种方法因计算耗时过长，只是 理论上可行而已。之前我们用 EMBL 的双序列比对工具做全局比对，虽然很快就出结果了， 但至少也要经历一两秒钟的时间。而数据库中有几百万条序列，全部比对一遍，耗时太长。 因此，我们需要快速的数据库相似性搜索工具。目前世界上广泛使用的就是 BLAST。它可 以在尽可能准确的前提下，快速的从数据库中找到跟某一条序列相似的序列\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3608442694663165in\u0026rdquo;}\n在实际应用 中，更多时候，输入值是一条序列。想要找到这条序列在数据库中的相关注释，是不能把这 条序列直接放到搜索条里搜索的。这个时候就得用 BLAST 搜索了！\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.55549321959755in\u0026rdquo;}\n。BLAST 从头至尾将两条序列扫描一遍并找出所有片段对，并在允许的阈值 范围内对片段对进行延伸，最终找出高分值片段对（high-scoring pairs, HSPs）\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4386646981627296in\u0026rdquo;}\nBLAST种类 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.441827427821522in\u0026rdquo;}\n为什么是 按 6 条链翻译？在无法得知翻译起始位点在情况下，翻译可能是从第一个碱基开始，三个三 个的往后翻译，也可能是从第 2 个碱基开始，也可能从第 3 个碱基开始。另外还有可能是从 这条链的互补链上开始，这样又有三个可能的开始位置，加起来一共会产生 6 条可能被翻译 出来的蛋白质序列。这 6 条中有些是真实存在的，有些是不存在，但是谁真谁假我们无从知 晓，所以 6 条序列都要到数据库中去搜索一下试试。接下来的问题是，既然是核酸序列，为 什么不做 BLASTn 直接到核酸数据库里去搜索，而是要到蛋白质数据库里搜索呢？我们说这 样做是有意义的，比如，从核酸序列数据库里找不到跟你手里这条核酸序列相似的序列，或 找到了相似的序列但这些找到的序列无法提供有意义的注释信息。这时，就可以去蛋白质数 据库试试，看看这条核酸序列的翻译产物能不能从蛋白质数据库里找到相似的序列以及有意 义的注释信息。或者说，你不是想找跟你这条核酸序列相似的核酸序列，而是想找跟你这条 核酸序列编码蛋白质相似的蛋白质序列，这时就要做 BLASTx\n反之，当你不是想找跟你手上这条蛋白质序列相似的蛋白质序列，而是想找跟编码这条 蛋白质序列的核酸序列相似的核酸序列的时候，就要做 tBLASTn\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.210229658792651in\u0026rdquo;}\nNCBI上的BLASTp NCBI 的 BLAST 工 具 为 例 尝 试 一 下 不 同 算 法 的 BLAST 工 具 （http://www.ncbi.nlm.nih.gov/）。BLAST 链接在 NCBI 主页右侧很显眼的地方。我们做 BLASTp（Protein BLAST），也就是用蛋白质序列搜索蛋白质序列数据库\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.8737740594925634in\u0026rdquo;}\n输入待搜索的蛋白质序列，这条序列可以在示例文 件 blast.fasta 里面找到。\n定搜索跟输入序列哪部分相似的序列，如果空着就是全长搜索。\n\u0026lt;!-- --\u0026gt; 如从50-100就是比较输入序列的第50位开始到100位相比 \u0026lt;!-- --\u0026gt; 给搜索任务起一个名字，如果输入的是 FASTA 格式的序列，那么在输入框里面点一下， 序列的名字就会被自动识别出来。\n如果在 Align two or more sequences 前面打勾的话，可以同时提交多个 BLAST 任务\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.282700131233596in\u0026rdquo;}\n被搜索的数据库（虽然是 NCBI 的 BLAST 工具，可以选择的数据库却不只 NCBI 下属的数据库，还包括其他组织的数据库，比如 PDB， Swissprot。事实上，各大数据库网站的 BLAST 工具都可以实现跨平台搜索）\nOrganism 可以把搜索范围限定在某一特定物种内，或者排除某一物种。\n在算法选择这一栏里，有之前提到的三种不同的 BLAST 算法，标准 BLAST，PSI-BLAST 和 PHI-BLAST。这一次我们先尝试标准 BLAST。所有参数设置完毕之 后，点 BLAST\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.210053587051618in\u0026rdquo;}\n在Graphic Summary结果里：\nBLAST 工具识别出输入序列的第 25 到第 170 个氨基酸这一段属于 TIR 蛋白质家族。这部分里彩色 线条构成的图告诉我们，一共从数据库中找到 50 个 hits，也就是高分匹配片段。注意这些 线代表的是 50 个高分匹配片段而不是 50 条序列。一个高分匹配片段有可能是一条全长的序 列，也就是全长匹配，也有可能只是某条序列的一部分，也就是局部匹配。代表这些高分匹 配片段的线拥有不同颜色和不同的长短。如果把鼠标放到某一条线上，可以看到这条匹配片 段的具体信息，包括他所在序列的数据库编号，序列的名字，匹配得分，期望值 E 值。匹 配得分在 200 以上的用红线表示，80 到 200 之间的用粉线，50 到 80 的绿线，40-50 的蓝线， 40 以下的黑线，所以颜色反映的是匹配的好坏程度。如果某一个高分匹配片段和输入序列 是从头到尾匹配，就是全长的线，比如最上面的三条红线。如果只匹配输入序列的一部分， 则是一条短线，短线所在的位置就是与输入序列匹配的位置\n{width=\u0026ldquo;5.777777777777778in\u0026rdquo; height=\u0026ldquo;9.208333333333334in\u0026rdquo;}\nDescriptions：\n是这 50 个高分匹配片段所在序列的详细信息列表。每条序列 都有一个匹配得分和覆盖度。这两项决定了第二部分彩图中每条线的颜色和长短。除了匹配 得分和覆盖度，表中还列出了其他指标。尤为重要的是 E-value。E-value 也叫做期望值或 E 值。E 值越接近零，说明输入序列与当前这条序列为同一条序列的可能性越大。\n第三部分的 表就是根据 E值由低到高排序的。随着 E 值增大，匹配得分是成反比逐渐降低的。但是一 致度与 E 值并非完全成反比。因为我们在前面讲 BLAST 核心思想的时候说过，BLAST 没有 做双序列比对，为了提高速度，它牺牲了一定的准确度。表中的一致度，是 BLAST 搜索完 成后，针对搜索到的这 50 条序列专门做双序列比对而得到的。BLAST 牺牲掉的准确度对高 度相似的序列，也就是亲缘关系近的序列构成不了威胁，不会把它们落掉，但是对于那些只 有一点点相似，也就是远源的序列，就有点麻烦了，它们很有可能被丢掉而没有被 BLAST 发现\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.635181539807524in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.333093832020998in\u0026rdquo;}\nNCBI上的PSI-BLAST 为了提高速度，标准 BLAST 牺牲了一定的准确度，牺牲掉的准确度对高度相似的序列， 也就是亲缘关系近的序列构成不了威胁，不会把它们落掉，但是对于那些只有一点点相似， 也就是远源的序列，就有点麻烦了。它们很可能被落掉而没有被 BLAST 发现\n要解决这个问题，可以用 PSI-BLAST。PSI-BLAST 可以搜罗出一个庞大的蛋白质家族， 当然也包括标准 BLAST 不小心漏掉的那些远房亲戚。换言之，标准 BLAST 找到了直接认识 的朋友，但朋友的朋友都丢掉了\n位置特异权重矩阵（Position-Specific Scoring Matrix，简称 PSSM）是 以矩阵的形式，统计一个多序列比对中，每个位置上不同残基出现的百分比。假设 A 的朋 友只有 B，B 朋友除 A 外还有 C。如果输入序列的第一个位置是 A，那么在第一轮没有 PSSM 辅助的情况下，只有第一个位置是 A 或 B 的序列被找到了。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.6025164041994753in\u0026rdquo;}\n用 NCBI 的 PSI-BLAST 工具再次搜索 blast.fasta 这条序列，仍然在 SwissProt 数据库 中搜索，算法选 PSI-BLAST\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.51123687664042in\u0026rdquo;}\n略有不同的是，在第三部分（Descriptions） 列表的最右侧多了两列。绿色标记的一列是将要为下一轮搜索创建 PSSM 所选择的序列，默 认第一轮搜索到的序列都将用来创建 PSSM。红色标记的一列是已在本轮搜索中用来创建 PSSM 的序列。因为是第一轮搜索，之前还没有搜索到任何序列，也就是第一轮的搜索过程；\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.435080927384077in\u0026rdquo;}\n接下来点\u0026quot;go\u0026quot;进行第二轮搜索。\u0026ldquo;go\u0026quot;左侧的输入 框里可以指定列出搜索到的前多少条序列。因为 PSI-BLAST 无休止的一直做下去，也许会 把数据库中的全部序列都找出来\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.514294619422572in\u0026rdquo;}\n红色标记的一列，也就是已在第二轮搜索中被用来创建 PSSM 的序列，大部分都打勾了。但是，再往后面会看到有些标黄的序列没有打勾。这些没 有打勾的标黄的序列就是在第二轮搜索中新找到的序列。它们将用于创建下一轮搜索使用的 PSSM，但是在本轮搜索中，它们没有被用到，所以没有打勾。而没有标黄的这些打勾的序 列，是在第一轮中就已经找到的序列。（好在通过 PSI-BLAST，它 们又被找回来了）\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.198655949256343in\u0026rdquo;}\nNCBI PHI-BLAST PSI-BLAST 是撒大网搜索，而 PHI-BLAST 则是精准搜索；其中，N 是天冬酰胺，P 是脯氨酸，S 是丝氨酸，T 是苏氨酸。{ }代表除大括号里的氨基酸 以外的任意氨基酸，[ ]代表中括号中的任意一个氨基酸；PHI-BLAST 可以根据给入的正则表达式对搜索到的相似序列 进行模式匹配，符合正则表达式的才会被作为结果输出。\n正则表达式{L}GExGASx(3,7)的意思是， 除 L 以外的任意一个字母，紧接 G，再紧接 E，再接一个任意字符，之后是 GAS 中的任意 一个，再接 LIVM 中的任意一个，最后再接 3 到 7 个任意字符；\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3166108923884514in\u0026rdquo;}\n在 NCBI BLAST 工具的输入页面，当算法选择了 PHI-BLAST 之后，会自动出现模式输入框，\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.4208245844269465in\u0026rdquo;}\n此外，PHI-BLAST 可以和PSI-BLAST 联合使用，以找到更多符合模式的远房亲戚们\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.177214566929134in\u0026rdquo;}\n三种BLAST的区别 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.716686351706037in\u0026rdquo;}\nNCBI的SmartBLAST {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4537674978127733in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.173571741032371in\u0026rdquo;}\n其他网站的BLAST的工具 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3123622047244092in\u0026rdquo;}\n多序列比对 前面我们讲的是双序列比对，为了看清楚每一列的保守情况和理化性 质，通常会给多序列比对根据不同的原则赋予丰富的色彩；\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4139774715660542in\u0026rdquo;}\n多序列比对的用途 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.031045494313211in\u0026rdquo;}\n多序列比对的算法 两条序列的比对需要构建一个二维表格，然后从 右下角到左上角找出一条最优路线。如果是做 3 条序列的比对，应该做一个三维立方体，从 (0,0,0)这个位置到(n,n,n)这个位置找到最优的贯穿路径。以此类推，如果是做 n 条序列的比对，就要创建一个 n 维空间。这个 n 维空间实在是难以想象，但是有一点是明确的， 就是到了 n 维我们已经没有办法再像二维那样精确的计算出比对结果了。\n由于计算量过于巨 大，所以目前所有的多序列比对工具都是不完美的。它们都使用一种近似的算法。目的就是 为了缩短计算时间，但也因此牺牲了一定的准确度。好在多序列比对并不像双序列比对对准 确度要求极高。通常，我们是要从多序列比对中看到一个趋势，一个大体的位置，所以牺牲 掉的这点儿准确度影响不大\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1168952318460192in\u0026rdquo;}\n多序列比对的注意事项 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.9399464129483817in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1097790901137357in\u0026rdquo;}\nEMBL - Clustal Omega 多序列比对工具 http://www.ebi.ac.uk/Tools/msa\n目前世界上最流行的多序列比对工具是 CLUSTAL 系列，TCOFFEE 和 MUSCLE。其中 CLUSTAL 系列使用率最高；TCOFFEE 最新，而且还有很多变形；MUSCLE 最快，而且胃 口大，能接受的序列数量是其他工具比不了的\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.523391294838145in\u0026rdquo;}\nEMBL 的多序 列比对工具很多，包括前面提到的 CLUSTAL 系列、TCOFFEE、MUSCLE。我们看 EMBL 这些比对工具中 CLUSTAL 系列的最新版本 Clustal Omega\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.364431321084864in\u0026rdquo;}\n在这个界面里可以点通过点击 More options，设置各种设置。在这个例子里，所有参数都使用默认值。参数里有输出格式（OUTPUT FORMAT）和输出顺序（ORDER）这两个参 数。\n输出格式里可以选择常用的多序列比对格式。我们选标准的 Clustal 格式。这是最常见 的多序列比对格式\n输出顺序参数可以设定多序列比对中各个序列的排列顺序。\u0026ldquo;aligned\u0026rdquo; 是按照比对过程中自动创建的计算顺序排列；\u0026ldquo;input\u0026quot;是按照输入序列的原始顺序排列。(输入序列是按照 TLR10、9、8、7\u0026hellip;这样的顺序排列的。输出顺序参数设定为 aligned，看看比 对结果里序列的排列顺序是否发生了变化)\nClustal 格式 的输出结果\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.499394138232721in\u0026rdquo;}\n可以看到，比对中序列的排列顺序跟输入的时候不一样了，这是按 照比对创建时的计算顺序排列的。请点击 Download Alignment File 保存将当前结果，以便后 面章节进一步加工。保存的文件后缀名是\u0026rdquo;.clustal\u0026rdquo;。它是一个纯文本文件，用写字板或者 记事本都可以打开\n如果想要添加色彩，点击\u0026quot;Show Colors\u0026quot;。之后，不同的氨基酸根据它们的理化性质 不同会显示出不同的颜色\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.559265091863517in\u0026rdquo;}\n除了颜色之外，多序列比对每一段的最后一行都有些星星点点的标记。这些标记和双序 列比对中的竖线、双点、单点的意思类似，但并不完全相同;这些星星点点的标记可以为我们指出保守区域所在的位置，即，星星点点特别密集的区域\n如果某一列是完全保守的一列， 也就是说这一列里的字母完全相同，那么这一列的下面就打一个\u0026quot;*\u0026quot;\n如果这一列的残基 有大致相似的分子大小及相同的亲疏水性，也就是这一列的字母要么相同要么相似，没有不 相似的，那么就打一个\u0026quot;：\u0026quot;\n如果这一列的残基 有大致相似的分子大小及相同的亲疏水性，也就是这一列的字母要么相同要么相似，没有不 相似的，那么就打一个\u0026quot;：\u0026quot;\n什么都不标记代表这一列是完全不保守的，也就是这一列的字母全部都不相似\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.4426038932633425in\u0026rdquo;}\nResult Summary 标签里，给出了全部结果信息的下载列表和一个 Jalview 的按钮（图 4）。 Jalview 是多序列比对编辑软件;。在下载列表里，如果打开 \u0026ldquo;Percent Identity Matrix\u0026quot;链接，可以得到所有序列两两之间的一致度矩阵。一致度矩阵的第 一行省略掉了。它和第一列完全相同，都是序列的名字并且按照相同的顺序排列。所以这个 矩阵是以对角线为轴对称的，并且对角线上是某条序列自己和自己的一致度，都是 100%。 这个矩阵可以帮助我们更好的了解这些序列之间的关系。比如我们可以从中发现，一致度最高的一对序列是 TLR1 和 TLR6,\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.170785214348206in\u0026rdquo;}\n还可以通过 Phylogenetic Tree 标签下的 Guide Tree 清楚的看出哪条序列和哪条序列更相似;Phylogenetic Tree 翻译成中文是系统发 生树。但是这里要特别注意，这不是真正意义上的系统发生树！它只是在创建多序列比对的 过程中用到的树（Guide Tree），没有经过距离校正，所以不能当作系统发生树来使用。如果 想要根据多序列比对结果构建系统发生树，可以在 Alignments 标签下，点击\u0026quot;Send to ClustalW2_Phylogeny\u0026quot;链接，把做好的多序列比对发送给专门做系统发生树的工具\nTCOFFEE - Expresso http://tcoffee.crg.cat\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.293115704286964in\u0026rdquo;}\nTCOFFEE 本身是一个标准的多序列比对工具，跟 CLUSTAL 没有什么区别。我们来看 它的变形，也就是根据比对序列种类的不同，TCOFFEE 网站下特有的比对工具\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.34495406824147in\u0026rdquo;}\nExpresso 最有特色，它是为序列加入结构信息后再做多序列比对的 工具。因为有结构信息的辅助，它可以大大提高比对的准确度\nM-Coffee 可以把多个比对的结果整合成一个\nTM-Coffee 专为穿膜蛋白打造\nPSI-Coffee 专为远源序列打造\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.415285433070866in\u0026rdquo;}\n做 Expresso 的序列我们选用网站提供的示例序列（图 2）。Show more options 下，可以 通过各种方式给入输入序列的结构信息。如果你有这些序列现成的结构文件，也就是 PDB 文件，可以直接把它们上传上来。三条序列对应三个上传链接。可以上传的结构文件不只限 于 PDB 数据库下载的，也包括还未正式发表的解析结构或者计算机预测的结构，只要是用 PDB 文件格式保存的，都可以\n如果没有现成的结构文件，但是这些序列在 PDB 数据库里有对应结构的话，你可以从 接下来的输入框里，按照规定的写法，指定哪条序列对应 PDB 数据库中的哪个结构\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.4290113735783025in\u0026rdquo;}\n如果这里输入了信息，Expresso 会自动从 PDB 网站下载指定的 PDB 文件。那些已经本地上 传的结构，Expresso 也会根据序列信息自动匹配出它是哪条序列的结构，不需要再在这里列 出了。如果对结构信息一无所知，只需要将\u0026quot;MODE_PDB\u0026quot;钩选。之后，Expresso 就会自 己在网络上为所有没有指定结构信息的序列搜索相应的结构。你提供给 Expresso 的结构信息越多，计算时间就会越短；你提供结构信息越少，计算时间就会越长。如果只勾选了 \u0026ldquo;MODE_PDB\u0026rdquo;，那么需要等待的时间会很长，因为 Expresso 首先要搜索，然后要下载， 最后要计算\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.470108267716536in\u0026rdquo;}\nTCOFFEE 系列各比对工具做出的多序列比对的颜色代表比对质量的好坏。越红质量越好， 越蓝质量越坏。这次的比对结果非常令人满意。如果配合上这些序列二级结构信息看一下的 话，你会发现，螺旋和螺旋很好的对在了一起，折叠和折叠很好的对在了一起。\n同样的序列做普通的 TCOFFEE，质量远不如 Expresso。可以看到二级结构全部 错位。所以，如果你有序列的结构信息的话，用 Expresso 相比用普通的比对工具会大大提 高比对质量。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.441287182852143in\u0026rdquo;}\n多序列比对的结果保存格式 比如有漂亮的网页格式的，标准的 Clustal 格式的，还有写完一条 序列再写下一条的 FASTA 格式的，以及方便下一步建树使用的 Phylip 格式的\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4409536307961504in\u0026rdquo;}\n要保存哪种格式主要看你下一步要干什么。在选择保存格式之前：\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3418744531933506in\u0026rdquo;}\n格式转换工具fmtseq http://www.bioinformatics.org/JaMBW/1/2\n它可以实现 20 多种格式间的转 换。其中总有一款是你想要的\n多序列比对的编辑和发布：Jalview http://www.jalview.org/\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.589284776902887in\u0026rdquo;}\n通过 Jalview 除了可以加工多序列比对，还可以针对比对中的序列做各种各样的 分析、比如构建系统发生树、预测蛋白质二级结构、查看结构域家族、从 PDB 数据库中查 询三级结构等\nJalview的使用简介 Jalview 打开之后，会自动展示许多 Demo 窗口。通过这些窗口，你可以了解到， Jalview 能加工序列比对、做进化树、分析结构，等等。功能确实不少。不过，你需要做的， 是把这些窗口统统关掉。打开你自己的多序列比对。\n点击 File 菜单 - Input Alignment - From File - 打开我们之前用Clustal Omega做出并保存的多序列比对结果\u0026quot;clustalo.clustal\u0026rdquo;（如 果你忘记保存了可以从附件中下载）。因为\u0026quot;.clustal\u0026quot;不是 Jalview 熟悉的后缀名，所以需要 把文件类型改成\u0026quot;所有文件\u0026quot;才能看到它\n{width=\u0026ldquo;5.513888888888889in\u0026rdquo; height=\u0026ldquo;8.819444444444445in\u0026rdquo;}\n它们体现了比对中每个位置的 保守度高低（Conservation）、比对质量高低（Score）、以及共有序列（Consensus）。从保守度 行，可以很清楚的找到保守区大致的位置。共有序列指的是某一列出现频率最高的那个字母， 比如第 58 列中 W 出现的频率最高，是 100%。如果某一列拥有的最高出现频率的字母是两 个或两个以上的话，会以\u0026quot;+\u0026ldquo;显示。把鼠标放在\u0026rdquo;+\u0026ldquo;上就可以看到是哪些字母出现的频 率一样高。共有序列可以一定程度上体现出某个保守区域所具有的序列特征。以后如果看到 和这段序列长相极其相似的序列，它很可能能跟这个保守区的功能相似。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.34794728783902in\u0026rdquo;}\n使用Jalview加工数据 上颜色 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.3141338582677164in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 另一个颜色方案 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.309044181977253in\u0026rdquo;} Clustalx的颜色方案\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.6097353455818024in\u0026rdquo;}\n对局部位置进行 手动调整 \u0026lt;!-- --\u0026gt; 比如，从前期实验我们得知，图中方框所示区域的 TLR2、10、6、1 这四条序 列的第 53 列应该往右挪一列，跟 TLR9、8、7 这三条序列的第 54 列对在一起。TLR2、10、 6、1 这四条序列的第 53 列补空位。其他位置不动\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.206905074365705in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 多序列比对的外观 \u0026lt;!-- --\u0026gt; 默认情况下，多序列比对是单行显示的。由于序列长，需要拖动窗口拉条才能浏览全部。这样不利于查看分析，也不利于将导出的比对图片插入文献。 如果想要让多序列比对根据 Jalview 窗口的宽度自动换行，可以在 Format 菜单下勾选\u0026quot;Wrap\u0026rdquo; 。此外，还可以通过\u0026quot;Font\u0026hellip;\u0026ldquo;窗口对字体格式、大小等进行调整\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.295605861767279in\u0026rdquo;}\n如果你只需要多序列比对，而不需要有关保守度等的注释行。可以关闭 annotations 标签下的 \u0026ldquo;Show annotations\u0026quot;选项，以达到去掉注释行的目的\n比如，可以按照序列的名字、 两两一致度或其他规则给比对中的序列重新排序以及为选中的两条序列做双序列全局比对 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.2624934383202095in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 为选中的一组序列计算各种系统发生树 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.104316491688539in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 或者用在线软件为某一条序列预 测二级结构 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.389772528433946in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 导出多序列比对为图片 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.246930227471566in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 其他编辑工具 \u0026lt;!-- --\u0026gt; Boxshade 擅长黑白制图。因很多学术期 刊只收取彩图的编辑费，所以黑白图可以节省科研经费。ESPript 的功能十分强大。MView 擅长把彩色多序列比对转换成 HTML 源代码。这样就可以将它直接插入网页，并方便以文 本形式选取。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.7402898075240596in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.6730686789151357in\u0026rdquo;}\n寻找保守区域 你究竟想从多序列比对中得到什么，答案是你想要找到序列中重要 的位置，说得更专业一点，就是要找到保守区域\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.6254932195975504in\u0026rdquo;}\n通过多序列比对下方的星星点点可以大致 发现图 1 中两个红框中的区域比较吸引眼球，因为星星点点特别多（\u0026rdquo;*\u0026ldquo;代表这一列残基完全相同；\u0026rdquo;:\u0026ldquo;代表这一列残基或者相同或者相似；\u0026rdquo;.\u0026rdquo; 代表这一列残基有相似的但也有不相似的；什么都没有代表这一列残基都不相似。所以我们 寻找的就是星星点点特别多的区域。当然用眼睛来数星星不那么靠谱。我们仍然需要借助软 件来更好的寻来保守区域）\n序列标识图 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.553226159230096in\u0026rdquo;}\n要创建序列标识图，首先需要一个多序列比对。多序列比对中的一列对应序列标识图中 的一个位置。然后分别计算每一列中不同残基出现的频率，再根据以下公式把频率转换成高 度值，最后根据高度值写出不同残基的彩色字母图形\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.460893482064742in\u0026rdquo;}\n如果某一列非常保守，字母高度就高。反之，如果某一列没有什么特征，各种残基都有 出现，杂乱无章，那么就会看到一堆比较矮的字母摞在一起\n这里再次强调，字母的高度和 它在某一列中出现的频率成正比，但是并不等于频率。试想一下，如果字母高度就是频率的 话，那么序列标识图中每个位置上字母摞起来的总高度应该是一样的，都是 100%。但是从图中可以看到，序列标识图上每个位置字母摞起来的总高度是不一样的，这是因为在字母 高度的计算过程中涉及了熵值。某一列中字母出现的情况越混乱，熵值越大，字母越矮。字 母出现的情况越有规律，熵值越小，字母越高。所以序列标识图可以很好的展现多序列比对 中每一列的保守程度，即，它们是杂乱无章的，还有有规律可循的。并且把可循的规律图形 化的展现出来。这就是我们为什么要给序列打上 logo 的原因\nWebLogo http://weblogo.threeplusone.com/\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.073949037620298in\u0026rdquo;}\n示例文件 promoter.fasta 是一组启动子序列的多序列比对， 以 FASTA 格式存储。FASTA 格式的多序列比对要求把多序列比对中的每一条序列连同插入 的空位一起按 FASTA 格式书写，写完一条序列再写下一条。这和之前讲过的 Clustal 格式不 太一样。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.292139107611549in\u0026rdquo;}\n从图中可以清晰的看到：输入的这些启动子序列上 TATA-Box 的共有特征序列，以及它们出现的位置\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.948219597550306in\u0026rdquo;}\n序列基序MEME http://meme-suite.org/\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4565299650043744in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.039199475065617in\u0026rdquo;}\nMEME 的 使用非常简单，只需要将待分析的序列上传即可（图 1）。而且，上传的序列为原始序列， 不需要提前为它们做多序列比对。你也可以指定返回排名前几的基序。MEME 的等待时间 稍长，大约 10 分钟以上，所以最好留下邮箱。\nMeme 的返回结果被保存成各种格式：HTML、XML、test 等。便于在线查看的是\u0026quot;MEME HTML output\u0026quot;，即网页格式\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1836154855643044in\u0026rdquo;}\n网页格式的 MEME 结果页面中，给出了找到的排名前三的基序，它们以序列标识图的形式展现出来\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.9849507874015746in\u0026rdquo;}\n同时还提供这三个基序在每条序列中的大体位置。如果要进一步了解 某个基序，可以点击序列标识图右侧的\u0026quot;More\u0026quot;下面的\u0026quot;下\u0026quot;箭头，以查看详细。 点击后，会得到大比例序列标识图，以及该基序在每条序列中对应的序列片段和它们出现的 具体位置\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.768046806649169in\u0026rdquo;}\n此外，还可以点击序列标识图右侧的\u0026quot;Submit/Download\u0026quot;下面的\u0026quot;右\u0026quot;箭头，将某个基序提交至各种数据库，并进行针对该基序的序列相似性搜索，已找到数据库中 含有该基序的序列，进而推测该基序的功能。这步操作是通过 The MEME Suite 软件套装下 的另一个软件 FIMO 来实现的\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.888888888888889in\u0026rdquo;}\nPRINTS 指纹图谱数据库 http://130.88.97.239/PRINTS/index.php\nhttp://www. bioinf.manchester.ac.uk/dbbrowser/PRINTS/\n蛋白质序列进行了充分的研究，而且早已发现并总结了这些 序列上的重要基序。相关研究成果汇入了 PRINTS 蛋白质序列指纹图谱数据库\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.506613079615048in\u0026rdquo;}\n要浏览 PRINTS 数据库，可以输入数据库编号、关键词、或标题等以查找某一个指纹图 谱。比如点击\u0026quot;By text\u0026quot;通过关键词搜索。输入条中输入\u0026quot;TRANSFERRIN\u0026quot;，也就 是搜索转铁蛋白家族的图谱。搜索返回转铁蛋白家族的指纹图谱链接\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.164479440069991in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.086452318460193in\u0026rdquo;}\n点击结果页面中的\u0026quot;TRANSFERRIN\u0026quot;链接后，会显示包括指纹图谱的基本信息、与其 他数据库之间的交叉链接、构建指纹图谱所使用的蛋白质序列、以及指纹图谱中每个基序等具体信息 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.2194083552056in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 点击\u0026quot;View alignment\u0026quot;链接后，可以看到创建指纹图谱所使用的多序列比对 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4101060804899386in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 点击\u0026quot;View structure\u0026quot;链接后，网页会打开一个三维视图插件，并以该家族中某一特征 蛋白质具有的三维结构为例，在线显示指纹图谱中各个基序在三维结构中的位置； 从该三维结构图中可以看出，紫色的基序在氨基酸序列水平上并不相邻，但是在三维空间结 构中是紧密联系在一起的，并形成蛋白质的重要功能区 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.183832020997375in\u0026rdquo;} \u0026lt;!-- --\u0026gt; PRINTS 还提供指纹匹配服务。也就是搜索某一序列所匹配的 指纹图谱。此功能通过 PRINTS 主页也上的\u0026quot;FPScan\u0026quot;链接实现，注意输入的待搜 索序列只能是\u0026quot;a raw sequence\u0026quot;，也就是纯序列。换言之，FASTA 格式中带大于号的第一行 不能拷贝进输入框。示例文件 prints.fasta 请从课程附件中下载 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.527737314085739in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.2331386701662295in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.975456036745407in\u0026rdquo;}\n点击排名第一的视紫红质家族的\u0026quot;Graphic\u0026quot;链接，可以得到该家族指纹图谱中各个基 序在输入序列中所匹配的位置。结果页面的下部还提供了视紫红质家族的 6 个基序 在输入序列中所对应的具体序列片段。由此，可以推测，输入序列属于视紫红质家族，并具 有该家族蛋白质的主要功能。事实上，输入序列确实是从 UniprotKB 数据库中下载的一条羊 的视紫红质的序列（P02700）。 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.9080457130358703in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.476358267716535in\u0026rdquo;}\n进化树 引言 Nothing in Biology Makes Sense Except in the Light of Evolution（如果生物学没有了进化， 那么一切都将黯然无光）\n分子进化概念 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1352602799650042in\u0026rdquo;}\n所谓变异速度是指一定时间内不同碱基或氨基酸突变的个数。这个 进化变异速度被认为是恒定的，跟物种没有关系。所以，拿蛋白质来说，两个蛋白质在序列 上越相似，他们距离共同祖先就越近。分子钟理论是进化研究领域被普遍认可的理论，但是至今也没有直接的证据证实。\n一些基本概念 同源（Homologs），相同来源。没错，但是它的确切定义是，来源于共同祖先的相似序 列为同源序列。也就是说，相似序列有两种，一种是来源于共同祖先的，那么他们可以叫同 源，另一种不是来源于共同祖先的，那么他们尽管相似也不能叫同源。 \u0026lt;!-- --\u0026gt; 第二种情况出现的概 率虽然低，但还是存在的，所以相似序列并不一定是同源序列。同源又分为三种，直系同源， 旁系同源和异同源。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.80228237095363in\u0026rdquo;}\n\u0026lt;!-- --\u0026gt; 直系同源（Orthologs）是指，来自于不同物种的由垂直家系，也就是物种形成，进化而 来的基因，并且典型的保留与原始基因相同的功能。也就是说，随着进化分支，一个基因进 入了不同的物种，并保留了原有功能。这时，不同物种中的这个基因就属于直系同源 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.332113954505687in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 旁系同源（Paralogs）是指在同一物种中的来源于基因复制的基因，可能会进化出新的 但与原功能相关的功能来。基因复制产生了两个重复的基因，多出来的这个有几种命运，一 个是又丢了。复制出来发现没有用，又删了。另一种命运是演化出了新的功能。如果这个新 功能是往好的方向发展，就会被保留下了，如果是往不好的方面发展，就会被自然选择淘汰。 还有一种命运，就是被放置不用。复制出来以后，又加了个终止子，既不表达，也不删除， 搁那里搁着不管，成了伪基因。被保留下来的具有新功能的基因与另一个复制出来的基因之 间就是旁系同源。 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.564267279090114in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 异同源（Xenologs）是指通过水平基因转移，来源于共生或病毒侵染所产生的相似基因。 异同源的产生不是垂直进化而来的，也不是平行复制产生的，而是由于原核生物与真核生物 的接触，比如病毒感染，在跨度巨大的物种间跳跃转移产生的 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.4842333770778655in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 不同的同源，概念很容易混淆。图 1 清楚的描述了各种同源之间的关系。首先，有个早 期的球蛋白基因，它通过基因复制，形成了α球蛋白基因和β球蛋白基因。后来随着进化，这 两种复制产生的基因也存在于不同的物种中。其中某一物种里的，比如老鼠里的α球蛋白基 因和β球蛋白基因就属于旁系同源。而某一个基因在不同物种中，比如青蛙里的α球蛋白基因 和鸡里的α球蛋白基因就属于直系同源。再比如，某个细菌，它没有早期的球蛋白基因，也 自然没有β球蛋白基因，但是通过与青蛙的共生，发生了基因水平转移。于是它从某一天就 起有了β球蛋白基因。那么这个细菌的β球蛋白基因和青蛙的β球蛋白基因就属于异同源 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0876760717410323in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 一个小知识点 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.27665791776028in\u0026rdquo;} 之前我们讲过两条序列的相似度如何计算，那么能不能定量描述同源呢？答案是不可以， 同源只是对性质的一种判定，只能定性描述，不能定量描述。所以\u0026quot;同源性等于 80%\u0026ldquo;这种 说法是错误的！\n树和网 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.575250437445319in\u0026rdquo;} 编织生命网 的要素之一就是水平基因转移 ，水平基因转移（horizontal gene transfer）是指生物将遗传物质传递给其他细胞而非其子代细胞的过程\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3388156167979in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.7101213910761155in\u0026rdquo;}\n系统发生树的样子 研究分子进化所要构建的系统发生树（Phylogenetic tree），也叫分子树\n对于一个未知的基因或蛋白质序列，可以利用系统发生树 确定与其亲缘关系最近的物种。比如你得到了一个新发现的细菌的核糖体 RNA，你可以把 它跟所有已知的核糖体 RNA 放在一起，然后用他们构建一棵系统发生树。这样就可以从树 上推测出谁和这个新细菌的关系最近\n系统发生树还可以预测一个新发现的基因或蛋白质的 功能。以基因为例，如果在树上与新基因关系十分密切的基因的功能已知，那么这个已知的 功能可以被延伸到这个新基因上\n构建系统发生树还有助于预测一个分子功能的走势。也就 是从树上可以看出某个基因是正在走向辉煌还是在逐渐衰落\n系统发生树还能帮助我 们追溯一个基因的起源。甚至当它从一个物种\u0026quot;跳\u0026quot;到另一个物种上，也就是发生了水平基 因转移时，系统发生树都可以很好的展示出来\n\u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.652173009623797in\u0026rdquo;} {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.0666207349081365in\u0026rdquo;}\n根和节都表示理论上曾经存 在的祖先，叶子是现存的物种。这一点很重要！比如我们要研究某个基因，于是搜集了很多 物种的这个基因的序列，用它们构建了一棵系统发生树。搜集到的物种都出现在叶子上，也 就是外节点上，没有在内节点上的。内节点上都是理论上曾经存在过的共同祖先，现在已经 不存在了！此外，枝子的长短也是有意义的\n根据建树所用序列的多少来选择不同形 状的树。如果序列非常多，那么圆形的看上去就要比方的或者三角的舒服得多，便于在文献 里排版。系统发生树上从任何一个点发出的枝子围着这个点旋转都不改变树的生物学意义， 只是视觉上有点儿差别而已。所以，旋转之后的两棵树是等价的，生物学意义完全相同\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.031599956255468in\u0026rdquo;}\n系统发生树的种类 系统发生树还分为有根树和无根树，顾名思义，有根树就是有根，无根树就是 无根。其实两者是可以互换的。如果我们按住无根树上某一个点，然后用把梳子将树上所有 的枝条都以这个点为中心向右梳理，就能把它梳成有根树的样子。按住的这个点就是根。所 以对于一棵树来说，根的位置是主观的，你想让他在哪它就在哪里。但是你不能随意指定哪 个内节点当根，毕竟根有其自身的生物学意义，它应该是所有叶子的共同祖先。那么我们如 何确定根的位置呢？可以通过外类群（outgroup）来确定，从而把无根树变成有根树\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5430380577427822in\u0026rdquo;}\n所谓外类群，就是你所研究的内容之 外的一个群。比如你要分析某一个基因在不同人种间的进化关系，那就可以额外选择黑猩猩 加入进来，作为外类群一同参与建树。或者你要分析哺乳动物，那就可以选鳄鱼、乌龟之类 的。总之，保证外类群在你要研究的内容之外，但又不能太远。外类群可以不只是一个物种， 而是多个，但也不要太多，两三个即可。为什么有了外类群之后，做出来的树就是有根树了 呢？因为你知道外类群和你研究的内容一定不是一伙的，所以外类群分支出的那个内节点就 是根\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.6395395888013997in\u0026rdquo;}\n讲的分子树跟物种树是有本质区别的。物种树是基于每个物种整体的进化关系， 也就是基于整个基因组构建的，而分子树是基于不同物种里某一个基因或蛋白质序列之间的 关系构建的。那么一个分子树表达出来的各物种之间的关系就可能与物种树完全不同。此时 说明这个基因经历了特殊的进化故事。也许是受到了特殊环境变化的影响，也许是发生了水 平基因转移等等。总之，这种区别的出现是很有研究价值的\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.768365048118985in\u0026rdquo;}\n系统发生树的构建方法 最常用的有基于距离的构建方法，包括非加权分组平均法 （Unweighted Pair Group Method with Arithmetic mean，UPGMA），最近邻居法（NeighborJoining method，NJ），最小二乘法（Generalized Least Squares，GLS）等。还有最大简约法 （Maximum Parsimony，MP），最大似然法（Maximum Likelihood，ML），贝叶斯推断法（Bayesian Inference，BI）等\n从计算速度来看，最快的是基于距离的方法， 几十条序列几秒钟即可完成。其次是最大简约法。最大似然法就要慢得多。最慢的是贝叶斯 法。但是从计算准确度来看，算得最慢的贝叶斯法确是最准确，而算得最快的基于距离法结 果确是最粗糙。从实用的角度，建议使用最大似然法。因为这种方法无论从速度还是准确度 都比较适中。最近邻居法虽然算得快，但是当序列多，彼此差别小的时候，这种方法不适合。 最大简约法，似乎是个掉空里的方法，高不成低不就，所以很少有人使用。贝叶斯法不是所 有的建树软件都提供，算法开发上还有待提高，而且计算时间过长\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.210430883639545in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.2344991251093616in\u0026rdquo;}\nhttps://www.megasoftware.net/\n以非加权分组平均法（UPGMA 法）：\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.7601345144356957in\u0026rdquo;} 在接下来的例子里，我们用序列间不同的碱基数目作为序列间遗传距 离的度量。首先，计算出每两条序列间有几个碱基不同，并以用矩阵的形 式记录下这些距离\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;6.116220472440945in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.522497812773404in\u0026rdquo;}\nMEGA 7 构建NJ树 http://www.megasoftware.net/\n初步使用 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.1582305336832897in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.8658530183727033in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.559730971128609in\u0026rdquo;}\n这个多序列比对作为中间结果保存下来\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.966666666666667in\u0026rdquo;} {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.6509426946631671in\u0026rdquo;}\n想进一步了解序列的保守性 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;8.394635826771653in\u0026rdquo;}\n准备工作到此全部完成\n构建 NJ 树 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.984195100612424in\u0026rdquo;} 步长检验是根据所选的建树方法，计算并绘制指定次数株系统发生树。因为大多数建树 方法的核心算法都是统计概率模型，所以每次计算出的树都会有所差别。而建好的系统发生 树上每个节点上都会标记一个数字，它代表了指定次数次计算所得出的系统发生树中有百分 之多少棵树都含有这一节点。一般来说，绝大多数节点上的数值都大于 70%的树才可信。个 别低于 70%的节点可以暂且容忍，或通过添加，删减序列来改善质量。\n第二个参数是，Substitution Model。它是选择计算遗传距离时使用的计算模型。理论上应该尝试各种模型，根据检验结果选择最合适的模型进行计算。但在实际操作中，可先尝试 选用较简单的距离模型，比如 p-distance\n第三个参数是 Gap/Missing Data Treatment。大多数建树方法会要求删除多序列比对中含 有空位的列。但是根据遗传距离度量方法的不同，删除原则也不同。如果是以序列间不同残 基的个数来度量遗传距离的话，这里需要选择 Complete deletion（全部删除）。如果是其他方 法，比如这里选用的 NJ 方法，可以选择 Partial deletion（部分删除）。删除程度定在 50%， 即，保留一半含有空位的列\n最终的树\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.5252919947506562in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.230919728783902in\u0026rdquo;}\n换进化树的形状 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;4.986111111111111in\u0026rdquo; height=\u0026ldquo;0.875in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 调整树枝的粗细或字体的大小\n可以从 View 下拉菜单 下的 Option 选项卡中调整。\n保存为图片\n\u0026lt;!-- --\u0026gt; 保存图片可以点 Image 下拉菜单，选择保存格式。或者将窗口放大，再点 按钮将树放大之后屏幕截图 蛋白质结构预测 蛋白质的结构 你将洞悉一个蛋白质到 底长什么样子（蛋白质三维结构），它和它的闺蜜手拉手拍合影的样子（蛋白质和蛋白质分 子对接），它嘴巴里塞满食物的样子（蛋白质和小分子分子对接），以及它在你身体里活动的 样子（分子动力学模拟)\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2800645231846017in\u0026rdquo;}\n一级结构也就是氨基酸序列，\n二 级结构是周期性的结构构象，比如α螺旋β折叠等\n三 级结构是整条多肽链的三维空间结构\n四级结构是几 个蛋白质分子形成的复合体结构，比如三聚体，四聚体等\n\u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.299537401574803in\u0026rdquo;}\n蛋白质是由氨基酸组成的，前一个氨基酸的羧基 和后一个氨基酸的氨基脱去一分子的水，缩合形成的 肽键。肽键将氨基酸连接起来形成肽链。成熟的肽链 并不是一根松散的毛线，它要经过折叠变成一个毛线 团，即，形成空间立体结构。拥有了空间立体结构之 后，蛋白质才能上岗工作。\n蛋白质的二级结构 DSSP指认 蛋白质经过折叠后会形成规则的片段，这些规则的片段构成了蛋白质的二级结构单元 （图 1）。三种常见的二级结构单元包括螺旋、β折叠、和转角。螺旋中最常见的就是α螺旋， 但不只有α螺旋，还有其他的螺旋，比如 3 转角螺旋，5 转角螺旋等。β折叠由平行排列的β 折片组成。这些折片在序列上可能相隔很远，但是在空间结构上并排在一起，彼此间形成氢 键。除了螺旋和折叠外，蛋白质结构中还存在大量的无规律松散结构 coil。如果这些无规律 的肽链突然发生了急转弯，这个转弯结构就叫做β转角\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.166084864391951in\u0026rdquo;}\n蛋白质的二级结构经常用图形来形象的描述。比如黄色的箭头代表对应的氨基酸 具有β折片结构。波浪线代表螺旋结构，小鼓包是转角。此外，以字母形式书写的二级结构 序列能够更加精准的描述。其中，E 代表β折叠，H 代表α螺旋，T 代表转角。没有写任何字 母的地方是松散的 coil 结构\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1464173228346457in\u0026rdquo;}\n研究人员根据 DSSP，也就是蛋白质二级结构定义词典，将三级结构里的二级结构单元指认出来的\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.261509186351706in\u0026rdquo;}\n然后再按照规定的格式，记录下蛋白质中每个氨基酸处于哪种二级结构单元。这样一 个记录蛋白质二级结构信息的文件叫做 DSSP 文件。蛋白质结构数据库 PDB 中的每一个蛋 白质三级结构都有自己对应的 DSSP 文件。DSSP 文件里不同字母所代表的不同二级结构单 元和 PDB 里面的记录方式是统一的\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.125272309711286in\u0026rdquo;}\nDSSP 的主页上，Introduction 部分有一个 Web server 链接，这个链接很容易让人误以为 可以通过它预测某条氨基酸序列的二级结构。这是不对的。DSSP 网站的 Web Server 可以指 认蛋白质结构文件，也就是 PDB 文件中的二级结构，并创建出相应的 DSSP 文件。提交的 PDB 文件可以是用实验方法刚刚解析出来，还没有提交 PDB 数据库的蛋白质三级结构，也 可以是用计算方法预测出来的蛋白质三级结构模型。总之，输入值必须是三级结构，而不是 一级的氨基酸序列（PDB ID 必须是小写的）\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.226353893263342in\u0026rdquo;}\nPDB获取 。从 序列图形化部分可以看到二级结构对应在一级结构上的图形化表示\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.311246719160105in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.671609798775153in\u0026rdquo;}\n点击左侧的\u0026quot;View Sequence \u0026amp; DSSP Image\u0026quot;可以获得直观的一级结构对二级结构的序列表示（图 2）。图 2 中的 序列有两行，上面的一行是一级结构，下面的是二级结构。这个页面看上去很不错，序列 10 个字母一间隔，50 个字母一行，而且不同的二级结构还对应不同的字母颜色。但是在接 下来的分析研究工作中，我们往往需要的是像氨基酸序列那样的 FASTA 格式的二级结构序 列。非常遗憾的是， PDB 里没有现成的针对某一个蛋白质的 FASTA 格式二级结构序列下载链接。\u0026ldquo;Download FASTA File\u0026quot;链接只能下载 FASTA 格式的一级结构序列，也就是氨基酸序列\n此外，PDB 数据库中有一个叫做\u0026quot;ss.txt\u0026quot;的文件：http://www.rcsb.org/pdb/files/ss.txt.gz (压缩文件 30.6M)。这个文件里面有 PDB 所有蛋白质结构的一级和二级结构的 FASTA 格式 序列。\nhttp://www.rcsb.org/pdb/files/ss.txt.gz\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.481858048993876in\u0026rdquo;}\n软件预测 但是，已知空间 结构的蛋白质在 PDB 数据库里毕竟只有 10 万多个。然而，UniprotKB 数据库里却有几百万 条蛋白质序列。也就是说，绝大多数蛋白质的空间结构还都未知。这些蛋白质的二级结构又 如何知晓呢？这，就需要用计算机软件来预测蛋白质的二级结构。预测的结果和真实情况会 有一定出入，究竟差多少，取决于预测软件的准确度。可以预测蛋白质二级结构的软件很多， 而且都可以在线使用\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.543543307086614in\u0026rdquo;}\n以 PSIPRED 为例，PSIPRED （http://bioinf.cs.ucl.ac.uk/psipred）是一个蛋白质序列分析平台，它不仅可以预测二级结构， 还有很多其他分析功能，比如预测三级结构\n选择第一个\u0026quot;PSIPRED\u0026quot;工具，即，预测蛋白 质二级结构的工具。输入的氨基酸序列（见附件 psipred.fasta）在 PDB 数据库中已有对应的 空间结构（PDB ID：3CIG），因此二级结构也是已知的。之所以输入这样一条序列，是为 了将预测结构和真实结构进行比较，从而评估一下软件的预测准确度如何。预测一般需要 30 分钟。可在线等待结果，也可以查收结果邮件。需要注意的是，像大多数在线软件一样， PSIPRED 不支持免费的商业邮箱，比如 hotmail 或者 QQ 邮箱等。此外，最好给预测任务起 个名字。最后，点击\u0026quot;Predict\u0026rdquo;。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.387323928258968in\u0026rdquo;}\n预测结果页面 Summary 标签下的内容所示，，粉红色的位置是α螺旋出现的位置， 黄色的是β折片，没有底色的是松散的 coil 结构。如果预测出有错乱的结构也会被标出。目 前的二级结构预测软件，都只能预测出α螺旋和β折片，对其他不常见的二级结构单元并不进 行预测\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0471073928258967in\u0026rdquo;}\n结果页面中，PSIPRED 标签下有全面的结果描述。描述一共有四行，最下面是 一级结构序列，往上是二级结构序列，再往上是二级结构图形化的描述，最上面的柱状图反 应的是每个位置预测的可信度\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.24167760279965in\u0026rdquo;}\n从结果页面的 Download 标签下可以下载纯文本格式的结果文件。纯文本格式的结果更 利于下一步分析加工\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.295230752405949in\u0026rdquo;}\n把 PSIPRED 预测结果和 DSSP 里的真实二级结构放在一起（图 6）。绿色的 是预测结果，粉色的是真实结构。可以看到图 6 中显示出的这部分结果里，绝大多数α螺旋和β折片，也就是 H 和 E，都被正确的预测出来了，只有少数几个短的β折片没有被预测出 来，准确度超过 90%\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1597222222222223in\u0026rdquo;}\n蛋白质的三级结构 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.6815758967629044in\u0026rdquo;}\n无法结晶的蛋白质，可以利用核磁共振法在 液体环境中进行结构测定。但是核磁共振法只能用于质量小于 70 千道尔顿的分子，大约对 应 200 个氨基酸的长度\n除此之外，还有一些不太常用的方法也可以测定分子的三维空间结构，比如冷冻电子显微镜技术（Cyro-Electron Microscopy）。无论用什么方法测定的空间结构， 都要提交到 PDB 数据库。所以我们获取蛋白质三级结构最直接的办法就是去 PDB 搜索\nhttp://www.rcsb.org/\n从 PDB 首页的搜索条里，可以通过搜索 PDB ID、分子名称、作者姓名等 关键词来查找蛋白质三级结构\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.382924321959755in\u0026rdquo;}\n此外，利用高级搜索工具，可以通过序列相似性搜索获得与 输入序列在序列水平上相似的蛋白质的三级结构。搜索方法选 BLAST，输入序列（示 例文件：pdb_search.fasta），点击\u0026quot;Result Count\u0026rdquo;。此次搜索一共找到 108 个在序列水平上和 输入序列相似的蛋白质。点击链接\u0026quot;108 PDB Entities\u0026quot;。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.22693460192476in\u0026rdquo;}\n搜索结果中，排在第一位结构是人的 dUTPase 蛋白的三维结构，PDB ID 为 2HQU。这个结构所对应的序列与输入序列中黄色片段之间的一致度是 100%。输入的序列中蓝 色区域是信号肽。信号肽在蛋白质到达亚细胞定位之后就被切掉了，所以解析的成熟蛋白质 结构里不会有这一段。此外，成熟肽段 N 端的一小部分，由于实验技术等原因，也没有被 解析出来，这在 PDB 结构中是很常见的。有时，在序列中间也会有未解析出的断口。甚至 有时，为了得到稳定的晶体状态，需要突变个别的氨基酸或者删除一截肽段。这些技术手段 都会使得结构中的序列和蛋白质本身的序列有所差别\n复习：PDB 文件是通过记录蛋白质中每一个氨基酸上的每一个原子的三维坐标来存储 空间结构信息的。这些原子坐标可以被三维可视化软件读取。三维可视化软件能够创建一个 三维空间，然后根据原子坐标以及原子的大小把原子展示在空间内，并根据原子间的距离给 它们连上化学键。这样一个立体的蛋白质结构就呈现在眼前了。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.343793744531934in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.548386920384952in\u0026rdquo;}\n三级结构可视化软件VMD 强 大 的 免 费蛋 白 质 三 维结 构可 视 化软 件， VMD （http://www.ks.uiuc.edu/Research/vmd）；VMD 的安装也极其简单。不需 要预装任何语言环境，完全图形化安装过程，绝对可以轻松搞定\nVMD 打开后， 会弹出三个窗口：VMD Main（主窗口），VMD Display（显示窗口），和命令窗口 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.9581353893263342in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 打开一个蛋白质结构\n主窗口中点 File -- New Molecule\n弹出新窗口 Molecule File Browser（文件读取窗口）\n文件读取窗口中点 Browse 自动打开 VMD 安装目录\n进入 proteins 文件夹\n选择 VMD 自带的演示结构 bpti.pdb文件读取窗口 中点 Load\n显示窗口出现蛋白质结构\n\u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5741808836395452in\u0026rdquo;} VMD 读取了 PDB 文件中的原子坐标，把每一个原子以细线的形 式展示在 3D 空间中。不 同的原子对应的细线颜 色不同；碳原子是青色 的，氮原子是蓝色的， 氧原子是红色的，氢原 子是白色的，还有少量 黄色的硫原子\n\u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.597222222222222in\u0026rdquo; height=\u0026ldquo;4.680555555555555in\u0026rdquo;} 主窗口上的 Mouse 菜单里可以切换鼠标模式\n{width=\u0026ldquo;4.930555555555555in\u0026rdquo; height=\u0026ldquo;1.6388888888888888in\u0026rdquo;}\n如何改变蛋白质结构的外观 \u0026lt;!-- --\u0026gt; 有关外观的设置在主窗口中的 Graphic 菜单下的 Representation 窗 口里（图 1）。一个 Representation（显示状态）由三个元素构成。第一个元素是用什么 样式（Style）显示，当前使用的样式是以细线显示原子（Lines）。第二个元素是用什么 颜色（Color）显示，当前使用的颜色是按原子名定义的不同颜色（Name）。最后一个元素 是要显示什么内容（Selection），当前显示的内容是所有原子（all）。这三个元素分别 在 Representation 窗口里的 Draw style 标签下的 Drawing Method 下拉条、 Coloring Method 下拉条和 Selected Atoms 输入框里进行设置\nDrawing Method 下拉条：Lines 以细线显示原子。CPK 以不同大小的球 来显示原子，原子间的连线是相应的化学键，比如碳与碳之间的共价键，硫与硫之间的二硫 键等。NewCartoon 只显示蛋白质的碳骨架（backbone），并形象的展示出不同的二级结 构。每一种 Drawing Method 都可以再进一步设置显示效 果。比如对于 CPK，可以调整原子球的大小（Sphere Scale）、 改变化学键的粗细（Bond Radius）、以及设置更高或更低 的分辨率（Sphere/Bond Resolution）\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.61000656167979in\u0026rdquo;}\n\u0026lt;!-- --\u0026gt; ```{=html} \u0026lt;!-- --\u0026gt; ``` - Coloring Method 下拉条，Name 颜色方案是一种原子一种颜色，常见的 比如碳原子青色、氧原子红色、氮原子蓝色、硫原子黄色。在 NewCartoon 显示样式下， 只有碳骨架被现实出来，再配以 Name 颜色方案，整个结构都是青色的。Secondary Structure 颜色方案可以为不同的二级结构赋予不同的颜色，常见的比如α螺旋紫色、β折 片黄色、转角青色、松散 coil 结构白色。颜色方案 Res Type 根据氨基酸类型的不同赋予 不同的颜色，比如非极性氨基酸白色、极性带正电荷的氨基酸（碱性的氨基酸）蓝色、极性 带负电荷的氨基酸（酸性的氨基酸）红色、极性不带电荷的氨基酸绿色。这种颜色方案适合 在显示氨基酸侧链的 Drawing Method 下观看，比如 CPK 样式。颜色方案 ResName 为 20 种氨基酸设置了 20 种不同的颜色 - ![](media/rId879.png){width=\u0026quot;5.833333333333333in\u0026quot; height=\u0026quot;2.1749278215223096in\u0026quot;} ```{=html} \u0026lt;!-- --\u0026gt; ``` - ```{=html} \u0026lt;!-- --\u0026gt; ``` - Selected Atoms 输入框：输入框里输入需要显示 的内容，比如，写\u0026quot;all\u0026quot;代表显示所有原子，也就是整个 蛋白质、写\u0026quot;backbone\u0026quot;代表显示碳骨架。输入框里允许 输入的关键词和语法在 Selections 标签下有详细定义。 可以写Singlewords里面单个的单词，也可以写Keyword 和 Value 组成的词组，还可以利用逻辑词\u0026quot;and/or/not\u0026quot; 把多个单词和词组串成句子 - 点击 Reset 清空输入框里的内容Keyword 里双 击 ResNameValue 里双击 ALA（输入框里出现\u0026quot;ResName ALA\u0026quot;）点击 Apply。显示名字为 Alanine 的氨基酸上的原 子，即显示所有丙氨酸。配合Drawing Method设置为CPK， Coloring Method 设置为 ResName - 点击 Reset 清空输入框里的内容Keyword 里双 击 ResidValue 里双击 1点击 Apply。显示第一个氨基 酸。利用这个 Keyword 和 Value 组合可以根据残基的编号 选择某个或某一段氨基酸，比如，想要显示第 1 到第 10 个氨 基酸，可以直接在输入框里输入\u0026quot;resid 1 to 10\u0026quot;，回车。 此时显示的就是前 10 个氨基酸 - Draw style 标签下，Drawing Method 设置为 NewCartoon，Coloring Method 设置为 Secondary StructureSelections 标签下，点击 Reset 清空输入 框里的内容Singlewords 里面双击 alpha_helix点 击\u0026quot;or\u0026quot; Singlewords 里面双击 beta_sheet（输入 框里出现\u0026quot;alpha_helix or beta_sheet\u0026quot;）点击 Apply。 通过逻辑词显示出所有的α helix 和β sheet {width=\u0026ldquo;2.8194444444444446in\u0026rdquo; height=\u0026ldquo;8.13888888888889in\u0026rdquo;}\n设置多个 representations（简称 rep），也就是将多个显示状态的试试 效果叠加在一起\n设置第一个 rep：当前处于编辑状态下的 rep 背景色为浅绿色。设置 Drawing Method 为 NewCartoon，Coloring Method 为 Secondary Structure，Selected Atoms 为\u0026quot;all\u0026quot;\n创建第二个 rep：点击 Create Rep。点击后，复制产生了和第一个一摸一样的第二 个 rep。浅绿色背景自动跳转到第二个 rep，即目前第二个 rep 处于可编辑状态。设置 Drawing Method 为 cpk，Coloring Method 为 colorid，并选择选\u0026quot;1 red\u0026quot;（红色）， Selected Atoms 为\u0026quot;resname PRO\u0026quot;，回车\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.178016185476815in\u0026rdquo;}\n\u0026lt;!-- --\u0026gt; ```{=html} \u0026lt;!-- --\u0026gt; ``` - 创建第三个 rep：点击 Create Rep；设置 Drawing Method 为 Surface，Coloring Method 为 colorid，并选择选\u0026quot;8 white\u0026quot;（白色），Selected Atoms 为\u0026quot;all\u0026quot;，回车，此时，可以看到填满肉的蛋白质，也就是蛋白质的外表面 - 继续设置 Coloring Method 右侧的 Material（材质）为 Transparent（透明材质）。 如果显卡支持，可以打开 GLSL 显示模式：主窗口中点 Display\\--Rendermode选中 GLSL。 GLSL 打开后的 Transparent Surface 变得更加柔美了 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.644080271216098in\u0026rdquo;}\n可以双击某一个 rep，比如双击第三个 rep，将其暂时隐藏，等需要的时候再双击它取消隐藏\n可 以 保 存 当 前 所 有 的 representations（注意保存的是显示状态，而不是结构）：主窗口中点击 File -- Save Visualization State -- 保存在桌面上，起名叫 mystate.vmd。接下来关闭 VMD 再重 新打开。这次我们不需要 Load 蛋白质结构，分别设置三个 rep，我们只需要直接载入刚刚保 存 的 mystate.vmd， 即 可 恢 复 刚 刚 的 显 示 状 态 ： 主 窗 口 中 点 击 File -- Load Visualization State -- 找到并打开 mystate.vmd。这时，之前保存的显示状态就自 动显示出来了\n\u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.447484689413823in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 调换背景颜色 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.7193919510061242in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 显示Label\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.2641240157480316in\u0026rdquo;} \u0026lt;!-- --\u0026gt; ```{=html} \u0026lt;!-- --\u0026gt; ``` - 把鼠标模式设置为 Lable 模式，让它标记原子。此时，鼠标 变为十字 。接下来在显示窗口中，要标记哪个原子就在哪里点一下。点击后出现文本显 示的氨基酸名字、氨基酸序号以及被点击的原子的名字 调整 Lable\n改变字体颜色 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.2463035870516186in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 改变显示位置和内容： \u0026lt;!-- --\u0026gt; 主窗口中点击 GraphicsLables弹出 Lable 窗 口- - Properties 标签下 -- 选中要调整的 Lable按住鼠标左键在 Offset 坐标系内移动 来改变 Lable 的位置写入 Format 来改变 Lable 的内容。默认 Format 为：%R（氨基 酸名字）%d（氨基酸序号）:%a（原子名字）。比如，只想显示氨基酸名字和编号，删掉\u0026quot;:%a\u0026quot; 这部分代表原子信息的代码即可\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.448558617672791in\u0026rdquo;}\n\u0026lt;!-- --\u0026gt; 改变字体大小/粗细 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.461884295713036in\u0026rdquo;} 保存图片\n\u0026lt;!-- --\u0026gt; 可以用主窗口下的 File -- Render -- 弹出 File Render Controls 窗口。File Render Controls 窗口里可以选择多种图片导 出方式 计算方法预测三级结构 常见的方法 预测蛋白质的三级结构，常用的方法有从头计算法（ab initio），同源建模法（homolog modeling），穿线法（threading）和综合法（ensemble method）。一般的蛋白质都能用这几种 方法之一预测出三级结构，但也不排除特殊情况\n同源建模法 SWISS-MODEL http://swissmodel.expasy.org\n能帮助完成上述步骤中从模板选取到创建序列比对，再到计算模型，以及最后的质量评估 的全部过程。你需要做的只是：输入目标序列，点 Build Model（创建模型），大约 三到五分钟之后就会返回结果\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.478650481189851in\u0026rdquo;}\n结果页面：\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.262516404199475in\u0026rdquo;}\n从结果页面里可以看到，SWISS-MODEL 挑选了 PDB ID 为 IFYV 的结构作为模板，它 和我们输入的目标序列之间的一致度是 36.18%，大于 30%，满足同源建模的基本条件。结 果中还给出了目标序列和模板序列之间的序列比对。点击 中的下拉箭头，可 以下载预测模型的 PDB 文件。从序列比对和概览图上都可以看到，模板序列较目标序列短 了一点，即，模板序列没有能覆盖目标序列两端的部分，所以模型两端的质量并不高，但是 模型整体的质量按照 SWISS-MODEL 自己的评估是合格的。这一点可以通过 QMEAN 打分 获知。\n如果目标序列与模板序列一致度极高，那么同源建模法是最准确的方法，如果 一致度如果达到 30%，那么模型的准确度就可以达到 80%，模型可以用于寻找功能位点以及 推测功能关系等。如果一致度达到 50%，模型准确度就可以达到 95%，可以根据模型设计定 点突变实验，甚至可以做晶体结构置换，也就是辅助完成真实结构的测定。如果一致度达到 70%以上的话，我们基本就可以认为预测模型完全代表了真实结构，并可以用它进行虚拟筛 选、分子对接、药物设计等结构功能研究\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.625246062992126in\u0026rdquo;}\n预测蛋白质三级结构的首选方法是同源建模法（homolog modeling），该方法基于原理： 相似的氨基酸序列对应着相似的蛋白质结构。比如三个蛋白质，它们在序列水平上十分相似， 解析出的结构也十分相似。第四个蛋白质的序列和前面三个也高度相似，那么就可以比着前 三个结构的样子\u0026quot;画\u0026quot;出第四个的样子。所以同源建模法的关键就是找到一个好的模板。好 的模板要求，在序列水平上模板（template）要与目标（target）蛋白质具有超过 30%的一致 度。同源建模法操作流程如下\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1400699912510937in\u0026rdquo;}\n确定模板 \u0026lt;!-- --\u0026gt; 找到与目标蛋白质同源的已知蛋白质结构作为模版（目标序列与模版序 列间的一致度要≥30%） \u0026lt;!-- --\u0026gt; 序列比对 \u0026lt;!-- --\u0026gt; 为目标序列与模板序列创建序列对比。模板可以选取多个，通过做多序 列比对，各取所长，让模板序列中与目标序列相似的片段尽可能多的覆盖整个目标序列，同 时要尽量避免没有模板参考的断口 \u0026lt;!-- --\u0026gt; 计算模型 \u0026lt;!-- --\u0026gt; 通过序列比对，将目标序列里的氨基酸替换到模板结构里对应的氨基酸 所在的空间位置上。这一步通过同源建模软件来实现 \u0026lt;!-- --\u0026gt; 质量评估 \u0026lt;!-- --\u0026gt; 同源建模软件输出结构模型后还需要进行质量评估，并根据评估结果更 换模板或修正序列比对，重新构建模型，再次评估。重复这个过程，直至模型质量合格为止 但是，\u0026ldquo;序列越相似，结构越相似\u0026quot;也有例外情况，虽然这种情况极其罕见，但是我们的一个研究结果发现，两个蛋白质在序列水平上高度相似，但它们的晶体结构却告诉我们，两个蛋白质的结构并没有像序列比对里那样一一对应，完全一致，而是发生了一个字母的错位；这一个字母的错位导致二者最后一个折片的结束方向发生了 60 度角的扭转，进而使得之后的 C 端残基的空间位置发生了改变，并因此导致了两者功能的差异。这种情况 下，如果我们按照传统同源建模法以其中一个为模板预测另一个的结构的话，是无法预测出 正确的结果的。好在这种情况极其罕见，但既然存在，就需要敲响警钟，为这一传统方法的 改进提出新的目标\n虽然同源建模法是蛋白质三级结构预测的首选方法，但是对于那些找不到\u0026quot;好\u0026quot;模板的 蛋白质，此方法并不适用 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.6592246281714784in\u0026rdquo;}\n穿线法 I-TASSER 前言 对于那些找不到合适模板（一致度大 于 30%的模板）的蛋白质，此方法并不适用。这种情况下可以尝试穿线法（threading）。穿 线法基于的原理是：不相似的氨基酸序列也可以对应着相似的蛋白质结构\n而具有同一结构拓扑的蛋白质，序列水平上有相似的，也有不相似的\n看穿到哪个结构 里最舒服，哪个结构就可以作为预测的模板，并根据最舒服的穿法，构建出最终模型。那怎 么知道穿的舒服不舒服呢？通过能量方程。穿的舒服，能量就低，穿的不舒服，能量就高。 这和我们穿衣服一样。穿上一件不合身的衣服，你肯定老在那扭啊扭的不得劲，这老动换能 量就高啊！要是穿上件合身的，那就能待住不动了，这能量不就降下来了。穿线法就是通过 计算目标序列穿到每一个已知结构中的每一种穿法下的能量，找到能量最低的那种穿法以及 所穿的结构，然后把目标序列中的氨基酸替换到模板结构里来构建结构模型的，显然这种方法的计算量较同源建模法要大得多，因此预测需要耗费更久的时间\nI-TASSER 的介绍 https://zhanggroup.org//I-TASSER/\n这个预测系统在 CASP 第 7 到第 11 届蛋白质结构预测比赛 中都名列第一。CASP 全称是蛋白质结构生物信息预测国际竞赛。两年一届。每次比赛，参 与者们会对一组即将公开的结构进行预测。再将预测模型和真实结构进行比较，看谁预测的 最准。咱们中国上海交大电子信息与电器工程学院的沈红斌教授的研究组在 CASP11 中取得 了第三名。这是中国代表队截至目前取得的最好成绩\nI-TASSER的使用 张阳教授的 I-TASSER 可以在线提交预测任务（图 3），不需要提前下载安装。提交氨基 酸序列进行预测前需注册获得用户名密码，注册是完全免费的。再给任务起个名字。最后点 Run I-TASSER\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3025634295713036in\u0026rdquo;}\n通过点击 Queue 链接，可以查看当前的所有任务进程，从图中的任务列表里可以看出，穿线法需要 的时间确实比同源建模法多得多，提交的这条示例序列需要计算大约 35 个小时。这里请注意，一个用户或一个 IP 地址一次只能提交一个任务\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5456299212598426in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3391196412948383in\u0026rdquo;}\n此外，还可以通过搜索链接，搜索任务号找到任务，或者搜索账号找到账号下所有的任 务，或者通过搜索序列查找\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.099502405949256in\u0026rdquo;}\nITASSER 预测出了 5 个模型，点击任务号 以查看具体信息\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.466124234470691in\u0026rdquo;}\nI-TASSER 除 了预测出了三级 结构，同时还预测除了二级结构和残基 可溶性\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0588713910761154in\u0026rdquo;}\n构建模型所 使用的模板及序列比对。 这些模板和序列别对不 是通过序列相似性创建 的，而是用穿线法穿出 来的\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2921905074365703in\u0026rdquo;}\nITASSER 给出了多个预测模型。这些模型根据 ITASSER 自带的质量评估系数 C-score 排序。C-score 从负 5 到正 2，分值越高模型越可信。除了 C-Score， 排名第一的模型还给出 了一个 TM-Score，这也是 评价模型质量的系数。 TM-socre \u0026gt;0.5 说明模型 具有正确的结构拓扑，可 信；\u0026lt;0.17 说明模型属于 随机模型，不可信。挑选 到合格的模型后，点击 download model 下载模型 的 PDB 文件\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1787762467191603in\u0026rdquo;}\n此外，I-TASSER 还为预测了该蛋白质的功能，它把排名第一的模型和 PDB 中 的结构进行了比较，把跟模型最相似的 PDB 结构找了出来。这样就可以通过这些已知结构 的功能他推测模型的功能了\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.161142825896763in\u0026rdquo;}\nI-TASSER 还预测出了模型可能结合的配体以及具体的配体结合位点。并给出了可能的功能\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1900153105861766in\u0026rdquo;}\n从头计算法QUARK 但是也有一些特殊的蛋白，穿线 法也解决不了。这种蛋白质，尽管可以用 ITASSER 做出模型，但如果看一下模型的质量评 估系数，都不合格。这时我们就得采取其他方法。可以尝试从头计算法（ab initio）\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.810605861767279in\u0026rdquo;}\n也就说，当 一个蛋白质被翻译出来之后，它上面的 氨基酸排列顺序就已经决定了这个蛋白 质长什么样子。一个蛋白子所应该具有 的天然模样就是它的氨基酸序列所能摆出的自由能最低的 pose，因为能量低意味着稳定和持 久。所以从头计算法会模拟肽段在三维空间内所有可能的存在姿态，并计算每一个姿态下的 自由能，最终给出自由能最低的那个姿态作为预测结果\nQUARK的介绍 https://zhanggroup.org/QUARK/\n注意，QUARK 只能为长度在 200 个氨基酸以内的蛋白质预测结 构。输入序列和用户信息。QUARK 需要单独注册，ITASSER 的账号在这里不通用。QUARK 需要很长的计算时间，大约需要 2 天以上才能计算出结果\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.208616579177603in\u0026rdquo;}\n结果页面里，QUARK 会给出排名前十的预测模型，根据每个模型的 TM-score 可以查 看模型是否合格可用\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.2631222659667545in\u0026rdquo;}\n综合法ROBETTA http://robetta.bakerlab.org/\n综合法（ensemble method）。 综合法综合了前三种方法，将氨基酸序列分段，情况不同的片段采用不同的方法预测;ROBETTA 是一款使用综合预测三级结构的软件，它综合了同源建模法和从头计算法两 种方法。能找到模板的区域用同源建模法，找不到的区域用从头计算法\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0375481189851268in\u0026rdquo;}\nROBETTA 需要的等待的时间是四种方法里最长的，首先任务要排队，至少要排两个小 时。排队排上之后，还要等几个小时，才会出现分析页面。分析页面里，输入的氨基酸序列 被分成了几段，比如前两段适合用从头计算法预测，后两段适合用同源建模法预测。 之后，点击分段链接，逐段进行预测。每一段的预测时间都不短。同源建模法预测的，需要 几个小时到几天的时间，从头计算法预测的，需要几天到几周的时间，这取决于目标序列的 预测\u0026quot;难易\u0026quot;程度\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3604396325459316in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.270201224846894in\u0026rdquo;}\n到底用谁 要预测一个蛋白质的三级结构，首先看能不能 找到一致度大于等于 30%的模板，如果能，用 Swiss-model，又快又准。如果不能，再看能 否舒用 I-TASSER 预测出合格的模型。如果能，搞定。如果不能，再看氨基酸序列的长度是 否小于 200，如果小于 200，用 QUARK。如果大于 200，用 ROBETTA！如果，ROBETTA 也做不出质量合格的模型，那说明蛋白质太特殊了，现有的软件都不适用。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.945774278215223in\u0026rdquo;}\n模型质量评估 模型质量评估软件并不比较预 测模型跟真实结构之间的差别大小，而是从空间几何学、立体化学和能量分布三方面评估一 个模型的自身合理性\n大多数预测软件都自带模型质量评估系数，比如 I-TASSER 有 C-score，Swiss-Model 有 QMEAN score，QUARK 有 TM-score 等；这些系数可以一定程度上反应模型的质量， 但不能完全反应。至少要有三个评估体系都认为模型可靠，模型才真的可靠。这就需要借助 第三方的评估软件\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4447364391951005in\u0026rdquo;}\nSAVES https://saves.mbi.ucla.edu/\nVerify3D 会根据模型质量，返回一个 3D-1Dscore。只有超过 80%的残基拥有大 于 0.2 的 3D/1D score，才能被认为是质量合格的模型。从 3D-1Dscore 分布图上可以查看， 模型中哪些位置分值比较低，然后针对这些低质量区域进一步修正\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.455648512685914in\u0026rdquo;}\nPROCHECK 可以做出模型结构的拉氏图。拉氏图检查 Cα的两面角是否合理。 合格的模型超过 90%的残基都应该位于红色的允许区域和正黄色的额外允许区域内。落到其 他区域的残基应当被查看并修正\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.40326334208224in\u0026rdquo;}\nProQ https://www.sbc.su.se/\n它的 评估结果十分明确，LGscore 和 MaxSub 两个值所处范围直接对应模型的好坏\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.0315573053368325in\u0026rdquo;}\nModFOLD https://www.reading.ac.uk/bioinf/ModFOLD/ModFOLD8_form.html\nModFOLD 通过 P-value，给出模型的可信度,但是 ModFOLD 较其他几种方法，评估需要较长的时间。其他的都是立等可取，ModFold 需 要半个小时。而且一个 email 地址一次只能提交一个任务.\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.172016622922135in\u0026rdquo;}\n三级结构的比对 蛋白质三级结构的比对不同于序列比对，它是针对蛋白质三维空间结构的相似性进行比 较，是蛋白质结构分析的重要手段之一。结构比对可用于探索蛋白质进化及同源关系、改进 序列比对的精度、改进蛋白质结构预测工具、为蛋白质结构分类提供依据，以及帮助了解蛋 白质功能等。结构比对的结果可以用很多种参数来衡量，最常用的是 root mean squared deviations (RMSD)。如果两个结构的 RMSD 为 0 埃，那么就认为它们结构一致，可以完全重 合。一般来说，RMSD 小于 3 埃时，认为两个结构相似\nSuperPose叠合 SuperPose 是一款在线蛋白质结构叠合软件（http://wishart.biology.ualberta.ca/SuperPose/）， 可以将两个结构比对在一起，并给出两者之间的 RMSD。以课程附件中的 A.pdb 和 B.pdb 为 例，分别上传，提交\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2431999125109363in\u0026rdquo;}\n结果页面里，3D 可视插件需要 java 支持。弹出窗口问是否加载所有模型，选择 yes。3D 窗口里可以看到红绿两个蛋白，即蛋白 A 和蛋白 B，被叠合在一起了。蛋白 A 和蛋 白 B 整体的结构还是比较相似的，但局部仍有差异。点击 RMSD Report 可以查看两者间具 体的 RMSD 值\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.914099956255468in\u0026rdquo;}\nRMSD Report 显示，如果考虑所有原子的话，整体结构两者间的 RMSD 是 5.55 埃，如果只考虑碳骨架的话，整体结构两者间的 RMSD 是 5.08 埃。通过 RMSD 值，我们可 以认为这两个蛋白质结构比较相似，但不是特别相似\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.109912510936133in\u0026rdquo;}\n在叠合的结果中，蛋白 B 没有移动位置，只有蛋白 A 移动了位置。所以只需要把移动 后的蛋白 A 的 PDB 文件保存下来，再与原始的蛋白 B 的 PDB 文件同时用可视软件，比如 VMD，打开，就可以进一步分析叠合后的结果了\nSPDBV选择叠合 SPDBV 是一款拥有强大结构分析功能的绿色软件，无需安装，下载后直接运行即可 （http://spdbv.vital-it.ch/）\nSDPBV 有分子查看器的功能，也是一个蛋白质同源建模 平台，同时它还能为两个结构进行整体智能叠合，或者进行选择性叠合。课程附件中的 C.pdb 和 D.pdb 这两个蛋白除了一个 loop 区域结构不同之外，其他区域的结构都是完全相同的。 如果我们想要把相同的结构分毫不差的叠合起来，以看清楚不同区域的差别的话，就需要用 到选择性叠合\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2603805774278216in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2199278215223095in\u0026rdquo;}\n两个蛋白叠合在一起了，但是 Magic Fit 考虑的是整体结构，所以 CD 两个蛋白中完全 相同的结构部分并没有被严丝合缝的重合在一起\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.209468503937008in\u0026rdquo;}\n尝试选择性叠合：\n首先需要选中参与叠合的氨基酸，也就是结构完全相同的这部分氨基酸（图 2）。结构 不同的这块不参与叠合的过程，但是会随着叠合同步移动\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.10090769903762in\u0026rdquo;}\n蛋白质分子表面性质 VMD创建PSF文件 蛋白质分子表面性质是蛋白质结构的重要研究内容之一，对了解蛋白质的功能至关重要。 蛋白质的表面性质包括表面性状、表面电荷分布、表面残基可溶性等。表面性状可以用 VMD 的 Surface 这种 Representation 表示，电荷分布和可溶性分布同样也可以用 VMD 计算并标识 出来\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;7.043827646544182in\u0026rdquo;}\nAPBS计算表面电荷分布 1.载入PDB文件 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.5195220909886265in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 2.载入 PSF 文件 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.096397637795276in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 3.设置 APBS 插件 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.980144356955381in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 4.计算电荷分布 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.8975896762904636in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 5.查看计算后新生成文件 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.2069433508311462in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 6.加载文件 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.5225842082239724in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 7.显示电荷分布 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;0.7475929571303587in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;7.107279090113736in\u0026rdquo;}\n蛋白质四级结构 蛋白质四级结构是独立的三级结构单元聚集形成的复合物，其中每个独立三级结构称为 亚基，也称为单体（monomer）。含两个亚基的蛋白质称为二聚体（dimer），比如图中 Toll 样受体 3 的两个单体聚集在一起形成二聚体识别病原双链 RNA 的结构。再有比如血红蛋白 的四聚体结构（tetramer）和热休克蛋白的六聚体结构（hexamer）\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.5637849956255467in\u0026rdquo;}\n如果蛋白质三级结构异常聚集，形成了不该形成的四级结构，就会导致疾病的发生。比 如诱发神经系统退行性病变的淀粉样蛋白 A 蛋白，是蛋白质序列相同但四级结构不 同而诱发疾病的典型代表。阿尔茨海默症在发生过程中会出现 A 蛋白。A 蛋白是由特殊水 解酶对其前体蛋白的水解作用产生的。A 蛋白有两种构象，一种为螺旋且可溶，存在于健康 的个体脑组织中，这类 A 蛋白为单体没有四级结构。另一种构象为片层，且是多个 A 蛋白 聚集形成的链间片层，这类 A 蛋白不溶，并且出现在阿尔茨海默症患者的脑组织中。诱发 A 蛋白从可溶螺旋转变成不溶片层聚集体的机制目前尚不清楚，但已被广泛证实，这种构象的 转变是阿尔茨海默症的重要诱因\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.2235290901137357in\u0026rdquo;}\n在研究蛋白质结构功能时，可通过实验方法获得它们的四级结构。实验方法主 要是采用 X 射线衍射法。这种方法可以获得复合体的真实结构，但是技术难度较大，主要 是因为复合体很难获取并成功结晶。此外还有冷冻电子显微镜技术，但这种方法不能像 X 射线衍射法像那样获得精准的真实结构。它只能捕获类似影子的外形轮廓，之后，再根据已 有的同源蛋白质晶体结构对影子中的单体进行同源建模，再把模型嵌入到影子里，以此建出 复合体的模型\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.4019597550306213in\u0026rdquo;}\n目前通过试验方法获得的四级结构都可以从 PDB 数据库中找到。此外，还有专门的蛋 白质相互作用关系数据库。比如，DIP 数据库，专门用于存储实验方法测定的蛋白 质之间的相互作用。BioGRID 数据库主要收集模式生物物种中涉及的蛋白质间相互作用。 STRING 数据库除了实验方法测定的蛋白质间相互作用外，还储存已发表的用计算方法预测 的蛋白质间相互作用\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.366846019247594in\u0026rdquo;}\n蛋白质-蛋白质分子对接 除了实验方法和从数据库中获取，还可以通过计算机预测蛋白质的四级结构。这种预测 技术叫分子对接（molecule docking）\nDocking 一词原意指船停泊进船坞，在生物信息学领域 docking 是指一个分子结合到另一个分子上。目前有很多做分子对接的软件，这些软 件的基本思路都是尝试分子间所有可能的结合方式，并根据结合后能量的高低给每种结合方 式打分、排名。分子结合后，能量越低说明这种结合方式越稳定。软件在对接的过程中会考 虑以下因素：1）形状互补，2）亲疏水性，3）表面电荷分布\n{width=\u0026ldquo;5.083333333333333in\u0026rdquo; height=\u0026ldquo;2.5277777777777777in\u0026rdquo;}分子对接分为蛋白质和蛋白质之间的分子对接以及蛋白质和小分子之间的分子对接。蛋 白质和蛋白质之间的分子对接又分为两种不同的方法，即刚性对接（Rigid Docking）和柔性 对接（Flexible Docking)。刚性对接是指对接过程中蛋白质是硬的，外形不可变，柔性对接 则反之。刚性对接实际上是一种近似计算。因为蛋白质在机体环境中是软的，他的表面是在 微小移动中保持着动态平衡。但是要在对接过程中考虑到这个实际情况则需要庞大的计算量， 所以能够模拟蛋白质真实状态的柔性对接软件非常少，且都是收费的软件。目前可用的大多 数免费软件都是刚性对接软件\n蛋白质和蛋白质之间的刚性对接软件常用的有 ZDOCK（http://zdock.umassmed.edu/） 和 GRAMM-X（http://vakser.bioinformatics.ku.edu/resources/gramm/grammx），这两个都是免 费的刚性对接软件。HadDock（http://haddock.chem.uu.nl/）可以做蛋白质局部柔性对接，但 需要和开发者联系以获取免费的使用权限。这些软件的输出值都是根据能量高低排序的多个 对接模型，能量低者排名靠前。至于选取哪个模型作为最终结果需要综合考虑各种因素，并非一定选取排名第一的模型。下面以 ZDOCK 为例，来演示蛋白质-蛋白质间的刚性对接。 ZDOCK 是一个全自动的在线对接工具\n常用对接软件ZDOCK 第一步，上传要对接的两个蛋白质结构： ZdockA.pdb 和 ZdockB.pdb（见富文 本中的附件）。接下来输入邮箱。注意邮箱必须输入，否则无法提交。对接需要大约 30 分钟， 完成结果链接会发送至填写的邮箱。最后的点击提交 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.7893930446194224in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 第二步，选择或者排除残基参与蛋白质相互作用。比如，从发表文献中已知 732 号赖氨酸是参与蛋白质相互作用的重要氨基酸，此时则需要在\u0026quot;Select Binding Site Residues\u0026rdquo; 窗口里选中 732 号氨基酸。点击提交BePISA \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.7444641294838146in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 第三步，确认选择无误后，点 ok。半小时后你的结果会显示在页面下方提供的网址中 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.8841382327209097in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 对接结果中，ZdockA.pdb，也就是 receptor 的位置没有改变，ZdockB.pdb，也就 是 ligand 产生了 500 个可能的对接状态，点\u0026quot;Top 10 Predictions\u0026quot;可下载排名前十的对接结 果。或者从输入框里输入数字以选择下载排名第几的对接结果。如果要下载更多或者全部 500 个，需要运行页面上的 JAVA 插件。一个对接结果模型对应一个 PDB 文件，每个 PDB 文件里都包含两条链，分别对应 receptor 和 ligand。综合考虑了各种因素之后，我们最终挑 选了排名第一的对接模型。接下来就可以对这个预测出来的复合体结构进行各种分析了 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.895980971128609in\u0026rdquo;} 蛋白质-小分子分子对接 AutoDock 相互作用面分析 PDBePISA 用 ZDOCK 软件预测出两个蛋白质最有可能的结合方式之后，还需要对预测的对接结 果进行进一步分析。PDBePISA（http://www.ebi.ac.uk/pdbe/pisa/）能够在线分析蛋白质复合 体中的相互作用面\n1.打开 PDBePISA 主页，点击\u0026quot;Launch PDBePisa\u0026quot;，启动在线分析网页 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.805002187226597in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 2.输入网页上上，可以输入 PDBID 查看现有复合体结构的相互作用面，也可以点 \u0026ldquo;Coordinate file\u0026rdquo;，上传本地的复合体结构。本地的复合体结构既可以是下载到本地的 PDB 数据库里的结构，也可以是通过预测软件，如 ZDOCK，预测出的结构。点击\u0026quot;选择文件\u0026quot;， 选取课程附件中的\u0026quot;complex.pdb\u0026quot;，即上一节课 ZDOCK 预测结果中，排名第一的复合体结 构。选取正确文件后，点击\u0026quot;Upload\u0026quot;上传文件 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2087817147856517in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 3.上传成功后，点击\u0026quot;Interfaces\u0026quot;查看相互作用面情况 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.8612423447069117in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 4.接合面列表（Interface List）里可以查看到蛋白质相互作用面的面积大小（Interface Area， A 2），以及该种对接方式下的自由能高低（ΔiG kcal/mol）。自由能越低，说明结构越稳定。通 常小于零的自由能才对应一个有意义的对接结果。如果自由能比较高，说明对接软件没有找 到合适的对接状态，这可能是软件的预测能力所限，但更可能是这两个蛋白质在天然环境中 不擅长结合在一起，或蛋白质三级结构有误。如要查看具体的结合位点，点击\u0026quot;Details\u0026quot;来 查看更多信息 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.968905293088364in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 5.\u0026ldquo;interfacing residues\u0026quot;列表里给出了以下信息：（1）两个蛋白质中所有参与相互作用 的残基，（2）这些残疾参与的化学键，比如氢键（H）、盐键（S）、二硫键（D）和共价键（C）， （3）这些残基埋入相互作用面中的面积比，竖线越多，说明埋入的面积所占比率越大。从 表中得知，732 号赖氨酸参与了相互作用并形成氢键。从氢键列表（Hydrogen Bonds）中可 以查看相互作用面上形成的所有氢键，以及形成氢键的原子和化学键的键长。如果有其他化 学键形成的话，也会有相应的列表出现 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1646052055993in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 6.页面上还提供了 Jmol 插件，以图形化显示出相互作用面和作用面上的残基 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.5796095800524936in\u0026rdquo;} 虚拟筛选 和反向对接 虚拟筛选的概念（Virtual screening VS） {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.577522965879265in\u0026rdquo;}\n说白就是使用计算机预测出哪些小分子与蛋白质结合最后，再买来这些小分子做实验验证\nZINC {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5573982939632547in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3442738407699037in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0543350831146108in\u0026rdquo;}\nZINC的使用例子 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.318856080489939in\u0026rdquo;}\n比如说我想知道某个蛋白除了能和苯甲酸结合外，还想知道还能和哪些结合；比如给出的结果文件中会含有全部的小分子配体，就是说结果是全部放在一起的，所以需要去写个程序分离出来\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2407403762029747in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5237554680664918in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0271926946631673in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.55549321959755in\u0026rdquo;}\nautodock vina可以进行虚拟筛选 看官方的教程\n反向对接 (Target Fishing) 基本介绍 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3827154418197725in\u0026rdquo;}\n靶标数据库scPDB 是画好了grid box的数据库\nhttp://bioinfo-pharma.u-strasbg.fr/scPDB/\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3230336832895886in\u0026rdquo;}\n分子动力学模拟 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2122747156605422in\u0026rdquo;}\nNAMD {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5090638670166228in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.392660761154856in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4291393263342083in\u0026rdquo;}\n高通量测序 （对于我，这章了解就好了) 基因组学与测序技术 Sanger 测序\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.018454724409449in\u0026rdquo;}\n高通量测序\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.314882983377078in\u0026rdquo;}\n得到一个整个的DNA，将其捣碎，加入测序相关的试剂进去，体系放在一个槽里，每个槽里就是每个测序反应（可加入荧光观察）； 会有个拍照系统，每个反应中每隔一段时间拍一张；\n测序在医学中有大作用\n数据本身的复杂性\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.381519028871391in\u0026rdquo;}\n测序本身都会有错误，所以在进行之前，要排查出来；\n海量数据的计算和挖掘成为主要瓶颈；用内存计算拼接；\n从头测序 de novo sequencing 片段化、零碎的信息拼接成染色体水平\nOverlap Graph : 基于read重叠区的，去找到他们的重合，然后再末端延申去获得这些片段;也就是说这里是将两两的序列作比较\nDe Bruijn Graph: 把read切成特定大小的长度\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.6531977252843393in\u0026rdquo;}\n都很难解决重复区域\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4084962817147857in\u0026rdquo;}\n重测序 转录组测序 测序对象是不是DNA序列，而是 DNA转录的产物\n表观基因组学 在DNA上的修饰，DNA上本身的甲基化（甲基化可以沉默基因的表达）、组蛋白的修饰，组蛋白的promoting的打开与否，\n来测试这种打开的信号\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.331119860017498in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.464051837270341in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.072636701662292in\u0026rdquo;}\n猛犸象基因组测序计划 简单的生物统计应用以及序列算法 Bayesj基础 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;8.125433070866142in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.6918602362204727in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.400382764654418in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.6757983377077865in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5953871391076118in\u0026rdquo;}\nBayes在生物学的应用 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.822579833770779in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0538134295713038in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2251596675415573in\u0026rdquo;}\n二元预测的灵敏度和特异度 基本介绍 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2344356955380578in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.480990813648294in\u0026rdquo;}\nA的灵敏度达到100%，说明它对发生很敏感，只要有发生就会探测到；A的特异度60%说明所引起的探测不一定是由于地震引起的\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3598140857392824in\u0026rdquo;}\n在生物学上的应用 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.343495188101487in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2347659667541557in\u0026rdquo;}\n位点特异性加权矩阵 每一行是一个LRR的序列（长度为11）（上半部分）, 来看看每一个氨基酸即20个氨基酸出现的概率（下半部分）；如第一行A在第一列（1）中 代表A这个氨基酸出现在第一个序列中的占比为0.3%\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0606332020997375in\u0026rdquo;}\n当作打分矩阵用来预测是哪里出现了LRR序列\n如：构造一个长度为11的小窗口，一个一个位置往后面扫描，每一次都打一次分，\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.589207130358705in\u0026rdquo;}\n如何打分：\n比如这个 LTVLMLLHNQL\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5772353455818022in\u0026rdquo;}\n在矩阵中第一列找到L中出现的百分比为： 75% 即 0.75；同样的方法找到后面的氨基酸的百分比之和，转为小数，相加求和\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.8992311898512684in\u0026rdquo;}\n量化的标准（阈值）的确定，即低于这个分值的不是LRR序列\n怎么找到一个合理的阈值（即灵敏度和特异性高）\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0096095800524933in\u0026rdquo;}\n看这两个的交点 （ 或者将这两条曲线叠加起来，纯数学求和，取最高值 ） {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1744925634295713in\u0026rdquo;}\n基本序列算法 基本概念 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.188888888888889in\u0026rdquo;}\n构建后缀树 后缀就是包含最后一个字符的子序列；\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.139086832895888in\u0026rdquo;}\n但要注意序列自己本身就是一个子序列\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.9196948818897637in\u0026rdquo;}\n将获得的后缀树画成一棵树，（相同的开头可以共用，注意都要从根点开始考虑）\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.7858081802274715in\u0026rdquo;}\n后缀树的使用 查找是否在序列中\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.7805358705161853in\u0026rdquo;}\n查找重复次数\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.7929286964129485in\u0026rdquo;}\n查找最长重复子序列\n先找到所有内节点，看其出现了最多字母的\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.861525590551181in\u0026rdquo;}\n$的作用\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0688932633420825in\u0026rdquo;}\n最高分-子序列 概念：\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.931441382327209in\u0026rdquo;}\n生物学的应用：\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.734915791776028in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.807363298337708in\u0026rdquo;}\n计算的复杂度\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.867911198600175in\u0026rdquo;}\n生物数据挖掘（这章讲的有点水） 大数据有四字箴言：大、快、杂、 疑，即大数据资料量庞大、变化飞快、种类繁杂、以及真伪存疑\n数据库系统 数据挖掘涉及三个领域：统计、数据库系统和机器学习。关于统计，有专门的统计课程 不属于这门课的主要讲授内容。这一章主要从数据库系统和机器学习这两部分入手来掌握数 据挖掘的基本方法。 数据库系统就是存放数据的数据库和管理数据库的管理软件加在一起，即，数据库+数 据库管理系统=数据库系统。这就像一个图书馆，除了书籍以外，还要有图书管理员，否则 书放不进去也拿不出来\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.278423009623797in\u0026rdquo;}\n数据库有很多类型，比如传统的关系型数据库，是以表格的形式存储数据的。还有近些 年越来越流行的面向对象型数据库，比如 XML 数据库。它是以 XML 格式存储数据的。Xml 格式的数据都是以尖括号括起来的标签开始，在标签名前加个斜线结束。标签中还可以再包 含子一级的标签。比如病人资料这个标签下就有两个病人标签，每个病人标签下还有四个记 录病人信息的标签。如果需要的话，疾病这个标签下还可以再加入更深一层的标签，比如疾 病的名字、分型等等。这样一层一层的，结构非常清晰而且灵活，特别适合存储复杂的生物 数据。这是传统的关系型数据库无法比拟的\n机器学习 主要任务 机器学习主要是设计和分析一些让计算机可以自动\u0026quot;学习\u0026quot;的算法。这些算法是一类从 数据中获得规律，并利用这些规律对未知数据进行预测的算法。比如有台电脑，我们想让他 学会辨认各种球，那么我们就拿来很多球让电脑学习，告诉它这样的球足球、这样的球是排 球，这样的球是篮球，这样的球是棒球。经过大量的学习后，电脑说，我已经学会了，可以 分清这四种球了。好，我们来考考他。学过的都能掌握，没学过的打死也不会，这就是机器 学习。如果机器学习到了一个很高的境界，能够主动学习了，并能正确掌握学习到的东西， 那就走向人工智能了。目前市面上比较火的公子小白就具备机器学习的功能，但可惜仍然是 比较初级的被动学习，还没有智能到主动学习\n现在回到最初的问题上，看看这个电脑是怎么学会识别各种球的。电脑没有眼睛，所以 我们也不是真的把球摆到电脑面前让他看。我们实际上是把电脑学习的物体转化成了向量， 让电脑读取向量值，也就是用向量来描述物体。我们可以用一个 5 维的向量来描述一个球。 这五个维度分别描述了球的直径、重量、颜色、材质和纹路。这样一个向量足够将各种球区 分开了。几乎所有物体我们都可以把他转化成多维向量。比如图片可以转换成颜色柱状图， 并由此创建一个 36 维的向量，一个维度对应一种颜色，每一维度上的值代表这种颜色在图 片中出现的频率。再比如，基因表达水平，可以用描述基因芯片上每个点的颜色及深浅的向 量来表示。甚至我们教室里的每一个人都可以向量化\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.9564643482064743in\u0026rdquo;}\n机器都能学些什么，也就是机器学习的任务。常见的机器学习的任务 有分类、聚类和回归。分类和聚类虽然名字很像，但他们的区别还是巨大的。之前教给小电 脑完成的任务就是分类任务\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4433650481189852in\u0026rdquo;}\n分类任务要有足够的背景知识去训练电脑，告诉电脑这个样的都是篮球，这个样的都是 足球，这样的都是排球。然后拿学习过的这些球以外的球，让电脑判断是哪一种球，这就是 分类\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.098248031496063in\u0026rdquo;}\n再来看聚类，要理解聚类可以设想这样一个场景，一个外星人来到地球，对球状物体非 常痴迷，从地球上搜集了大量的球状物体。它不知道这些球状物都是什么，为了更好的研究 它们，外星人把长得差不多的球都放在了一起，并且将它们命名为球 1、 球 2 和球 3。这个 过程就叫聚类。由此可见，聚类和分类最大的区别就是聚类不需要背景知识，而分类需要\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.219550524934383in\u0026rdquo;}\n最后看看回归。回归跟分类一样，也是通过对背景知识的学习达到预测未知的目的。只 不过，这次预测的不是属于哪一类，而是预测一个具体的数值。比如房产公司可以根据房屋 情况利用回归算法估算房屋价格。房产公司拥有大量已成交房屋的数据，包括房子大小、地 皮大小、卧室数量、建材是否是花岗岩、有没有地下室以及房屋的最终售价。现在他们想要让电脑学习这些数据，从而建立起房屋的各项指标和价格之间的定量关系，这样他们就能够 更加准确的给待出售的房屋定价了。通过机器学习，最终返回了一个公式。公式体现了 x1、 x2、x3、x4 和 x5 与 y 之间的定量关系。对于新登记的房屋，只要把 x1 到 x5 这五个指标数 值带入公式，就可以估算出房屋的售价\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.138747812773403in\u0026rdquo;}\nK次交叉检验 回归和分类都需要背景知识，也就是训练组数据来训练出预测模型，而聚类不需要。预 测模型训练好之后还需要从训练组数据中拿出一部分作为测试组数据来测试模型的准确度。 这里问题就来了。哪些数据可以用来做测试组呢？\n理论上，所有已知结果的数据都应该拿来训练，这样训练才充分。可是，这些训练数据 以外的都是不知道结果的，无法拿来做测试？用训练数据本身做测试，肯定不行！因为你的 算法就是基于你的这组数据产生，肯定适合你的这些数据，但是换这些数据以外的数据可能 就不适合了，属于过学习。那么，我们从训练组数据里拿出一小部分做测试用，用剩下的做 训练，这样是否可行呢？也不行，因为有一部分知识预测模型没有学习到，属于欠学习\n这时，我们可以使用 K 次交叉检验（K-fold cross validation）。所谓 K 次交叉检验就是把 所有能够搜集到的已知结果的数据，分成 K 份。我们以 4 份为例。将第一份拿出来作为测 试组数据，其余 3 份作为训练组数据。用选定的算法根据训练组数据训练出一个模型。再用 测试组数据测试模型的准确度。然后将第二份拿出来作为测试组数据，其余 3 份作为训练组 数据。用选定的算法根据训练组数据训练出一个模型。再用测试组数据测试模型的准确度。 依次类推，让每一份都作一次测试组数据。这样就会用同一种选定的算法构建出 4 个模型， 进行 4 次测试，得到 4 个准确度。最后取 4 个准确度的平均值，得到的平均准确度就是用全 部训练组数据训练出的最终模型的准确度。利用 K 次交叉检验可以有效避免过学习的现象。 因为测试组数据并未参与相应模型的构建。同时也避免了欠学习的现象，因为所有训练组数 据都学习到了\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.735444006999125in\u0026rdquo;}\n常见算法 机器学习能完成诸如分类、聚类、回归等任务。那他是如何完成这些任务的呢？是通过 各种机器学习的算法来完成的。常见的算法有贝叶斯、最近邻居、决策树、支持向量机、人 工神经网络、遗传算法等。我们没有足够的时间从理论上完全掌握这些算法，但是应该从科 普的角度至少知道他们是怎么回事。这里挑其中一二给大家介绍一下\n所谓贝叶斯法就是基于贝叶斯原理的一种概率统计算法。关于贝叶斯定理我们已经在前 面的章节给大家详细介绍过了。这里就不多说了\n{width=\u0026ldquo;5.305555555555555in\u0026rdquo; height=\u0026ldquo;2.5in\u0026rdquo;}\n再来看最近邻居法。它是把已知物体根据它们的属性，标记在一个坐标系中。比如各种 钉子根据它们的外形是红色这堆，各种螺丝是绿色这堆，蓝色这堆是各种螺母。因为螺母外 形上跟钉子和螺丝差别较大，所以在坐标系中，它离另两种物体比较远。接下来我们根据新 物体的外形把它也标记在坐标系中。然后看离它最近的邻居是什么，它就是什么。这里，离 新物体最近的是螺母，那么新物体就应被分类为螺母\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.9826541994750655in\u0026rdquo;}\n决策树非常好理解，给大家举个例子就明白了。这是个保险公司统计的被保人所处年龄、 所开车型与出险率高低的统计表。历年数据显示，23 岁开家庭车的司机风险高，17 岁开跑 车的高，简直就是马路杀手，43 岁开跑车的也高，看来跑车风险就是高。68 岁开家庭车的 低。32 岁开大卡车的也低。别问什么，这是德国教材上的例子。现在我们来看看保险公司 利用这些数据都做了什么。他们根据这些数据画了一棵树，并利用这棵树对即将投保的人进 行风险评估。先判断车型，大卡车的风险低，不是大卡车的再判断年龄，60 岁以上低，60 岁以下高。有了这棵树，只要知道新投保人的年龄和车型，就可以判断出风险高低。这就是 决策树\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.56538823272091in\u0026rdquo;}\n支持向量机和最近邻居差不多，也是把已知物体根据属性标记在坐标系中，然后画一条 线。画的这条线要尽可能的把不同物体全部分开。新物体落在线的哪一侧就属于哪种物体。 问题的关键就是这条线怎么话。绿色和蓝色两种画法肯定都没有黄色画法好。比如绿色这条 线，如果新物体落在这个位置，因为他在绿线的右侧，所以他会和黄点归入一类，但实际上 他跟这堆蓝点离得更近。如果，这条线画成黄线这样，就不会出现刚才的问题。所以我们要 画这样一条线，首先这条线要把不同的已知物体尽可能的分开，且线两边距离线最近的物体 到线的垂直距离要尽 可能的大。也就是所 谓的间隔最大化，也 叫做最大最小距离。 只有这样画出线才能 成功完成任务\n{width=\u0026ldquo;5.666666666666667in\u0026rdquo; height=\u0026ldquo;3.1666666666666665in\u0026rdquo;}\nWEKA 介绍一款做数据挖掘的傻瓜级软件 WEKA\nhttp://www.cs.waikato.ac.nz/ml/weka/\nWEKA 的全名是怀卡托智能分析环境。WEKA 也是新西兰一种鸟的名字。WEKA 的主要开 发者来自新西兰怀卡托大学。WEKA 是免费的，它可以完成各种各样的数据挖掘任务，就 像傻瓜相机一样，算法的事儿完全不需要你操心，你只要输入数据，告诉 WEKA 你要完成 什么样的挖掘任务，再选择现成的算法，WEKA 就会为你返回想要的结果模型\nWEKA 不能读取 Excel 数据。WEKA 的数据 存储格式是 ARFF 格式。这种格式的文件其实就是一个纯文本文件，可以用写字板或记事本 打开。在 WEKA 安装目录下的 data 文件夹里有许多 ARFF 文件。我们用记事本打开其中的 weather_numeric.arff\n（右）是 ARFF 数据的原始容貌。我们也可以用 WEKA 的 ARFF viewer，将这些文本样式的数据转换成表格形式（左）。这组数据记录了不同的 天气与是否适合打球之间的对应关系。比如晴天，温度 85，湿度 85，不刮风的时候不适合 打球，而阴天，温度 80，湿度 90，不刮风的时候适合打球\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.348228346456693in\u0026rdquo;}\nWEAK中的术语 表格里的一行叫做一个实例（Instance），相当于统计学中的一个样本， 或者数据库中的一条记录。这组数据一共有 14 行，也就是 14 个实例。表格里的一列称为一 个属性（Attribute），这个表格一共有 5 个属性，分别是 outlook（天气概况）、temperature（温 度）、humidity（湿度），windy（是否刮风）以及 play（是否打球）。前四个属性是关于天气 的描述，最后一个属性是要判断的关键性问题。这个表格实际上就是反映了前四个属性跟最 后一个属性之间的关系。所以 WEKA 管一个表格叫一个关系（Relation）。这个关系的名字 是 weather（天气）\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2301662292213473in\u0026rdquo;}\nARFF文件的格式 WEKA 读取 ARFF 文件的重要依据是分行和空格，因此不能在这种 文件里随意的断行，以及随意加入空格。空行以及全是空格的行将被忽略。打开一个 ARFF 文件，经常会看到大段%开头的内容，这些是关于数据的注释。WEKA 在读取文件时会自动 忽略这些行。除去注释后，整个 ARFF 文件可以分为两个部分，第一部分头信息（head information）是对关系和属性的定义，第二部分数据信息（data information）就是数据值\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.330139982502187in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.262023184601925in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.249320866141732in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.19873687664042in\u0026rdquo;}\n属性类型和转换 ARFF 格式中共有四种属性类型，分别是数值型（numeric）、标称型（nominal）、字符串 型（string）和时间日期型（date）\n数值型 numeric {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2014971566054244in\u0026rdquo;}\n标称型 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2938462379702536in\u0026rdquo;}\n字符串型 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.220544619422572in\u0026rdquo;}\n时间日期型 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2407403762029747in\u0026rdquo;}\n转换 WEKA 虽然不能读取 Excel 文件，但是它可以读取 csv 文件。更加值得庆幸的是， Excel 也可以读写 csv 文件。比如示例文件 test.xls 是 Excel 文件。数据的第一行说明了 每一列是什么内容，之后共有四行数据。我们可以将这个 Excel 文件另存为 csv 文件，起名 为 test.csv。csv 文件是用逗号分隔各列的纯文本文件，可以用记事本打开，也可以用 Excel 打开，还可以用 WEKA 的 Arff viewer 打开。用 WEKA 的 Arff viewer 打开 test.csv 之后， 就可以通过 save as（另存为）将数据保存为 ARFF 文件了，起名为 test.arff。用记事 本打开 test.arff，可以看到 WEKA 将文件名作为关系的名字，并且根据原来 Excel 里第 一行的内容自动定义了属性，并根据后面数据的情况，自动给属性分配了属性类型。注意， 自动分配的属性类型可能不准确。比如 name（名字）这个属性，很显然不应该是标称型的， 而应该是字符串型的，所以需要人工更正。接下来的数据部分会自动用逗号分隔，缺失数据 也会自动填补问号\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3501115485564306in\u0026rdquo;}\nExplorer界面介绍 数据准备好了，我们可以开始挖掘了。 WEKA 主窗口打开 Explorer 界面。Explorer 界面可以帮助我们完成数据预处理和各种挖 掘任务。点 open file，载入我们要挖掘的数据，weather_numeric.arff 文件\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3918864829396327in\u0026rdquo;}\n数据 载入后，整个界面根据功能不同，可分成 8 个区域\n区域 1 的几个选项卡是用来切换不同的挖掘任务面板的。我们当前所处的面板是数据预 处理面板，可对数据进行预览和预处理。后面还有分类任务面板，聚类任务面板等 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.303781714785652in\u0026rdquo;} \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1282852143482063in\u0026rdquo;} \u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.381901793525809in\u0026rdquo;} \u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.572516404199475in\u0026rdquo;} \u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.876922572178478in\u0026rdquo;} \u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.753149606299212in\u0026rdquo;} \u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; 区域7 下方的直方图会给出具有不同outlook标称的实例中play 是yes 和no 的分布情况。 比如 outlook 是 sunny 的这五条实例中，play 是 yes 的是蓝色这些，play 是 no 的是红色这些。 outlook 是 overcast 这 4 条实例中，play 都 是蓝色的，也就是都是 yes。Outlook 是 rainy 的 5 条实例中，play 是 yes 的是蓝色的三条， play 是 no 的是红色的两条\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.125in\u0026rdquo;}\n\u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; 现在我们点 Visualize All 看一下所有属性的直方图。这时看到，标称型属性和数值型属 性的直方图是有差别的，数值型的直方图是根据平均值划分出两个区段，分别统计每个区段 里 class 属性的分布。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4766666666666666in\u0026rdquo;}\n\u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.688869203849519in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 数据预处理 在进行挖掘任务之前，通常还需要对数据进行预处理，比如更换属性类型或者增加删减 属性等。这些预处理工作主要是通过 Explorer 界面下的 Filter 下拉菜单里的各种函数来实现 的。比如在实际应用中，我们经常会需要把数值型的属性改成标称型的属性。这时可以用 Filter 下的 unsupervised 下的 attribute 下的 discretize 离散化函数来实现\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.389717847769029in\u0026rdquo;}\n选中 discretize 函数后，点击选中后出现的参数框。弹出参数设置窗口。从 AttributeIndices （属性代号参数）指定要更改哪个属性的属性类型。比如我们更改第二个属性 temperature （温度）和第三个属性 humidity（湿度），这两个数值型的属性，那么这里就写\u0026quot;2,3\u0026rdquo;。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.350938320209974in\u0026rdquo;}\ndiscretize 函数会将所有实例中对应属性下的数值离散化成几个区段，每个区段赋予一个标称，同一区 段里的数值都转化成这一区段的标称。所以我们还需要指定一下，要离散化成几个区段。这 里我们定为 3 个，那么新属性将具有 3 个标称。其他参数不变，点 ok，窗口关闭，再点 apply\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.28882217847769in\u0026rdquo;}\n现在看一下 temperature（温度）这个属性，原来是数值型的。离散化函数处理之后，变 成了标称型。函数将所有温度数值离散化成了三个区段，\u0026rsquo;(-inf-71]\u0026rsquo;（温度值小于 71） 的都归入了第一区段，拥有第一个标称。标称的具体写法虽然怪异，但是它很清楚的告诉了 我们哪些数值归入了第一个标称。\u0026rsquo;(71-78]\u0026rsquo;（温度值在 71 到 78 之间）的归入第二区段， 拥有第二个标称；\u0026rsquo;(78-inf)\u0026rsquo;（温度值大于 78）的归入第三区段，拥有第三个标称\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.941175634295713in\u0026rdquo;}\n可以看到，现在标称型的温度属性的直方图已经变成三个离散的柱子了，而不再是根据 平均值划分统计了。除了 discretize 离散化函数，NumericToNominal 函数也可以将数值型的 属性转化成标称型。这两个函数虽然达到的最终目的是一样的，但是具体的转化方法是不一 样的。究竟哪里不一样，请大家自己尝试比较一下\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3717694663167106in\u0026rdquo;}\n通过 AddExpression 函数可以增加一个属性。比如增加新属性的值等于温度除以湿度。 重新打开 weather_numeric.arff 文件。此时，温度和湿度的属性都是数值型的。Filter →unsupervised→attribute→AddExpression。从参数设置窗口设置新属性的公式，温度除以湿 度，也就是第二个属性 a2 除以第三个属性 a3，即a2/a3。再定义新属性的名字为 temp/humi。 点 ok，点 apply，新属性就产生了。 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.350224190726159in\u0026rdquo;}\n注意新创建的属性都会添加在属性列表的最后，这就会 影响 WEKA 对 class 属性的判断，需要手动将 class 属性重新选为 play\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.241373578302712in\u0026rdquo;}\n执行挖掘任务 WEKA 把分类和回归两种挖掘任务都放在\u0026quot;Classify\u0026quot;选项卡中，是有原因的。因为这 两种挖掘任务都有一个目标属性，也就是输出变量，并且都是根据实例的一组特征属性，也 就是输入变量，对目标属性进行预测。比如之前讲过的分类例子是根据各种天气特征属性预 测是否打球这个目标属性。回归例子里是根据房屋特征属性来预测房屋价格这个目标属性。 为了实现这一目的，首先需要有一个训练数据集，这个数据集中每个实例的输入和输出都是 已知的。观察训练集中的实例，可以建立起预测模型。有了这个模型，就可以对新的输出未 知的数据进行预测了。衡量模型的好坏就在于预测的准确程度如何。目标属性是标称型的， 就是分类任务，目标属性是数值型的就是回归任务。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.218390201224847in\u0026rdquo;}\nWEKA 自带的典型分类算法有贝叶斯、人工神经网络、支持向量机、决策树等等。典 型的回归算法有线性回归和简单线性回归等\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.346788057742782in\u0026rdquo;}\n一个具体的任务\nWEKA 的安装目录下有一个有关糖尿病的 ARFF 文件：WEKA 安装目录/data /diabetes.arff。里面记录了 768 位 21 岁以上女性的 8 种生理指标跟糖尿病检测结果是 阴性还是阳性之间的关系。我们要用这组数据来创建一个预测模型，通过这个模型只要输入 某位未做过糖尿病检测的女士的这 8 种生理指标就可以预测出她是否患有糖尿病\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.045634295713036in\u0026rdquo;}\n具体的操作\nExplorer 界面下，首先从 preprocess 标签下打开，diabetes.arff 文件，导入数据。然后在 classify 标签下选择分类算法。选 classifiers→tree（决策树）→random tree（随机决策树）来 创建预测模型。预测模型准确度的检验方法选 10 次交叉检验，再次确认目标属性是否选择 正确。最后点 start。任务开始后会看到 WEKA 鸟走了两步又坐下了，说明任务完成\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.1958333333333333in\u0026rdquo;}\n任务完成后，output 窗口里显示出了一个文本形式的决策树。这样的决策树方便进一步 编写程序以实现预测模型的自动化。此外，还有图形样式的决策树。从任务列表里找到刚刚 完成的任务，右键点击，选 Visualize Tree。这时一棵图形化的决策树就出现了。因为树太大， 窗口又太小，所以全都挤在一起了。为了看清楚，可以放大窗口，再右键点击，选 fit to screen 将树放大\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.260825678040245in\u0026rdquo;}\n有了这棵树，我们只需要输入 8 种生理指标，就可以从树根开始一层一层的判断直到末 端的叶子。叶子上写的总是目标属性，也就是预测的糖尿病检测结果是阴性还是阳性。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.2964370078740157in\u0026rdquo;}\n有了预测模型只是第一步，我们还需要知道预测模型的准确度如何，也就是判断模型是 否可信。10 次交叉检验的返回结果告诉我们，交叉检验下，68%的实例能够预测出正确结果， 32%的预测结果是错误的。除此之外，还提供了其他很多准确度衡量参数。交叉检验后得出的 Confusion Matirx 是个很有用的矩阵。它提供了 TPFPTNFN 的统计值，用以计算上述的各 种准确度衡量参数\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.708195538057743in\u0026rdquo;}\n由此得出的 TP、FP、TN、FN 可以带入公式计算各种准确度衡量参数，包括 sensitivity 和 specificity。当把 test_negative 定义为阳性结果，test_positive 定义为阴性结果的时候，会得 出另外一套 TP、FP、TN、FN，从而计算出另一套准确度衡量参数。两次统计结果可分别 体现出目标属性中每个标称的预测质量\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;6.325597112860892in\u0026rdquo;}\nLinux {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.6345844269466316in\u0026rdquo;}\n有一定的基础所以不做笔记了\nPerl 可以去看其他的课程\n","date":"2021-11-04T00:00:00Z","permalink":"https://example.com/p/%E5%B1%B1%E4%B8%9C%E5%A4%A7%E7%94%9F%E4%BF%A1note/","title":"山东大生信note"},{"content":" 写在前面的话：这个笔记不过多深入知识，因为做入门有，大概知道什么时候该用哪个，以及现在有什么技术\n序列比较 基础概念 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3103608923884513in\u0026rdquo;}\n序列相似性 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.227069116360455in\u0026rdquo;}\n相似的序列说明可能来自同一祖先而且可能具有相似的结构和功能\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3227121609798775in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3592727471566053in\u0026rdquo;}\n例子的一致度为：50%；\n替换计分矩阵 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4233748906386703in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.468004155730534in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2975951443569556in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.382060367454068in\u0026rdquo;}\nPAM后面的数体现的是序列差异度，而BLOSUM后面的数字体现的是相似性\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.6440846456692912in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0868766404199475in\u0026rdquo;}\n其他两种蛋白质序列比对的替换计分矩阵\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.353906386701662in\u0026rdquo;}\n那现在来解决下前面遗留的相似度问题：\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4432042869641295in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.406619641294838in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5194849081364827in\u0026rdquo;}\n（2+1）:代表两对相同的，一对相似的\n那么问题来了，两个序列的长度不相同怎么办呢？\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5037270341207347in\u0026rdquo;}\n先学习下两个序列的比较方法\n序列两两比较的方法 打点法 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.021797900262467in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.142396106736658in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.716813210848644in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3695647419072614in\u0026rdquo;}\n打点法在线软件 Dotlet [http://myhits.isb-sib.ch/cgi-bin/dotlet]\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3341786964129483in\u0026rdquo;}\nInput中复制序列进去\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0793339895013125in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.550419947506562in\u0026rdquo;}\n如果选其他的， 比如选择 15，那就是一次比较 15 个字母，也就是看 15 个字母长度的序列整体的相似度如 何来确定打不打点。注意这里不是比较完前 15 个字母，然后再从第 16 个字母开始比较后面 的 15 个字母，而是第 1 次比较第 1 到第 15 个字母，然后再比较第 2 到第 16 个字母，再是 第 3 到第 17 个字母，依次类推；\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3186100174978126in\u0026rdquo;}\n注意默认的颜色方案是在越相 似的地方打的点的颜色越浅，越不相似的地方颜色越深；\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.076650262467192in\u0026rdquo;}\n这里我们可以通过调整灰度条，来屏蔽大多数低分值的点，让他们统统变成黑色背景， 并且强化高分值的点，让他们以纯白色突出显示出来\n接下来打点两条独立的序列\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.522202537182852in\u0026rdquo;}\n这时我们看到除去主对角线外，还有很多条对角线（图 4）。说明序列中存在串联重复 序列。前面我们讲过，半个矩阵范围内，数数包括主对角线在内，有多少条等距平行线，就 说明重复了多少次，最短的平行线就是一个重复单元\n接下来使用串联重复序列\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.3725973315835525in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5668208661417324in\u0026rdquo;}\n序列比对法 但是用打点法只能让你大致了解两条序列是否相似，无 法定量的描述。如果想要精确地知道两条序列到底有多相似，就需要使用序列比对法 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.8823195538057744in\u0026rdquo;}\n就是通过插入空位，让上下两 行中尽可能多的一致的和相似的字符对在一起。这不是随便摆摆看看就能完成的，需要使用 专门的序列比对算法\n双序列全局比对以及算法 有替换积分矩阵以确定不同字母间的相似度得分，以及空位罚分,空位 罚分就是当字母对空位的时候应该得几分。我们还是希望一致或相似的字母尽可能的对在一 起，字母对空位的情况和不相似的字母对在一起的情况一样，都不是我们希望的，还是少出 现为好，所以通常字母对空位会得到一个负分，这个负分就叫做空位罚分。这里我们让空位 罚分，也就是 gap 分值为-5 分\n不过要注意，得分矩阵 p 和 q 的前面各留一个空列和一个空行，也就是第 0 列和第 0 行\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.985601487314086in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.919752843394576in\u0026rdquo;}\n我们再回过头来看一下第一行和第一列 （图 6）。其实，第一行的每一个值都是从左边的格加 gap 来的。所以我们给它们补上向左 的箭头。第一列的每一个值都是从上边的格加 gap 来的。所以我们给它们补上向上的箭头。 至此，所有的箭头和数值就都填好了\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.7441163604549432in\u0026rdquo;}\n追溯箭头是从右下角到左上角，但 是写全局比对是从左上角开始，如果是斜箭头则是字符对字符，如果是水平箭头或垂直箭头 则是字符对空位，箭头指着的序列为空位。我们看第一个是斜箭头，字母对字母，就是 A 对 A，第二个是水平箭头，字母对空位，箭头指着的序列是空位，也就是 C 对空位。然后 斜箭头 G 对 A，斜箭头 T 对 T，斜箭头 C 对 C，一直写到右下角，全局比对就出现了\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.999339457567804in\u0026rdquo;}\n双序列局部比对 得分越高，越相似，这个例子告诉我 们，对于像这样一长一短的两条序列，比较局部比比较全长更有意义。这就是为什么除了全 局比对，还有局部比对\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.294534120734908in\u0026rdquo;}\n局部比对的计算公式在全局比对的基础上增加了第四个元素\u0026quot;0\u0026quot;\n从 s(1,1)开始要选择四个值中的最大值。除了上面格 s(0,1)+gap=0+-5=-5，左边 格 s(1,0)+gap=0+-5=-5，斜上格 s(0,0)+w(1,1)=0+-3=-3，还有一个 0。max(-5, -5,-3,0)=0。并且这个 0 既不是从上面格，也不是从左边格，以及斜上格三个方向来的， 而是来自于公式里增加的\u0026quot;0\u0026quot;，所以不用画箭头\n与全局比对不同，局部比对的得分不是在右 下角，而是在整个矩阵中找最大值。这个最大值才是局部比对的最终得分，他可能出现在任，何一个位置。这次箭头追溯也不是从右下角到左上角，而是从刚刚找到的最大值开始追溯到 没有箭头为止。追溯箭头终止的位置也可以是得分矩阵中的任何一个位置。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4164982502187224in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.602577646544182in\u0026rdquo;}\n一致度和相似性 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.381994750656168in\u0026rdquo;}\n在线双序列比对工具 EMBL http://www.ebi.ac.uk/Tools/psa 首先看下全局alignment，输入值非常简单，把要比较的两条 蛋白质序列贴在输入框里或者上传。可以使用示例文件 global.fasta 里面的两条序列。如果想 要进一步设置比对的参数，可以点 More options。从这里可以选择使用哪种替换记分矩阵。 按照之前讲过的原则，选择 PAM 矩阵或 BLOSUM 矩阵。如果实在不知道选哪个矩阵，就 闭着眼睛选 BLOSUME62 吧！下拉菜单里默认选的就是 BLOSUM62。除了选择替换记分矩 阵，这里还可以设置空位罚分（gap open），也就是 gap 的分值。这里实际上是让你选空位对字母的情况 罚几分，所以显示的是正数，但在计算的过程中还是按照负数处理\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5439063867016625in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5545942694663166in\u0026rdquo;}\n结果：\n在结果页面里，上部是比 对使用的参数，以及得出的序列比对的长度、一致度、相似度、空位比例和比对得分。下部 就是序列比对。在序列比对里，左边是两条序列的名字，因为输入的是 FASTA 格式的序列， 所以程序自动识别出了序列的名字。右边的序列比对分 3 行。上下两行是序列，里面插入了 许多的空位。中间这行标记出了哪些位置上下两个字母是完全相同的，用竖线表示。上下两 个字母相似，用双点表示。上下不相似，用单点表示。字母对空位的情况，用空格表示。这 样，我们只要数数比对结果里竖线的个数（40 个），再除以比对的长度（196 个），就可计算 出一致度。再用竖线的个数加上双点的个数（40+29=69 个），除以比对长度（196 个），就 是相似度。整个比对里一共插入了 65 个空位，占整个比对长度的 33%。序列两边的数是这 一行中的字母在序列中的位置数，而不是这一行的长度。比如第二行是 seq1 的第 49 个字母 到第 97 个字母，是 seq2 的第 27 个字母到第 75 个字母\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.9740212160979875in\u0026rdquo;}\ngap EMBL 比对工具将 gap 分为两种，一种叫\u0026quot;gap 开头（GAP OPEN）\u0026quot;，另一种叫\u0026quot;gap 延长（GAP EXTEND）\u0026quot;（图 1）。gap 开头就是连续的一串 gap 里面打头的那一个，可以当 它是队长。gap 延长就是剩下的那些 gap，也就是队长后面跟着的小兵\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.141512467191601in\u0026rdquo;}\n这一串里，第一个 gap 是 gap 开头，后面的都是 gap 延长。单独的一个 gap 按 gap 开头算。gap 开头和 gap 延长 可以分别定义不同的罚分。默认情况下，gap 开头罚分多，gap 延长罚分少。全局比对的例 子里我们就是用这种搭配组合方案做出的比对。这次我们反过来试试，让 gap 开头罚分少， 让 gap 延长罚分多。比如 gap 开头选罚 1 分，gap 延长选罚 5 分，其他参数不变，再作一次 看看结果发生了什么变化\n当 gap 开头小，gap 延长大的时候，做出来的比对里面，gap 很分散，极少有连续长串 的 gap 出现（图 2-A）。开头的一串 gap 是个例外，因为 seq2 太短， seq1 的这一段只能跟 gap 相对。其他部分的 gap 都是分散出现的。这和我们第一次做出来的比对结果是截然不同的（图 2-B）。在第一次做的结果里，也就是 gap 开头大，gap 延长小的时候，gap 很集中，有很多成 长串出现的 gap。大家可以想想其中的奥妙。当 gap 开头大，gap 延长小的时候，说明在连 续的字母里插入一个 gap 打开一个缺口要付出很大的代价，因为 gap 开头罚分大。但是这个 缺口一旦打开了，也就是一旦有了第一个 gap，后面再接更多的 gap 就容易了，因为 gap 延 长罚分小。所以这种情况下，gap 都集中连成长串出现\n而反过来，当 gap 开头小，gap 延长大的时候，说明在连续的字母里插入一个 gap 打开 一个缺口很容易，并不需要付出太大代价，因为 gap 开头罚分小。但是想在第一个 gap 后面 再接一个 gap 就难了，因为 gap 延长罚分大。所以这种情况下很难有长串的 gap 出现，gap 每延长一个都要付出巨大代价。因此在第二次我们做的结果里都是分散的 gap。 除了开头一段因两条序列长短不同而不得已出现的长串 gap 外，没有其他的长串 gap 了\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.8938046806649167in\u0026rdquo;}\n这就是说，通过调整 gap 开头和 gap 延长，我们可以把序列比对做成我们期待的样子。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1637390638670166in\u0026rdquo;}\n举两个例子，看看应该怎样调整 gap 最合理:\n第一个例子，你知道要比对的两条序列很相似， 是同源序列，所以它们的结构和功能也应该都差不多。其中一条序列的结构已知，另一条未 知。你想把它们很好的比对在一起，用其中已知结构的序列做模板，来预测另一个序列的结 构。这时候我们期待得到的是 gap 分散的比对结果还是 gap 集中的呢？另一例子，你知道要 比对的两条序列绝大部分区域都很相似，但是其中一条序列的一个功能区在另一条序列中是 缺失的。你想要通过序列比对把这个功能区找出来。这时候我们要怎么设置 gap 开头和 gap 延长呢？这两个例子告诉我们，在实际应用中，需要根据不同的情况选取不同的 gap 罚分， 以满足不同的生物学意义。如果你对结果没有什么预期，那就请保持默认的参数。\n除此之外，结尾的 gap 也可以划分出不同的种类并赋予不同的罚分，如果把 END GAP PENALTY 选成 true，就可以设置结尾的 gap 罚分了。结尾 gap 不太常用，特别是在做亲缘 关系较近的序列比对时，是否设置结尾 gap，比对结果差别不大。\nEMBL 局部双序列比对 EMBL 的局部双序列比对工具可以选择经典的 Smith-Waterman 算法\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4712434383202098in\u0026rdquo;}\n从比对结果可以看出，只有中间黑色的相似的部分出现在比对结果中了，两头 红色的不相似的部分被忽略掉了。也就是只返回了局部最相似，得分最高的片段的比对结果。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.6701870078740155in\u0026rdquo;}\n用这两条序列再做一次全局比对，从两次的比对结果可以更清楚的看出，全局 比对里前面和后面对得不好的部分在局部比对里就都被忽略了\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4096686351706036in\u0026rdquo;}\n除了一长一短两条序列适合做局部比对，有的时候两条差不多长的序列也可以做局部比 对，以找出它们最相似的局部片段。为了让相似的部分突出出来，我们把 gap 都调大，gap 开头调到 10，gap 延长调到 5，提交。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.272180664916885in\u0026rdquo;}\n比对结果中，只有黑色的相似的部分出现在最终的比对结果中了，两头红色的不相似的 部分全部被忽略了\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.649401793525809in\u0026rdquo;}\n如果给这两条序列做全局比对的话，会发现，绝大部分位置对得都很差，只有中间这一 段对的还不错，所以，有时候两条序列并不同源，它们只是有一个功能相似的区域， 这时用局部比对我们就能很快找到这一区域在两条序列中的位置。但是如果做全局比对的话， 结果就不如局部比对明显了。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.247603893263342in\u0026rdquo;}\n其他在线序列比对工具 可以做双序列比对的工具很多（表 1）。不同网站都有自己的比对工具，所使用的算法 也不尽相同，但是它们的核心算法都是讲过的 Nidelmann-Wunsch 和 Smith-Waterman 算法\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.09249343832021in\u0026rdquo;}\nBLAST http://www.ncbi.nlm.nih.gov/\n简介 们已经学会如何做双序列比对，那可不可以拿一条序列和数据库中的每条序列逐一进 行双序列比对，通过这种方法来找相似呢？这确实是一个办法。这样我们只要根据比对后得 出的相似度排序，就可以找到最相似的那条序列了。但是，这种方法因计算耗时过长，只是 理论上可行而已。之前我们用 EMBL 的双序列比对工具做全局比对，虽然很快就出结果了， 但至少也要经历一两秒钟的时间。而数据库中有几百万条序列，全部比对一遍，耗时太长。 因此，我们需要快速的数据库相似性搜索工具。目前世界上广泛使用的就是 BLAST。它可 以在尽可能准确的前提下，快速的从数据库中找到跟某一条序列相似的序列\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3608442694663165in\u0026rdquo;}\n在实际应用 中，更多时候，输入值是一条序列。想要找到这条序列在数据库中的相关注释，是不能把这 条序列直接放到搜索条里搜索的。这个时候就得用 BLAST 搜索了！\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.55549321959755in\u0026rdquo;}\n。BLAST 从头至尾将两条序列扫描一遍并找出所有片段对，并在允许的阈值 范围内对片段对进行延伸，最终找出高分值片段对（high-scoring pairs, HSPs）\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4386646981627296in\u0026rdquo;}\nBLAST种类 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.441827427821522in\u0026rdquo;}\n为什么是 按 6 条链翻译？在无法得知翻译起始位点在情况下，翻译可能是从第一个碱基开始，三个三 个的往后翻译，也可能是从第 2 个碱基开始，也可能从第 3 个碱基开始。另外还有可能是从 这条链的互补链上开始，这样又有三个可能的开始位置，加起来一共会产生 6 条可能被翻译 出来的蛋白质序列。这 6 条中有些是真实存在的，有些是不存在，但是谁真谁假我们无从知 晓，所以 6 条序列都要到数据库中去搜索一下试试。接下来的问题是，既然是核酸序列，为 什么不做 BLASTn 直接到核酸数据库里去搜索，而是要到蛋白质数据库里搜索呢？我们说这 样做是有意义的，比如，从核酸序列数据库里找不到跟你手里这条核酸序列相似的序列，或 找到了相似的序列但这些找到的序列无法提供有意义的注释信息。这时，就可以去蛋白质数 据库试试，看看这条核酸序列的翻译产物能不能从蛋白质数据库里找到相似的序列以及有意 义的注释信息。或者说，你不是想找跟你这条核酸序列相似的核酸序列，而是想找跟你这条 核酸序列编码蛋白质相似的蛋白质序列，这时就要做 BLASTx\n反之，当你不是想找跟你手上这条蛋白质序列相似的蛋白质序列，而是想找跟编码这条 蛋白质序列的核酸序列相似的核酸序列的时候，就要做 tBLASTn\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.210229658792651in\u0026rdquo;}\nNCBI上的BLASTp NCBI 的 BLAST 工 具 为 例 尝 试 一 下 不 同 算 法 的 BLAST 工 具 （http://www.ncbi.nlm.nih.gov/）。BLAST 链接在 NCBI 主页右侧很显眼的地方。我们做 BLASTp（Protein BLAST），也就是用蛋白质序列搜索蛋白质序列数据库\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.8737740594925634in\u0026rdquo;}\n输入待搜索的蛋白质序列，这条序列可以在示例文 件 blast.fasta 里面找到。\n定搜索跟输入序列哪部分相似的序列，如果空着就是全长搜索。\n\u0026lt;!-- --\u0026gt; 如从50-100就是比较输入序列的第50位开始到100位相比 \u0026lt;!-- --\u0026gt; 给搜索任务起一个名字，如果输入的是 FASTA 格式的序列，那么在输入框里面点一下， 序列的名字就会被自动识别出来。\n如果在 Align two or more sequences 前面打勾的话，可以同时提交多个 BLAST 任务\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.282700131233596in\u0026rdquo;}\n被搜索的数据库（虽然是 NCBI 的 BLAST 工具，可以选择的数据库却不只 NCBI 下属的数据库，还包括其他组织的数据库，比如 PDB， Swissprot。事实上，各大数据库网站的 BLAST 工具都可以实现跨平台搜索）\nOrganism 可以把搜索范围限定在某一特定物种内，或者排除某一物种。\n在算法选择这一栏里，有之前提到的三种不同的 BLAST 算法，标准 BLAST，PSI-BLAST 和 PHI-BLAST。这一次我们先尝试标准 BLAST。所有参数设置完毕之 后，点 BLAST\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.210053587051618in\u0026rdquo;}\n在Graphic Summary结果里：\nBLAST 工具识别出输入序列的第 25 到第 170 个氨基酸这一段属于 TIR 蛋白质家族。这部分里彩色 线条构成的图告诉我们，一共从数据库中找到 50 个 hits，也就是高分匹配片段。注意这些 线代表的是 50 个高分匹配片段而不是 50 条序列。一个高分匹配片段有可能是一条全长的序 列，也就是全长匹配，也有可能只是某条序列的一部分，也就是局部匹配。代表这些高分匹 配片段的线拥有不同颜色和不同的长短。如果把鼠标放到某一条线上，可以看到这条匹配片 段的具体信息，包括他所在序列的数据库编号，序列的名字，匹配得分，期望值 E 值。匹 配得分在 200 以上的用红线表示，80 到 200 之间的用粉线，50 到 80 的绿线，40-50 的蓝线， 40 以下的黑线，所以颜色反映的是匹配的好坏程度。如果某一个高分匹配片段和输入序列 是从头到尾匹配，就是全长的线，比如最上面的三条红线。如果只匹配输入序列的一部分， 则是一条短线，短线所在的位置就是与输入序列匹配的位置\n{width=\u0026ldquo;5.777777777777778in\u0026rdquo; height=\u0026ldquo;9.208333333333334in\u0026rdquo;}\nDescriptions：\n是这 50 个高分匹配片段所在序列的详细信息列表。每条序列 都有一个匹配得分和覆盖度。这两项决定了第二部分彩图中每条线的颜色和长短。除了匹配 得分和覆盖度，表中还列出了其他指标。尤为重要的是 E-value。E-value 也叫做期望值或 E 值。E 值越接近零，说明输入序列与当前这条序列为同一条序列的可能性越大。\n第三部分的 表就是根据 E值由低到高排序的。随着 E 值增大，匹配得分是成反比逐渐降低的。但是一 致度与 E 值并非完全成反比。因为我们在前面讲 BLAST 核心思想的时候说过，BLAST 没有 做双序列比对，为了提高速度，它牺牲了一定的准确度。表中的一致度，是 BLAST 搜索完 成后，针对搜索到的这 50 条序列专门做双序列比对而得到的。BLAST 牺牲掉的准确度对高 度相似的序列，也就是亲缘关系近的序列构成不了威胁，不会把它们落掉，但是对于那些只 有一点点相似，也就是远源的序列，就有点麻烦了，它们很有可能被丢掉而没有被 BLAST 发现\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.635181539807524in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.333093832020998in\u0026rdquo;}\nNCBI上的PSI-BLAST 为了提高速度，标准 BLAST 牺牲了一定的准确度，牺牲掉的准确度对高度相似的序列， 也就是亲缘关系近的序列构成不了威胁，不会把它们落掉，但是对于那些只有一点点相似， 也就是远源的序列，就有点麻烦了。它们很可能被落掉而没有被 BLAST 发现\n要解决这个问题，可以用 PSI-BLAST。PSI-BLAST 可以搜罗出一个庞大的蛋白质家族， 当然也包括标准 BLAST 不小心漏掉的那些远房亲戚。换言之，标准 BLAST 找到了直接认识 的朋友，但朋友的朋友都丢掉了\n位置特异权重矩阵（Position-Specific Scoring Matrix，简称 PSSM）是 以矩阵的形式，统计一个多序列比对中，每个位置上不同残基出现的百分比。假设 A 的朋 友只有 B，B 朋友除 A 外还有 C。如果输入序列的第一个位置是 A，那么在第一轮没有 PSSM 辅助的情况下，只有第一个位置是 A 或 B 的序列被找到了。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.6025164041994753in\u0026rdquo;}\n用 NCBI 的 PSI-BLAST 工具再次搜索 blast.fasta 这条序列，仍然在 SwissProt 数据库 中搜索，算法选 PSI-BLAST\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.51123687664042in\u0026rdquo;}\n略有不同的是，在第三部分（Descriptions） 列表的最右侧多了两列。绿色标记的一列是将要为下一轮搜索创建 PSSM 所选择的序列，默 认第一轮搜索到的序列都将用来创建 PSSM。红色标记的一列是已在本轮搜索中用来创建 PSSM 的序列。因为是第一轮搜索，之前还没有搜索到任何序列，也就是第一轮的搜索过程；\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.435080927384077in\u0026rdquo;}\n接下来点\u0026quot;go\u0026quot;进行第二轮搜索。\u0026ldquo;go\u0026quot;左侧的输入 框里可以指定列出搜索到的前多少条序列。因为 PSI-BLAST 无休止的一直做下去，也许会 把数据库中的全部序列都找出来\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.514294619422572in\u0026rdquo;}\n红色标记的一列，也就是已在第二轮搜索中被用来创建 PSSM 的序列，大部分都打勾了。但是，再往后面会看到有些标黄的序列没有打勾。这些没 有打勾的标黄的序列就是在第二轮搜索中新找到的序列。它们将用于创建下一轮搜索使用的 PSSM，但是在本轮搜索中，它们没有被用到，所以没有打勾。而没有标黄的这些打勾的序 列，是在第一轮中就已经找到的序列。（好在通过 PSI-BLAST，它 们又被找回来了）\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.198655949256343in\u0026rdquo;}\nNCBI PHI-BLAST PSI-BLAST 是撒大网搜索，而 PHI-BLAST 则是精准搜索；其中，N 是天冬酰胺，P 是脯氨酸，S 是丝氨酸，T 是苏氨酸。{ }代表除大括号里的氨基酸 以外的任意氨基酸，[ ]代表中括号中的任意一个氨基酸；PHI-BLAST 可以根据给入的正则表达式对搜索到的相似序列 进行模式匹配，符合正则表达式的才会被作为结果输出。\n正则表达式{L}GExGASx(3,7)的意思是， 除 L 以外的任意一个字母，紧接 G，再紧接 E，再接一个任意字符，之后是 GAS 中的任意 一个，再接 LIVM 中的任意一个，最后再接 3 到 7 个任意字符；\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3166108923884514in\u0026rdquo;}\n在 NCBI BLAST 工具的输入页面，当算法选择了 PHI-BLAST 之后，会自动出现模式输入框，\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.4208245844269465in\u0026rdquo;}\n此外，PHI-BLAST 可以和PSI-BLAST 联合使用，以找到更多符合模式的远房亲戚们\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.177214566929134in\u0026rdquo;}\n三种BLAST的区别 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.716686351706037in\u0026rdquo;}\nNCBI的SmartBLAST {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4537674978127733in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.173571741032371in\u0026rdquo;}\n其他网站的BLAST的工具 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3123622047244092in\u0026rdquo;}\n多序列比对 前面我们讲的是双序列比对，为了看清楚每一列的保守情况和理化性 质，通常会给多序列比对根据不同的原则赋予丰富的色彩；\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4139774715660542in\u0026rdquo;}\n多序列比对的用途 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.031045494313211in\u0026rdquo;}\n多序列比对的算法 两条序列的比对需要构建一个二维表格，然后从 右下角到左上角找出一条最优路线。如果是做 3 条序列的比对，应该做一个三维立方体，从 (0,0,0)这个位置到(n,n,n)这个位置找到最优的贯穿路径。以此类推，如果是做 n 条序列的比对，就要创建一个 n 维空间。这个 n 维空间实在是难以想象，但是有一点是明确的， 就是到了 n 维我们已经没有办法再像二维那样精确的计算出比对结果了。\n由于计算量过于巨 大，所以目前所有的多序列比对工具都是不完美的。它们都使用一种近似的算法。目的就是 为了缩短计算时间，但也因此牺牲了一定的准确度。好在多序列比对并不像双序列比对对准 确度要求极高。通常，我们是要从多序列比对中看到一个趋势，一个大体的位置，所以牺牲 掉的这点儿准确度影响不大\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1168952318460192in\u0026rdquo;}\n多序列比对的注意事项 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.9399464129483817in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1097790901137357in\u0026rdquo;}\nEMBL - Clustal Omega 多序列比对工具 http://www.ebi.ac.uk/Tools/msa\n目前世界上最流行的多序列比对工具是 CLUSTAL 系列，TCOFFEE 和 MUSCLE。其中 CLUSTAL 系列使用率最高；TCOFFEE 最新，而且还有很多变形；MUSCLE 最快，而且胃 口大，能接受的序列数量是其他工具比不了的\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.523391294838145in\u0026rdquo;}\nEMBL 的多序 列比对工具很多，包括前面提到的 CLUSTAL 系列、TCOFFEE、MUSCLE。我们看 EMBL 这些比对工具中 CLUSTAL 系列的最新版本 Clustal Omega\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.364431321084864in\u0026rdquo;}\n在这个界面里可以点通过点击 More options，设置各种设置。在这个例子里，所有参数都使用默认值。参数里有输出格式（OUTPUT FORMAT）和输出顺序（ORDER）这两个参 数。\n输出格式里可以选择常用的多序列比对格式。我们选标准的 Clustal 格式。这是最常见 的多序列比对格式\n输出顺序参数可以设定多序列比对中各个序列的排列顺序。\u0026ldquo;aligned\u0026rdquo; 是按照比对过程中自动创建的计算顺序排列；\u0026ldquo;input\u0026quot;是按照输入序列的原始顺序排列。(输入序列是按照 TLR10、9、8、7\u0026hellip;这样的顺序排列的。输出顺序参数设定为 aligned，看看比 对结果里序列的排列顺序是否发生了变化)\nClustal 格式 的输出结果\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.499394138232721in\u0026rdquo;}\n可以看到，比对中序列的排列顺序跟输入的时候不一样了，这是按 照比对创建时的计算顺序排列的。请点击 Download Alignment File 保存将当前结果，以便后 面章节进一步加工。保存的文件后缀名是\u0026rdquo;.clustal\u0026rdquo;。它是一个纯文本文件，用写字板或者 记事本都可以打开\n如果想要添加色彩，点击\u0026quot;Show Colors\u0026quot;。之后，不同的氨基酸根据它们的理化性质 不同会显示出不同的颜色\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.559265091863517in\u0026rdquo;}\n除了颜色之外，多序列比对每一段的最后一行都有些星星点点的标记。这些标记和双序 列比对中的竖线、双点、单点的意思类似，但并不完全相同;这些星星点点的标记可以为我们指出保守区域所在的位置，即，星星点点特别密集的区域\n如果某一列是完全保守的一列， 也就是说这一列里的字母完全相同，那么这一列的下面就打一个\u0026quot;*\u0026quot;\n如果这一列的残基 有大致相似的分子大小及相同的亲疏水性，也就是这一列的字母要么相同要么相似，没有不 相似的，那么就打一个\u0026quot;：\u0026quot;\n如果这一列的残基 有大致相似的分子大小及相同的亲疏水性，也就是这一列的字母要么相同要么相似，没有不 相似的，那么就打一个\u0026quot;：\u0026quot;\n什么都不标记代表这一列是完全不保守的，也就是这一列的字母全部都不相似\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.4426038932633425in\u0026rdquo;}\nResult Summary 标签里，给出了全部结果信息的下载列表和一个 Jalview 的按钮（图 4）。 Jalview 是多序列比对编辑软件;。在下载列表里，如果打开 \u0026ldquo;Percent Identity Matrix\u0026quot;链接，可以得到所有序列两两之间的一致度矩阵。一致度矩阵的第 一行省略掉了。它和第一列完全相同，都是序列的名字并且按照相同的顺序排列。所以这个 矩阵是以对角线为轴对称的，并且对角线上是某条序列自己和自己的一致度，都是 100%。 这个矩阵可以帮助我们更好的了解这些序列之间的关系。比如我们可以从中发现，一致度最高的一对序列是 TLR1 和 TLR6,\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.170785214348206in\u0026rdquo;}\n还可以通过 Phylogenetic Tree 标签下的 Guide Tree 清楚的看出哪条序列和哪条序列更相似;Phylogenetic Tree 翻译成中文是系统发 生树。但是这里要特别注意，这不是真正意义上的系统发生树！它只是在创建多序列比对的 过程中用到的树（Guide Tree），没有经过距离校正，所以不能当作系统发生树来使用。如果 想要根据多序列比对结果构建系统发生树，可以在 Alignments 标签下，点击\u0026quot;Send to ClustalW2_Phylogeny\u0026quot;链接，把做好的多序列比对发送给专门做系统发生树的工具\nTCOFFEE - Expresso http://tcoffee.crg.cat\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.293115704286964in\u0026rdquo;}\nTCOFFEE 本身是一个标准的多序列比对工具，跟 CLUSTAL 没有什么区别。我们来看 它的变形，也就是根据比对序列种类的不同，TCOFFEE 网站下特有的比对工具\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.34495406824147in\u0026rdquo;}\nExpresso 最有特色，它是为序列加入结构信息后再做多序列比对的 工具。因为有结构信息的辅助，它可以大大提高比对的准确度\nM-Coffee 可以把多个比对的结果整合成一个\nTM-Coffee 专为穿膜蛋白打造\nPSI-Coffee 专为远源序列打造\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.415285433070866in\u0026rdquo;}\n做 Expresso 的序列我们选用网站提供的示例序列（图 2）。Show more options 下，可以 通过各种方式给入输入序列的结构信息。如果你有这些序列现成的结构文件，也就是 PDB 文件，可以直接把它们上传上来。三条序列对应三个上传链接。可以上传的结构文件不只限 于 PDB 数据库下载的，也包括还未正式发表的解析结构或者计算机预测的结构，只要是用 PDB 文件格式保存的，都可以\n如果没有现成的结构文件，但是这些序列在 PDB 数据库里有对应结构的话，你可以从 接下来的输入框里，按照规定的写法，指定哪条序列对应 PDB 数据库中的哪个结构\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.4290113735783025in\u0026rdquo;}\n如果这里输入了信息，Expresso 会自动从 PDB 网站下载指定的 PDB 文件。那些已经本地上 传的结构，Expresso 也会根据序列信息自动匹配出它是哪条序列的结构，不需要再在这里列 出了。如果对结构信息一无所知，只需要将\u0026quot;MODE_PDB\u0026quot;钩选。之后，Expresso 就会自 己在网络上为所有没有指定结构信息的序列搜索相应的结构。你提供给 Expresso 的结构信息越多，计算时间就会越短；你提供结构信息越少，计算时间就会越长。如果只勾选了 \u0026ldquo;MODE_PDB\u0026rdquo;，那么需要等待的时间会很长，因为 Expresso 首先要搜索，然后要下载， 最后要计算\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.470108267716536in\u0026rdquo;}\nTCOFFEE 系列各比对工具做出的多序列比对的颜色代表比对质量的好坏。越红质量越好， 越蓝质量越坏。这次的比对结果非常令人满意。如果配合上这些序列二级结构信息看一下的 话，你会发现，螺旋和螺旋很好的对在了一起，折叠和折叠很好的对在了一起。\n同样的序列做普通的 TCOFFEE，质量远不如 Expresso。可以看到二级结构全部 错位。所以，如果你有序列的结构信息的话，用 Expresso 相比用普通的比对工具会大大提 高比对质量。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.441287182852143in\u0026rdquo;}\n多序列比对的结果保存格式 比如有漂亮的网页格式的，标准的 Clustal 格式的，还有写完一条 序列再写下一条的 FASTA 格式的，以及方便下一步建树使用的 Phylip 格式的\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4409536307961504in\u0026rdquo;}\n要保存哪种格式主要看你下一步要干什么。在选择保存格式之前：\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3418744531933506in\u0026rdquo;}\n格式转换工具fmtseq http://www.bioinformatics.org/JaMBW/1/2\n它可以实现 20 多种格式间的转 换。其中总有一款是你想要的\n多序列比对的编辑和发布：Jalview http://www.jalview.org/\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.589284776902887in\u0026rdquo;}\n通过 Jalview 除了可以加工多序列比对，还可以针对比对中的序列做各种各样的 分析、比如构建系统发生树、预测蛋白质二级结构、查看结构域家族、从 PDB 数据库中查 询三级结构等\nJalview的使用简介 Jalview 打开之后，会自动展示许多 Demo 窗口。通过这些窗口，你可以了解到， Jalview 能加工序列比对、做进化树、分析结构，等等。功能确实不少。不过，你需要做的， 是把这些窗口统统关掉。打开你自己的多序列比对。\n点击 File 菜单 - Input Alignment - From File - 打开我们之前用Clustal Omega做出并保存的多序列比对结果\u0026quot;clustalo.clustal\u0026rdquo;（如 果你忘记保存了可以从附件中下载）。因为\u0026quot;.clustal\u0026quot;不是 Jalview 熟悉的后缀名，所以需要 把文件类型改成\u0026quot;所有文件\u0026quot;才能看到它\n{width=\u0026ldquo;5.513888888888889in\u0026rdquo; height=\u0026ldquo;8.819444444444445in\u0026rdquo;}\n它们体现了比对中每个位置的 保守度高低（Conservation）、比对质量高低（Score）、以及共有序列（Consensus）。从保守度 行，可以很清楚的找到保守区大致的位置。共有序列指的是某一列出现频率最高的那个字母， 比如第 58 列中 W 出现的频率最高，是 100%。如果某一列拥有的最高出现频率的字母是两 个或两个以上的话，会以\u0026quot;+\u0026ldquo;显示。把鼠标放在\u0026rdquo;+\u0026ldquo;上就可以看到是哪些字母出现的频 率一样高。共有序列可以一定程度上体现出某个保守区域所具有的序列特征。以后如果看到 和这段序列长相极其相似的序列，它很可能能跟这个保守区的功能相似。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.34794728783902in\u0026rdquo;}\n使用Jalview加工数据 上颜色 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.3141338582677164in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 另一个颜色方案 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.309044181977253in\u0026rdquo;} Clustalx的颜色方案\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.6097353455818024in\u0026rdquo;}\n对局部位置进行 手动调整 \u0026lt;!-- --\u0026gt; 比如，从前期实验我们得知，图中方框所示区域的 TLR2、10、6、1 这四条序 列的第 53 列应该往右挪一列，跟 TLR9、8、7 这三条序列的第 54 列对在一起。TLR2、10、 6、1 这四条序列的第 53 列补空位。其他位置不动\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.206905074365705in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 多序列比对的外观 \u0026lt;!-- --\u0026gt; 默认情况下，多序列比对是单行显示的。由于序列长，需要拖动窗口拉条才能浏览全部。这样不利于查看分析，也不利于将导出的比对图片插入文献。 如果想要让多序列比对根据 Jalview 窗口的宽度自动换行，可以在 Format 菜单下勾选\u0026quot;Wrap\u0026rdquo; 。此外，还可以通过\u0026quot;Font\u0026hellip;\u0026ldquo;窗口对字体格式、大小等进行调整\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.295605861767279in\u0026rdquo;}\n如果你只需要多序列比对，而不需要有关保守度等的注释行。可以关闭 annotations 标签下的 \u0026ldquo;Show annotations\u0026quot;选项，以达到去掉注释行的目的\n比如，可以按照序列的名字、 两两一致度或其他规则给比对中的序列重新排序以及为选中的两条序列做双序列全局比对 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.2624934383202095in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 为选中的一组序列计算各种系统发生树 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.104316491688539in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 或者用在线软件为某一条序列预 测二级结构 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.389772528433946in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 导出多序列比对为图片 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.246930227471566in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 其他编辑工具 \u0026lt;!-- --\u0026gt; Boxshade 擅长黑白制图。因很多学术期 刊只收取彩图的编辑费，所以黑白图可以节省科研经费。ESPript 的功能十分强大。MView 擅长把彩色多序列比对转换成 HTML 源代码。这样就可以将它直接插入网页，并方便以文 本形式选取。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.7402898075240596in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.6730686789151357in\u0026rdquo;}\n寻找保守区域 你究竟想从多序列比对中得到什么，答案是你想要找到序列中重要 的位置，说得更专业一点，就是要找到保守区域\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.6254932195975504in\u0026rdquo;}\n通过多序列比对下方的星星点点可以大致 发现图 1 中两个红框中的区域比较吸引眼球，因为星星点点特别多（\u0026rdquo;*\u0026ldquo;代表这一列残基完全相同；\u0026rdquo;:\u0026ldquo;代表这一列残基或者相同或者相似；\u0026rdquo;.\u0026rdquo; 代表这一列残基有相似的但也有不相似的；什么都没有代表这一列残基都不相似。所以我们 寻找的就是星星点点特别多的区域。当然用眼睛来数星星不那么靠谱。我们仍然需要借助软 件来更好的寻来保守区域）\n序列标识图 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.553226159230096in\u0026rdquo;}\n要创建序列标识图，首先需要一个多序列比对。多序列比对中的一列对应序列标识图中 的一个位置。然后分别计算每一列中不同残基出现的频率，再根据以下公式把频率转换成高 度值，最后根据高度值写出不同残基的彩色字母图形\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.460893482064742in\u0026rdquo;}\n如果某一列非常保守，字母高度就高。反之，如果某一列没有什么特征，各种残基都有 出现，杂乱无章，那么就会看到一堆比较矮的字母摞在一起\n这里再次强调，字母的高度和 它在某一列中出现的频率成正比，但是并不等于频率。试想一下，如果字母高度就是频率的 话，那么序列标识图中每个位置上字母摞起来的总高度应该是一样的，都是 100%。但是从图中可以看到，序列标识图上每个位置字母摞起来的总高度是不一样的，这是因为在字母 高度的计算过程中涉及了熵值。某一列中字母出现的情况越混乱，熵值越大，字母越矮。字 母出现的情况越有规律，熵值越小，字母越高。所以序列标识图可以很好的展现多序列比对 中每一列的保守程度，即，它们是杂乱无章的，还有有规律可循的。并且把可循的规律图形 化的展现出来。这就是我们为什么要给序列打上 logo 的原因\nWebLogo http://weblogo.threeplusone.com/\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.073949037620298in\u0026rdquo;}\n示例文件 promoter.fasta 是一组启动子序列的多序列比对， 以 FASTA 格式存储。FASTA 格式的多序列比对要求把多序列比对中的每一条序列连同插入 的空位一起按 FASTA 格式书写，写完一条序列再写下一条。这和之前讲过的 Clustal 格式不 太一样。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.292139107611549in\u0026rdquo;}\n从图中可以清晰的看到：输入的这些启动子序列上 TATA-Box 的共有特征序列，以及它们出现的位置\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.948219597550306in\u0026rdquo;}\n序列基序MEME http://meme-suite.org/\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4565299650043744in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.039199475065617in\u0026rdquo;}\nMEME 的 使用非常简单，只需要将待分析的序列上传即可（图 1）。而且，上传的序列为原始序列， 不需要提前为它们做多序列比对。你也可以指定返回排名前几的基序。MEME 的等待时间 稍长，大约 10 分钟以上，所以最好留下邮箱。\nMeme 的返回结果被保存成各种格式：HTML、XML、test 等。便于在线查看的是\u0026quot;MEME HTML output\u0026quot;，即网页格式\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1836154855643044in\u0026rdquo;}\n网页格式的 MEME 结果页面中，给出了找到的排名前三的基序，它们以序列标识图的形式展现出来\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.9849507874015746in\u0026rdquo;}\n同时还提供这三个基序在每条序列中的大体位置。如果要进一步了解 某个基序，可以点击序列标识图右侧的\u0026quot;More\u0026quot;下面的\u0026quot;下\u0026quot;箭头，以查看详细。 点击后，会得到大比例序列标识图，以及该基序在每条序列中对应的序列片段和它们出现的 具体位置\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.768046806649169in\u0026rdquo;}\n此外，还可以点击序列标识图右侧的\u0026quot;Submit/Download\u0026quot;下面的\u0026quot;右\u0026quot;箭头，将某个基序提交至各种数据库，并进行针对该基序的序列相似性搜索，已找到数据库中 含有该基序的序列，进而推测该基序的功能。这步操作是通过 The MEME Suite 软件套装下 的另一个软件 FIMO 来实现的\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.888888888888889in\u0026rdquo;}\nPRINTS 指纹图谱数据库 http://130.88.97.239/PRINTS/index.php\nhttp://www. bioinf.manchester.ac.uk/dbbrowser/PRINTS/\n蛋白质序列进行了充分的研究，而且早已发现并总结了这些 序列上的重要基序。相关研究成果汇入了 PRINTS 蛋白质序列指纹图谱数据库\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.506613079615048in\u0026rdquo;}\n要浏览 PRINTS 数据库，可以输入数据库编号、关键词、或标题等以查找某一个指纹图 谱。比如点击\u0026quot;By text\u0026quot;通过关键词搜索。输入条中输入\u0026quot;TRANSFERRIN\u0026quot;，也就 是搜索转铁蛋白家族的图谱。搜索返回转铁蛋白家族的指纹图谱链接\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.164479440069991in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.086452318460193in\u0026rdquo;}\n点击结果页面中的\u0026quot;TRANSFERRIN\u0026quot;链接后，会显示包括指纹图谱的基本信息、与其 他数据库之间的交叉链接、构建指纹图谱所使用的蛋白质序列、以及指纹图谱中每个基序等具体信息 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.2194083552056in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 点击\u0026quot;View alignment\u0026quot;链接后，可以看到创建指纹图谱所使用的多序列比对 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4101060804899386in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 点击\u0026quot;View structure\u0026quot;链接后，网页会打开一个三维视图插件，并以该家族中某一特征 蛋白质具有的三维结构为例，在线显示指纹图谱中各个基序在三维结构中的位置； 从该三维结构图中可以看出，紫色的基序在氨基酸序列水平上并不相邻，但是在三维空间结 构中是紧密联系在一起的，并形成蛋白质的重要功能区 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.183832020997375in\u0026rdquo;} \u0026lt;!-- --\u0026gt; PRINTS 还提供指纹匹配服务。也就是搜索某一序列所匹配的 指纹图谱。此功能通过 PRINTS 主页也上的\u0026quot;FPScan\u0026quot;链接实现，注意输入的待搜 索序列只能是\u0026quot;a raw sequence\u0026quot;，也就是纯序列。换言之，FASTA 格式中带大于号的第一行 不能拷贝进输入框。示例文件 prints.fasta 请从课程附件中下载 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.527737314085739in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.2331386701662295in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.975456036745407in\u0026rdquo;}\n点击排名第一的视紫红质家族的\u0026quot;Graphic\u0026quot;链接，可以得到该家族指纹图谱中各个基 序在输入序列中所匹配的位置。结果页面的下部还提供了视紫红质家族的 6 个基序 在输入序列中所对应的具体序列片段。由此，可以推测，输入序列属于视紫红质家族，并具 有该家族蛋白质的主要功能。事实上，输入序列确实是从 UniprotKB 数据库中下载的一条羊 的视紫红质的序列（P02700）。 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.9080457130358703in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.476358267716535in\u0026rdquo;}\n","date":"2021-11-03T00:00:00Z","permalink":"https://example.com/p/%E5%B1%B1%E4%B8%9C%E5%A4%A7%E7%94%9F%E4%BF%A1_%E5%BA%8F%E5%88%97%E6%AF%94%E8%BE%83_note/","title":"山东大生信_序列比较_note"},{"content":" 写在前面的话：这个笔记不过多深入知识，因为做入门有，大概知道什么时候该用哪个，以及现在有什么技术\n教程地址：https://www.bilibili.com/video/BV13t411372E?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click\n写在前面的话：这个笔记不过多深入知识，因为做入门有，大概知道什么时候该用哪个，以及现在有什么技术\n生物数据库 PubMed的使用（原核生物） PubMed 是拥有超过两百六十万生物医学文献的数据库。这些文献来源于 MEDLINE， 也就是生物医学文献数据库、生命科学领域学术杂志以及在线的专业书籍。他们大部分提供 全文链接。注意，提供的是链接，你有没有权限通过这个链接打开或者下载全文另当别论。 不管怎么说，看上去还不错。PubMed 主页（http://www.ncbi.nlm.nih.gov/pubmed）上有个搜 索条。不管三七二十一，先把家说的 dUTPase 敲到搜索条里，点搜索。找到了五百多条文 献。每个文献有题目，作者，刊物，出版时间等等。如果列出的这些信息还不够，或者无法 满足你的要求，你从页面上方设置每个文献是要显示内容、总结、摘要，还是其他。还可以 控制每页显示几个文献，以及按照你期望的顺序进行排序\n如果找到的文献太多，一时看不完，可以把他们保存到本地。只要选中你要保存的文献， 然后通过发送按钮，选择文件，再选择保存的内容以及顺序，最后点创建文件。这样你选中 文章的信息就以纯文本文件的形式保存到本地电脑上了。\nPubmed 提供文 献的摘要和全文链接。这里有两个全文链接。其中一个链接的图标上有 free 字样。Jim 很幸 运，这篇文章是 open access 的，也就是免费阅读的。两个链接，第一个是期刊提供的全文 链接，第二个是 PubMed 中心提供的全文链接。点其中一个链接，就可以在线浏览文献全文 了，或者下载全文的 PDF 文件到本地。至此，JIM 总算找到了些许有用的信息\n回到 PUBMED 搜索结果页面，在显示内容格式这个下拉菜单里，除了总结，摘要，还 有个叫 MEDLINE 的项目。你可以把它简单的理解为数据库中文献记录的内部结构。每条 文献都是以这样的内部结构存储在数据库中的。一篇文献的所以信息被分割成小节，每个小 节都有自己的索引名，比如 TI 代表题目，AB 代表摘要，AU 代表作者等等。这些由几个字 母组成的索引名是规定好的。\n了解了 MEDLINE 结构，我们就可以在搜索条中通过引入索引名，来按照不同的规则 搜索。比如搜索 Down 这个词。我们在 Down 的后面加上空格，中括号 AU（Down [AU]）， 就会返回所有作者名里有 Down 这个词的文献。如果加上[TI]，则返回题目中有 Down 的 文献。中括号 AD 是搜索发表单位。如果什么限制都没有，只写 Down 的话是在任意地方搜 索。我们看到，引入不同索引名后，搜索到的文献数量是不相同的\n现在，我们的 Jim 已经学会了 PubMed 的基本使用。他打算趁暑假回北京期间找个相关 的专家拜访一下。于是他在 PubMed 上搜索，题目和摘要里有 dUTPase 的文献，而且文献的 发表单位是北京的（dUTPase [TIAB] Beijing [AD]）。搜索后，找到了这四篇文章。 其中第二篇文章是研究 dUTPase 晶体结构的，Jim 很感兴趣。点进去看一下，找到最后一位 的作者。我们说，通常课题的主要负责人会以通讯作者的身份出现在作者列队的最后。那么， 这个研究团队的主要负责人就是这位 Su XD，Su 教授。他的单位是北京大学。之后的任务 就可以交给度娘来完成了。度娘很快就找到了苏教授的地址和电话，而且连照片都有。Jim 通过这些信息，成功的拜访了北京大学生命科学学院的苏晓东教授\n除了搜索条，我们还可以利用高级搜索工具更精确的查找。高级搜索可以无限的添加条 件。比如发表日期从哪天到哪天，文章类型是 research article 还是 review，语言是英语发表 的还是其他语言发表的等等。我们可以查找比如 2000 年至今发表的，题目有 dUTPase 关键 词的，英语的，review 文章。一共找到五篇，相信 Jim 同学看完这 5 篇文章之后，一定会对 dUPTase 有一个很好的了解\n关于 PubMed 还有几点要说明的。在搜索时，可以使用引号。引号里的词会被当作一个 整体来看待，而不会被拆开。这个小技巧同样适用于度娘等网络搜索引擎。也可以使用逻辑词 AND OR NOT。比如你可以规定，题目里有 dUPTase，并且题目里有 bacteria，但是作者 里不要有 Smith（dUTPase [TI] AND bacteria [TI] NOT Smith [AU]）。此外，如 果知名知姓，可以利用正确的名字缩写来提高搜索的准确度。还有非常关键的一点，每篇文 章都有自己唯一的 PubMed ID（PMID）。通过这个号，可以直接找到某一篇文章。最后，不 得不说的是，有的时候 PubMed 也帮不了你。比如，搜索 1995 年以前的文献中排名十位以 后的作者是白费力气。搜索 1976 年以前的文献是没有摘要的。搜索 1965 年以前的文献，就 别想了。关于 PubMed 就给大家介绍到这里\n一级核酸数据库：GenBank原核生物核酸序列 看一级核酸数据库，他主要包括三大核酸数据库和基因组数据库。三大核 酸数据库包括 NCBI 的 Genbank，EMBL 的 ENA 和 DDBJ，它们共同构成国际核酸序列数据 库。三大核酸数据库，美国一个，欧洲一个，亚洲一个。美国的 Genbank 由美国国家生物 技术信息中心 NCBI 开发并负责维护。NCBI 隶属于美国国立卫生研究院 NIH。欧洲核苷酸 序列数据集 ENA 由欧洲分子生物学研究室 EMBL 开发并负责维护。亚洲的核酸数据库 DDBJ 由位于日本静冈的日本国立遗传学研究所 NIG 开发并负责维护。Genbank，EMBL 与 DDBJ 共同构成国际核酸序列数据库合作联盟 INSDC。通过 INSDC，三大核酸数据库的信息每日 相互交换，更新汇总。这使得他们几乎在任何时候都享有相同的数据\n我们以 NCBI 的 Genbank 为例，教你如何解读一级核酸数据库。我们将分别浏览一个原 核生物的基因和一个真核生物的基因。为此，请先跟我复习一下原核生物与真核生物基因的 不同之处。原核生物基因组小，真核生物基因组大。原核生物基因密度高，1000 个碱基里 就有 1 个基因，而真核生物基因密度低，比如人，要 10 万个碱基才有 1 个基因。与此对应， 原核生物编码区含量高，而真核生物低。此外，原核生物的基因是呈线性分布的，而真核生 物的基因是非线性的，因为翻译蛋白质的外显子被内含子分隔开来。也就是真核生物的 mRNA 要经历剪切的过程，剪切后的成熟 mRNA 才能进行翻译。这是原核生物和真核生物 基因的最大区别，即，原核生物没有内含子，真核生物有内含子。这个巨大的区别，将导致 两种基因在数据库中不同的存储及注释方式\n我们首先来看一条原核生物的 DNA 序列，它是编码大肠杆菌 dUTPase 的基因，在 Genbank 里的数据库编号是 X01714。从 NCBI 的主页（http://www.ncbi.nlm.nih.gov/）选择 Genbank 数据库。Nucleotide 数据库就是 Genbank 数据库，然后在搜索条中直接写入这条序 列对应的数据库编号 X01714，点击\u0026quot;搜索\u0026quot;。结果返回编号为 X01714 的序列在 Genbank 中 详细记录。从这条记录的标题我们得知，dUTPase 是脱氧尿苷焦磷酸酶，编码他的基因叫 dut 基因，所属物种是大肠杆菌。下面是关于这个基因的详细注释，我们逐条浏览一下：\nLOCUS 这一行里包括基因座的名字，核酸序列长度，分子的类别，拓扑类型，原核生 物的基因拓扑类型都是线性的，最后是更新日期\nDEFINITION 是这条序列的简短定义，也就是前面看到的标题\nACCESSION 就是在搜索条中输入的那个数据库编号，也叫做检索号，每条记录的检索 号在数据库中是唯一且不变的。即使数据提交者改变了数据内容，Accession 也不会变。 你会发现，这条记录里，Accession 和 Locus 是一样的。这是因为这个基因在录入数据 库之前并没有起名字，因此录入数据的时候便将检索号作为了基因的名字。但是有些基因， 在录入数据库之前已经有了自己的名字，那么这些基因所对应的 Accession 和 Locus 就 不一样了。你可以这样理解，Locus 是一个同学的真实姓名，而 Accession 是这个同学 的学号。同一个人在不同的学校里会有不同的学号，而名字只有一个。基因也是一样，同一 个基因在不同的数据库中会有不同的检索号，而基因的名字只有一个\nVersion 版本号和 Locus，Accession 长得差不多。版本号的格式是\u0026quot;检索号点上 一个数字\u0026quot;。版本号于 1999 年 2 月由三大数据库采纳使用。主要用于识别数据库中一条单一 的特定核苷酸序列。在数据库中，如果某条序列发生了改变，即使是单碱基的改变，它的版 本号都将增加，而它的 Accession 也就是检索号保持不变。比如，版本号由 U12345.1 变 为 U12345.2，而检索号依然是 U12345。版本号后面还有个 GI 号。GI 号与前面的版本号系 统是平行运行的。当一条序列改变后，它将被赋予一个新的 GI 号，同时它的版本号将增加\nKEYWORDS 提供能够大致描述该条目的几个关键词，可用于数据库搜索。\nSOURCE，基因序列所属物种的俗名。他下面还有一个子条目，ORGANISM，是对所属 物种更详细的定义，包括他的科学分类\nREFERENCE 是基因序列来源的科学文献。有时一条基因序列的不同片段可能来源于不 同的文献，那样的话，就会有很多个 REFERENCE 条目出现。REFERENCE 的子条目包括文 献的作者、题目和刊物。刊物下面还包括 PubMed ID 作为其子条目\nCOMMENT 是自由撰写的内容，比如致谢，或者是无法归入前面几项的内容\nFEATURES 是非常重要的注释内容，它描述了核酸序列中各个已确定的片段区域，包含 很多子条目，比如来源，启动子，核糖体结合位点等等\npromoter 列出了启动子的位置。细菌有两个启动子区，-35 区和-10 区。-35 区位于第 286 个碱基到第 291 个碱基 ，-10 区位于第 310 个碱基到第 316 个碱基\nmisc_feature 列出了一些杂项，比如，这条说明了从第 322 个碱基到第 324 个碱基 是一个推测的，但无实验证实的转录起始位置\nRBS 是核糖体结合位点的位置\nCDS，Coding Segment，编码区。对于原核生物来讲，CDS 记录了一个开放阅读框，从 第 343 个碱基开始的起始密码子 ATG 到第 798 个碱基结束的结束密码子 TAA。除了位置信 息，还包括翻译产物的诸多信息。翻译产物蛋白的名字是 dUTPase，这个编码区编码该蛋白 的第 1 到第 151 个氨基酸。翻译的起始位置和翻译所使用的密码本，以及计算机使用翻译密 码本根据核酸序列翻译出的蛋白质序列。需要强调的是，这不是生物自然翻译的，而是计算 机翻译的。事实上，蛋白质数据库中的大多数蛋白质序列都是根据核酸序列由计算机根据翻译密码本自动翻译出来的。中间部分是翻译出的蛋白在各种蛋白质数据库中对应的检索号。 通过这些检索号可以轻松的链接到其他数据库 （此外，X01714 这条核酸序列还包含第二个\u0026quot;潜在的\u0026quot;基因，也就是计算机预测出来的 基因。它编码的蛋白目前的数据库里没有详细记录，是个未知的蛋白。像这样，一条核酸序 列包含多个基因的情况在 Genbank 里是很常见的）\nORIGIN 作为最后一个条目记录的是核酸序列，并以双斜线作为整条记录的结束符。至 此整条记录就浏览完了\n有时你可能会想要保存这条序列，但是直接从这里拷贝，序列里既有空格，又有数字， 不是纯序列，手动删除这些又很麻烦。这时，你可以在这条记录的标题下面找到一个叫做 FASTA 的链接。点击他，你会获得 FASTA 格式的核酸序列。FASTA 格式是最常用的序列书 写格式，他由两部分组成，第一部分就是第一行，以大于号开始。大于号后面接序列的名称 或注释。第二部分就是第二行以后的纯序列部分，这部分只能写序列，不能有其他内容，比 如空格，注释，行号之类的都不能在序列部分出现。早期的 FASTA 格式要求序列部分每行 60 个字母。但这个规定早已被打破，每行 80，或每行 100，都可以\n标题下方，除了 FASTA 链接，还有一个图形化链接，点击可以看到 Features 里的注 释信息以图形的形式更直观的展示出来。可以看到这条序列包含的两个基因，他们的启动子 的位置，核糖体结合位点的位置等。其中一条基因是编码 dUTPase 的 dut 基因，另一个是编 码未知蛋白的潜在的通过计算预测出的基因\n如果想要保存这条记录，最好的方法是像保存 PubMed 文献列表那样，点击发送链接， 然后选择以纯文本文件的形式保存整条记录到本地电脑上\n一级核酸数据库：GenBank真核生物核酸序列 真核生物的基因与原核生物不同，是非线性排列的，也 就是基因里有外显子和内含子。因此真核生物核酸序列的数据库记录要要比原核生物复杂。 有时需要几条记录拼凑在一起才能描述出一个完整的基因。我们先来看看编码人 dUTPase 的成熟 mRNA 序列。成熟 mRNA 是已经剪切掉内含子，只剩外显子的序列，所以这条成熟 mRNA 序列和我们之前看到的原核生物的 DNA 序列从拓扑结构上看是几乎一样的，都是线 性的。输入这条成熟 mRNA 序列的检索号 U90223，搜索\n打开数据库记录，基本的注释内容和原核生物的差不多，这里只挑两点特别的地方说一 下。大家看到 KEYWORDS 后面只有一个点。这个点提示我们，数据库并不是完美的，所有 数据库都存在数据不完整的问题。再有，JOURNAL 后面我们看到是写的是未正式发表。但 事实上，这篇文章早在 1997 年就已经发表在 JBC 上了。因此，忠言逆耳：别指望 Genbank 或任何一个数据库能够百分百做到数据无误且实时更新\nFeatures 里的注释内容与原核生物的数据库记录相似，CDS 指出了从 63 到 821 是一 段编码区，在这段编码区里基因是连续的，因为是经过剪切后的成熟 mRNA，它将被翻译 成线粒体型 dUTPase 蛋白。下面/translation 里给出的是计算机翻译出的该蛋白的序列\n在 Features 里还有两个新的条目之前没有见到过。sig_peptide 和 mat_peptide。 sig_peptide，也就是 signal peptide，指出了编码信号肽的碱基的位置。信号肽决定了蛋 白质的亚细胞定位，也就是蛋白质工作的地方。mat_peptide，也就是 mature peptide，指 出了编码成熟肽链的碱基的位置。他从信号肽后面开始，到编码区结尾提前三个碱基结束。 编码区一直到第 821 号碱基，而编码成熟蛋白的最后一个碱基是第 818 号碱基，这中间差了 3 个碱基，那最后的这三个碱基干嘛去了呢？编码区的最后三个碱基是终止密码子，不翻译。 这条真核生物序列的 Genbank 注释看起来和原核生物的差不多，这是因为我们很小心的挑 了一条成熟 mRNA 的序列\n基因组里的 DNA 序列，是非线性分布的基因序列。我们仍然浏览编码人的 dUTPase 的 dut 基因序列。输入检索号 AF018430，搜索\n这个检索号下的序列标题是\u0026quot;人 dut 基因的第三号外显子\u0026quot;。人的 dut 基因肯定包含多 个外显子，而当前的这条 DNA 序列里只包含了一个外显子。其他的外显子在别的数据库记 录里。从 SEGMENT 处可以看到，人的 dut 基因序列被分成了 4 个片段，并且分别存储在 4 条数据库记录中。也就是说，只有把四个片段全部凑在一起，才能拼凑出完整的基因。当前 这条数据库记录是所有四个片段里的第二个。这个片段里只包含一个外显子，是第三号外显 子。需要注意的是，一个片段可以只包含一个外显子，也可以包含不止一个外显子。另外， 这个例子告诉我们，LOCUS 和 ACCESSION 是可以不相同的\n从 FEATURES 里可以找到这个序列片段在染色体上的具体位置。是在 15 号染色体的长 臂上，位置在 15 到 21.1 条带之间\nGENE 这部分指出了拼出完整基因所需的所有四个片段的检索号，以及具体的位置。也 就是 AF018429 这条序列的 1 到 1735 号碱基，连上当前这条序列的 1 到 1177 号碱基，连上 AF018431 这条序列的 1 到 45 号碱基，连上 AF018432 这条序列的 658 到 732 号碱基和 884 到 954 号碱基，以及 1391 到 1447 号碱基。后面给出了基因的名字，dut\nmRNA 给出了拼凑出上面基因的各个片段中外显子的位置。也就是说，GENE 里的片段 拼在一起是完整的基因，mRNA 里的片段拼在一起就等于完成了剪切的过程，相当于成熟 mRNA。值得注意的是，剪接后形成的 mRNA 有两种，其中一种比另一种在前端多了一个 外显子。多的这一段将被翻译成定位线粒体的信号肽，从而翻译出线粒体型的蛋白质。而另 一种没有信号肽的将形成细胞核型蛋白质\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.9825699912510937in\u0026rdquo;}\n上表清晰的列出了四个片段中所有外显子的位置。能够清楚的看到，线粒体型的比细胞 核型的多了一个翻译信号肽的外显子，其他的翻译成熟肽段的外显子都是一样的。虽然信号 肽最终也会被切掉，但是由此产生了两种亚细胞定位的蛋白质。有信号肽的会到线粒体中去， 没有的将留在细胞核里\n最后 exon 条目明确的告诉我们，当前这条序列里 560 到 651 号碱基是 dut 基因的第三 个外显子。至此，大家应该看得出来，解读真核生物的 DNA 序列要比原核生物复杂得多。 但是，只要你熟知基因的结构和 Genbank 的存储方式，这本天书不难看懂\n基因组数据库 Ensemble 在查看人的整个基因组之前，需要先搞清楚几件事。人的基因组有 33 亿个碱基分布在 23 个染色体上。我们现在已经获得了人的全基因组序列。起初拿到手的就是 33 亿个字母， 下一步面临的巨大挑战就是给它们添加注释，也就是做一个详细的 FEATURES 表。全世界 每时每刻关于人类基因及其功能都有新的发现。研究基因的方法五花八门，层出不穷，不可 能全部学会，只能是用到哪学到哪\n我们从 Ensembl 数据库（http://www.ensembl.org）查看人的基因组。Ensembl 是由欧洲 生物信息学研究所 EBI 和英国桑格研究院合作开发的。它收入了各种动物的基因组，特别 是那些离我们人类近的脊椎动物的基因组。这些基因组的注释都是通过配套开发的软件自动 添加的。Ensembl 主页左下角有人，老鼠，斑马鱼这三个点击率最高的基因组的快速链接。 其中，人的基因组有两个。右边是 2009 年获得的基因组信息，左边是 2013 年重新测序获得 的基因组信息。我们看右边这个最新的\n点击进入之后，我们点这个查看染色体，就可以看到人的所有染色体的图例。不知到大 家还记不记得，之前看到的某些信息似乎和 15 号染色体有点儿什么关系！没错，前面一直 研究的那个编码 dUPTase 的 dut 基因就在 15 号染色体上。点一下 15 号染色体，在弹出窗口 中选择染色体概要（chromosome summary）。这时我们会得到 15 号染色体的一个一览图。里 面包括编码蛋白的基因、非编码基因、假基因分别在染色体上不同区段内的含量，以及 GC 百分比（红线），和卫星 DNA 百分比（黑线）。染色体统计表给出了 15 号染色体的长度， 以及各种类型的基因的个数\n从 Genbank 我们了解到，dut 基因的第三号外显子位于 15 号染色体的长臂条带 21.1 附 近。所以我们进一步进入这个条带看一下。点击条带 21.1，选择区间链接。这时，这个区间 内所有的基因就都被显示在一张图上。如果眼力好的话，可以从这个图谱上直接找到 dut 基 因，并以他为中心放大。如果找不到，也可以通过搜索条输入基因的名字进行查找\n在以 dut 基因为中心显示的放大图谱中，点击 dut 或者对应的区域，在弹出的概况窗口 中选择 Ensemble 数据库的检索号。之后就会出现dut基因在 Ensemble 数据库中的详细记录\n微生物宏基因组数据库JCVI http://www.jcvi.org/\n微生物宏基因组数据库是非常有用的一级核酸数据库资源。说到微生物宏基因组学，不 得不介绍的是美国基因组研究所 TIGR 和克莱格反特研究所 JCVI。美国基因组研究所致力 于微生物基因组的研究，也有部分植物基因组项目。它是克莱格·凡特研究所的一部分。自 1995 年成立之初的两个基因组，至今已拥有超过 700 个基因组，而且还将更多。TIGR 是 NCBI 基因组资源的有力补充，因为它不仅拥有已完成测序的基因组，还有那些测序中的基 因组信息。在植物基因组项目中可以找到拟南芥、玉米、苜蓿和柳树的基因组信息。在微生 物与环境基因组目中，特别值得关注的是\u0026quot;人类微生物组计划\u0026quot;，HMP\nHMP 由美国 NIH 发起，由 4 个四个测序中心共同完成，其中一个就是克莱格凡特学院。 \u0026ldquo;人类微生物组计划\u0026quot;堪比\u0026quot;人类基因组计划\u0026rdquo;。我们目前认知的微生物不到 1%，生活在我 们肠道中的微生物细胞，是人体细胞的 10 倍。这些微生物基因组之和是人类基因组的 100 倍。微生物影响并超越我们的生老病死，有一天人死了，但身体中的微生物却还活着。除了 近年来少量的有关糖尿病等与肠道微生物的研究外，我们完全不清楚肠道微生物，呼吸道微 生物，还有体表微生物等在人体内做了什么，他们的喜怒哀乐与我们的生老病死有什么关系。 所以世界上诸多科学家都呼吁完成微生物组的研究计划。HMP 就是其中之一。目前，HMP 主要包括了人类鼻腔、口腔、皮肤、胃肠道和泌尿生殖道的宏基因组样本数据和分析流程\n从 JCVI 主页（http://www.jcvi.org/）的统计表中我们可以看到不同器官中有多少微生物 基因组已被测序并被注释。点击下方的统计链接。可以得到 HMP 中已研究的所有微生物基 因组。这些微生物在人体中存在的位置，测序及注释是已完成还是在进行中。已完成的基因 组后面会有三个链接\nWGS 是全基因组鸟枪法测序项目数据库记录的链接\nSRA 是高通量测序数据库记录的链接。这两个链接里记录的是测序的信息。相比之下对 大家更为有意义的是\nANNOTATION 链接里的内容，他列出了某个基因组在 Genbank 中所有注释的链接。 比如微生物 Acinetobacter radioresistens SK82 的基因组共分成 82 条序列记录在 Genbank 数据 库中。通过前面章节的学习，解读这些序列并不困难\n二级核酸数据库 二级核酸数据库包括的内容非常多。其中 NCBI 下属的三个数据库经常会用到。他们是 RefSeq 数据库，dbEST 数据库和 Gene 数据库。RefSeq 数据库，也叫参考序列数据库，是通 过自动及人工精选出的非冗余数据库，包括基因组序列、转录序列和蛋白质序列。凡是叫 ref 什么的数据库都是非冗余数据库，就是已经帮你把重复的内容去除掉了。dbEST 数据库， 也就是表达序列标签数据库，存储的是不同物种的表达序列标签。Gene 数据库以基因为记 录对象为用户提供基因序列注释和检索服务，收录了来自 5300 多个物种的 430 万条基因记录\n此外，非编码 RNA 数据库，提供非编码 RNA 的序列和功能信息。非编码 RNA 不编码 蛋白质但在细胞中起调节作用。目前该数据库包含来源于 99 种细菌，古细菌和真核生物的 3 万多条序列。microRNA 数据库主要存放已发表的 microRNA 序列和注释。这个数据库可 以分析 microRNA 在基因组中的定位和挖掘 microRNA 序列间的关系\n{width=\u0026ldquo;5.430555555555555in\u0026rdquo; height=\u0026ldquo;2.2916666666666665in\u0026rdquo;}\n一级蛋白质序列数据库：蛋白质序列数据库UniProtKB {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.90625in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.049759405074366in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.521139545056868in\u0026rdquo;}\ninteraction {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.125106080489939in\u0026rdquo;}\nstructure {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.9633880139982502in\u0026rdquo;}\nFamily \u0026amp; Domains {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.799250874890639in\u0026rdquo;}\nSeq {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.96869750656168in\u0026rdquo;}\nCross-references {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.9376826334208226in\u0026rdquo;}\nPublication {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.113636264216973in\u0026rdquo;}\nEntry information {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.8024682852143483in\u0026rdquo;}\nMiscellaneous {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.044243219597551in\u0026rdquo;}\nSimillar proteins {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.033374890638671in\u0026rdquo;}\nText纯文本数据库记录 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.949798775153106in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.309232283464567in\u0026rdquo;}\n一级蛋白质数据库 蛋白质结构数据库PDB 蛋白质的基础结构概念 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4897779965004374in\u0026rdquo;}\nPDB库的概况 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.485168416447944in\u0026rdquo;}\n在PubMed上查找的资料\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.354765966754156in\u0026rdquo;}\nPDB库的使用 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.182671697287839in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.186359361329834in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.13661854768154in\u0026rdquo;}\nPDB文件的介绍 HEADER : 分子类别 ， 日期， PDB ID\nTITLE : 结构的标题，一般就是相关文献的标题\nCOMPD： 对各个分子的描述\n\u0026lt;!-- --\u0026gt; 如这里是由三条链构成的三聚体结构 \u0026lt;!-- --\u0026gt; SOURCE : 结构中包括的每一个分子的实验来源 （生物学 / 化学）\nKEYWDS： 用于数据库搜索的关键词\nEXPDTA: 测定结构所采用的实验方法\n\u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1316940069991253in\u0026rdquo;} \u0026lt;!-- --\u0026gt; AUTHOR: 结构测定者\nREVDAT: 历史上曾对该数据库记录进行过的修改\nJRNL：发表这个结构的文献\nREMARK: 无法归入其他部分的注释\nDBREF: 该蛋白质在蛋白质序列数据库看看里的检索号等信息\nSEQRES: 氨基酸序列\nMODRES:对标准残基上的修饰\n\u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.598545494313211in\u0026rdquo;} \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.450038276465442in\u0026rdquo;} \u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; LINK: 残基间的化学键，比如肽键，氢键，二硫键 \u0026lt;!-- --\u0026gt; 如 107号上的N 与 106号上的C 脱去一分子的水形成1.32A \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5027252843394576in\u0026rdquo;} \u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; 3D坐标部分 \u0026lt;!-- --\u0026gt; 每一行是一个原子，如这是A链上的20号氨基酸PRO上的N原子，\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.6044969378827645in\u0026rdquo;}\n\u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;6.395739282589676in\u0026rdquo;} \u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.050152012248469in\u0026rdquo;} \u0026lt;!-- --\u0026gt; PDB在线3D查看 JSMOL {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.514746281714786in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3333333333333335in\u0026rdquo;}\n二级蛋白质数据库 Pfam简述 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3306846019247596in\u0026rdquo;}\npfam上的SEARCH可以帮我们查找某条序列上有哪些结构域\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.252445319335083in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.5911154855643046in\u0026rdquo;}\nSummary可以获得这个结构域的功能注释以及结构信息\nDomin organisation 可以看出在当前有多少蛋白质拥有TIR结构域以及结构域之间的关系Sturctures : 会列出当前TIR结构域的蛋白质结构以及在UNIport中的链接\nCATH {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.225036089238845in\u0026rdquo;}\nCATH分类代码\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.581871172353456in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.195364173228347in\u0026rdquo;}\nCATH的使用 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.249998906386701in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.985738188976378in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.630951443569554in\u0026rdquo;}\n3H6X是三条链的\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.715883639545057in\u0026rdquo;}\n另外一个有趣的图\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.42660542432196in\u0026rdquo;}\nSCOP2 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.298932633420822in\u0026rdquo;}\nSCOP2的使用 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.632825896762904in\u0026rdquo;}\n四层分类\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.670062335958005in\u0026rdquo;}\n专用数据库 KEGG 首先看个图（生物代谢图）\n每一个点就是一个化合物，每一条线就是一个生化反应\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5799781277340332in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;0.7091765091863517in\u0026rdquo;}\n正式开始KEGG的介绍\n以代谢通路来讲解\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.208489720034995in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.919989063867017in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.53876968503937in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.893409886264217in\u0026rdquo;}\n放大看看\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.316514654418198in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.3274660979877515in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.547490157480315in\u0026rdquo;}\n接下来看下Organismal Systems\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.6585640857392825in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.440069991251094in\u0026rdquo;}\n这是人的各种Toll样受体信号传导图\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.378748906386702in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.241024715660543in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.016052055993001in\u0026rdquo;}\nOMIM 人类遗传病数据库 简介\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.003987314085739in\u0026rdquo;}\n例子\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.1526695100612425in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.201524496937883in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.979549431321085in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.9959448818897636in\u0026rdquo;}\n可以查看这个基因的全部信息\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.5044149168853895in\u0026rdquo;}\n","date":"2021-11-02T00:00:00Z","permalink":"https://example.com/p/%E5%B1%B1%E4%B8%9C%E5%A4%A7%E7%94%9F%E4%BF%A1_%E7%94%9F%E7%89%A9%E6%95%B0%E6%8D%AE%E5%BA%93_note/","title":"山东大生信_生物数据库_note"},{"content":" 写在前面的话：这个笔记不过多深入知识，因为做入门有，大概知道什么时候该用哪个，以及现在有什么技术\n生物数据挖掘（这章讲的有点水） 大数据有四字箴言：大、快、杂、 疑，即大数据资料量庞大、变化飞快、种类繁杂、以及真伪存疑\n数据库系统 数据挖掘涉及三个领域：统计、数据库系统和机器学习。关于统计，有专门的统计课程 不属于这门课的主要讲授内容。这一章主要从数据库系统和机器学习这两部分入手来掌握数 据挖掘的基本方法。 数据库系统就是存放数据的数据库和管理数据库的管理软件加在一起，即，数据库+数 据库管理系统=数据库系统。这就像一个图书馆，除了书籍以外，还要有图书管理员，否则 书放不进去也拿不出来\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.278423009623797in\u0026rdquo;}\n数据库有很多类型，比如传统的关系型数据库，是以表格的形式存储数据的。还有近些 年越来越流行的面向对象型数据库，比如 XML 数据库。它是以 XML 格式存储数据的。Xml 格式的数据都是以尖括号括起来的标签开始，在标签名前加个斜线结束。标签中还可以再包 含子一级的标签。比如病人资料这个标签下就有两个病人标签，每个病人标签下还有四个记 录病人信息的标签。如果需要的话，疾病这个标签下还可以再加入更深一层的标签，比如疾 病的名字、分型等等。这样一层一层的，结构非常清晰而且灵活，特别适合存储复杂的生物 数据。这是传统的关系型数据库无法比拟的\n机器学习 主要任务 机器学习主要是设计和分析一些让计算机可以自动\u0026quot;学习\u0026quot;的算法。这些算法是一类从 数据中获得规律，并利用这些规律对未知数据进行预测的算法。比如有台电脑，我们想让他 学会辨认各种球，那么我们就拿来很多球让电脑学习，告诉它这样的球足球、这样的球是排 球，这样的球是篮球，这样的球是棒球。经过大量的学习后，电脑说，我已经学会了，可以 分清这四种球了。好，我们来考考他。学过的都能掌握，没学过的打死也不会，这就是机器 学习。如果机器学习到了一个很高的境界，能够主动学习了，并能正确掌握学习到的东西， 那就走向人工智能了。目前市面上比较火的公子小白就具备机器学习的功能，但可惜仍然是 比较初级的被动学习，还没有智能到主动学习\n现在回到最初的问题上，看看这个电脑是怎么学会识别各种球的。电脑没有眼睛，所以 我们也不是真的把球摆到电脑面前让他看。我们实际上是把电脑学习的物体转化成了向量， 让电脑读取向量值，也就是用向量来描述物体。我们可以用一个 5 维的向量来描述一个球。 这五个维度分别描述了球的直径、重量、颜色、材质和纹路。这样一个向量足够将各种球区 分开了。几乎所有物体我们都可以把他转化成多维向量。比如图片可以转换成颜色柱状图， 并由此创建一个 36 维的向量，一个维度对应一种颜色，每一维度上的值代表这种颜色在图 片中出现的频率。再比如，基因表达水平，可以用描述基因芯片上每个点的颜色及深浅的向 量来表示。甚至我们教室里的每一个人都可以向量化\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.9564643482064743in\u0026rdquo;}\n机器都能学些什么，也就是机器学习的任务。常见的机器学习的任务 有分类、聚类和回归。分类和聚类虽然名字很像，但他们的区别还是巨大的。之前教给小电 脑完成的任务就是分类任务\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4433650481189852in\u0026rdquo;}\n分类任务要有足够的背景知识去训练电脑，告诉电脑这个样的都是篮球，这个样的都是 足球，这样的都是排球。然后拿学习过的这些球以外的球，让电脑判断是哪一种球，这就是 分类\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.098248031496063in\u0026rdquo;}\n再来看聚类，要理解聚类可以设想这样一个场景，一个外星人来到地球，对球状物体非 常痴迷，从地球上搜集了大量的球状物体。它不知道这些球状物都是什么，为了更好的研究 它们，外星人把长得差不多的球都放在了一起，并且将它们命名为球 1、 球 2 和球 3。这个 过程就叫聚类。由此可见，聚类和分类最大的区别就是聚类不需要背景知识，而分类需要\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.219550524934383in\u0026rdquo;}\n最后看看回归。回归跟分类一样，也是通过对背景知识的学习达到预测未知的目的。只 不过，这次预测的不是属于哪一类，而是预测一个具体的数值。比如房产公司可以根据房屋 情况利用回归算法估算房屋价格。房产公司拥有大量已成交房屋的数据，包括房子大小、地 皮大小、卧室数量、建材是否是花岗岩、有没有地下室以及房屋的最终售价。现在他们想要让电脑学习这些数据，从而建立起房屋的各项指标和价格之间的定量关系，这样他们就能够 更加准确的给待出售的房屋定价了。通过机器学习，最终返回了一个公式。公式体现了 x1、 x2、x3、x4 和 x5 与 y 之间的定量关系。对于新登记的房屋，只要把 x1 到 x5 这五个指标数 值带入公式，就可以估算出房屋的售价\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.138747812773403in\u0026rdquo;}\nK次交叉检验 回归和分类都需要背景知识，也就是训练组数据来训练出预测模型，而聚类不需要。预 测模型训练好之后还需要从训练组数据中拿出一部分作为测试组数据来测试模型的准确度。 这里问题就来了。哪些数据可以用来做测试组呢？\n理论上，所有已知结果的数据都应该拿来训练，这样训练才充分。可是，这些训练数据 以外的都是不知道结果的，无法拿来做测试？用训练数据本身做测试，肯定不行！因为你的 算法就是基于你的这组数据产生，肯定适合你的这些数据，但是换这些数据以外的数据可能 就不适合了，属于过学习。那么，我们从训练组数据里拿出一小部分做测试用，用剩下的做 训练，这样是否可行呢？也不行，因为有一部分知识预测模型没有学习到，属于欠学习\n这时，我们可以使用 K 次交叉检验（K-fold cross validation）。所谓 K 次交叉检验就是把 所有能够搜集到的已知结果的数据，分成 K 份。我们以 4 份为例。将第一份拿出来作为测 试组数据，其余 3 份作为训练组数据。用选定的算法根据训练组数据训练出一个模型。再用 测试组数据测试模型的准确度。然后将第二份拿出来作为测试组数据，其余 3 份作为训练组 数据。用选定的算法根据训练组数据训练出一个模型。再用测试组数据测试模型的准确度。 依次类推，让每一份都作一次测试组数据。这样就会用同一种选定的算法构建出 4 个模型， 进行 4 次测试，得到 4 个准确度。最后取 4 个准确度的平均值，得到的平均准确度就是用全 部训练组数据训练出的最终模型的准确度。利用 K 次交叉检验可以有效避免过学习的现象。 因为测试组数据并未参与相应模型的构建。同时也避免了欠学习的现象，因为所有训练组数 据都学习到了\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.735444006999125in\u0026rdquo;}\n常见算法 机器学习能完成诸如分类、聚类、回归等任务。那他是如何完成这些任务的呢？是通过 各种机器学习的算法来完成的。常见的算法有贝叶斯、最近邻居、决策树、支持向量机、人 工神经网络、遗传算法等。我们没有足够的时间从理论上完全掌握这些算法，但是应该从科 普的角度至少知道他们是怎么回事。这里挑其中一二给大家介绍一下\n所谓贝叶斯法就是基于贝叶斯原理的一种概率统计算法。关于贝叶斯定理我们已经在前 面的章节给大家详细介绍过了。这里就不多说了\n{width=\u0026ldquo;5.305555555555555in\u0026rdquo; height=\u0026ldquo;2.5in\u0026rdquo;}\n再来看最近邻居法。它是把已知物体根据它们的属性，标记在一个坐标系中。比如各种 钉子根据它们的外形是红色这堆，各种螺丝是绿色这堆，蓝色这堆是各种螺母。因为螺母外 形上跟钉子和螺丝差别较大，所以在坐标系中，它离另两种物体比较远。接下来我们根据新 物体的外形把它也标记在坐标系中。然后看离它最近的邻居是什么，它就是什么。这里，离 新物体最近的是螺母，那么新物体就应被分类为螺母\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.9826541994750655in\u0026rdquo;}\n决策树非常好理解，给大家举个例子就明白了。这是个保险公司统计的被保人所处年龄、 所开车型与出险率高低的统计表。历年数据显示，23 岁开家庭车的司机风险高，17 岁开跑 车的高，简直就是马路杀手，43 岁开跑车的也高，看来跑车风险就是高。68 岁开家庭车的 低。32 岁开大卡车的也低。别问什么，这是德国教材上的例子。现在我们来看看保险公司 利用这些数据都做了什么。他们根据这些数据画了一棵树，并利用这棵树对即将投保的人进 行风险评估。先判断车型，大卡车的风险低，不是大卡车的再判断年龄，60 岁以上低，60 岁以下高。有了这棵树，只要知道新投保人的年龄和车型，就可以判断出风险高低。这就是 决策树\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.56538823272091in\u0026rdquo;}\n支持向量机和最近邻居差不多，也是把已知物体根据属性标记在坐标系中，然后画一条 线。画的这条线要尽可能的把不同物体全部分开。新物体落在线的哪一侧就属于哪种物体。 问题的关键就是这条线怎么话。绿色和蓝色两种画法肯定都没有黄色画法好。比如绿色这条 线，如果新物体落在这个位置，因为他在绿线的右侧，所以他会和黄点归入一类，但实际上 他跟这堆蓝点离得更近。如果，这条线画成黄线这样，就不会出现刚才的问题。所以我们要 画这样一条线，首先这条线要把不同的已知物体尽可能的分开，且线两边距离线最近的物体 到线的垂直距离要尽 可能的大。也就是所 谓的间隔最大化，也 叫做最大最小距离。 只有这样画出线才能 成功完成任务\n{width=\u0026ldquo;5.666666666666667in\u0026rdquo; height=\u0026ldquo;3.1666666666666665in\u0026rdquo;}\nWEKA 介绍一款做数据挖掘的傻瓜级软件 WEKA\nhttp://www.cs.waikato.ac.nz/ml/weka/\nWEKA 的全名是怀卡托智能分析环境。WEKA 也是新西兰一种鸟的名字。WEKA 的主要开 发者来自新西兰怀卡托大学。WEKA 是免费的，它可以完成各种各样的数据挖掘任务，就 像傻瓜相机一样，算法的事儿完全不需要你操心，你只要输入数据，告诉 WEKA 你要完成 什么样的挖掘任务，再选择现成的算法，WEKA 就会为你返回想要的结果模型\nWEKA 不能读取 Excel 数据。WEKA 的数据 存储格式是 ARFF 格式。这种格式的文件其实就是一个纯文本文件，可以用写字板或记事本 打开。在 WEKA 安装目录下的 data 文件夹里有许多 ARFF 文件。我们用记事本打开其中的 weather_numeric.arff\n（右）是 ARFF 数据的原始容貌。我们也可以用 WEKA 的 ARFF viewer，将这些文本样式的数据转换成表格形式（左）。这组数据记录了不同的 天气与是否适合打球之间的对应关系。比如晴天，温度 85，湿度 85，不刮风的时候不适合 打球，而阴天，温度 80，湿度 90，不刮风的时候适合打球\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.348228346456693in\u0026rdquo;}\nWEAK中的术语 表格里的一行叫做一个实例（Instance），相当于统计学中的一个样本， 或者数据库中的一条记录。这组数据一共有 14 行，也就是 14 个实例。表格里的一列称为一 个属性（Attribute），这个表格一共有 5 个属性，分别是 outlook（天气概况）、temperature（温 度）、humidity（湿度），windy（是否刮风）以及 play（是否打球）。前四个属性是关于天气 的描述，最后一个属性是要判断的关键性问题。这个表格实际上就是反映了前四个属性跟最 后一个属性之间的关系。所以 WEKA 管一个表格叫一个关系（Relation）。这个关系的名字 是 weather（天气）\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2301662292213473in\u0026rdquo;}\nARFF文件的格式 WEKA 读取 ARFF 文件的重要依据是分行和空格，因此不能在这种 文件里随意的断行，以及随意加入空格。空行以及全是空格的行将被忽略。打开一个 ARFF 文件，经常会看到大段%开头的内容，这些是关于数据的注释。WEKA 在读取文件时会自动 忽略这些行。除去注释后，整个 ARFF 文件可以分为两个部分，第一部分头信息（head information）是对关系和属性的定义，第二部分数据信息（data information）就是数据值\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.330139982502187in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.262023184601925in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.249320866141732in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.19873687664042in\u0026rdquo;}\n属性类型和转换 ARFF 格式中共有四种属性类型，分别是数值型（numeric）、标称型（nominal）、字符串 型（string）和时间日期型（date）\n数值型 numeric {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2014971566054244in\u0026rdquo;}\n标称型 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2938462379702536in\u0026rdquo;}\n字符串型 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.220544619422572in\u0026rdquo;}\n时间日期型 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2407403762029747in\u0026rdquo;}\n转换 WEKA 虽然不能读取 Excel 文件，但是它可以读取 csv 文件。更加值得庆幸的是， Excel 也可以读写 csv 文件。比如示例文件 test.xls 是 Excel 文件。数据的第一行说明了 每一列是什么内容，之后共有四行数据。我们可以将这个 Excel 文件另存为 csv 文件，起名 为 test.csv。csv 文件是用逗号分隔各列的纯文本文件，可以用记事本打开，也可以用 Excel 打开，还可以用 WEKA 的 Arff viewer 打开。用 WEKA 的 Arff viewer 打开 test.csv 之后， 就可以通过 save as（另存为）将数据保存为 ARFF 文件了，起名为 test.arff。用记事 本打开 test.arff，可以看到 WEKA 将文件名作为关系的名字，并且根据原来 Excel 里第 一行的内容自动定义了属性，并根据后面数据的情况，自动给属性分配了属性类型。注意， 自动分配的属性类型可能不准确。比如 name（名字）这个属性，很显然不应该是标称型的， 而应该是字符串型的，所以需要人工更正。接下来的数据部分会自动用逗号分隔，缺失数据 也会自动填补问号\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3501115485564306in\u0026rdquo;}\nExplorer界面介绍 数据准备好了，我们可以开始挖掘了。 WEKA 主窗口打开 Explorer 界面。Explorer 界面可以帮助我们完成数据预处理和各种挖 掘任务。点 open file，载入我们要挖掘的数据，weather_numeric.arff 文件\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3918864829396327in\u0026rdquo;}\n数据 载入后，整个界面根据功能不同，可分成 8 个区域\n区域 1 的几个选项卡是用来切换不同的挖掘任务面板的。我们当前所处的面板是数据预 处理面板，可对数据进行预览和预处理。后面还有分类任务面板，聚类任务面板等 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.303781714785652in\u0026rdquo;} \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1282852143482063in\u0026rdquo;} \u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.381901793525809in\u0026rdquo;} \u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.572516404199475in\u0026rdquo;} \u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.876922572178478in\u0026rdquo;} \u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.753149606299212in\u0026rdquo;} \u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; 区域7 下方的直方图会给出具有不同outlook标称的实例中play 是yes 和no 的分布情况。 比如 outlook 是 sunny 的这五条实例中，play 是 yes 的是蓝色这些，play 是 no 的是红色这些。 outlook 是 overcast 这 4 条实例中，play 都 是蓝色的，也就是都是 yes。Outlook 是 rainy 的 5 条实例中，play 是 yes 的是蓝色的三条， play 是 no 的是红色的两条\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.125in\u0026rdquo;}\n\u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; 现在我们点 Visualize All 看一下所有属性的直方图。这时看到，标称型属性和数值型属 性的直方图是有差别的，数值型的直方图是根据平均值划分出两个区段，分别统计每个区段 里 class 属性的分布。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4766666666666666in\u0026rdquo;}\n\u0026lt;!-- --\u0026gt; \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.688869203849519in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 数据预处理 在进行挖掘任务之前，通常还需要对数据进行预处理，比如更换属性类型或者增加删减 属性等。这些预处理工作主要是通过 Explorer 界面下的 Filter 下拉菜单里的各种函数来实现 的。比如在实际应用中，我们经常会需要把数值型的属性改成标称型的属性。这时可以用 Filter 下的 unsupervised 下的 attribute 下的 discretize 离散化函数来实现\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.389717847769029in\u0026rdquo;}\n选中 discretize 函数后，点击选中后出现的参数框。弹出参数设置窗口。从 AttributeIndices （属性代号参数）指定要更改哪个属性的属性类型。比如我们更改第二个属性 temperature （温度）和第三个属性 humidity（湿度），这两个数值型的属性，那么这里就写\u0026quot;2,3\u0026quot;。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.350938320209974in\u0026rdquo;}\ndiscretize 函数会将所有实例中对应属性下的数值离散化成几个区段，每个区段赋予一个标称，同一区 段里的数值都转化成这一区段的标称。所以我们还需要指定一下，要离散化成几个区段。这 里我们定为 3 个，那么新属性将具有 3 个标称。其他参数不变，点 ok，窗口关闭，再点 apply\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.28882217847769in\u0026rdquo;}\n现在看一下 temperature（温度）这个属性，原来是数值型的。离散化函数处理之后，变 成了标称型。函数将所有温度数值离散化成了三个区段，\u0026rsquo;(-inf-71]\u0026rsquo;（温度值小于 71） 的都归入了第一区段，拥有第一个标称。标称的具体写法虽然怪异，但是它很清楚的告诉了 我们哪些数值归入了第一个标称。\u0026rsquo;(71-78]\u0026rsquo;（温度值在 71 到 78 之间）的归入第二区段， 拥有第二个标称；\u0026rsquo;(78-inf)\u0026rsquo;（温度值大于 78）的归入第三区段，拥有第三个标称\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.941175634295713in\u0026rdquo;}\n可以看到，现在标称型的温度属性的直方图已经变成三个离散的柱子了，而不再是根据 平均值划分统计了。除了 discretize 离散化函数，NumericToNominal 函数也可以将数值型的 属性转化成标称型。这两个函数虽然达到的最终目的是一样的，但是具体的转化方法是不一 样的。究竟哪里不一样，请大家自己尝试比较一下\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3717694663167106in\u0026rdquo;}\n通过 AddExpression 函数可以增加一个属性。比如增加新属性的值等于温度除以湿度。 重新打开 weather_numeric.arff 文件。此时，温度和湿度的属性都是数值型的。Filter →unsupervised→attribute→AddExpression。从参数设置窗口设置新属性的公式，温度除以湿 度，也就是第二个属性 a2 除以第三个属性 a3，即a2/a3。再定义新属性的名字为 temp/humi。 点 ok，点 apply，新属性就产生了。 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.350224190726159in\u0026rdquo;}\n注意新创建的属性都会添加在属性列表的最后，这就会 影响 WEKA 对 class 属性的判断，需要手动将 class 属性重新选为 play\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.241373578302712in\u0026rdquo;}\n执行挖掘任务 WEKA 把分类和回归两种挖掘任务都放在\u0026quot;Classify\u0026quot;选项卡中，是有原因的。因为这 两种挖掘任务都有一个目标属性，也就是输出变量，并且都是根据实例的一组特征属性，也 就是输入变量，对目标属性进行预测。比如之前讲过的分类例子是根据各种天气特征属性预 测是否打球这个目标属性。回归例子里是根据房屋特征属性来预测房屋价格这个目标属性。 为了实现这一目的，首先需要有一个训练数据集，这个数据集中每个实例的输入和输出都是 已知的。观察训练集中的实例，可以建立起预测模型。有了这个模型，就可以对新的输出未 知的数据进行预测了。衡量模型的好坏就在于预测的准确程度如何。目标属性是标称型的， 就是分类任务，目标属性是数值型的就是回归任务。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.218390201224847in\u0026rdquo;}\nWEKA 自带的典型分类算法有贝叶斯、人工神经网络、支持向量机、决策树等等。典 型的回归算法有线性回归和简单线性回归等\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.346788057742782in\u0026rdquo;}\n一个具体的任务\nWEKA 的安装目录下有一个有关糖尿病的 ARFF 文件：WEKA 安装目录/data /diabetes.arff。里面记录了 768 位 21 岁以上女性的 8 种生理指标跟糖尿病检测结果是 阴性还是阳性之间的关系。我们要用这组数据来创建一个预测模型，通过这个模型只要输入 某位未做过糖尿病检测的女士的这 8 种生理指标就可以预测出她是否患有糖尿病\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.045634295713036in\u0026rdquo;}\n具体的操作\nExplorer 界面下，首先从 preprocess 标签下打开，diabetes.arff 文件，导入数据。然后在 classify 标签下选择分类算法。选 classifiers→tree（决策树）→random tree（随机决策树）来 创建预测模型。预测模型准确度的检验方法选 10 次交叉检验，再次确认目标属性是否选择 正确。最后点 start。任务开始后会看到 WEKA 鸟走了两步又坐下了，说明任务完成\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.1958333333333333in\u0026rdquo;}\n任务完成后，output 窗口里显示出了一个文本形式的决策树。这样的决策树方便进一步 编写程序以实现预测模型的自动化。此外，还有图形样式的决策树。从任务列表里找到刚刚 完成的任务，右键点击，选 Visualize Tree。这时一棵图形化的决策树就出现了。因为树太大， 窗口又太小，所以全都挤在一起了。为了看清楚，可以放大窗口，再右键点击，选 fit to screen 将树放大\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.260825678040245in\u0026rdquo;}\n有了这棵树，我们只需要输入 8 种生理指标，就可以从树根开始一层一层的判断直到末 端的叶子。叶子上写的总是目标属性，也就是预测的糖尿病检测结果是阴性还是阳性。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.2964370078740157in\u0026rdquo;}\n有了预测模型只是第一步，我们还需要知道预测模型的准确度如何，也就是判断模型是 否可信。10 次交叉检验的返回结果告诉我们，交叉检验下，68%的实例能够预测出正确结果， 32%的预测结果是错误的。除此之外，还提供了其他很多准确度衡量参数。交叉检验后得出的 Confusion Matirx 是个很有用的矩阵。它提供了 TPFPTNFN 的统计值，用以计算上述的各 种准确度衡量参数\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.708195538057743in\u0026rdquo;}\n由此得出的 TP、FP、TN、FN 可以带入公式计算各种准确度衡量参数，包括 sensitivity 和 specificity。当把 test_negative 定义为阳性结果，test_positive 定义为阴性结果的时候，会得 出另外一套 TP、FP、TN、FN，从而计算出另一套准确度衡量参数。两次统计结果可分别 体现出目标属性中每个标称的预测质量\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;6.325597112860892in\u0026rdquo;}\n","date":"2021-11-02T00:00:00Z","permalink":"https://example.com/p/%E5%B1%B1%E4%B8%9C%E5%A4%A7%E7%94%9F%E4%BF%A1_%E7%94%9F%E7%89%A9%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98_note/","title":"山东大生信_生物数据挖掘_note"},{"content":" 写在前面的话：这个笔记不过多深入知识，因为做入门有，大概知道什么时候该用哪个，以及现在有什么技术\n进化树 引言 Nothing in Biology Makes Sense Except in the Light of Evolution（如果生物学没有了进化， 那么一切都将黯然无光）\n分子进化概念 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1352602799650042in\u0026rdquo;}\n所谓变异速度是指一定时间内不同碱基或氨基酸突变的个数。这个 进化变异速度被认为是恒定的，跟物种没有关系。所以，拿蛋白质来说，两个蛋白质在序列 上越相似，他们距离共同祖先就越近。分子钟理论是进化研究领域被普遍认可的理论，但是至今也没有直接的证据证实。\n一些基本概念 同源（Homologs），相同来源。没错，但是它的确切定义是，来源于共同祖先的相似序 列为同源序列。也就是说，相似序列有两种，一种是来源于共同祖先的，那么他们可以叫同 源，另一种不是来源于共同祖先的，那么他们尽管相似也不能叫同源。 \u0026lt;!-- --\u0026gt; 第二种情况出现的概 率虽然低，但还是存在的，所以相似序列并不一定是同源序列。同源又分为三种，直系同源， 旁系同源和异同源。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.80228237095363in\u0026rdquo;}\n\u0026lt;!-- --\u0026gt; 直系同源（Orthologs）是指，来自于不同物种的由垂直家系，也就是物种形成，进化而 来的基因，并且典型的保留与原始基因相同的功能。也就是说，随着进化分支，一个基因进 入了不同的物种，并保留了原有功能。这时，不同物种中的这个基因就属于直系同源 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.332113954505687in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 旁系同源（Paralogs）是指在同一物种中的来源于基因复制的基因，可能会进化出新的 但与原功能相关的功能来。基因复制产生了两个重复的基因，多出来的这个有几种命运，一 个是又丢了。复制出来发现没有用，又删了。另一种命运是演化出了新的功能。如果这个新 功能是往好的方向发展，就会被保留下了，如果是往不好的方面发展，就会被自然选择淘汰。 还有一种命运，就是被放置不用。复制出来以后，又加了个终止子，既不表达，也不删除， 搁那里搁着不管，成了伪基因。被保留下来的具有新功能的基因与另一个复制出来的基因之 间就是旁系同源。 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.564267279090114in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 异同源（Xenologs）是指通过水平基因转移，来源于共生或病毒侵染所产生的相似基因。 异同源的产生不是垂直进化而来的，也不是平行复制产生的，而是由于原核生物与真核生物 的接触，比如病毒感染，在跨度巨大的物种间跳跃转移产生的 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.4842333770778655in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 不同的同源，概念很容易混淆。图 1 清楚的描述了各种同源之间的关系。首先，有个早 期的球蛋白基因，它通过基因复制，形成了α球蛋白基因和β球蛋白基因。后来随着进化，这 两种复制产生的基因也存在于不同的物种中。其中某一物种里的，比如老鼠里的α球蛋白基 因和β球蛋白基因就属于旁系同源。而某一个基因在不同物种中，比如青蛙里的α球蛋白基因 和鸡里的α球蛋白基因就属于直系同源。再比如，某个细菌，它没有早期的球蛋白基因，也 自然没有β球蛋白基因，但是通过与青蛙的共生，发生了基因水平转移。于是它从某一天就 起有了β球蛋白基因。那么这个细菌的β球蛋白基因和青蛙的β球蛋白基因就属于异同源 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0876760717410323in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 一个小知识点 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.27665791776028in\u0026rdquo;} 之前我们讲过两条序列的相似度如何计算，那么能不能定量描述同源呢？答案是不可以， 同源只是对性质的一种判定，只能定性描述，不能定量描述。所以\u0026quot;同源性等于 80%\u0026ldquo;这种 说法是错误的！\n树和网 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.575250437445319in\u0026rdquo;} 编织生命网 的要素之一就是水平基因转移 ，水平基因转移（horizontal gene transfer）是指生物将遗传物质传递给其他细胞而非其子代细胞的过程\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3388156167979in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.7101213910761155in\u0026rdquo;}\n系统发生树的样子 研究分子进化所要构建的系统发生树（Phylogenetic tree），也叫分子树\n对于一个未知的基因或蛋白质序列，可以利用系统发生树 确定与其亲缘关系最近的物种。比如你得到了一个新发现的细菌的核糖体 RNA，你可以把 它跟所有已知的核糖体 RNA 放在一起，然后用他们构建一棵系统发生树。这样就可以从树 上推测出谁和这个新细菌的关系最近\n系统发生树还可以预测一个新发现的基因或蛋白质的 功能。以基因为例，如果在树上与新基因关系十分密切的基因的功能已知，那么这个已知的 功能可以被延伸到这个新基因上\n构建系统发生树还有助于预测一个分子功能的走势。也就 是从树上可以看出某个基因是正在走向辉煌还是在逐渐衰落\n系统发生树还能帮助我 们追溯一个基因的起源。甚至当它从一个物种\u0026quot;跳\u0026quot;到另一个物种上，也就是发生了水平基 因转移时，系统发生树都可以很好的展示出来\n\u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.652173009623797in\u0026rdquo;} {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.0666207349081365in\u0026rdquo;}\n根和节都表示理论上曾经存 在的祖先，叶子是现存的物种。这一点很重要！比如我们要研究某个基因，于是搜集了很多 物种的这个基因的序列，用它们构建了一棵系统发生树。搜集到的物种都出现在叶子上，也 就是外节点上，没有在内节点上的。内节点上都是理论上曾经存在过的共同祖先，现在已经 不存在了！此外，枝子的长短也是有意义的\n根据建树所用序列的多少来选择不同形 状的树。如果序列非常多，那么圆形的看上去就要比方的或者三角的舒服得多，便于在文献 里排版。系统发生树上从任何一个点发出的枝子围着这个点旋转都不改变树的生物学意义， 只是视觉上有点儿差别而已。所以，旋转之后的两棵树是等价的，生物学意义完全相同\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.031599956255468in\u0026rdquo;}\n系统发生树的种类 系统发生树还分为有根树和无根树，顾名思义，有根树就是有根，无根树就是 无根。其实两者是可以互换的。如果我们按住无根树上某一个点，然后用把梳子将树上所有 的枝条都以这个点为中心向右梳理，就能把它梳成有根树的样子。按住的这个点就是根。所 以对于一棵树来说，根的位置是主观的，你想让他在哪它就在哪里。但是你不能随意指定哪 个内节点当根，毕竟根有其自身的生物学意义，它应该是所有叶子的共同祖先。那么我们如 何确定根的位置呢？可以通过外类群（outgroup）来确定，从而把无根树变成有根树\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5430380577427822in\u0026rdquo;}\n所谓外类群，就是你所研究的内容之 外的一个群。比如你要分析某一个基因在不同人种间的进化关系，那就可以额外选择黑猩猩 加入进来，作为外类群一同参与建树。或者你要分析哺乳动物，那就可以选鳄鱼、乌龟之类 的。总之，保证外类群在你要研究的内容之外，但又不能太远。外类群可以不只是一个物种， 而是多个，但也不要太多，两三个即可。为什么有了外类群之后，做出来的树就是有根树了 呢？因为你知道外类群和你研究的内容一定不是一伙的，所以外类群分支出的那个内节点就 是根\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.6395395888013997in\u0026rdquo;}\n讲的分子树跟物种树是有本质区别的。物种树是基于每个物种整体的进化关系， 也就是基于整个基因组构建的，而分子树是基于不同物种里某一个基因或蛋白质序列之间的 关系构建的。那么一个分子树表达出来的各物种之间的关系就可能与物种树完全不同。此时 说明这个基因经历了特殊的进化故事。也许是受到了特殊环境变化的影响，也许是发生了水 平基因转移等等。总之，这种区别的出现是很有研究价值的\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.768365048118985in\u0026rdquo;}\n系统发生树的构建方法 最常用的有基于距离的构建方法，包括非加权分组平均法 （Unweighted Pair Group Method with Arithmetic mean，UPGMA），最近邻居法（NeighborJoining method，NJ），最小二乘法（Generalized Least Squares，GLS）等。还有最大简约法 （Maximum Parsimony，MP），最大似然法（Maximum Likelihood，ML），贝叶斯推断法（Bayesian Inference，BI）等\n从计算速度来看，最快的是基于距离的方法， 几十条序列几秒钟即可完成。其次是最大简约法。最大似然法就要慢得多。最慢的是贝叶斯 法。但是从计算准确度来看，算得最慢的贝叶斯法确是最准确，而算得最快的基于距离法结 果确是最粗糙。从实用的角度，建议使用最大似然法。因为这种方法无论从速度还是准确度 都比较适中。最近邻居法虽然算得快，但是当序列多，彼此差别小的时候，这种方法不适合。 最大简约法，似乎是个掉空里的方法，高不成低不就，所以很少有人使用。贝叶斯法不是所 有的建树软件都提供，算法开发上还有待提高，而且计算时间过长\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.210430883639545in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.2344991251093616in\u0026rdquo;}\nhttps://www.megasoftware.net/\n以非加权分组平均法（UPGMA 法）：\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.7601345144356957in\u0026rdquo;} 在接下来的例子里，我们用序列间不同的碱基数目作为序列间遗传距 离的度量。首先，计算出每两条序列间有几个碱基不同，并以用矩阵的形 式记录下这些距离\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;6.116220472440945in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.522497812773404in\u0026rdquo;}\nMEGA 7 构建NJ树 http://www.megasoftware.net/\n初步使用 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.1582305336832897in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.8658530183727033in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.559730971128609in\u0026rdquo;}\n这个多序列比对作为中间结果保存下来\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.966666666666667in\u0026rdquo;} {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.6509426946631671in\u0026rdquo;}\n想进一步了解序列的保守性 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;8.394635826771653in\u0026rdquo;}\n准备工作到此全部完成\n构建 NJ 树 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.984195100612424in\u0026rdquo;} 步长检验是根据所选的建树方法，计算并绘制指定次数株系统发生树。因为大多数建树 方法的核心算法都是统计概率模型，所以每次计算出的树都会有所差别。而建好的系统发生 树上每个节点上都会标记一个数字，它代表了指定次数次计算所得出的系统发生树中有百分 之多少棵树都含有这一节点。一般来说，绝大多数节点上的数值都大于 70%的树才可信。个 别低于 70%的节点可以暂且容忍，或通过添加，删减序列来改善质量。\n第二个参数是，Substitution Model。它是选择计算遗传距离时使用的计算模型。理论上应该尝试各种模型，根据检验结果选择最合适的模型进行计算。但在实际操作中，可先尝试 选用较简单的距离模型，比如 p-distance\n第三个参数是 Gap/Missing Data Treatment。大多数建树方法会要求删除多序列比对中含 有空位的列。但是根据遗传距离度量方法的不同，删除原则也不同。如果是以序列间不同残 基的个数来度量遗传距离的话，这里需要选择 Complete deletion（全部删除）。如果是其他方 法，比如这里选用的 NJ 方法，可以选择 Partial deletion（部分删除）。删除程度定在 50%， 即，保留一半含有空位的列\n最终的树\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.5252919947506562in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.230919728783902in\u0026rdquo;}\n换进化树的形状 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;4.986111111111111in\u0026rdquo; height=\u0026ldquo;0.875in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 调整树枝的粗细或字体的大小\n可以从 View 下拉菜单 下的 Option 选项卡中调整。\n保存为图片\n\u0026lt;!-- --\u0026gt; 保存图片可以点 Image 下拉菜单，选择保存格式。或者将窗口放大，再点 按钮将树放大之后屏幕截图 ","date":"2021-11-01T00:00:00Z","permalink":"https://example.com/p/%E5%B1%B1%E4%B8%9C%E5%A4%A7%E7%94%9F%E4%BF%A1_%E8%BF%9B%E5%8C%96%E6%A0%91_note_/","title":"山东大生信_进化树_note_"},{"content":" 写在前面的话：这个笔记不过多深入知识，因为做入门有，大概知道什么时候该用哪个，以及现在有什么技术\n简单的生物统计应用以及序列算法 Bayesj基础 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;8.125433070866142in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.6918602362204727in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.400382764654418in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.6757983377077865in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5953871391076118in\u0026rdquo;}\nBayes在生物学的应用 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.822579833770779in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0538134295713038in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2251596675415573in\u0026rdquo;}\n二元预测的灵敏度和特异度 基本介绍 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2344356955380578in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.480990813648294in\u0026rdquo;}\nA的灵敏度达到100%，说明它对发生很敏感，只要有发生就会探测到；A的特异度60%说明所引起的探测不一定是由于地震引起的\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3598140857392824in\u0026rdquo;}\n在生物学上的应用 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.343495188101487in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2347659667541557in\u0026rdquo;}\n位点特异性加权矩阵 每一行是一个LRR的序列（长度为11）（上半部分）, 来看看每一个氨基酸即20个氨基酸出现的概率（下半部分）；如第一行A在第一列（1）中 代表A这个氨基酸出现在第一个序列中的占比为0.3%\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0606332020997375in\u0026rdquo;}\n当作打分矩阵用来预测是哪里出现了LRR序列\n如：构造一个长度为11的小窗口，一个一个位置往后面扫描，每一次都打一次分，\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.589207130358705in\u0026rdquo;}\n如何打分：\n比如这个 LTVLMLLHNQL\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5772353455818022in\u0026rdquo;}\n在矩阵中第一列找到L中出现的百分比为： 75% 即 0.75；同样的方法找到后面的氨基酸的百分比之和，转为小数，相加求和\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.8992311898512684in\u0026rdquo;}\n量化的标准（阈值）的确定，即低于这个分值的不是LRR序列\n怎么找到一个合理的阈值（即灵敏度和特异性高）\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0096095800524933in\u0026rdquo;}\n看这两个的交点 （ 或者将这两条曲线叠加起来，纯数学求和，取最高值 ） {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1744925634295713in\u0026rdquo;}\n基本序列算法 基本概念 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.188888888888889in\u0026rdquo;}\n构建后缀树 后缀就是包含最后一个字符的子序列；\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.139086832895888in\u0026rdquo;}\n但要注意序列自己本身就是一个子序列\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.9196948818897637in\u0026rdquo;}\n将获得的后缀树画成一棵树，（相同的开头可以共用，注意都要从根点开始考虑）\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.7858081802274715in\u0026rdquo;}\n后缀树的使用 查找是否在序列中\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.7805358705161853in\u0026rdquo;}\n查找重复次数\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.7929286964129485in\u0026rdquo;}\n查找最长重复子序列\n先找到所有内节点，看其出现了最多字母的\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.861525590551181in\u0026rdquo;}\n$的作用\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0688932633420825in\u0026rdquo;}\n最高分-子序列 概念：\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.931441382327209in\u0026rdquo;}\n生物学的应用：\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.734915791776028in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.807363298337708in\u0026rdquo;}\n计算的复杂度\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.867911198600175in\u0026rdquo;}\n","date":"2021-10-31T00:00:00Z","permalink":"https://example.com/p/%E5%B1%B1%E4%B8%9C%E5%A4%A7%E7%94%9F%E4%BF%A1_%E7%AE%80%E5%8D%95%E7%9A%84%E7%94%9F%E7%89%A9%E7%BB%9F%E8%AE%A1%E5%BA%94%E7%94%A8%E4%BB%A5%E5%8F%8A%E5%BA%8F%E5%88%97%E7%AE%97%E6%B3%95/","title":"山东大生信_简单的生物统计应用以及序列算法"},{"content":" ","date":"2021-10-30T00:00:00Z","permalink":"https://example.com/p/%E7%94%9F%E7%89%A9%E5%8C%96%E5%AD%A6_%E4%B8%8A/","title":"《生物化学_上》"},{"content":" 写在前面的话：这个笔记不过多深入知识，因为做入门有，大概知道什么时候该用哪个，以及现在有什么技术\n高通量测序 （对于我，这章了解就好了) 基因组学与测序技术 Sanger 测序\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.018454724409449in\u0026rdquo;}\n高通量测序\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.314882983377078in\u0026rdquo;}\n得到一个整个的DNA，将其捣碎，加入测序相关的试剂进去，体系放在一个槽里，每个槽里就是每个测序反应（可加入荧光观察）； 会有个拍照系统，每个反应中每隔一段时间拍一张；\n测序在医学中有大作用\n数据本身的复杂性\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.381519028871391in\u0026rdquo;}\n测序本身都会有错误，所以在进行之前，要排查出来；\n海量数据的计算和挖掘成为主要瓶颈；用内存计算拼接；\n从头测序 de novo sequencing 片段化、零碎的信息拼接成染色体水平\nOverlap Graph : 基于read重叠区的，去找到他们的重合，然后再末端延申去获得这些片段;也就是说这里是将两两的序列作比较\nDe Bruijn Graph: 把read切成特定大小的长度\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.6531977252843393in\u0026rdquo;}\n都很难解决重复区域\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4084962817147857in\u0026rdquo;}\n重测序 转录组测序 测序对象是不是DNA序列，而是 DNA转录的产物\n表观基因组学 在DNA上的修饰，DNA上本身的甲基化（甲基化可以沉默基因的表达）、组蛋白的修饰，组蛋白的promoting的打开与否，\n来测试这种打开的信号\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.331119860017498in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.464051837270341in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.072636701662292in\u0026rdquo;}\n猛犸象基因组测序计划 ","date":"2021-10-30T00:00:00Z","permalink":"https://example.com/p/%E5%B1%B1%E4%B8%9C%E5%A4%A7%E7%94%9F%E4%BF%A1_%E9%AB%98%E9%80%9A%E9%87%8F%E6%B5%8B%E5%BA%8F_note/","title":"山东大生信_高通量测序_note"},{"content":" 写在前面的话：这个笔记不过多深入知识，因为做入门有，大概知道什么时候该用哪个，以及现在有什么技术\n蛋白质结构预测\n蛋白质的结构 你将洞悉一个蛋白质到 底长什么样子（蛋白质三维结构），它和它的闺蜜手拉手拍合影的样子（蛋白质和蛋白质分 子对接），它嘴巴里塞满食物的样子（蛋白质和小分子分子对接），以及它在你身体里活动的 样子（分子动力学模拟)\n一级结构也就是氨基酸序列， 二 级结构是周期性的结构构象，比如α螺旋β折叠等 三 级结构是整条多肽链的三维空间结构 四级结构是几 个蛋白质分子形成的复合体结构，比如三聚体，四聚体等 蛋白质是由氨基酸组成的，前一个氨基酸的羧基和后一个氨基酸的氨基脱去一分子的水，缩合形成的肽键。肽键将氨基酸连接起来形成肽链。成熟的肽链 并不是一根松散的毛线，它要经过折叠变成一个线团，即，形成空间立体结构。拥有了空间立体结之后，蛋白质才能上岗工作。 蛋白质的二级结构 DSSP指认 蛋白质经过折叠后会形成规则的片段，这些规则的片段构成了蛋白质的二级结构单元 （图 1）。三种常见的二级结构单元包括螺旋、β折叠、和转角。螺旋中最常见的就是α螺旋， 但不只有α螺旋，还有其他的螺旋，比如 3 转角螺旋，5 转角螺旋等。β折叠由平行排列的β 折片组成。这些折片在序列上可能相隔很远，但是在空间结构上并排在一起，彼此间形成氢 键。除了螺旋和折叠外，蛋白质结构中还存在大量的无规律松散结构 coil。如果这些无规律 的肽链突然发生了急转弯，这个转弯结构就叫做β转角\n蛋白质的二级结构经常用图形来形象的描述。比如黄色的箭头代表对应的氨基酸 具有β折片结构。波浪线代表螺旋结构，小鼓包是转角。此外，以字母形式书写的二级结构 序列能够更加精准的描述。其中，E 代表β折叠，H 代表α螺旋，T 代表转角。没有写任何字 母的地方是松散的 coil 结构\n研究人员根据 DSSP，也就是蛋白质二级结构定义词典，将三级结构里的二级结构单元指认出来的\n然后再按照规定的格式，记录下蛋白质中每个氨基酸处于哪种二级结构单元。这样一 个记录蛋白质二级结构信息的文件叫做 DSSP 文件。蛋白质结构数据库 PDB 中的每一个蛋 白质三级结构都有自己对应的 DSSP 文件。DSSP 文件里不同字母所代表的不同二级结构单 元和 PDB 里面的记录方式是统一的\nDSSP 的主页上，Introduction 部分有一个 Web server 链接，这个链接很容易让人误以为 可以通过它预测某条氨基酸序列的二级结构。这是不对的。DSSP 网站的 Web Server 可以指 认蛋白质结构文件，也就是 PDB 文件中的二级结构，并创建出相应的 DSSP 文件。提交的 PDB 文件可以是用实验方法刚刚解析出来，还没有提交 PDB 数据库的蛋白质三级结构，也 可以是用计算方法预测出来的蛋白质三级结构模型。总之，输入值必须是三级结构，而不是 一级的氨基酸序列（PDB ID 必须是小写的）\nPDB获取 从 序列图形化部分可以看到二级结构对应在一级结构上的图形化表示\n点击左侧的\u0026quot;View Sequence \u0026amp; DSSP Image\u0026quot;可以获得直观的一级结构对二级结构的序列表示（图 2）。图 2 中的 序列有两行，上面的一行是一级结构，下面的是二级结构。这个页面看上去很不错，序列 10 个字母一间隔，50 个字母一行，而且不同的二级结构还对应不同的字母颜色。但是在接 下来的分析研究工作中，我们往往需要的是像氨基酸序列那样的 FASTA 格式的二级结构序 列。非常遗憾的是， PDB 里没有现成的针对某一个蛋白质的 FASTA 格式二级结构序列下载链接。\u0026ldquo;Download FASTA File\u0026quot;链接只能下载 FASTA 格式的一级结构序列，也就是氨基酸序列\n此外，PDB 数据库中有一个叫做\u0026quot;ss.txt\u0026quot;的文件：http://www.rcsb.org/pdb/files/ss.txt.gz (压缩文件 30.6M)。这个文件里面有 PDB 所有蛋白质结构的一级和二级结构的 FASTA 格式 序列。\nhttp://www.rcsb.org/pdb/files/ss.txt.gz\n软件预测 但是，已知空间 结构的蛋白质在 PDB 数据库里毕竟只有 10 万多个。然而，UniprotKB 数据库里却有几百万 条蛋白质序列。也就是说，绝大多数蛋白质的空间结构还都未知。这些蛋白质的二级结构又 如何知晓呢？这，就需要用计算机软件来预测蛋白质的二级结构。预测的结果和真实情况会 有一定出入，究竟差多少，取决于预测软件的准确度。可以预测蛋白质二级结构的软件很多， 而且都可以在线使用\n以 PSIPRED 为例，PSIPRED （http://bioinf.cs.ucl.ac.uk/psipred）是一个蛋白质序列分析平台，它不仅可以预测二级结构， 还有很多其他分析功能，比如预测三级结构\n选择第一个\u0026quot;PSIPRED\u0026quot;工具，即，预测蛋白 质二级结构的工具。输入的氨基酸序列（见附件 psipred.fasta）在 PDB 数据库中已有对应的 空间结构（PDB ID：3CIG），因此二级结构也是已知的。之所以输入这样一条序列，是为 了将预测结构和真实结构进行比较，从而评估一下软件的预测准确度如何。预测一般需要 30 分钟。可在线等待结果，也可以查收结果邮件。需要注意的是，像大多数在线软件一样， PSIPRED 不支持免费的商业邮箱，比如 hotmail 或者 QQ 邮箱等。此外，最好给预测任务起 个名字。最后，点击\u0026quot;Predict\u0026rdquo;。\n预测结果页面 Summary 标签下的内容所示，，粉红色的位置是α螺旋出现的位置， 黄色的是β折片，没有底色的是松散的 coil 结构。如果预测出有错乱的结构也会被标出。目 前的二级结构预测软件，都只能预测出α螺旋和β折片，对其他不常见的二级结构单元并不进 行预测\n结果页面中，PSIPRED 标签下有全面的结果描述。描述一共有四行，最下面是 一级结构序列，往上是二级结构序列，再往上是二级结构图形化的描述，最上面的柱状图反 应的是每个位置预测的可信度\n从结果页面的 Download 标签下可以下载纯文本格式的结果文件。纯文本格式的结果更 利于下一步分析加工\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.295230752405949in\u0026rdquo;}\n把 PSIPRED 预测结果和 DSSP 里的真实二级结构放在一起（图 6）。绿色的 是预测结果，粉色的是真实结构。可以看到图 6 中显示出的这部分结果里，绝大多数α螺旋和β折片，也就是 H 和 E，都被正确的预测出来了，只有少数几个短的β折片没有被预测出 来，准确度超过 90%\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1597222222222223in\u0026rdquo;}\n蛋白质的三级结构 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.6815758967629044in\u0026rdquo;}\n无法结晶的蛋白质，可以利用核磁共振法在 液体环境中进行结构测定。但是核磁共振法只能用于质量小于 70 千道尔顿的分子，大约对 应 200 个氨基酸的长度\n除此之外，还有一些不太常用的方法也可以测定分子的三维空间结构，比如冷冻电子显微镜技术（Cyro-Electron Microscopy）。无论用什么方法测定的空间结构， 都要提交到 PDB 数据库。所以我们获取蛋白质三级结构最直接的办法就是去 PDB 搜索\nhttp://www.rcsb.org/\n从 PDB 首页的搜索条里，可以通过搜索 PDB ID、分子名称、作者姓名等 关键词来查找蛋白质三级结构\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.382924321959755in\u0026rdquo;}\n此外，利用高级搜索工具，可以通过序列相似性搜索获得与 输入序列在序列水平上相似的蛋白质的三级结构。搜索方法选 BLAST，输入序列（示 例文件：pdb_search.fasta），点击\u0026quot;Result Count\u0026quot;。此次搜索一共找到 108 个在序列水平上和 输入序列相似的蛋白质。点击链接\u0026quot;108 PDB Entities\u0026quot;。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.22693460192476in\u0026rdquo;}\n搜索结果中，排在第一位结构是人的 dUTPase 蛋白的三维结构，PDB ID 为 2HQU。这个结构所对应的序列与输入序列中黄色片段之间的一致度是 100%。输入的序列中蓝 色区域是信号肽。信号肽在蛋白质到达亚细胞定位之后就被切掉了，所以解析的成熟蛋白质 结构里不会有这一段。此外，成熟肽段 N 端的一小部分，由于实验技术等原因，也没有被 解析出来，这在 PDB 结构中是很常见的。有时，在序列中间也会有未解析出的断口。甚至 有时，为了得到稳定的晶体状态，需要突变个别的氨基酸或者删除一截肽段。这些技术手段 都会使得结构中的序列和蛋白质本身的序列有所差别\n复习：PDB 文件是通过记录蛋白质中每一个氨基酸上的每一个原子的三维坐标来存储 空间结构信息的。这些原子坐标可以被三维可视化软件读取。三维可视化软件能够创建一个 三维空间，然后根据原子坐标以及原子的大小把原子展示在空间内，并根据原子间的距离给 它们连上化学键。这样一个立体的蛋白质结构就呈现在眼前了。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.343793744531934in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.548386920384952in\u0026rdquo;}\n三级结构可视化软件VMD 强 大 的 免 费蛋 白 质 三 维结 构可 视 化软 件， VMD （http://www.ks.uiuc.edu/Research/vmd）；VMD 的安装也极其简单。不需 要预装任何语言环境，完全图形化安装过程，绝对可以轻松搞定\nVMD 打开后， 会弹出三个窗口：VMD Main（主窗口），VMD Display（显示窗口），和命令窗口 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.9581353893263342in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 打开一个蛋白质结构\n主窗口中点 File -- New Molecule\n弹出新窗口 Molecule File Browser（文件读取窗口）\n文件读取窗口中点 Browse 自动打开 VMD 安装目录\n进入 proteins 文件夹\n选择 VMD 自带的演示结构 bpti.pdb文件读取窗口 中点 Load\n显示窗口出现蛋白质结构\n\u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5741808836395452in\u0026rdquo;} VMD 读取了 PDB 文件中的原子坐标，把每一个原子以细线的形 式展示在 3D 空间中。不 同的原子对应的细线颜 色不同；碳原子是青色 的，氮原子是蓝色的， 氧原子是红色的，氢原 子是白色的，还有少量 黄色的硫原子\n\u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.597222222222222in\u0026rdquo; height=\u0026ldquo;4.680555555555555in\u0026rdquo;} 主窗口上的 Mouse 菜单里可以切换鼠标模式\n{width=\u0026ldquo;4.930555555555555in\u0026rdquo; height=\u0026ldquo;1.6388888888888888in\u0026rdquo;}\n如何改变蛋白质结构的外观 \u0026lt;!-- --\u0026gt; 有关外观的设置在主窗口中的 Graphic 菜单下的 Representation 窗 口里（图 1）。一个 Representation（显示状态）由三个元素构成。第一个元素是用什么 样式（Style）显示，当前使用的样式是以细线显示原子（Lines）。第二个元素是用什么 颜色（Color）显示，当前使用的颜色是按原子名定义的不同颜色（Name）。最后一个元素 是要显示什么内容（Selection），当前显示的内容是所有原子（all）。这三个元素分别 在 Representation 窗口里的 Draw style 标签下的 Drawing Method 下拉条、 Coloring Method 下拉条和 Selected Atoms 输入框里进行设置\nDrawing Method 下拉条：Lines 以细线显示原子。CPK 以不同大小的球 来显示原子，原子间的连线是相应的化学键，比如碳与碳之间的共价键，硫与硫之间的二硫 键等。NewCartoon 只显示蛋白质的碳骨架（backbone），并形象的展示出不同的二级结 构。每一种 Drawing Method 都可以再进一步设置显示效 果。比如对于 CPK，可以调整原子球的大小（Sphere Scale）、 改变化学键的粗细（Bond Radius）、以及设置更高或更低 的分辨率（Sphere/Bond Resolution）\nColoring Method 下拉条，Name 颜色方案是一种原子一种颜色，常见的 比如碳原子青色、氧原子红色、氮原子蓝色、硫原子黄色。在 NewCartoon 显示样式下， 只有碳骨架被现实出来，再配以 Name 颜色方案，整个结构都是青色的。Secondary Structure 颜色方案可以为不同的二级结构赋予不同的颜色，常见的比如α螺旋紫色、β折 片黄色、转角青色、松散 coil 结构白色。颜色方案 Res Type 根据氨基酸类型的不同赋予 不同的颜色，比如非极性氨基酸白色、极性带正电荷的氨基酸（碱性的氨基酸）蓝色、极性 带负电荷的氨基酸（酸性的氨基酸）红色、极性不带电荷的氨基酸绿色。这种颜色方案适合 在显示氨基酸侧链的 Drawing Method 下观看，比如 CPK 样式。颜色方案 ResName 为 20 种氨基酸设置了 20 种不同的颜色\n```\nSelected Atoms 输入框：输入框里输入需要显示 的内容，比如，写\u0026quot;all\u0026quot;代表显示所有原子，也就是整个 蛋白质、写\u0026quot;backbone\u0026quot;代表显示碳骨架。输入框里允许 输入的关键词和语法在 Selections 标签下有详细定义。 可以写Singlewords里面单个的单词，也可以写Keyword 和 Value 组成的词组，还可以利用逻辑词\u0026quot;and/or/not\u0026quot; 把多个单词和词组串成句子\n点击 Reset 清空输入框里的内容Keyword 里双 击 ResNameValue 里双击 ALA（输入框里出现\u0026quot;ResName ALA\u0026quot;）点击 Apply。显示名字为 Alanine 的氨基酸上的原 子，即显示所有丙氨酸。配合Drawing Method设置为CPK， Coloring Method 设置为 ResName\n点击 Reset 清空输入框里的内容Keyword 里双 击 ResidValue 里双击 1点击 Apply。显示第一个氨基 酸。利用这个 Keyword 和 Value 组合可以根据残基的编号 选择某个或某一段氨基酸，比如，想要显示第 1 到第 10 个氨 基酸，可以直接在输入框里输入\u0026quot;resid 1 to 10\u0026quot;，回车。 此时显示的就是前 10 个氨基酸\nDraw style 标签下，Drawing Method 设置为 NewCartoon，Coloring Method 设置为 Secondary StructureSelections 标签下，点击 Reset 清空输入 框里的内容Singlewords 里面双击 alpha_helix点 击\u0026quot;or\u0026quot; Singlewords 里面双击 beta_sheet（输入 框里出现\u0026quot;alpha_helix or beta_sheet\u0026quot;）点击 Apply。 通过逻辑词显示出所有的α helix 和β sheet\n{width=\u0026ldquo;2.8194444444444446in\u0026rdquo; height=\u0026ldquo;8.13888888888889in\u0026rdquo;}\n设置多个 representations（简称 rep），也就是将多个显示状态的试试 效果叠加在一起\n设置第一个 rep：当前处于编辑状态下的 rep 背景色为浅绿色。设置 Drawing Method 为 NewCartoon，Coloring Method 为 Secondary Structure，Selected Atoms 为\u0026quot;all\u0026quot;\n创建第二个 rep：点击 Create Rep。点击后，复制产生了和第一个一摸一样的第二 个 rep。浅绿色背景自动跳转到第二个 rep，即目前第二个 rep 处于可编辑状态。设置 Drawing Method 为 cpk，Coloring Method 为 colorid，并选择选\u0026quot;1 red\u0026quot;（红色）， Selected Atoms 为\u0026quot;resname PRO\u0026quot;，回车\n创建第三个 rep：点击 Create Rep；设置 Drawing Method 为 Surface，Coloring Method 为 colorid，并选择选\u0026quot;8 white\u0026quot;（白色），Selected Atoms 为\u0026quot;all\u0026quot;，回车，此时，可以看到填满肉的蛋白质，也就是蛋白质的外表面\n继续设置 Coloring Method 右侧的 Material（材质）为 Transparent（透明材质）。 如果显卡支持，可以打开 GLSL 显示模式：主窗口中点 Display--Rendermode选中 GLSL。 GLSL 打开后的 Transparent Surface 变得更加柔美了\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.644080271216098in\u0026rdquo;}\n可以双击某一个 rep，比如双击第三个 rep，将其暂时隐藏，等需要的时候再双击它取消隐藏\n可 以 保 存 当 前 所 有 的 representations（注意保存的是显示状态，而不是结构）：主窗口中点击 File -- Save Visualization State -- 保存在桌面上，起名叫 mystate.vmd。接下来关闭 VMD 再重 新打开。这次我们不需要 Load 蛋白质结构，分别设置三个 rep，我们只需要直接载入刚刚保 存 的 mystate.vmd， 即 可 恢 复 刚 刚 的 显 示 状 态 ： 主 窗 口 中 点 击 File -- Load Visualization State -- 找到并打开 mystate.vmd。这时，之前保存的显示状态就自 动显示出来了\n\u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.447484689413823in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 调换背景颜色 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.7193919510061242in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 显示Label\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.2641240157480316in\u0026rdquo;} \u0026lt;!-- --\u0026gt; ```{=html} \u0026lt;!-- --\u0026gt; ``` - 把鼠标模式设置为 Lable 模式，让它标记原子。此时，鼠标 变为十字 。接下来在显示窗口中，要标记哪个原子就在哪里点一下。点击后出现文本显 示的氨基酸名字、氨基酸序号以及被点击的原子的名字 调整 Lable\n改变字体颜色 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.2463035870516186in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 改变显示位置和内容： \u0026lt;!-- --\u0026gt; 主窗口中点击 GraphicsLables弹出 Lable 窗 口- - Properties 标签下 -- 选中要调整的 Lable按住鼠标左键在 Offset 坐标系内移动 来改变 Lable 的位置写入 Format 来改变 Lable 的内容。默认 Format 为：%R（氨基 酸名字）%d（氨基酸序号）:%a（原子名字）。比如，只想显示氨基酸名字和编号，删掉\u0026quot;:%a\u0026quot; 这部分代表原子信息的代码即可\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.448558617672791in\u0026rdquo;}\n\u0026lt;!-- --\u0026gt; 改变字体大小/粗细 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.461884295713036in\u0026rdquo;} 保存图片\n\u0026lt;!-- --\u0026gt; 可以用主窗口下的 File -- Render -- 弹出 File Render Controls 窗口。File Render Controls 窗口里可以选择多种图片导 出方式 计算方法预测三级结构 常见的方法 预测蛋白质的三级结构，常用的方法有从头计算法（ab initio），同源建模法（homolog modeling），穿线法（threading）和综合法（ensemble method）。一般的蛋白质都能用这几种 方法之一预测出三级结构，但也不排除特殊情况\n同源建模法 SWISS-MODEL http://swissmodel.expasy.org\n能帮助完成上述步骤中从模板选取到创建序列比对，再到计算模型，以及最后的质量评估 的全部过程。你需要做的只是：输入目标序列，点 Build Model（创建模型），大约 三到五分钟之后就会返回结果\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.478650481189851in\u0026rdquo;}\n结果页面：\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.262516404199475in\u0026rdquo;}\n从结果页面里可以看到，SWISS-MODEL 挑选了 PDB ID 为 IFYV 的结构作为模板，它 和我们输入的目标序列之间的一致度是 36.18%，大于 30%，满足同源建模的基本条件。结 果中还给出了目标序列和模板序列之间的序列比对。点击 中的下拉箭头，可 以下载预测模型的 PDB 文件。从序列比对和概览图上都可以看到，模板序列较目标序列短 了一点，即，模板序列没有能覆盖目标序列两端的部分，所以模型两端的质量并不高，但是 模型整体的质量按照 SWISS-MODEL 自己的评估是合格的。这一点可以通过 QMEAN 打分 获知。\n如果目标序列与模板序列一致度极高，那么同源建模法是最准确的方法，如果 一致度如果达到 30%，那么模型的准确度就可以达到 80%，模型可以用于寻找功能位点以及 推测功能关系等。如果一致度达到 50%，模型准确度就可以达到 95%，可以根据模型设计定 点突变实验，甚至可以做晶体结构置换，也就是辅助完成真实结构的测定。如果一致度达到 70%以上的话，我们基本就可以认为预测模型完全代表了真实结构，并可以用它进行虚拟筛 选、分子对接、药物设计等结构功能研究\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.625246062992126in\u0026rdquo;}\n预测蛋白质三级结构的首选方法是同源建模法（homolog modeling），该方法基于原理： 相似的氨基酸序列对应着相似的蛋白质结构。比如三个蛋白质，它们在序列水平上十分相似， 解析出的结构也十分相似。第四个蛋白质的序列和前面三个也高度相似，那么就可以比着前 三个结构的样子\u0026quot;画\u0026quot;出第四个的样子。所以同源建模法的关键就是找到一个好的模板。好 的模板要求，在序列水平上模板（template）要与目标（target）蛋白质具有超过 30%的一致 度。同源建模法操作流程如下\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1400699912510937in\u0026rdquo;}\n确定模板 找到与目标蛋白质同源的已知蛋白质结构作为模版（目标序列与模版序 列间的一致度要≥30%） 序列比对 为目标序列与模板序列创建序列对比。模板可以选取多个，通过做多序 列比对，各取所长，让模板序列中与目标序列相似的片段尽可能多的覆盖整个目标序列，同 计算模型 通过序列比对，将目标序列里的氨基酸替换到模板结构里对应的氨基酸 所在的空间位置上。这一步通过同源建模软件来实现 质量评估 同源建模软件输出结构模型后还需要进行质量评估，并根据评估结果更 换模板或修正序列比对，重新构建模型，再次评估。重复这个过程，直至模型质量合格为止 但是，\u0026ldquo;序列越相似，结构越相似\u0026quot;也有例外情况，虽然这种情况极其罕见，但是我们的一个研究结果发现，两个蛋白质在序列水平上高度相似，但它们的晶体结构却告诉我们，两个蛋白质的结构并没有像序列比对里那样一一对应，完全一致，而是发生了一个字母的错位；这一个字母的错位导致二者最后一个折片的结束方向发生了 60 度角的扭转，进而使得之后的 C 端残基的空间位置发生了改变，并因此导致了两者功能的差异。这种情况 下，如果我们按照传统同源建模法以其中一个为模板预测另一个的结构的话，是无法预测出 正确的结果的。好在这种情况极其罕见，但既然存在，就需要敲响警钟，为这一传统方法的 改进提出新的目标\n虽然同源建模法是蛋白质三级结构预测的首选方法，但是对于那些找不到\u0026quot;好\u0026quot;模板的 蛋白质，此方法并不适用 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.6592246281714784in\u0026rdquo;}\n穿线法 I-TASSER 前言 对于那些找不到合适模板（一致度大 于 30%的模板）的蛋白质，此方法并不适用。这种情况下可以尝试穿线法（threading）。穿 线法基于的原理是：不相似的氨基酸序列也可以对应着相似的蛋白质结构\n而具有同一结构拓扑的蛋白质，序列水平上有相似的，也有不相似的\n看穿到哪个结构 里最舒服，哪个结构就可以作为预测的模板，并根据最舒服的穿法，构建出最终模型。那怎 么知道穿的舒服不舒服呢？通过能量方程。穿的舒服，能量就低，穿的不舒服，能量就高。 这和我们穿衣服一样。穿上一件不合身的衣服，你肯定老在那扭啊扭的不得劲，这老动换能 量就高啊！要是穿上件合身的，那就能待住不动了，这能量不就降下来了。穿线法就是通过 计算目标序列穿到每一个已知结构中的每一种穿法下的能量，找到能量最低的那种穿法以及 所穿的结构，然后把目标序列中的氨基酸替换到模板结构里来构建结构模型的，显然这种方法的计算量较同源建模法要大得多，因此预测需要耗费更久的时间\nI-TASSER 的介绍 https://zhanggroup.org//I-TASSER/\n这个预测系统在 CASP 第 7 到第 11 届蛋白质结构预测比赛 中都名列第一。CASP 全称是蛋白质结构生物信息预测国际竞赛。两年一届。每次比赛，参 与者们会对一组即将公开的结构进行预测。再将预测模型和真实结构进行比较，看谁预测的 最准。咱们中国上海交大电子信息与电器工程学院的沈红斌教授的研究组在 CASP11 中取得 了第三名。这是中国代表队截至目前取得的最好成绩\nI-TASSER的使用 张阳教授的 I-TASSER 可以在线提交预测任务（图 3），不需要提前下载安装。提交氨基 酸序列进行预测前需注册获得用户名密码，注册是完全免费的。再给任务起个名字。最后点 Run I-TASSER\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3025634295713036in\u0026rdquo;}\n通过点击 Queue 链接，可以查看当前的所有任务进程，从图中的任务列表里可以看出，穿线法需要 的时间确实比同源建模法多得多，提交的这条示例序列需要计算大约 35 个小时。这里请注意，一个用户或一个 IP 地址一次只能提交一个任务\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5456299212598426in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3391196412948383in\u0026rdquo;}\n此外，还可以通过搜索链接，搜索任务号找到任务，或者搜索账号找到账号下所有的任 务，或者通过搜索序列查找\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.099502405949256in\u0026rdquo;}\nITASSER 预测出了 5 个模型，点击任务号 以查看具体信息\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.466124234470691in\u0026rdquo;}\nI-TASSER 除 了预测出了三级 结构，同时还预测除了二级结构和残基 可溶性\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0588713910761154in\u0026rdquo;}\n构建模型所 使用的模板及序列比对。 这些模板和序列别对不 是通过序列相似性创建 的，而是用穿线法穿出 来的\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2921905074365703in\u0026rdquo;}\nITASSER 给出了多个预测模型。这些模型根据 ITASSER 自带的质量评估系数 C-score 排序。C-score 从负 5 到正 2，分值越高模型越可信。除了 C-Score， 排名第一的模型还给出 了一个 TM-Score，这也是 评价模型质量的系数。 TM-socre \u0026gt;0.5 说明模型 具有正确的结构拓扑，可 信；\u0026lt;0.17 说明模型属于 随机模型，不可信。挑选 到合格的模型后，点击 download model 下载模型 的 PDB 文件\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1787762467191603in\u0026rdquo;}\n此外，I-TASSER 还为预测了该蛋白质的功能，它把排名第一的模型和 PDB 中 的结构进行了比较，把跟模型最相似的 PDB 结构找了出来。这样就可以通过这些已知结构 的功能他推测模型的功能了\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.161142825896763in\u0026rdquo;}\nI-TASSER 还预测出了模型可能结合的配体以及具体的配体结合位点。并给出了可能的功能\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1900153105861766in\u0026rdquo;}\n从头计算法QUARK 但是也有一些特殊的蛋白，穿线 法也解决不了。这种蛋白质，尽管可以用 ITASSER 做出模型，但如果看一下模型的质量评 估系数，都不合格。这时我们就得采取其他方法。可以尝试从头计算法（ab initio）\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.810605861767279in\u0026rdquo;}\n也就说，当 一个蛋白质被翻译出来之后，它上面的 氨基酸排列顺序就已经决定了这个蛋白 质长什么样子。一个蛋白子所应该具有 的天然模样就是它的氨基酸序列所能摆出的自由能最低的 pose，因为能量低意味着稳定和持 久。所以从头计算法会模拟肽段在三维空间内所有可能的存在姿态，并计算每一个姿态下的 自由能，最终给出自由能最低的那个姿态作为预测结果\nQUARK的介绍 https://zhanggroup.org/QUARK/\n注意，QUARK 只能为长度在 200 个氨基酸以内的蛋白质预测结 构。输入序列和用户信息。QUARK 需要单独注册，ITASSER 的账号在这里不通用。QUARK 需要很长的计算时间，大约需要 2 天以上才能计算出结果\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.208616579177603in\u0026rdquo;}\n结果页面里，QUARK 会给出排名前十的预测模型，根据每个模型的 TM-score 可以查 看模型是否合格可用\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.2631222659667545in\u0026rdquo;}\n综合法ROBETTA http://robetta.bakerlab.org/\n综合法（ensemble method）。 综合法综合了前三种方法，将氨基酸序列分段，情况不同的片段采用不同的方法预测;ROBETTA 是一款使用综合预测三级结构的软件，它综合了同源建模法和从头计算法两 种方法。能找到模板的区域用同源建模法，找不到的区域用从头计算法\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0375481189851268in\u0026rdquo;}\nROBETTA 需要的等待的时间是四种方法里最长的，首先任务要排队，至少要排两个小 时。排队排上之后，还要等几个小时，才会出现分析页面。分析页面里，输入的氨基酸序列 被分成了几段，比如前两段适合用从头计算法预测，后两段适合用同源建模法预测。 之后，点击分段链接，逐段进行预测。每一段的预测时间都不短。同源建模法预测的，需要 几个小时到几天的时间，从头计算法预测的，需要几天到几周的时间，这取决于目标序列的 预测\u0026quot;难易\u0026quot;程度\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3604396325459316in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.270201224846894in\u0026rdquo;}\n到底用谁 要预测一个蛋白质的三级结构，首先看能不能 找到一致度大于等于 30%的模板，如果能，用 Swiss-model，又快又准。如果不能，再看能 否舒用 I-TASSER 预测出合格的模型。如果能，搞定。如果不能，再看氨基酸序列的长度是 否小于 200，如果小于 200，用 QUARK。如果大于 200，用 ROBETTA！如果，ROBETTA 也做不出质量合格的模型，那说明蛋白质太特殊了，现有的软件都不适用。\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.945774278215223in\u0026rdquo;}\n模型质量评估 模型质量评估软件并不比较预 测模型跟真实结构之间的差别大小，而是从空间几何学、立体化学和能量分布三方面评估一 个模型的自身合理性\n大多数预测软件都自带模型质量评估系数，比如 I-TASSER 有 C-score，Swiss-Model 有 QMEAN score，QUARK 有 TM-score 等；这些系数可以一定程度上反应模型的质量， 但不能完全反应。至少要有三个评估体系都认为模型可靠，模型才真的可靠。这就需要借助 第三方的评估软件\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4447364391951005in\u0026rdquo;}\nSAVES https://saves.mbi.ucla.edu/\nVerify3D 会根据模型质量，返回一个 3D-1Dscore。只有超过 80%的残基拥有大 于 0.2 的 3D/1D score，才能被认为是质量合格的模型。从 3D-1Dscore 分布图上可以查看， 模型中哪些位置分值比较低，然后针对这些低质量区域进一步修正\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.455648512685914in\u0026rdquo;}\nPROCHECK 可以做出模型结构的拉氏图。拉氏图检查 Cα的两面角是否合理。 合格的模型超过 90%的残基都应该位于红色的允许区域和正黄色的额外允许区域内。落到其 他区域的残基应当被查看并修正\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.40326334208224in\u0026rdquo;}\nProQ https://www.sbc.su.se/\n它的 评估结果十分明确，LGscore 和 MaxSub 两个值所处范围直接对应模型的好坏\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.0315573053368325in\u0026rdquo;}\nModFOLD https://www.reading.ac.uk/bioinf/ModFOLD/ModFOLD8_form.html\nModFOLD 通过 P-value，给出模型的可信度,但是 ModFOLD 较其他几种方法，评估需要较长的时间。其他的都是立等可取，ModFold 需 要半个小时。而且一个 email 地址一次只能提交一个任务.\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.172016622922135in\u0026rdquo;}\n三级结构的比对 蛋白质三级结构的比对不同于序列比对，它是针对蛋白质三维空间结构的相似性进行比 较，是蛋白质结构分析的重要手段之一。结构比对可用于探索蛋白质进化及同源关系、改进 序列比对的精度、改进蛋白质结构预测工具、为蛋白质结构分类提供依据，以及帮助了解蛋 白质功能等。结构比对的结果可以用很多种参数来衡量，最常用的是 root mean squared deviations (RMSD)。如果两个结构的 RMSD 为 0 埃，那么就认为它们结构一致，可以完全重 合。一般来说，RMSD 小于 3 埃时，认为两个结构相似\nSuperPose叠合 SuperPose 是一款在线蛋白质结构叠合软件（http://wishart.biology.ualberta.ca/SuperPose/）， 可以将两个结构比对在一起，并给出两者之间的 RMSD。以课程附件中的 A.pdb 和 B.pdb 为 例，分别上传，提交\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2431999125109363in\u0026rdquo;}\n结果页面里，3D 可视插件需要 java 支持。弹出窗口问是否加载所有模型，选择 yes。3D 窗口里可以看到红绿两个蛋白，即蛋白 A 和蛋白 B，被叠合在一起了。蛋白 A 和蛋 白 B 整体的结构还是比较相似的，但局部仍有差异。点击 RMSD Report 可以查看两者间具 体的 RMSD 值\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.914099956255468in\u0026rdquo;}\nRMSD Report 显示，如果考虑所有原子的话，整体结构两者间的 RMSD 是 5.55 埃，如果只考虑碳骨架的话，整体结构两者间的 RMSD 是 5.08 埃。通过 RMSD 值，我们可 以认为这两个蛋白质结构比较相似，但不是特别相似\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.109912510936133in\u0026rdquo;}\n在叠合的结果中，蛋白 B 没有移动位置，只有蛋白 A 移动了位置。所以只需要把移动 后的蛋白 A 的 PDB 文件保存下来，再与原始的蛋白 B 的 PDB 文件同时用可视软件，比如 VMD，打开，就可以进一步分析叠合后的结果了\nSPDBV选择叠合 SPDBV 是一款拥有强大结构分析功能的绿色软件，无需安装，下载后直接运行即可 （http://spdbv.vital-it.ch/）\nSDPBV 有分子查看器的功能，也是一个蛋白质同源建模 平台，同时它还能为两个结构进行整体智能叠合，或者进行选择性叠合。课程附件中的 C.pdb 和 D.pdb 这两个蛋白除了一个 loop 区域结构不同之外，其他区域的结构都是完全相同的。 如果我们想要把相同的结构分毫不差的叠合起来，以看清楚不同区域的差别的话，就需要用 到选择性叠合\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2603805774278216in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2199278215223095in\u0026rdquo;}\n两个蛋白叠合在一起了，但是 Magic Fit 考虑的是整体结构，所以 CD 两个蛋白中完全 相同的结构部分并没有被严丝合缝的重合在一起\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.209468503937008in\u0026rdquo;}\n尝试选择性叠合：\n首先需要选中参与叠合的氨基酸，也就是结构完全相同的这部分氨基酸（图 2）。结构 不同的这块不参与叠合的过程，但是会随着叠合同步移动\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.10090769903762in\u0026rdquo;}\n蛋白质分子表面性质 VMD创建PSF文件 蛋白质分子表面性质是蛋白质结构的重要研究内容之一，对了解蛋白质的功能至关重要。 蛋白质的表面性质包括表面性状、表面电荷分布、表面残基可溶性等。表面性状可以用 VMD 的 Surface 这种 Representation 表示，电荷分布和可溶性分布同样也可以用 VMD 计算并标识 出来\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;7.043827646544182in\u0026rdquo;}\nAPBS计算表面电荷分布 1.载入PDB文件 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.5195220909886265in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 2.载入 PSF 文件 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.096397637795276in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 3.设置 APBS 插件 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;5.980144356955381in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 4.计算电荷分布 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.8975896762904636in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 5.查看计算后新生成文件 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.2069433508311462in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 6.加载文件 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;4.5225842082239724in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 7.显示电荷分布 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;0.7475929571303587in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;7.107279090113736in\u0026rdquo;}\n蛋白质四级结构 蛋白质四级结构是独立的三级结构单元聚集形成的复合物，其中每个独立三级结构称为 亚基，也称为单体（monomer）。含两个亚基的蛋白质称为二聚体（dimer），比如图中 Toll 样受体 3 的两个单体聚集在一起形成二聚体识别病原双链 RNA 的结构。再有比如血红蛋白 的四聚体结构（tetramer）和热休克蛋白的六聚体结构（hexamer）\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;1.5637849956255467in\u0026rdquo;}\n如果蛋白质三级结构异常聚集，形成了不该形成的四级结构，就会导致疾病的发生。比 如诱发神经系统退行性病变的淀粉样蛋白 A 蛋白，是蛋白质序列相同但四级结构不 同而诱发疾病的典型代表。阿尔茨海默症在发生过程中会出现 A 蛋白。A 蛋白是由特殊水 解酶对其前体蛋白的水解作用产生的。A 蛋白有两种构象，一种为螺旋且可溶，存在于健康 的个体脑组织中，这类 A 蛋白为单体没有四级结构。另一种构象为片层，且是多个 A 蛋白 聚集形成的链间片层，这类 A 蛋白不溶，并且出现在阿尔茨海默症患者的脑组织中。诱发 A 蛋白从可溶螺旋转变成不溶片层聚集体的机制目前尚不清楚，但已被广泛证实，这种构象的 转变是阿尔茨海默症的重要诱因\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.2235290901137357in\u0026rdquo;}\n在研究蛋白质结构功能时，可通过实验方法获得它们的四级结构。实验方法主 要是采用 X 射线衍射法。这种方法可以获得复合体的真实结构，但是技术难度较大，主要 是因为复合体很难获取并成功结晶。此外还有冷冻电子显微镜技术，但这种方法不能像 X 射线衍射法像那样获得精准的真实结构。它只能捕获类似影子的外形轮廓，之后，再根据已 有的同源蛋白质晶体结构对影子中的单体进行同源建模，再把模型嵌入到影子里，以此建出 复合体的模型\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.4019597550306213in\u0026rdquo;}\n目前通过试验方法获得的四级结构都可以从 PDB 数据库中找到。此外，还有专门的蛋 白质相互作用关系数据库。比如，DIP 数据库，专门用于存储实验方法测定的蛋白 质之间的相互作用。BioGRID 数据库主要收集模式生物物种中涉及的蛋白质间相互作用。 STRING 数据库除了实验方法测定的蛋白质间相互作用外，还储存已发表的用计算方法预测 的蛋白质间相互作用\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.366846019247594in\u0026rdquo;}\n蛋白质-蛋白质分子对接 除了实验方法和从数据库中获取，还可以通过计算机预测蛋白质的四级结构。这种预测 技术叫分子对接（molecule docking）\nDocking 一词原意指船停泊进船坞，在生物信息学领域 docking 是指一个分子结合到另一个分子上。目前有很多做分子对接的软件，这些软 件的基本思路都是尝试分子间所有可能的结合方式，并根据结合后能量的高低给每种结合方 式打分、排名。分子结合后，能量越低说明这种结合方式越稳定。软件在对接的过程中会考 虑以下因素：1）形状互补，2）亲疏水性，3）表面电荷分布\n{width=\u0026ldquo;5.083333333333333in\u0026rdquo; height=\u0026ldquo;2.5277777777777777in\u0026rdquo;}分子对接分为蛋白质和蛋白质之间的分子对接以及蛋白质和小分子之间的分子对接。蛋 白质和蛋白质之间的分子对接又分为两种不同的方法，即刚性对接（Rigid Docking）和柔性 对接（Flexible Docking)。刚性对接是指对接过程中蛋白质是硬的，外形不可变，柔性对接 则反之。刚性对接实际上是一种近似计算。因为蛋白质在机体环境中是软的，他的表面是在 微小移动中保持着动态平衡。但是要在对接过程中考虑到这个实际情况则需要庞大的计算量， 所以能够模拟蛋白质真实状态的柔性对接软件非常少，且都是收费的软件。目前可用的大多 数免费软件都是刚性对接软件\n蛋白质和蛋白质之间的刚性对接软件常用的有 ZDOCK（http://zdock.umassmed.edu/） 和 GRAMM-X（http://vakser.bioinformatics.ku.edu/resources/gramm/grammx），这两个都是免 费的刚性对接软件。HadDock（http://haddock.chem.uu.nl/）可以做蛋白质局部柔性对接，但 需要和开发者联系以获取免费的使用权限。这些软件的输出值都是根据能量高低排序的多个 对接模型，能量低者排名靠前。至于选取哪个模型作为最终结果需要综合考虑各种因素，并非一定选取排名第一的模型。下面以 ZDOCK 为例，来演示蛋白质-蛋白质间的刚性对接。 ZDOCK 是一个全自动的在线对接工具\n常用对接软件ZDOCK 第一步，上传要对接的两个蛋白质结构： ZdockA.pdb 和 ZdockB.pdb（见富文 本中的附件）。接下来输入邮箱。注意邮箱必须输入，否则无法提交。对接需要大约 30 分钟， 完成结果链接会发送至填写的邮箱。最后的点击提交 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.7893930446194224in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 第二步，选择或者排除残基参与蛋白质相互作用。比如，从发表文献中已知 732 号赖氨酸是参与蛋白质相互作用的重要氨基酸，此时则需要在\u0026quot;Select Binding Site Residues\u0026rdquo; 窗口里选中 732 号氨基酸。点击提交BePISA \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.7444641294838146in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 第三步，确认选择无误后，点 ok。半小时后你的结果会显示在页面下方提供的网址中 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.8841382327209097in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 对接结果中，ZdockA.pdb，也就是 receptor 的位置没有改变，ZdockB.pdb，也就 是 ligand 产生了 500 个可能的对接状态，点\u0026quot;Top 10 Predictions\u0026quot;可下载排名前十的对接结 果。或者从输入框里输入数字以选择下载排名第几的对接结果。如果要下载更多或者全部 500 个，需要运行页面上的 JAVA 插件。一个对接结果模型对应一个 PDB 文件，每个 PDB 文件里都包含两条链，分别对应 receptor 和 ligand。综合考虑了各种因素之后，我们最终挑 选了排名第一的对接模型。接下来就可以对这个预测出来的复合体结构进行各种分析了 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.895980971128609in\u0026rdquo;} 蛋白质-小分子分子对接 AutoDock 相互作用面分析 PDBePISA 用 ZDOCK 软件预测出两个蛋白质最有可能的结合方式之后，还需要对预测的对接结 果进行进一步分析。PDBePISA（http://www.ebi.ac.uk/pdbe/pisa/）能够在线分析蛋白质复合 体中的相互作用面\n1.打开 PDBePISA 主页，点击\u0026quot;Launch PDBePisa\u0026quot;，启动在线分析网页 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.805002187226597in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 2.输入网页上上，可以输入 PDBID 查看现有复合体结构的相互作用面，也可以点 \u0026ldquo;Coordinate file\u0026rdquo;，上传本地的复合体结构。本地的复合体结构既可以是下载到本地的 PDB 数据库里的结构，也可以是通过预测软件，如 ZDOCK，预测出的结构。点击\u0026quot;选择文件\u0026quot;， 选取课程附件中的\u0026quot;complex.pdb\u0026quot;，即上一节课 ZDOCK 预测结果中，排名第一的复合体结 构。选取正确文件后，点击\u0026quot;Upload\u0026quot;上传文件 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2087817147856517in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 3.上传成功后，点击\u0026quot;Interfaces\u0026quot;查看相互作用面情况 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.8612423447069117in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 4.接合面列表（Interface List）里可以查看到蛋白质相互作用面的面积大小（Interface Area， A 2），以及该种对接方式下的自由能高低（ΔiG kcal/mol）。自由能越低，说明结构越稳定。通 常小于零的自由能才对应一个有意义的对接结果。如果自由能比较高，说明对接软件没有找 到合适的对接状态，这可能是软件的预测能力所限，但更可能是这两个蛋白质在天然环境中 不擅长结合在一起，或蛋白质三级结构有误。如要查看具体的结合位点，点击\u0026quot;Details\u0026quot;来 查看更多信息 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.968905293088364in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 5.\u0026ldquo;interfacing residues\u0026quot;列表里给出了以下信息：（1）两个蛋白质中所有参与相互作用 的残基，（2）这些残疾参与的化学键，比如氢键（H）、盐键（S）、二硫键（D）和共价键（C）， （3）这些残基埋入相互作用面中的面积比，竖线越多，说明埋入的面积所占比率越大。从 表中得知，732 号赖氨酸参与了相互作用并形成氢键。从氢键列表（Hydrogen Bonds）中可 以查看相互作用面上形成的所有氢键，以及形成氢键的原子和化学键的键长。如果有其他化 学键形成的话，也会有相应的列表出现 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.1646052055993in\u0026rdquo;} \u0026lt;!-- --\u0026gt; 6.页面上还提供了 Jmol 插件，以图形化显示出相互作用面和作用面上的残基 \u0026lt;!-- --\u0026gt; {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;2.5796095800524936in\u0026rdquo;} 虚拟筛选 和反向对接 虚拟筛选的概念（Virtual screening VS） {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.577522965879265in\u0026rdquo;}\n说白就是使用计算机预测出哪些小分子与蛋白质结合最后，再买来这些小分子做实验验证\nZINC {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5573982939632547in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3442738407699037in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0543350831146108in\u0026rdquo;}\nZINC的使用例子 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.318856080489939in\u0026rdquo;}\n比如说我想知道某个蛋白除了能和苯甲酸结合外，还想知道还能和哪些结合；比如给出的结果文件中会含有全部的小分子配体，就是说结果是全部放在一起的，所以需要去写个程序分离出来\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2407403762029747in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5237554680664918in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.0271926946631673in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.55549321959755in\u0026rdquo;}\nautodock vina可以进行虚拟筛选 看官方的教程\n反向对接 (Target Fishing) 基本介绍 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3827154418197725in\u0026rdquo;}\n靶标数据库scPDB 是画好了grid box的数据库\nhttp://bioinfo-pharma.u-strasbg.fr/scPDB/\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.3230336832895886in\u0026rdquo;}\n分子动力学模拟 {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.2122747156605422in\u0026rdquo;}\nNAMD {width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.5090638670166228in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.392660761154856in\u0026rdquo;}\n{width=\u0026ldquo;5.833333333333333in\u0026rdquo; height=\u0026ldquo;3.4291393263342083in\u0026rdquo;}\n","date":"2021-10-29T00:00:00Z","permalink":"https://example.com/p/%E5%B1%B1%E4%B8%9C%E5%A4%A7%E7%94%9F%E4%BF%A1_%E8%9B%8B%E7%99%BD%E7%BB%93%E6%9E%84%E9%A2%84%E6%B5%8B_note/","title":"山东大生信_蛋白结构预测_note"},{"content":"首先明确啥是变化率\n（后一个值-前一个值）／前一个值\npandas 中的方法pct_change()方法可以用来计算变化率\nimport pandas as pd test = pd.Series([1,2,3,4,5]) test result: test.pct_change() result: code:\nimport numpy as np d = np.random.randint(0,20,(5,5)) test2 = pd.DataFrame(d) test2 result: code:\ntest2.pct_change() result: ","date":"2021-10-27T00:00:00Z","permalink":"https://example.com/p/pd.pct_change%E8%AE%A1%E7%AE%97%E5%8F%98%E5%8C%96%E7%8E%87/","title":"pd.pct_change()计算变化率"},{"content":"fillna（）是用来填充NaN值的 参数：\ninplace True/False 是否在直接修改元原对象，False的话会创建一个副本 method 填充方法; {‘pad’, ‘ffill’,‘backfill’, ‘bfill’, None}, default None pad/ffill：用前一个非缺失值去填充该缺失值 backfill/bfill：用下一个非缺失值填充该缺失值 -None：指定一个值去替换缺失值（缺省默认这种方式） -limit参数：限制填充个数 -axis参数 ：修改填充方向 code:\nimport pandas as pd import numpy as np from numpy import nan as NAN df1 = pd.DataFrame([[1,2,3],[NAN,NAN,2],[NAN,NAN,NAN],[8,8,NAN]]) df1 result: code:\ndf1.fillna(22) result: code:\ndf1.fillna(method=\u0026#34;pad\u0026#34;) result: code:\ndf1.fillna(method=\u0026#34;ffill\u0026#34;) result: code:\ndf1 result: code:\ndf1.fillna(method=\u0026#34;bfill\u0026#34;) result: ","date":"2021-10-19T00:00:00Z","permalink":"https://example.com/p/pd.findna%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3/","title":"pd.findna函数详解"},{"content":"先看示例：\nimport pandas as pd import numpy as np test = pd.Categorical([\u0026#39;a\u0026#39;,\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;c\u0026#39;]) test result:\n[\u0026#39;a\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;c\u0026#39;] Categories (3, object): [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;] code:\ntest.dtype result:\nCategoricalDtype(categories=[\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;], ordered=False) code:\ntest.codes result:\narray([0, 0, 1, 2, 2], dtype=int8) code:\ntest.categories result:\nIndex([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;], dtype=\u0026#39;object\u0026#39;) 由上面的例子可以看出pandas的Categorical对象，实际上是计算一个列表型数据中的类别数，即不重复项，它返回的是一个CategoricalDtype 类型的对象，相当于在原来数据上附加上类别信息\ncodes可以使得数据中相同的值的index变成一样的，返回一个新的index categories 可以返回类别的值 ","date":"2021-10-17T00:00:00Z","permalink":"https://example.com/p/pd.categorical%E7%B1%BB%E5%88%AB%E5%AF%B9%E5%BA%94%E7%94%A8%E6%B3%95/","title":"pd.Categorical类别对应用法"},{"content":"pd.merge()\npandas.merge(left, right, how=\u0026#39;inner\u0026#39;, on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=(\u0026#39;_x\u0026#39;, \u0026#39;_y\u0026#39;), copy=True, indicator=False, validate=None) how : {‘left’, ‘right’, ‘outer’, ‘inner’}, default ‘inner’（设置数据连接的集合操作规则） left: 返回的结果只包含左列 right: 返回的结果只包含右列 inner: 交集 outer: 并集 on ：label or list（此参数只有在两个DataFrame有共同列名的时候才可以使用） left_on与right_on: label or list, or array-like（合并两个列名不同的数据集） left_index与right_index : bool, default False（合并索引） suffixes : tuple of (str, str), default (\u0026rsquo;_x\u0026rsquo;, \u0026lsquo;_y\u0026rsquo;)（为重复列名自定义后缀） # 简单连接 # 只有一个共同列名时参数 on 可省略 import pandas as pd df1 = pd.DataFrame({\u0026#39;Warframe\u0026#39;:[\u0026#39;saryn\u0026#39;,\u0026#39;volt\u0026#39;,\u0026#39;trinity\u0026#39;,\u0026#39;loki\u0026#39;], \u0026#39;group\u0026#39;:[\u0026#39;A\u0026#39;,\u0026#39;B\u0026#39;,\u0026#39;C\u0026#39;,\u0026#39;D\u0026#39;]}) df2 = pd.DataFrame({\u0026#39;Warframe\u0026#39;:[\u0026#39;volt\u0026#39;,\u0026#39;loki\u0026#39;,\u0026#39;saryn\u0026#39;,\u0026#39;trinity\u0026#39;], \u0026#39;support\u0026#39;:[2004,2008,2012,2014]}) print(df1) print(df2) pd.merge(df1,df2) #就是说当数据的行label一样时，可以自动匹配合并上去 result: code:\n# left_on和right_on df3 = pd.DataFrame({\u0026#39;name\u0026#39;:[\u0026#39;saryn\u0026#39;,\u0026#39;volt\u0026#39;,\u0026#39;loki\u0026#39;,\u0026#39;trinity\u0026#39;], \u0026#39;support\u0026#39;:[2012,2004,2008,2014]}) df3 result: code:\ndf1 result: code\npd.merge(df1,df3,left_on=\u0026#34;Warframe\u0026#34;,right_on=\u0026#34;name\u0026#34;)#传入两份数据的第一列的名字 result code\n# left_index和right_index #set_index可以赋予行index df11 = df1.set_index(\u0026#39;Warframe\u0026#39;) df22 = df2.set_index(\u0026#39;Warframe\u0026#39;) df11 result: code\ndf22 result code:\npd.merge(df11,df22,left_index=True,right_index=True) #left_index/righr_index = True 会自动匹配行索引的数据 result: code:\n# how参数 df5 = pd.DataFrame({\u0026#39;name\u0026#39;:[\u0026#39;Ember\u0026#39;,\u0026#39;Frost\u0026#39;,\u0026#39;Garuda\u0026#39;], \u0026#39;ability\u0026#39;:[\u0026#39;Fire\u0026#39;,\u0026#39;Ice\u0026#39;,\u0026#39;Blood\u0026#39;]}, columns=[\u0026#39;name\u0026#39;,\u0026#39;ability\u0026#39;]) df6 = pd.DataFrame({\u0026#39;name\u0026#39;:[\u0026#39;Garuda\u0026#39;,\u0026#39;Hydroid\u0026#39;], \u0026#39;face\u0026#39;:[\u0026#39;beatiful\u0026#39;,\u0026#39;emmm\u0026#39;]}, columns=[\u0026#39;name\u0026#39;,\u0026#39;face\u0026#39;]) print(df5) print(\u0026#34;===================================\u0026#34;) print(df6) print(\u0026#34;===================================\u0026#34;) print(pd.merge(df5,df6)) print(\u0026#34;=============ineer=====================\u0026#34;) print(pd.merge(df5,df6,how=\u0026#39;inner\u0026#39;)) print(\u0026#34;=============left====================\u0026#34;) print(pd.merge(df5,df6,how=\u0026#39;left\u0026#39;)) print(\u0026#34;============right=====================\u0026#34;) print(pd.merge(df5,df6,how=\u0026#39;right\u0026#39;)) print(\u0026#34;=============outer==================\u0026#34;) print(pd.merge(df5,df6,how=\u0026#39;outer\u0026#39;)) #可以看出当一个数据中不存在另一个数据源的列的数据时会自动填充NAN result: ","date":"2021-10-08T00:00:00Z","permalink":"https://example.com/p/%E4%BD%BF%E7%94%A8pd.merge%E5%AE%9E%E7%8E%B0%E6%95%B0%E6%8D%AE%E5%90%88%E5%B9%B6/","title":"使用pd.merge实现数据合并"},{"content":"resample与groupby的区别：\nresample：在给定的时间单位内重取样\ngroupby：对给定的数据条目进行统计\n函数原型：\nDataFrame.resample(rule, how=None, axis=0, fill_method=None, closed=None, label=None, convention=\u0026lsquo;start\u0026rsquo;, kind=None, loffset=None, limit=None, base=0)\n其中，参数how已经废弃了。\n附：常见时间频率\nA year\nM month\nW week\nD day\nH hour\nT minute\nS second\nimport pandas as pd import numpy as np index = pd.date_range(\u0026#34;15/1/2021\u0026#34;,periods=9,freq=\u0026#34;T\u0026#34;) index result: code:\nseries = pd.Series(range(9),index=index) series result: code:\nseries.resample(\u0026#34;3T\u0026#34;).sum() #在给定的时间单位内重取样 result: series.resample(\u0026#34;3T\u0026#34;,label=\u0026#34;right\u0026#34;,closed=\u0026#34;right\u0026#34;).sum() result: ","date":"2021-10-04T00:00:00Z","permalink":"https://example.com/p/pd.resample%E5%AF%B9%E7%BB%99%E5%AE%9A%E7%9A%84%E6%97%B6%E9%97%B4%E5%8D%95%E4%BD%8D%E5%86%85%E9%87%8D%E5%8F%96%E6%A0%B7/","title":"pd.resample()对给定的时间单位内重取样"},{"content":"pandas DataFrame.shift()函数可以把数据移动指定的位数\nperiod参数指定移动的步幅,可以为正为负.axis指定移动的轴\nimport pandas as pd data1 = pd.DataFrame({ \u0026#34;a\u0026#34;:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9], \u0026#39;b\u0026#39;: [9, 8, 7, 6, 5, 4, 3, 2, 1, 0] }) data1 result: 如果想让 a和b的数据都往下移动一位:\n没有的话，会以NAN填充\ndata2 = data1.shift(axis=0) data2 result: code:\ndata1 result: code:\ndata3 = data1.shift(axis=1) data3 result: data1 result: 如果想往上或者往左移动,可以指定(periods=-1): 因为periods是用来指定步长\ndata4 = data1.shift(periods=-1,axis=0) data4 result: 一个例子:\n这里有一组某车站各个小时的总进站人数和总出站人数的数据:\nentries_and_exits = pd.DataFrame({ \u0026#39;ENTRIESn\u0026#39;: [3144312, 3144335, 3144353, 3144424, 3144594, 3144808, 3144895, 3144905, 3144941, 3145094], \u0026#39;EXITSn\u0026#39;: [1088151, 1088159, 1088177, 1088231, 1088275, 1088317, 1088328, 1088331, 1088420, 1088753] }) entries_and_exits result: 要求计算每个小时该车站进出站人数\n思路: 把第n+1小时的总人数-第n小时的总人数,就是这个小时里的进出站人数\na = entries_and_exits - entries_and_exits.shift(axis=0) a.fillna(0) result: ","date":"2021-10-04T00:00:00Z","permalink":"https://example.com/p/pd.shift%E5%87%BD%E6%95%B0%E5%8F%AF%E4%BB%A5%E6%8A%8A%E6%95%B0%E6%8D%AE%E7%A7%BB%E5%8A%A8%E6%8C%87%E5%AE%9A%E7%9A%84%E4%BD%8D%E6%95%B0/","title":"pd.shift()函数可以把数据移动指定的位数"},{"content":"Dataframe.aggregate()函数用于在一个或多个列上应用某些聚合。使用callable，string，dict或string /callables列表进行聚合。最常用的聚合是\nsum:返回所请求轴的值之和 min:返回所请求轴的最小值 max:返回所请求轴的最大值 用法：DataFrame.aggregate(func, axis=0, *args, **kwargs)\n参数：\nfunc:可调用，字符串，字典或字符串/可调用列表，用于汇总数据的函数 如果是函数，则必须在传递DataFrame或传递给DataFrame.apply时起作用 对于DataFrame，如果键是DataFrame列名，则可以传递dict axis:(默认0){0或“索引”，1或“列”} 0或“索引”：将函数应用于每个列。 1或“列”：将函数应用于每一行 范例1： 汇总 DataFrame 中所有列的“和”和“最小”函数 范例2 在Pandas中，我们还可以在不同的列上应用不同的聚合函数。为此，我们需要传递一个字典，该字典的键包含列名称，值包含任何特定列的聚合函数列表 ","date":"2021-10-02T00:00:00Z","permalink":"https://example.com/p/pd.aggregate%E8%81%9A%E7%B1%BB%E4%BD%9C%E7%94%A8/","title":"pd.aggregate聚类作用"},{"content":"机器学习练习 5 Scikit-learn的介绍\n整理编译：黄海广 haiguang2000@wzu.edu.cn,光城\n在本节教程中将会绘制几个图形，于是我们激活matplotlib,使得在notebook中显示内联图。\n%matplotlib inline import matplotlib.pyplot as plt 为什么要出这个教程？ scikit-learn 提供最先进的机器学习算法。 但是，这些算法不能直接用于原始数据。 原始数据需要事先进行预处理。 因此，除了机器学习算法之外，scikit-learn还提供了一套预处理方法。此外，scikit-learn 提供用于流水线化这些估计器的连接器(即转换器，回归器，分类器，聚类器等)。\n在本教程中,将介绍scikit-learn 函数集，允许流水线估计器、评估这些流水线、使用超参数优化调整这些流水线以及创建复杂的预处理步骤。\n基本用例：训练和测试分类器 对于第一个示例，我们将在数据集上训练和测试一个分类器。 我们将使用此示例来回忆scikit-learn的API。\n我们将使用digits数据集，这是一个手写数字的数据集。\nfrom sklearn.datasets import load_digits X, y = load_digits(return_X_y=True) X.shape result:\n(1797, 64) X中的每行包含64个图像像素的强度。 对于X中的每个样本，我们得到表示所写数字对应的y。\nplt.imshow(X[0].reshape(8, 8), cmap=\u0026#39;gray\u0026#39;);# 下面完成灰度图的绘制 # 灰度显示图像 plt.axis(\u0026#39;off\u0026#39;)# 关闭坐标轴 print(\u0026#39;The digit in the image is {}\u0026#39;.format(y[0]))# 格式化打印 result:\n在机器学习中，我们应该通过在不同的数据集上进行训练和测试来评估我们的模型。train_test_split 是一个用于将数据拆分为两个独立数据集的效用函数。stratify参数可强制将训练和测试数据集的类分布与整个数据集的类分布相同。\ncode:\ny result:\narray([0, 1, 2, ..., 8, 9, 8]) code:\nfrom sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split( X, y, stratify=y, test_size=0.25, random_state=42) # 划分数据为训练集与测试集,添加stratify参数，以使得训练和测试数据集的类分布与整个数据集的类分布相同。 一旦我们拥有独立的培训和测试集，我们就可以使用fit方法学习机器学习模型。 我们将使用score方法来测试此方法，依赖于默认的准确度指标。\ncode:\nfrom sklearn.linear_model import LogisticRegression # 求出Logistic回归的精确度得分 clf = LogisticRegression( solver=\u0026#39;lbfgs\u0026#39;, multi_class=\u0026#39;ovr\u0026#39;, max_iter=5000, random_state=42) clf.fit(X_train, y_train) accuracy = clf.score(X_test, y_test) print(\u0026#39;Accuracy score of the {} is {:.4f}\u0026#39;.format(clf.__class__.__name__, accuracy)) result:\nAccuracy score of the LogisticRegression is 0.96 # ?clf.score scikit-learn的API在分类器中是一致的。因此，我们可以通过RandomForestClassifier轻松替换LogisticRegression分类器。这些更改很小，仅与分类器实例的创建有关。\nfrom sklearn.ensemble import RandomForestClassifier # RandomForestClassifier轻松替换LogisticRegression分类器 clf = RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=42) clf.fit(X_train, y_train) accuracy = clf.score(X_test, y_test) print(\u0026#39;Accuracy score of the {} is {:.2f}\u0026#39;.format(clf.__class__.__name__, accuracy)) result:\nAccuracy score of the RandomForestClassifier is 0.97 from xgboost import XGBClassifier clf = XGBClassifier(n_estimators=1000) clf.fit(X_train, y_train) accuracy = clf.score(X_test, y_test) print(\u0026#39;Accuracy score of the {} is {:.2f}\u0026#39;.format(clf.__class__.__name__, accuracy)) result:\nAccuracy score of the XGBClassifier is 0.96 code:\nfrom sklearn.ensemble import GradientBoostingClassifier clf = GradientBoostingClassifier(n_estimators=100, random_state=0) clf.fit(X_train, y_train) accuracy = clf.score(X_test, y_test) print(\u0026#39;Accuracy score of the {} is {:.2f}\u0026#39;.format(clf.__class__.__name__, accuracy)) result:\nAccuracy score of the GradientBoostingClassifier is 0.96 code:\nfrom sklearn.metrics import balanced_accuracy_score y_pred = clf.predict(X_test) accuracy = balanced_accuracy_score(y_pred, y_test) print(\u0026#39;Accuracy score of the {} is {:.2f}\u0026#39;.format(clf.__class__.__name__, accuracy)) result:\nAccuracy score of the GradientBoostingClassifier is 0.96 code:\nfrom sklearn.svm import SVC, LinearSVC clf = SVC() clf.fit(X_train, y_train) accuracy = clf.score(X_test, y_test) print(\u0026#39;Accuracy score of the {} is {:.2f}\u0026#39;.format(clf.__class__.__name__, accuracy)) result:\nAccuracy score of the SVC is 0.99 code:\nclf = LinearSVC() clf.fit(X_train, y_train) accuracy = clf.score(X_test, y_test) print(\u0026#39;Accuracy score of the {} is {:.2f}\u0026#39;.format(clf.__class__.__name__, accuracy)) result:\nAccuracy score of the LinearSVC is 0.94 标准化您的数据 在学习模型之前可能需要预处理。例如，一个用户可能对创建手工制作的特征或者算法感兴趣，那么他可能会对数据进行一些先验假设。\n在我们的例子中，线性模型使用的求解器期望数据被规范化。因此，我们需要在训练模型之前标准化数据。为了观察这个必要条件，我们将检查训练模型所需的迭代次数。\nMinMaxScaler变换器用于归一化数据，StandardScaler用于标准化数据。该标量应该以下列方式应用：学习（即，fit方法）训练集上的统计数据并标准化（即，transform方法）训练集和测试集。 最后，我们将训练和测试这个模型并得到归一化后的数据集。\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler scaler = MinMaxScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) clf = LinearSVC() clf.fit(X_train_scaled, y_train) accuracy = clf.score(X_test_scaled, y_test) print(\u0026#39;Accuracy score of the {} is {:.2f}\u0026#39;.format(clf.__class__.__name__, accuracy)) result:\nAccuracy score of the LinearSVC is 0.97 from sklearn.preprocessing import MinMaxScaler,StandardScaler scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) clf = LinearSVC() clf.fit(X_train_scaled, y_train) accuracy = clf.score(X_test_scaled, y_test) print(\u0026#39;Accuracy score of the {} is {:.2f}\u0026#39;.format(clf.__class__.__name__, accuracy)) result:\nAccuracy score of the LinearSVC is 0.95 from sklearn.metrics import confusion_matrix, classification_report y_pred = clf.predict(X_test_scaled) print(confusion_matrix(y_pred, y_test)) result: import pandas as pd pd.DataFrame( (confusion_matrix(y_pred, y_test)), columns=range(10), index=range(10)) result: print(classification_report(y_pred, y_test)) result: 交叉验证 分割数据对于评估统计模型性能是必要的。 但是，它减少了可用于学习模型的样本数量。 因此，应尽可能使用交叉验证。有多个拆分也会提供有关模型稳定性的信息。\nscikit-learn提供了三个函数：cross_val_score，cross_val_predict和cross_validate。 后者提供了有关拟合时间，训练和测试分数的更多信息。 我也可以一次返回多个分数。\nfrom sklearn.model_selection import cross_validate clf = LogisticRegression( solver=\u0026#39;lbfgs\u0026#39;, multi_class=\u0026#39;auto\u0026#39;, max_iter=1000, random_state=42) scores = cross_validate( clf, X_train_scaled, y_train, cv=3, return_train_score=True) clf.get_params() result: code:\nimport pandas as pd df_scores = pd.DataFrame(scores) df_scores result: 网格搜索调参 可以通过穷举搜索来优化超参数。GridSearchCV提供此类实用程序，并通过参数网格进行交叉验证的网格搜索。\n如下例子，我们希望优化LogisticRegression分类器的C和penalty参数。\ncode:\nfrom sklearn.model_selection import GridSearchCV clf = LogisticRegression( solver=\u0026#39;saga\u0026#39;, multi_class=\u0026#39;auto\u0026#39;, random_state=42, max_iter=5000) param_grid = { \u0026#39;logisticregression__C\u0026#39;: [0.01, 0.1, 1], \u0026#39;logisticregression__penalty\u0026#39;: [\u0026#39;l2\u0026#39;, \u0026#39;l1\u0026#39;] } tuned_parameters = [{ \u0026#39;C\u0026#39;: [0.01, 0.1, 1, 10], \u0026#39;penalty\u0026#39;: [\u0026#39;l2\u0026#39;, \u0026#39;l1\u0026#39;], }] grid = GridSearchCV( clf, tuned_parameters, cv=3, n_jobs=-1, return_train_score=True) grid.fit(X_train_scaled, y_train) result: 我们可以使用get_params()检查管道的所有参数。 code:\ngrid.get_params() result: code:\ndf_grid = pd.DataFrame(grid.cv_results_) df_grid result: 流水线操作 scikit-learn引入了Pipeline对象。它依次连接多个转换器和分类器（或回归器）。我们可以创建一个如下管道：\nimport pandas as pd from sklearn.preprocessing import MinMaxScaler from sklearn.linear_model import LogisticRegression from sklearn.pipeline import make_pipeline from sklearn.model_selection import GridSearchCV from sklearn.model_selection import cross_validate X = X_train y = y_train pipe = make_pipeline( MinMaxScaler(), LogisticRegression( solver=\u0026#39;saga\u0026#39;, multi_class=\u0026#39;auto\u0026#39;, random_state=42, max_iter=5000)) param_grid = { \u0026#39;logisticregression__C\u0026#39;: [0.1, 1.0, 10], \u0026#39;logisticregression__penalty\u0026#39;: [\u0026#39;l2\u0026#39;, \u0026#39;l1\u0026#39;] } grid = GridSearchCV(pipe, param_grid=param_grid, cv=3, n_jobs=-1) scores = pd.DataFrame( cross_validate(grid, X, y, cv=3, n_jobs=-1, return_train_score=True)) scores[[\u0026#39;train_score\u0026#39;, \u0026#39;test_score\u0026#39;]].boxplot() result: code:\npipe.fit(X_train, y_train) accuracy = pipe.score(X_test, y_test) print(\u0026#39;Accuracy score of the {} is {:.2f}\u0026#39;.format(pipe.__class__.__name__, accuracy)) result:\nAccuracy score of the Pipeline is 0.96 我们可以使用get_params()检查管道的所有参数。\npipe.get_params() result: 此外，可以将网格搜索称为任何其他分类器以进行预测。\n练习 异构数据：当您使用数字以外的数据时 import os data = pd.read_csv(\u0026#39;data/titanic_openml.csv\u0026#39;, na_values=\u0026#39;?\u0026#39;) data.head() result: 泰坦尼克号数据集包含分类，文本和数字特征。 我们将使用此数据集来预测乘客是否在泰坦尼克号中幸存下来。\n让我们将数据拆分为训练和测试集，并将幸存列用作目标。\ny = data[\u0026#39;survived\u0026#39;] X = data.drop(columns=\u0026#39;survived\u0026#39;) X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) 首先，可以尝试使用LogisticRegression分类器，看看它的表现有多好。\nclf = LogisticRegression() clf.fit(X_train, y_train)#这里肯定会报错。 大多数分类器都设计用于处理数值数据。 因此，我们需要将分类数据转换为数字特征。 最简单的方法是使用OneHotEncoder对每个分类特征进行读热编码。 让我们以sex与embarked列为例。 请注意，我们还会遇到一些缺失的数据。 我们将使用SimpleImputer用常量值替换缺失值。\nfrom sklearn.impute import SimpleImputer from sklearn.preprocessing import OneHotEncoder ohe = make_pipeline(SimpleImputer(strategy=\u0026#39;constant\u0026#39;), OneHotEncoder()) X_encoded = ohe.fit_transform(X_train[[\u0026#39;sex\u0026#39;, \u0026#39;embarked\u0026#39;]]) X_encoded.toarray() result: 这样，可以对分类特征进行编码。 但是，我们也希望标准化数字特征。 因此，我们需要将原始数据分成2个子组并应用不同的预处理：（i）分类数据的独热编；（ii）数值数据的标准缩放(归一化)。 我们还需要处理两种情况下的缺失值： 对于分类列，我们将字符串\u0026rsquo;missing_values\u0026lsquo;替换为缺失值，该字符串将自行解释为类别。 对于数值数据，我们将用感兴趣的特征的平均值替换缺失的数据。\ncode:\ncol_cat = [\u0026#39;sex\u0026#39;, \u0026#39;embarked\u0026#39;] col_num = [\u0026#39;age\u0026#39;, \u0026#39;sibsp\u0026#39;, \u0026#39;parch\u0026#39;, \u0026#39;fare\u0026#39;] X_train_cat = X_train[col_cat] X_train_num = X_train[col_num] X_test_cat = X_test[col_cat] X_test_num = X_test[col_num] from sklearn.preprocessing import StandardScaler scaler_cat = make_pipeline(SimpleImputer(strategy=\u0026#39;constant\u0026#39;), OneHotEncoder()) X_train_cat_enc = scaler_cat.fit_transform(X_train_cat) X_test_cat_enc = scaler_cat.transform(X_test_cat) scaler_num = make_pipeline(SimpleImputer(strategy=\u0026#39;mean\u0026#39;), StandardScaler()) X_train_num_scaled = scaler_num.fit_transform(X_train_num) X_test_num_scaled = scaler_num.transform(X_test_num) import numpy as np from scipy import sparse #转为稀疏矩阵 X_train_scaled = sparse.hstack((X_train_cat_enc, sparse.csr_matrix(X_train_num_scaled))) X_test_scaled = sparse.hstack((X_test_cat_enc, sparse.csr_matrix(X_test_num_scaled))) 转换完成后，我们现在可以组合所有数值的信息。最后，我们使用LogisticRegression分类器作为模型。\ncode\nclf = LogisticRegression(solver=\u0026#39;lbfgs\u0026#39;) clf.fit(X_train_scaled, y_train) accuracy = clf.score(X_test_scaled, y_test) print(\u0026#39;Accuracy score of the {} is {:.4f}\u0026#39;.format(clf.__class__.__name__, accuracy)) result:\nAccuracy score of the LogisticRegression is 0.7866 ","date":"2021-09-30T00:00:00Z","permalink":"https://example.com/p/wzu_scikit_learn_%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/","title":"WZU_scikit_learn_代码学习记录"},{"content":"有时候我们的数据是按某个频率收集的，比如每日、每月、每15分钟，那么我们怎么产生对应频率的索引呢？pandas中的date_range可用于生成指定长度的DatetimeIndex。\n我们先看一下怎么生成日期范围：pd.date_range(startdate,enddate)\n1.生成指定开始日期和结束日期的时间范围：\nimport pandas as pd #月日年 index = pd.date_range(\u0026#34;4/1/2021\u0026#34;,\u0026#34;5/2/2021\u0026#34;) print(index) result: 也可以只指定开始日期或结束日期，但这时必须要输入一个时间长度，并且指定输入的是开始时间还是结束时间，如果不指定默认是开始时间。\ncode:\n#periods指定时间长度 pd.date_range(start=\u0026#34;4/1/2021\u0026#34;,periods=10) result: 现在我们已经知道怎么生成日期范围了，但是上面我们生成的日期的时间间隔都是天，接下来告诉大家怎么生成其他时间频率的日期范围。\n要生成按某个频率计算的日期范围，只需要在date_range后加上freq就可以了。比如，生成每小时间隔的时间：\npd.date_range(\u0026#34;4/1/2021\u0026#34;,periods=10,freq=\u0026#34;h\u0026#34;) result: 生成时间间隔为1小时30分的时间：\npd.date_range(\u0026#34;4/1/2021\u0026#34;,periods=10,freq=\u0026#34;1h30min\u0026#34;) result:\npython还可以生成其他不规则频率的时间，比如每月的第一个工作日，每月的第一个日历日等\n生成每月的第一个工作日：\npd.date_range(\u0026#34;1/1/2021\u0026#34;,periods=12,freq=\u0026#34;BMS\u0026#34;) result: 生成每月的第一个日历日： code:\npd.date_range(\u0026#34;1/1/2021\u0026#34;,periods=12,freq=\u0026#34;MS\u0026#34;) result: 有一种很实用的频率类，为“WOM”，即每月的几个星期几。比如每月的第三个星期五。如果我们每月的第三个星期五发工资，这样就可以很方便的知道今年每个月的工资日了。\npd.date_range(\u0026#34;1/1/2021\u0026#34;,periods=12,freq=\u0026#34;WOM-3FRI\u0026#34;) result: ","date":"2021-09-28T00:00:00Z","permalink":"https://example.com/p/pandas%E4%B8%AD%E7%9A%84date_range%E5%8F%AF%E7%94%A8%E4%BA%8E%E7%94%9F%E6%88%90%E6%8C%87%E5%AE%9A%E9%95%BF%E5%BA%A6%E7%9A%84datetimeindex/","title":"pandas中的date_range可用于生成指定长度的DatetimeIndex"},{"content":"从多个 Excel 文件中读取数据并且在一个 dataframe 将这些数据合并在一起。\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = \u0026#34;SimHei\u0026#34; plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) import os 创建 3 个 Excel 文件\n#创建DataFrame #一个key就对应一个columns d = {\u0026#34;Channel\u0026#34;:[1],\u0026#34;Number\u0026#34;:[255]} df = pd.DataFrame(d) df result: # 导出到 Excel 文件中 df.to_excel(\u0026#34;./test1.xlsx\u0026#34;,sheet_name=\u0026#34;test1\u0026#34;,index=False) df.to_excel(\u0026#34;./test2.xlsx\u0026#34;,sheet_name=\u0026#34;test2\u0026#34;,index=False) df.to_excel(\u0026#34;./test3.xlsx\u0026#34;,sheet_name=\u0026#34;test3\u0026#34;,index=False) print(\u0026#34;Done\u0026#34;) result:\nDone 把 3 个 Excel 文件数据读入一个 DataFrame\n把 Excel 文件名读入到一个 list 中，并确保目录下没有其他 Excel 文件。\n#存放文件名的list FileNames = [] os.chdir(\u0026#34;./\u0026#34;) #找到所有文件拓展名是.xlsx的文件 #os.listdir 返回指定路径下的文件和文件夹列表。 [ FileNames.append(file) for file in os.listdir(\u0026#34;.\u0026#34;) if file.endswith(\u0026#34;.xlsx\u0026#34;)] FileNames [\u0026#39;test1.xlsx\u0026#39;, \u0026#39;test2.xlsx\u0026#39;, \u0026#39;test3.xlsx\u0026#39;] 创建一个函数来处理所有的 Excel 文件。\ndef GetFile(file): df = pd.read_excel(file,0) df[\u0026#34;File\u0026#34;] = file # 把 \u0026#39;File\u0026#39; 列作为索引即hanglabel return df.set_index([\u0026#34;File\u0026#34;]) 对每一个文件创建一个 dataframe，把所有的 dataframe 放到一个 list 中。\n即, df_list = [df, df, df]\n#创建一个datagrame 的 list df_list = [GetFile(file) for file in FileNames] df_list result: # 把 list 中所有的 dataframe 合并成一个 big_df = pd.concat(df_list) big_df#列名一致，所以自动补到后面了 result: big_df.dtypes result:\nChannel int64 Number int64 dtype: object #画一张图 #可以看出pd的plot函数会自动以index行lable作为 big_df[\u0026#34;Channel\u0026#34;].plot.bar() ","date":"2021-09-25T00:00:00Z","permalink":"https://example.com/p/%E5%AD%A6%E4%B9%A0pandas_%E7%AC%AC%E5%8D%81%E4%B8%80%E8%AF%BE/","title":"学习Pandas_第十一课"},{"content":"import pandas as pd import numpy as np import matplotlib.pyplot as plt import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = \u0026#34;SimHei\u0026#34; plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False 这里来讲下离群值Outlier\n#创建一个dataframe，用日期做索引 States = [\u0026#39;NY\u0026#39;, \u0026#39;NY\u0026#39;, \u0026#39;NY\u0026#39;, \u0026#39;NY\u0026#39;, \u0026#39;FL\u0026#39;, \u0026#39;FL\u0026#39;, \u0026#39;GA\u0026#39;, \u0026#39;GA\u0026#39;, \u0026#39;FL\u0026#39;,\u0026#39;FL\u0026#39;] data = [1.0, 2, 3, 4, 5, 6, 7, 8, 9, 10] idx = pd.date_range(\u0026#34;20210101\u0026#34;,periods=10,freq=\u0026#34;MS\u0026#34;)#MS每月第一个日历日 df1 = pd.DataFrame(data,index=idx,columns=[\u0026#34;Revenue\u0026#34;]) df1[\u0026#34;State\u0026#34;] = States df1 result: data2 = [10.0, 10.0, 9, 9, 8, 8, 7, 7, 6, 6] idx2 = pd.date_range(\u0026#34;20220101\u0026#34;,periods=10,freq=\u0026#34;MS\u0026#34;) #columns是对传入的data的列名，数量要与data列数量保持一致 df2 = pd.DataFrame(data2,index=idx2,columns=[\u0026#34;Revenue\u0026#34;]) df2[\u0026#34;State\u0026#34;] = States df2 #concat 对于两个数据中列名是相同的，则会自动补到后面即合并 #注意将合并的dataframe 用 [ ]封装起来 df = pd.concat([df1,df2]) df result: 计算离群值的方法\n注意: 均值(average)和标准差(Standard Deviation)只对高斯分布(gaussian distribution)有意义。\n#方法1 #直接算 #不过这个是对整个数据而言的，比如你的std就是整个数据而言的 newdf = df.copy() newdf[\u0026#34;x-Mean\u0026#34;] = abs(newdf[\u0026#34;Revenue\u0026#34;]-newdf[\u0026#34;Revenue\u0026#34;].mean()) newdf[\u0026#34;1.96*std\u0026#34;] = 1.96*newdf[\u0026#34;Revenue\u0026#34;].std() newdf[\u0026#34;Outlier\u0026#34;] = abs(newdf[\u0026#34;Revenue\u0026#34;] - newdf[\u0026#34;Revenue\u0026#34;].mean()) \u0026gt; 1.96*newdf[\u0026#34;Revenue\u0026#34;].std() newdf result: #方法2 #分组的方法 newdf = df.copy() #按照state分组 States = newdf.groupby(\u0026#34;State\u0026#34;) #体会下分类的好处！！！ #这样就可以实现，每个State中的类只减去自己对应类的均值了 #就是说它会将第一个类中的全部数据去实现transform(这时候只需要考虑这一类中含有的数据), 再去第二个类。。。。 newdf[\u0026#34;Outlier\u0026#34;] = States.transform(lambda x:abs(x-x.mean()) \u0026gt; 1.96*x.std() ) newdf[\u0026#34;x-Meaan\u0026#34;] = States.transform(lambda x:abs(x-x.mean())) newdf[\u0026#34;1.96*std\u0026#34;] = States.transform(lambda x:1.96*x.std()) newdf result: #方法3 #将分组封装为一个函数，再apply用 newdf = df.copy() States = df.groupby(\u0026#34;State\u0026#34;) #和上面的道理一样 def s(group): group[\u0026#34;x-mean\u0026#34;] = abs(group[\u0026#34;Revenue\u0026#34;]- group[\u0026#34;Revenue\u0026#34;].mean()) group[\u0026#34;1.96*std\u0026#34;] = 1.96*group[\u0026#34;Revenue\u0026#34;].std() group[\u0026#34;Outlier\u0026#34;] = abs(group[\u0026#34;Revenue\u0026#34;] - group[\u0026#34;Revenue\u0026#34;].mean()) \u0026gt; 1.96*group[\u0026#34;Revenue\u0026#34;].std() return group Newdf2 = States.apply(s) Newdf2 result: #方法3 #按多个条件分组 #其实就是groupby中又分组了 newdf = df.copy() #再次强调下这个groupby的大作用，是将数据分组，一组又一组作为一个对象去运行函数之类的 States_month = newdf.groupby([\u0026#34;State\u0026#34;,lambda x:x.month])#就又分了月份，即把每年相同月份的数据放在一起了 [i for i in States_month] result: def s(group): group[\u0026#39;x-Mean\u0026#39;] = abs(group[\u0026#39;Revenue\u0026#39;] - group[\u0026#39;Revenue\u0026#39;].mean()) group[\u0026#39;1.96*std\u0026#39;] = 1.96*group[\u0026#39;Revenue\u0026#39;].std() group[\u0026#39;Outlier\u0026#39;] = abs(group[\u0026#39;Revenue\u0026#39;] - group[\u0026#39;Revenue\u0026#39;].mean()) \u0026gt; 1.96*group[\u0026#39;Revenue\u0026#39;].std() return group Newdf2 = States_month.apply(s) Newdf2 result: ","date":"2021-09-24T00:00:00Z","permalink":"https://example.com/p/%E5%AD%A6%E4%B9%A0pandas_%E7%AC%AC%E4%B8%83%E8%AF%BE/","title":"学习Pandas_第七课"},{"content":"import pandas as pd import numpy as np import matplotlib.pyplot as plt import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = \u0026#34;SimHei\u0026#34; plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False 从DataFrame 到 Excel 从Excel 到 DataFrame 从DataFrame到 JSON 从JSON到DataFrame #创建一个DataFrame d = [1,2,3,4,5,6,7,8,9] df = pd.DataFrame(d,columns=[\u0026#34;Number\u0026#34;]) df result: #导出到excel df.to_excel(\u0026#34;./Lesson10.xlsx\u0026#34;,sheet_name=\u0026#34;testing\u0026#34;,index=False) print(\u0026#34;Done\u0026#34;) result:\nDone 从 Excel 到 DataFram\ndf = pd.read_excel(r\u0026#34;./Lesson10.xlsx\u0026#34;,sheet_name=0) df.head() result: df.dtypes result:\number int64 dtype: object df.tail() 从 DataFrame 到 JSON\ndf.to_json(\u0026#34;./Lesson10.json\u0026#34;) print(\u0026#34;Done\u0026#34;) result:\nDone 从 JSON 到 DataFram\ndf2 = pd.read_json(r\u0026#34;./Lesson10.json\u0026#34;) df2 result: df.dtypes result:\nNumber int64 dtype: object 到这里看来就只是read和to的方法名不一样而已\n","date":"2021-09-24T00:00:00Z","permalink":"https://example.com/p/%E5%AD%A6%E4%B9%A0pandas_%E7%AC%AC%E5%8D%81%E8%AF%BE/","title":"学习Pandas_第十课"},{"content":"import pandas as pd import numpy as np import matplotlib.pyplot as plt import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = [\u0026#34;SimHei\u0026#34;] plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False #小数据集 d = {\u0026#34;one\u0026#34;:[1,1],\u0026#34;two\u0026#34;:[2,2]} i = [\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;] df = pd.DataFrame(d,index=i) df result: df.index result:\nIndex([\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;], dtype=\u0026#39;object\u0026#39;) #把列名(columns)放置到索引位置 srack叠积 stack = df.stack() stack result: #现在索引包含了原来的列名 stack.index result: unstack = df.unstack()#如果是unstack则是解除叠积(这是对一个已经stack了的而言的) #对于没有stack的而言，unstack则是花结构加过来，而且交换行index的位置 unstack result: unstack.index 用 T (转置)，我们可以把列和索引交换位置。\ndf transpose = df.T transpose transpose.index result:\nIndex([\u0026#39;one\u0026#39;, \u0026#39;two\u0026#39;], dtype=\u0026#39;object\u0026#39;) ","date":"2021-09-23T00:00:00Z","permalink":"https://example.com/p/%E5%AD%A6%E4%B9%A0pandas_%E7%AC%AC%E4%BA%94%E8%AF%BE/","title":"学习Pandas_第五课"},{"content":"import pandas as pd import numpy as np import matplotlib.pyplot as plt import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = [\u0026#34;SimHei\u0026#34;] plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False #小数据集 d = {\u0026#34;one\u0026#34;:[1,1,1,1,1], \u0026#34;two\u0026#34;:[2,2,2,2,2], \u0026#34;letter\u0026#34;:[\u0026#34;a\u0026#34;,\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;b\u0026#34;,\u0026#34;c\u0026#34;]} df = pd.DataFrame(d) df result: #创建一个groupby对象 one = df.groupby(\u0026#34;letter\u0026#34;)#根据letter分组 即根据letter的结果分组，相同的放一起 one.sum() result: #创建一个groupby对象 one = df.groupby(\u0026#34;letter\u0026#34;)#根据letter分组 即根据letter的结果分组，相同的放一起 one.sum() #多个分组依据记得[]封起来 letterone = df.groupby([\u0026#34;letter\u0026#34;,\u0026#34;one\u0026#34;]).sum() letterone letterone.index result: 你可能不想把用来分组的列名字作为索引，像下面的做法很容易实现。\n#参数as_index=False会取消把groupby的对象作为index letterone = df.groupby([\u0026#34;letter\u0026#34;,\u0026#34;one\u0026#34;],as_index=False).sum() letterone letterone.index result: ","date":"2021-09-23T00:00:00Z","permalink":"https://example.com/p/%E5%AD%A6%E4%B9%A0pandas_%E7%AC%AC%E5%85%AD%E8%AF%BE/","title":"学习Pandas_第六课"},{"content":"这是WZU老师搭配的决策树的code，自己略作修改\n1．分类决策树模型是表示基于特征对实例进行分类的树形结构。决策树可以转换成一个if-then规则的集合，也可以看作是定义在特征空间划分上的类的条件概率分布。\n2．决策树学习旨在构建一个与训练数据拟合很好，并且复杂度小的决策树。因为从可能的决策树中直接选取最优决策树是NP完全问题。现实中采用启发式方法学习次优的决策树。\n决策树学习算法包括3部分：特征选择、树的生成和树的剪枝。常用的算法有ID3、 C4.5和CART。\n3．特征选择的目的在于选取对训练数据能够分类的特征。特征选择的关键是其准则。常用的准则自己去MD中看\n4．决策树的生成。通常使用信息增益最大、信息增益比最大或基尼指数最小作为特征选择的准则。决策树的生成往往通过计算信息增益或其他指标，从根结点开始，递归地产生决策树。这相当于用信息增益或其他准则不断地选取局部最优的特征，或将训练集分割为能够基本正确分类的子集。\n5．决策树的剪枝。由于生成的决策树存在过拟合问题，需要对它进行剪枝，以简化学到的决策树。决策树的剪枝，往往从已生成的树上剪掉一些叶结点或叶结点以上的子树，并将其父结点或根结点作为新的叶结点，从而简化生成的决策树。\n#导库 import numpy as np import pandas as pd import math from sklearn import tree import matplotlib.pyplot as plt import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = [\u0026#34;SimHei\u0026#34;] plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False #原始数据 def create_data(): datasets = [[\u0026#39;青年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;一般\u0026#39;, \u0026#39;否\u0026#39;], [\u0026#39;青年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;好\u0026#39;, \u0026#39;否\u0026#39;], [\u0026#39;青年\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;好\u0026#39;, \u0026#39;是\u0026#39;], [\u0026#39;青年\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;一般\u0026#39;, \u0026#39;是\u0026#39;], [\u0026#39;青年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;一般\u0026#39;, \u0026#39;否\u0026#39;], [\u0026#39;中年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;一般\u0026#39;, \u0026#39;否\u0026#39;], [\u0026#39;中年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;好\u0026#39;, \u0026#39;否\u0026#39;], [\u0026#39;中年\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;好\u0026#39;, \u0026#39;是\u0026#39;], [\u0026#39;中年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;非常好\u0026#39;, \u0026#39;是\u0026#39;], [\u0026#39;中年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;非常好\u0026#39;, \u0026#39;是\u0026#39;], [\u0026#39;老年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;非常好\u0026#39;, \u0026#39;是\u0026#39;], [\u0026#39;老年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;好\u0026#39;, \u0026#39;是\u0026#39;], [\u0026#39;老年\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;好\u0026#39;, \u0026#39;是\u0026#39;], [\u0026#39;老年\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;非常好\u0026#39;, \u0026#39;是\u0026#39;], [\u0026#39;老年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;一般\u0026#39;, \u0026#39;否\u0026#39;], ] labels = [u\u0026#39;年龄\u0026#39;, u\u0026#39;有工作\u0026#39;, u\u0026#39;有自己的房子\u0026#39;, u\u0026#39;信贷情况\u0026#39;, u\u0026#39;类别\u0026#39;] # 返回数据集和每个维度的名称 return datasets, labels datasets,label = create_data() train_data = pd.DataFrame(datasets,columns=label) train_data result: #定义信息论中的entropy datasets = np.array(datasets) #信息熵 def calc_ent(datasets): datasets_length = len(datasets) datasets_label_count = {} for i in range(datasets_length): datasets_label_count.setdefault(datasets[i,-1],datasets[:,-1].tolist().count(datasets[i][-1])) # print(datasets_label_count) OK的 ent = -sum([(i/datasets_length)*(math.log(i/datasets_length,2)) for i in datasets_label_count.values()]) # print(ent) OK的 return ent #条件熵 def cond_ent(datasets,feature_colums=0): #featurn_colums是来指定要哪个特征列的熵的即条件 datasets_length = len(datasets) datasets_label_e = {} for i in range(datasets_length): datasets_label_e.setdefault(datasets[i,feature_colums],[]) datasets_label_e[datasets[i,feature_colums]].append(datasets[i])#用[]装同一特征中同一特征值 cond_ent = sum([(len(i)/datasets_length)*calc_ent(np.array(i)) for i in datasets_label_e.values()]) #OK的 # print(ent) # print(datasets_label_e) return ent #信息增益 def info_gain(ent,cond_ent): return ent - cond_ent #信息增益率 def info_gain_rate(ent,cond_ent,datasets): return info_gain(ent,cond_ent)/calc_ent(datasets) #获取信息增益 def info_gain_train(datasets): count = len(datasets[0]) - 1 #特征数,因为要判断哪个特征的信息增益大 best_feature = [] ent = calc_ent(datasets) #样本熵 for c in range(count): c_info_gain = info_gain(ent,cond_ent(datasets,c)) best_feature.append((c,c_info_gain))#(特征,信息增益)搭配 print(\u0026#34;特征({})的信息增益为:{:.3f}\u0026#34;.format(label[c],c_info_gain)) #获得最佳的信息增益特征 best_ = max(best_feature,key=lambda x:x[-1])#按每个的信息增益大小排序 # print(\u0026#34;特征({})的信息增益最大,选择为根节点特征\u0026#34;.format(label[best_[0]]))#这就是为什么前面用 (特征,信息增益)搭配 return \u0026#34;特征({})的信息增益最大,选择为根节点特征\u0026#34;.format(label[best_[0]]) #OK的 info_gain_train(datasets) result: 利用ID3算法生成decisionTree 特征选择是根据信息增益 #定义节点类 二叉树 class Node: def __init__(self,root=True,label=None,feature_name=None,feature=None): self.root = root self.label = label self.feature_name = feature_name self.feature = feature self.tree = { \u0026#34;label\u0026#34;:self.label, \u0026#34;feature\u0026#34;:self.feature, \u0026#34;tree\u0026#34;:self.tree } def __repr__(self): return \u0026#34;{}\u0026#34;.formata(self.result) def add_node(self, val, node): self.tree[val] = node def predict(self,feature): if self.root is True: return self.label return self.tree[feature[self.feature]].predict(feature) class DTree: def __init__(self,epsilon=0.1): self.epsilon = epsilon self._tree = {} #信心熵 @staticmethod def calc_ent(datasets): datasets_length = len(datasets) datasets_label_count = {} for i in range(datasets_length): datasets_label_count.setdefault(datasets[i,-1],datasets[:,-1].tolist().count(datasets[i][-1])) # print(datasets_label_count) OK的 ent = -sum([(i/datasets_length)*(math.log(i/datasets_length,2)) for i in datasets_label_count.values()]) # print(ent) OK的 return ent #条件熵 @staticmethod def cond_ent(datasets,feature_colums=0): #featurn_colums是来指定要哪个特征列的熵的即条件 datasets_length = len(datasets) datasets_label_e = {} for i in range(datasets_length): datasets_label_e.setdefault(datasets[i,feature_colums],[]) datasets_label_e[datasets[i,feature_colums]].append(datasets[i])#用[]装同一特征中同一特征值 cond_ent = sum([(len(i)/datasets_length)*calc_ent(np.array(i)) for i in datasets_label_e.values()]) #OK的 # print(ent) # print(datasets_label_e) return cond_ent #信息增益 @staticmethod #信息增益率 def info_gain_rate(ent,cond_ent,datasets): return info_gain(ent,cond_ent)/calc_ent(datasets) #获取信息增益 @staticmethod def info_gain_train(datasets): count = len(datasets[0]) - 1 #特征数,因为要判断哪个特征的信息增益大 best_feature = [] ent = calc_ent(datasets) #样本熵 for c in range(count): c_info_gain = info_gain(ent,cond_ent(datasets,c)) best_feature.append((c,c_info_gain))#(特征,信息增益)搭配 # print(\u0026#34;特征({})的信息增益为:{:.3f}\u0026#34;.format(label[c],c_info_gain)) #获得最佳的信息增益特征 best_ = max(best_feature,key=lambda x:x[-1])#按每个的信息增益大小排序 return best_ #到这里就是算法的流程了!! def train(self, train_data): \u0026#34;\u0026#34;\u0026#34; input:数据集D(DataFrame格式)，特征集A，阈值eta output:决策树T \u0026#34;\u0026#34;\u0026#34; _, y_train, features = train_data.iloc[:, : -1], train_data.iloc[:, -1], train_data.columns[: -1] # 1,若D中实例属于同一类Ck，则T为单节点树，并将类Ck作为结点的类标记，返回T if len(y_train.value_counts()) == 1: return Node(root=True, label=y_train.iloc[0]) # 2, 若A为空，则T为单节点树，将D中实例树最大的类Ck作为该节点的类标记，返回T if len(features) == 0: return Node( root=True, label=y_train.value_counts().sort_values( ascending=False).index[0]) # 3,计算最大信息增益 同5.1,Ag为信息增益最大的特征 max_feature, max_info_gain = self.info_gain_train(np.array(train_data)) max_feature_name = features[max_feature] # 4,Ag的信息增益小于阈值eta,则置T为单节点树，并将D中是实例数最大的类Ck作为该节点的类标记，返回T if max_info_gain \u0026lt; self.epsilon: return Node( root=True, label=y_train.value_counts().sort_values( ascending=False).index[0]) # 5,构建Ag子集 node_tree = Node( root=False, feature_name=max_feature_name, feature=max_feature) feature_list = train_data[max_feature_name].value_counts().index for f in feature_list: sub_train_df = train_data.loc[train_data[max_feature_name] == f].drop([max_feature_name], axis=1) # 6, 递归生成树 sub_tree = self.train(sub_train_df) node_tree.add_node(f, sub_tree) # pprint.pprint(node_tree.tree) return node_tree def fit(self, train_data): self._tree = self.train(train_data) return self._tree def predict(self, X_test): return self._tree.predict(X_test) datasets, labels = create_data() data_df = pd.DataFrame(datasets, columns=labels) dt = DTree() tree = dt.fit(data_df) result: 这里出现了些小问题,暂时不讨论\n接下来是sklearn的实列 from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from collections import Counter#计数器 #数据采用iris iris = load_iris() iris_pd = pd.DataFrame(iris.data,columns=iris.feature_names) print(iris_pd) result: iris_pd[\u0026#34;label\u0026#34;] = iris.target iris_pd.columns = [\u0026#34;sepal length\u0026#34;,\u0026#34;sepal width\u0026#34;,\u0026#34;petal length\u0026#34;,\u0026#34;petal width\u0026#34;,\u0026#34;label\u0026#34;] iris_pd result: code:\ndata = np.array(iris_pd.iloc[:100,[0,1,-1]]) data#这里就只取出了两个特征以及类别(标签) result: X,y,feature_name = data[:,:2],data[:,-1],iris.feature_names[0:2] X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3) X_train result: code:\ny_train result: 决策树分类 from sklearn.tree import DecisionTreeClassifier from sklearn.tree import export_graphviz from sklearn import tree # import graphviz import pydotplus D_model = DecisionTreeClassifier() D_model.fit(X_train,y_train) D_model.score(X_test,y_test) result:\n0.9666666666666667 一旦经过训练,就可以用plot_tree函数绘制决策树了\ntree.plot_tree(D_model) result: #当然也可以使用export_graphviz导出决策树图 tree_pic = export_graphviz(D_model,out_file=\u0026#34;mytree.pdf\u0026#34;) #或者，还可以使用函数 export_text以文本格式导出树。此方法不需要安装外部库，而且更紧凑： from sklearn.tree import export_text r = export_text(D_model,feature_name)#记得传入特征名 print(r) result: 决策树回归 #导包 from sklearn.tree import DecisionTreeRegressor import numpy as np import matplotlib.pyplot as plt #数据 X = np.sort(5*np.random.RandomState(1).rand(80,1),axis=0)#生成均匀分布的样本 y = np.sin(X).ravel() y[::5] += 3* (0.5 - np.random.RandomState(1).rand(16)) X result: code:\ny result: code:\n#训练模型 regr_1 = DecisionTreeRegressor(max_depth=2) regr_2 = DecisionTreeRegressor(max_depth=5) regr_1.fit(X,y) regr_2.fit(X,y) result:\nDecisionTreeRegressor(max_depth=5) code:\n#预测 X_test = np.arange(0.0,5.0,0.01)[:,np.newaxis]#[:,np.newaxis]插入新维度 y_1 = regr_1.predict(X_test) y_2 = regr_2.predict(X_test) result:\nDecisionTreeRegressor(max_depth=5) code:\n#预测 X_test = np.arange(0.0,5.0,0.01)[:,np.newaxis]#[:,np.newaxis]插入新维度 y_1 = regr_1.predict(X_test) y_2 = regr_2.predict(X_test) #可视化结果 plt.figure() plt.scatter(X,y,s=20,edgecolors=\u0026#34;black\u0026#34;,c=\u0026#34;darkorange\u0026#34;,label=\u0026#34;data\u0026#34;) plt.plot(X_test,y_1,color = \u0026#34;cornflowerblue\u0026#34;,label=\u0026#34;max_depth=2\u0026#34;,linewidth=2) plt.plot(X_test,y_2,color = \u0026#34;yellowgreen\u0026#34;,label=\u0026#34;max_depth\u0026#34;,linewidth=2) plt.xlabel(\u0026#34;data\u0026#34;) plt.ylabel(\u0026#34;target\u0026#34;) plt.title(\u0026#34;Decision Tree Regression\u0026#34;) plt.legend() plt.show() 决策树调参 from sklearn.tree import DecisionTreeClassifier from sklearn.tree import DecisionTreeRegressor from sklearn import datasets from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt from sklearn.model_selection import GridSearchCV#交叉验证网格调参 from sklearn import metrics #数据集 X = datasets.load_iris() data = X.data targrt = X.target name = X.target_names x,y = datasets.load_iris(return_X_y=True)#这样可以一次性取出两个 print(x.shape,y.shape) print(x[:4])#可以看出直接取出的是特征和标签 nice了 print(y[:4]) x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=100) result: code:\n#同网格搜索调参(字典) param = { \u0026#34;criterion\u0026#34;:[\u0026#34;gini\u0026#34;],#分裂节点时评价准则是Gini指数 \u0026#34;max_depth\u0026#34;:[30,50,60,100], \u0026#34;min_samples_leaf\u0026#34;:[2,3,5,10],#叶子节点最少样本数 \u0026#34;min_impurity_decrease\u0026#34;:[0.1,0.2,0.5]#阈值,不纯度 对于基尼指数来说值越低越纯就越好 } grid = GridSearchCV(DecisionTreeClassifier(), param_grid=param, cv=6)#这里就是MD中提及的通过交叉验证得出最优的参数 grid.fit(x_train,y_train) print(\u0026#34;最优分类器:\u0026#34;,grid.best_params_,\u0026#34;最优分数{:.6f}\u0026#34;.format(grid.best_score_)) result:\n最优分类器: {\u0026#39;criterion\u0026#39;: \u0026#39;gini\u0026#39;, \u0026#39;max_depth\u0026#39;: 30, \u0026#39;min_impurity_decrease\u0026#39;: 0.1, \u0026#39;min_samples_leaf\u0026#39;: 2} 最优分数0.941667 ","date":"2021-09-22T00:00:00Z","permalink":"https://example.com/p/my_decisiontree_code/","title":"my_decisionTree_code"},{"content":"import pandas as pd import numpy as np #一个小数据集 d = [0,1,2,3,4,5,6,7,8,9] #创建一个dataframe df = pd.DataFrame(d) df result: #修改列名 df.columns = [\u0026#34;Rev\u0026#34;] df #增加一列 服从广播机制 df[\u0026#34;NewCol\u0026#34;] = 5 df result: #修改一下增加这一列的值 df[\u0026#34;NewCol\u0026#34;] = df[\u0026#34;NewCol\u0026#34;] + 1 df result: #可以删除这一列 del df[\u0026#34;NewCol\u0026#34;] df result: # 让我们增加几列。 译者注: 当使用 dataframe 没有的列时，dataframe 自动增加这个新列 df[\u0026#34;test\u0026#34;] = 3 df[\u0026#34;col\u0026#34;] = df[\u0026#34;Rev\u0026#34;] df result: # 如果有需要，可以改变索引(index)的名字 注意数量 i = [\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;,\u0026#39;d\u0026#39;,\u0026#39;e\u0026#39;,\u0026#39;f\u0026#39;,\u0026#39;g\u0026#39;,\u0026#39;h\u0026#39;,\u0026#39;i\u0026#39;,\u0026#39;j\u0026#39;] df.index = i df result: # 通过使用 *loc，我们可以选择 dataframe 中的部分数据 loc要传入值，不能是数字，而iloc可以 df.loc[\u0026#34;a\u0026#34;] #loc是不能单独取列的 result: # df.loc[起始索引(包含):终止索引(包含)] 即行，列 但是列要是列名 df.loc[\u0026#34;a\u0026#34;:\u0026#34;d\u0026#34;] result: # df.iloc[起始索引(包含):终止索引(不包含)] df.iloc[0:3] result: # 也可以通过列名选择一列的值。 df[\u0026#34;Rev\u0026#34;] result: #如果要取多列，记得用[]包起来 #用一个列的list来选择多个列 df[[\u0026#34;Rev\u0026#34;,\u0026#34;test\u0026#34;]] result: # df.ix[行范围, 列范围] 已经抛弃了 # df.ix[0:3,\u0026#34;Rev\u0026#34;] # 选择 top-N 个记录 (默认是 5 个) df.head() # 选择 bottom-N 个记录 (默认是 5 个) df.tail() ","date":"2021-09-22T00:00:00Z","permalink":"https://example.com/p/%E5%AD%A6%E4%B9%A0pandas_%E7%AC%AC%E5%9B%9B%E8%AF%BE/","title":"学习Pandas_第四课"},{"content":"数据异常常见的情况：\n数据缺失 missing data 数据不一致 inconsistent 在正常范围之外 out of place #导入相关库 import pandas as pd import matplotlib.pyplot as plt import sys #导入这个是为了确认py的版本 import matplotlib #这样导入matplotlib只是为了显示一下其版本号 # 初始化matplotlib，用inline方式显示图形 %matplotlib inline print(\u0026#34;Python 版本\u0026#34; + sys.version) print(\u0026#34;pd版本\u0026#34; + pd.__version__) print(\u0026#34;plt版本\u0026#34; + matplotlib.__version__) result: 创建数据 #初始化数据集: 婴儿名字和出生率 names = [\u0026#39;Bob\u0026#39;,\u0026#39;Jessica\u0026#39;,\u0026#39;Mary\u0026#39;,\u0026#39;John\u0026#39;,\u0026#39;Mel\u0026#39;] births = [968, 155, 77, 578, 973] #zip函数可以将多个列并起来为一个大的list 即拼接在一起如names中的第一个和births中的第一个放在一起，以此类推 zip? #这样可以查看zip函数的说明 #zip函数进行直到某列没有数据，停止 BabyDataSet = list(zip(names,births)) BabyDataSet result: df 是一个 DataFrame对象。 你可以把这个对象理解为包含了 BabyDataset 的 内容而格式非常象一个 sql 表格或者 Excel 的数据表。 让我们看看 df 中的内容。\n#创建DataFrame时可以通过参数columns赋予列的名称 df = pd.DataFrame(data=BabyDataSet,columns=[\u0026#34;name\u0026#34;,\u0026#34;births\u0026#34;]) df``` result ![](picture/第一章-2.png) 将 dataframe 导出到一个 csv 文件中。 我们将导出文件命名为 births1880.csv。 导出 csv 文件的函数是 to_csv。 除非你指定了其他的文件目录，否则导出的文件 将保存在和 notebook 文件相同的位置。 我们会使用的参数是 index 和 header。 将这两个参数设置为 False 将会防止索 引(index)和列名(header names)被导出到文件中。 你可以试着改变这两个参数值来 更好的理解这两个参数的作用。 ```python #to_csv可以将dataframe导出为csv，index/header可以是否导出索引和列名 df.to_csv(\u0026#34;births1880.csv\u0026#34;,index=False,header=False) 获取数据 我们将使用 pandas 的 read_csv 函数从 csv 文件中获取数据。\n#查看帮助,这个函数有很多的参数 pd.read_csv? 注意字符串之前的 r 。 因为斜线(slash)是一个特殊字符，在字符串之前放置前导的\nr 将会防止整个字符串进行转义(escape)\n比如：你想在string中输出\u0026rsquo;单引号，则需要用到\u0026rsquo;才行\n\u0026lsquo;r\u0026rsquo;是防止字符转义的， 如果字符串中出现\u0026rsquo;\\n\u0026rsquo;的话 ，不加r的话，\\n就会被转义成换行符,\n而加了\u0026rsquo;r\u0026rsquo;之后\u0026rsquo;\\n\u0026rsquo;就能保留原有的样子。。\ncode:\nLocation = r\u0026#34;./births1880.csv\u0026#34;#从当前脚本所在的文件夹位置读取 csv 文件 df = pd.read_csv(Location) df result: 这里出现了一个问题。 read_csv 函数将 csv 文件中的第一行作为了每列的列名\n(head names)。 这明显不对，因为数据文件没有提供列名。\n要修正这个错误，我们需要给 read_csv 函数传入 header 这个参数，并设置为\nNone (Python中 null 的意思)。\n#read_csv的参数header可以决定是否将数据第一行作为列名 df = pd.read_csv(\u0026#34;./births1880.csv\u0026#34;,header=None) df result: 如果我们需要为每一列指定一个名字，我们可以传入另外一个参数 names，同时\n去掉 header 这个参数。\n#read_csv的参数names可以赋予数据列名 df = pd.read_csv(\u0026#34;./births1880.csv\u0026#34;,names=[\u0026#34;names\u0026#34;,\u0026#34;births\u0026#34;]) df result: 你可以把数字 [0,1,2,3,4] 设想为 Excel 文件中的行标 (row number)。 在 pandas中，这些是 索引 (index) 的一部分。 你可以把索引(index)理解为一个sql表中的主键(primary key)，但是索引(index)是可以重复的。[Names, Births] 是列名，和sql表或者Excel数据表中的列名(column header)是类似的\n#删除刚刚生成的csv import os os.remove(\u0026#34;./births1880.csv\u0026#34;) 准备数据 我们的数据包含了1880年出生的婴儿及其数量。 我们已经知道我们有5条记录而且 没有缺失值(所有值都是非空 non-null 的)。\nNames 列是由字母和数字串组成的婴儿名字。 这一列也许会出现一些脏数据但我 们现在不需要有太多顾虑。 Births 列应该是通过整型数字(integers)表示一个指定 年份指定婴儿名字的出生率。 我们可以检查一下是否这一列的数字都是整型。这一 列出现浮点型(float)是没有意义的。但我们不需要担心这一列出现任何可能的离群 值(outlier)。\n请注意在目前这个阶段，简单地看一下dataframe中的数据来检查\u0026quot;Names\u0026quot;列已经足 够了。 在之后我们做数据分析的过程中，我们还有很多机会来发现数据中的问题。\n#dtypes 可以查看每一列的数据类型 df.dtypes result: #列名.dtypes 可以查看指定列的数据类型 df.births.dtype result 分析数据 要找到最高出生率的婴儿名或者是最热门的婴儿名字，我们可以这么做。\n将 dataframe 排序并且找到第一行 使用 max() 属性找到最大值 # 方法1： #sort_values的方法可以传入列名（整个数据集按这一列排序），ascending(升序)来决定排序的升/降 Sorted = df.sort_values([\u0026#34;births\u0026#34;],ascending=False) Sorted result: Sorted.head(1) result: #方法2 #列名.max()可以返回这一列中数据最大的这个这一列的值 df[\u0026#34;births\u0026#34;].max() result: 表现数据 我们可以将 Births 这一列标记在图形上向用户展示数值最大的点。 对照数据表， 用户就会有一个非常直观的画面 Mel 是这组数据中最热门的婴儿名字。\npandas 使用非常方便的 plot() 让你用 dataframe 中的数据来轻松制图。 刚才我们 在 Births 列找到了最大值，现在我们要找到 973 这个值对应的婴儿名字。\n[df[\u0026#34;births\u0026#34;] == df[\u0026#34;births\u0026#34;].max()] # 在 Births 列中找到值是973 的所有记录 不等于 df[....] #可以看出是遍历了全部的数据的，只有等于时才会返回Tue，佛则为False result: df[\u0026#34;names\u0026#34;][df[\u0026#34;births\u0026#34;] == df[\u0026#34;births\u0026#34;].max()] #返回符合条件的指定的列的值 #的意思是 在 Names列中挑选出 [Births 列的值等于 973] 也就是相当于sql的where了 result: #也可以这样，Sorted之前排序过了 Sorted[\u0026#34;names\u0026#34;].head(1) result:\n4 Mel Name: names, dtype: object Sorted[\u0026#34;names\u0026#34;].head(1).values#返回值且没有index result:\narray([\u0026#39;Mel\u0026#39;], dtype=object) str() 可以将一个对象转换为字符串。\n#绘图 df[\u0026#34;births\u0026#34;].plot.bar()#条形图 #births中的最大值 max_value = df[\u0026#34;births\u0026#34;].max() max_name = df[\u0026#34;names\u0026#34;][df[\u0026#34;births\u0026#34;] == df[\u0026#34;births\u0026#34;].max()].values #显示的文本 text = str(max_value) + \u0026#34; - \u0026#34; + max_name #将文本显示在图中 annotate plt.annotate(text,xy=(1,max_value),xytext=(8,0), xycoords=(\u0026#34;axes fraction\u0026#34;,\u0026#39;data\u0026#39;),textcoords=\u0026#34;offset points\u0026#34;) print(\u0026#34;The most popular name\u0026#34;) df[df[\u0026#39;births\u0026#39;] == df[\u0026#39;births\u0026#39;].max()]#df[条件]返回的是符合条件的整行数据 result: ","date":"2021-09-21T00:00:00Z","permalink":"https://example.com/p/%E5%AD%A6%E4%B9%A0pandas_%E7%AC%AC%E4%B8%80%E8%AF%BE/","title":"学习Pandas_第一课"},{"content":"获取数据 - 我们的数据在一个 Excel 文件中，包含了每一个日期的客户数量。 我们 将学习如何读取 Excel 文件的内容并处理其中的数据。\n准备数据 - 这组时间序列的数据并不规整而且有重复。 我们的挑战是整理这些数据 并且预测下一个年度的客户数。\n分析数据 - 我们将使用图形来查看趋势情况和离群点。我们会使用一些内置的计算 工具来预测下一年度的客户数。\n表现数据 - 结果将会被绘制成图形。\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) %matplotlib inline plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = [\u0026#34;SimHei\u0026#34;] plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False pd.date_range? #date_range()函数用来生成时间序列的 #参数freq是单位 #设置种子 np.random.seed(2021) #生成测试数据的函数 def create_dataset(Number = 1): output = [] for _ in range(Number): #创建一个按周期计算的日期的范围（每周一起始） ==》 W-MON rng = pd.date_range(start=\u0026#34;1/1/2021\u0026#34;,end=\u0026#34;12/31/2024\u0026#34;,freq=\u0026#34;W-MON\u0026#34;) #创建一些随机数 data = np.random.randint(25,1000,size=len(rng)) #状态池 status = [1,2,3] #创建一个随机的状态列表 random_status = [status[np.random.randint(0,len(status))] for i in range(len(rng))] #行政洲state的列表 states = [\u0026#34;GA\u0026#34;,\u0026#34;FL\u0026#34;,\u0026#34;fl\u0026#34;,\u0026#34;NY\u0026#34;,\u0026#34;NJ\u0026#34;,\u0026#34;TX\u0026#34;] #创建一个行政周期的随机列表 random_states = [states[np.random.randint(0,len(states))]for _ in range(len(rng))] output.extend(zip(random_states,random_status,data,rng)) return output pd.date_range(start=\u0026#34;1/1/2021\u0026#34;,end=\u0026#34;12/31/2024\u0026#34;,freq=\u0026#34;W-MON\u0026#34;) result: 现在我们有了一个生成测试数据的函数，我们来创建一些数据并放到一个dataframe 中\ndataset = create_dataset(4) #read_csv 才是 names/header参数 df = pd.DataFrame(dataset,columns=[\u0026#34;States\u0026#34;,\u0026#34;Status\u0026#34;,\u0026#34;CustomerCount\u0026#34;,\u0026#34;StatusDate\u0026#34;]) df.info() result: df.head(8) result: 现在我们将这个 dataframe 保存到 Excel 文件中，然后再读取出来放回到 dataframe 中。 我们简单地展示一下如何读写 Excel 文件。\n我们不会把索引值(index)写到 Excel 文件中，这些索引值不是我们的测试数据的一 部分。\n#j结果保存到excel中，跟csv是差不多的 #要openpyxl 包 df.to_excel(\u0026#34;./Lesson3.xlsx\u0026#34;,index=False,encoding=\u0026#34;UTF-8\u0026#34;)#这里不知道哪里有问题 #破案了是notebook本身看不了xlxs的问题，实际上生成的文件没问题 # df.to_csv(\u0026#34;./Lesson3.csv\u0026#34;,index=False,encoding=\u0026#34;UTF-8\u0026#34;) print(\u0026#34;Done\u0026#34;) result:\nDone pd.read_excel? # 读取第一个页签(sheetname)，并指定索引列index_col是 StatusDate #需要xlrd包 df = pd.read_excel(\u0026#34;./Lesson3.xlsx\u0026#34;,sheet_name=0,index_col=\u0026#34;StatusDate\u0026#34;) # df = pd.read_csv(\u0026#34;./Lesson3.csv\u0026#34;,index_col=\u0026#34;StatusDate\u0026#34;) df.dtypes result: df.index df.head(8) 这一部分，我们尝试将数据进行清洗以备分析:\n确保 state 列都是大写 只选择 Status = 1 的那些记录 对 State 列中的 NJ 和 NY，都合并为 NY 去除一些离群中 (数据集中一些特别奇异的结果) 让我们先快速看一下 State 列中的大小写情况。\ndf[\u0026#34;States\u0026#34;].unique() result:\narray([\u0026#39;NJ\u0026#39;, \u0026#39;fl\u0026#39;, \u0026#39;TX\u0026#39;, \u0026#39;FL\u0026#39;, \u0026#39;GA\u0026#39;, \u0026#39;NY\u0026#39;], dtype=object) 我们用 upper() 函数和 dataframe 的 apply 属性将 State 的值都转换为大写。\nlambda 函数简单地将 upper() 函数应用到 State 列中的每一个值上。\n#清洗State列，全部转换为大写 df[\u0026#34;States\u0026#34;] = df.States.apply(lambda x:x.upper()) df[\u0026#34;States\u0026#34;].unique() result:\narray([\u0026#39;NJ\u0026#39;, \u0026#39;FL\u0026#39;, \u0026#39;TX\u0026#39;, \u0026#39;GA\u0026#39;, \u0026#39;NY\u0026#39;], dtype=object) df.apply?#df.apply(func)就是对df数据的每个元素使用传入的func #只保留Status == 1 # df[df.[\u0026#34;Status\u0026#34;] == 1] mask = df[\u0026#34;Status\u0026#34;] == 1 df = df[mask] df result: 将 NJ 转换为 NY，仅需简单地:\n#也可以用个mask #注意某的.号 df[\u0026#34;States\u0026#34;][df[\u0026#34;States\u0026#34;]==\u0026#34;NJ\u0026#34;] = \u0026#34;NY\u0026#34; 现在我们看一下，我们有了一个更加干净的数据集了。\ndf[\u0026#34;States\u0026#34;].unique() result:\narray([\u0026#39;NY\u0026#39;, \u0026#39;FL\u0026#39;, \u0026#39;GA\u0026#39;, \u0026#39;TX\u0026#39;], dtype=object) 这时也许我们可以将数据绘制成图形查看一下数据中是否有任何离群值(outliers)或 者不一致(inconsistencies)。\n我们使用 dataframe 中的 plot() 函数。\ndf[\u0026#34;CustomerCount\u0026#34;].plot(figsize=(15,5)) #之前读取的时候已经指定了index，所以这里的轴就是这个的 result: 如果我们看这些数据，我们会意识到同一个 State, StatusDate 和 Status 的组合会 有多个值。 这可能意味着我们在处理的数据是脏数据/坏数据/不精确的数据 (dirty/bad/inaccurate)，但我们不这样假设。 我们假设这个数据集是一个更大更大 数据集的一个子集(subset)，并且如果我们简单的将 State, StatusDate 和 Status 组 合下的 CustomerCount 累加起来, 我们将得到每一天的 全部客户数量 (Total Customer Count)。\nsortdf = df[df[\u0026#34;States\u0026#34;]==\u0026#34;NY\u0026#34;].sort_index(axis=0)#索引为第零列即StatusDate sortdf.head(20) result: 我们的任务是创建一个新的 dataframe，然后对数据进行压缩处理，是的每一个\nState 和 StatusDate 组合代表一天的客户数量。 我们可以忽略 Status 列因为这一\n列我们之前处理过只有 1 这个值了。 要完成这个操作，我们使用 dataframe 的\ngroupyby() 和 sum() 这两个函数。\n注意，我们要使用 reset_index。 如果我们不这么做，我们将无法同时用 State 和\nStatusDate 这两列来做分组，因为 groupby 函数需要列(columns)来做为输入(译者\n注: StatusDate 目前是 index，不是 column)。 reset_index 函数将把 dataframen\n中作为索引(index)的 StatusDate 变回普通的列。\n#reset_index可以取消之前定义的索引列 #仔细体会下groupby的分组功能 # 先 reset_index，然后按照 State 和 StatusDate 来做分组 (groupby) 多个group依据时记得用[]阔起来 Daily = df.reset_index().groupby([\u0026#34;States\u0026#34;,\u0026#34;StatusDate\u0026#34;]).sum() #sum是因为有可能有重复的日期 Daily.head() result: 在 Daily 这个 dataframe 中，State 和 StatusDate 这两列被自动设置为了索引 (index)。 就是说groupby后会自动列为index\n你可以将 index 设想为数据库表中的逐渐(primary key)，只不过没有唯一 性(unique)的限制。\n索引中的这些列让我们更容易的可以选择，绘图和执行一些计算。\n接下去我们将 Status 删掉，它的值就是 1，没有多少用途了。\n#可以直接用py的del删除 del Daily[\u0026#34;Status\u0026#34;] Daily.head(8) result: # 看一下 dataframe 中的索引(index) Daily.index result: #对于index使用levels[]可以过的指定的索引类别 #选择State这个索引 Daily.index.levels[0] result:\nIndex([\u0026#39;FL\u0026#39;, \u0026#39;GA\u0026#39;, \u0026#39;NY\u0026#39;, \u0026#39;TX\u0026#39;], dtype=\u0026#39;object\u0026#39;, name=\u0026#39;States\u0026#39;) #选择StatusDate这个索引 Daily.index.levels[1] 我们按照每一个州来绘制一下图表。\n正如你所看到的，将图表按照不同的 State 区分开，我们能看到更清晰的数据。\n你能看到任何离群值(outlier)吗?\nDaily.loc? #loc是用来根据label(行的名字)获取数据的 Daily.loc[\u0026#34;FL\u0026#34;].plot() Daily.loc[\u0026#34;GA\u0026#34;].plot() Daily.loc[\u0026#34;NY\u0026#34;].plot() Daily.loc[\u0026#34;TX\u0026#34;].plot() result: 我们也可以指定一个日期，比如 2012，来绘制图表。 We can also just plot the data on a specific date, like 2012. 我们能够清晰地看到这些州的数据分布很广。 因 为这些数据包含了每周的客户数量，数据的变化情况看上去是可以预测的。 在这个 教程里面，我们假设没有坏数据并继续往下。\n#注意这里是没有.号的 Daily.loc[\u0026#34;FL\u0026#34;][\u0026#34;2022\u0026#34;:].plot() Daily.loc[\u0026#34;GA\u0026#34;][\u0026#34;2022\u0026#34;:].plot() Daily.loc[\u0026#34;NY\u0026#34;][\u0026#34;2022\u0026#34;:].plot() Daily.loc[\u0026#34;TX\u0026#34;][\u0026#34;2022\u0026#34;:].plot() result: 我们假设每个月的客户数量应该是保持相对稳定的。 在一个月内任何在这个特定范 围之外的数据都可以从数据集中移除。 最终的结果应该更加的平滑并且图形不会有 尖刺。\nStateYearMonth - 这里我们通过 State, StatusDate 中的年份(year)和月份(Month) 来分组。\nDaily[\u0026lsquo;Outlier\u0026rsquo;] - 一个布尔(boolean)变量值 (True 或者 False)，从而我们会知道 CustomerCount 值是否在一个可接受的范围之内。\n我们将会用到 transform 而不是 apply。 原因是， transform 将会保持 dataframe 矩阵的形状(shape)(就是行列数不变)而 apply 会改变矩阵的形状。 看过前面的图形 我们意识到这些图形不是高斯分布的(gaussian distribution)，这意味着我们不能使 用均值(mean)和标准差(stDev)这些统计量。 我们将使用百分位数(percentile)。 请 注意这里也会有把好数据消除掉的风险。\nDaily.index.get_level_values? #这种情况下只对xlsx有用，而对csv没用 Daily.index.get_level_values(1).year result: #计算离群值 #index.get_level_values()是用来获取某个行label的所有值 State_year_month = Daily.groupby([Daily.index.get_level_values(0),#state Daily.index.get_level_values(1).year,#statusdate的年 Daily.index.get_level_values(1).month])#statusdate的月 #可以这样赋予新的一列 #transform和apply的用处一样，但是它不会原始数据的shape #quantile是求分位数 Daily[\u0026#39;Lower\u0026#39;] = State_year_month[\u0026#39;CustomerCount\u0026#39;].transform( lambda x: x.quantile(q=.25) - (1.5*x.quantile(q=.75)-x.quantile(q=.25)) ) Daily[\u0026#39;Upper\u0026#39;] = State_year_month[\u0026#39;CustomerCount\u0026#39;].transform( lambda x: x.quantile(q=.75) + (1.5*x.quantile(q=.75)-x.quantile(q=.25)) ) Daily[\u0026#39;Outlier\u0026#39;] = (Daily[\u0026#39;CustomerCount\u0026#39;] \u0026lt; Daily[\u0026#39;Lower\u0026#39;]) | (Daily[\u0026#39;CustomerCount\u0026#39;] \u0026gt; Daily[\u0026#39;Upper\u0026#39;]) # 移除离群值 Daily = Daily[Daily[\u0026#39;Outlier\u0026#39;] == False] Daily 这个 dataframe 按照每天来汇总了客户数量。 而原始的数据则可能每一天会 有多个记录。 我们现在保留下一个用 State 和 StatusDate 来做索引的数据集。 Outlier 列如果是 False 的化代表这条记录不是一个离群值。\nDaily.head() result: 我们创建一个单独的 dataframe，叫 ALL，仅用 StatusDate 来为 Daily 数据集做索 引。\n我们简单地去掉 State 这一列。 Max 列则代表了每一个月最大的客户数量。\nMax 列是用来是的图形更顺滑的。\n# 按日期计算出最大的客户数 ALL = pd.DataFrame(Daily[\u0026#34;CustomerCount\u0026#34;].groupby(Daily.index.get_level_values(1)).sum()) #重命名列名 ALL.columns = [\u0026#34;CustomerCount\u0026#34;] #按照年和月来分组 Year_month = ALL.groupby([lambda x:x.year,lambda x:x.month]) #找出每一个年和月的组合中最大的客户数 ALL[\u0026#34;Max\u0026#34;] = Year_month[\u0026#34;CustomerCount\u0026#34;].transform(lambda x:x.max()) ALL.head() result 从上面的 ALL dataframe 中可以看到，在 2021年1月的这个月份，最大的客户数是 1235。 如果我们使用 apply 的方式，我们将会得到一个以 (年 和 月）组合作为索引 的 dataframe ，只有 Max 这一列有1235这个值。\n如果当前的客户数达到了公司制定的一定的目标值，这也会是一个很有趣的度量 值。\n现在的任务是可视化的展示当前的客户数是否达到了下面列出的目标值。\n我们把这些目标叫做 BHAG (Big Hairy Annual Goal，年度战略目标)。\n12/31/2011 - 1,000 客户\n12/31/2012 - 2,000 客户\n12/31/2013 - 3,000 客户\n我们将用 date_range 函数来创建日期。\n定义: date_range(start=None, end=None, periods=None, freq=\u0026lsquo;D\u0026rsquo;, tz=None, normalize=False, name=None, closed=None)\n文档(Docstring): 返回一个固定频率的日期时间索引，用日历天作为默认的频率 把频率设定为 A 或者是“年度” 我们将等到上述三个年份。\n#创建BHAG data = [1000,2000,3000] idx = pd.date_range(\u0026#34;12/31/2021\u0026#34;,\u0026#34;12/31/2023\u0026#34;,freq=\u0026#34;A\u0026#34;)#\u0026#39;A\u0026#39;年度划分 BHAG = pd.DataFrame(data,index=idx, columns=[\u0026#34;BHAG\u0026#34;])#参数index可以自定义行label BHAG result: 我们之前学过用 concat 把 dataframe 合并。\n记住，当我们设置 axis = 0 时我们按列合并 (row wise, 译者注: 即直接把列合并进去，行方向数据缺失用NaN来填 充）。\n# 把 BHAG 和 ALL 两个数据集合并在一起 #contact是合并DataFrame 合并的对象记得[]装在一起 axis可以指定方式，0为列，1为行 combined = pd.concat([ALL,BHAG],axis=0) combined = combined.sort_index(axis=0)#索引为第零列且排序，而sorted_value()不会产生索引的变化 combined.tail() result: #绘图 fig,axes = plt.subplots(figsize=(12,7)) #fillna是用来填充NAN值的 method可以指定填充的方法： # pad/ffill：用前一个非缺失值去填充该缺失值 # backfill/bfill：用下一个非缺失值填充该缺失值 combined[\u0026#34;BHAG\u0026#34;].fillna(method=\u0026#34;pad\u0026#34;).plot(color=\u0026#34;green\u0026#34;,label=\u0026#34;BHAS\u0026#34;) combined[\u0026#34;Max\u0026#34;].plot(color=\u0026#34;blue\u0026#34;,label=\u0026#34;All Market\u0026#34;) plt.legend(loc=\u0026#34;best\u0026#34;) 这里还有一个需求是预测下一个年度的客户数，我们之后会通过几个简单的步骤完 成。 我们想把已经合并的 dataframe 按照 Year 来分组，并且计算出年度的最大客 户数。 这样每一行就是一个年度的数据。\nYear = combined.groupby(lambda x:x.year).max() Year result: # 增加一列，表示为每一年比上一年变化的百分比 #pct_change用来计算变化率的 （后一个值-前一个值）／前一个值 Year[\u0026#34;YR_PCT_Change\u0026#34;] = Year[\u0026#34;Max\u0026#34;].pct_change(periods=1) Year result: 要得到下一个年度末的客户数，我们假定当前的增长速率是维持恒定的。 我们按照 这个增长速率来预测下一个年度的客户数量。\n#ix是loc和iloc的结合 #新库ix直接变为了iloc (1 + Year.iloc[2023,\u0026#34;YR_PCT_Change\u0026#34;]) * Year.iloc[2023,\u0026#34;Max\u0026#34;] #pd可以直接用本身的函数plot画图 #前面传入y的数据，plot后面指定图的大小、位置等 x轴会默认为传入的数据的行索引 # 第一张图是整个市场的 ALL[\u0026#39;Max\u0026#39;].plot(figsize=(10, 5)) plt.title(\u0026#39;ALL Markets\u0026#39;) # 后面四张 fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 10)) fig.subplots_adjust(hspace=1.0)#可以理解为排版美观 Daily.loc[\u0026#39;FL\u0026#39;][\u0026#39;CustomerCount\u0026#39;][\u0026#39;2012\u0026#39;:].fillna(method=\u0026#39;pad\u0026#39;).plot(ax=axes[0,0]) Daily.loc[\u0026#39;GA\u0026#39;][\u0026#39;CustomerCount\u0026#39;][\u0026#39;2012\u0026#39;:].fillna(method=\u0026#39;pad\u0026#39;).plot(ax=axes[0,1]) Daily.loc[\u0026#39;TX\u0026#39;][\u0026#39;CustomerCount\u0026#39;][\u0026#39;2012\u0026#39;:].fillna(method=\u0026#39;pad\u0026#39;).plot(ax=axes[1,0]) Daily.loc[\u0026#39;NY\u0026#39;][\u0026#39;CustomerCount\u0026#39;][\u0026#39;2012\u0026#39;:].fillna(method=\u0026#39;pad\u0026#39;).plot(ax=axes[1,1]) # 增加图表的抬头 axes[0,0].set_title(\u0026#39;Florida\u0026#39;) axes[0,1].set_title(\u0026#39;Georgia\u0026#39;) axes[1,0].set_title(\u0026#39;Texas\u0026#39;) axes[1,1].set_title(\u0026#39;North East\u0026#39;) Daily.loc[\u0026#34;FC\u0026#34;][\u0026#34;\u0026#34;][2022] ","date":"2021-09-21T00:00:00Z","permalink":"https://example.com/p/%E5%AD%A6%E4%B9%A0pandas_%E7%AC%AC%E4%B8%89%E8%AF%BE/","title":"学习Pandas_第三课"},{"content":"#导入库 import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = [\u0026#34;SimHei\u0026#34;] names = [\u0026#34;Bob\u0026#34;,\u0026#34;Jessica\u0026#34;,\u0026#34;Mary\u0026#34;,\u0026#34;John\u0026#34;,\u0026#34;Mel\u0026#34;] 使用上面的5个名字来创建一个有1,000个婴儿名字的随机列表，我们要做如下一些\n操作:\n生成一个 0 到 4 之间的随机数\n我们会用到 seed，randint, len, range 和 zip 这几个函数。\n#随机种子保证随机的一致性 np.random.seed? np.random.seed(2021)#随机种子 #用loop产生随机数（index），再去找值 random_names = [names[np.random.randint(0,len(names))]for _ in range(1000)] random_names[:10] result: #同理birth ## 1880年，不同婴儿名字对应的出生数量 np.random.seed(2021) births = [np.random.randint(0,1000) for _ in range(1000)] births[:10] result: #用 zip 函数把 names 和 births 这两个数据集合并在一起。 bady_data_set = list(zip(random_names,births)) bady_data_set[:10] result: 我们基本上完成了数据集的创建工作。 现在我们要用 pandas 库将这个数据集导出 到一个 csv 文件中。\ndf 是一个 DataFrame对象。 你可以把这个对象理解为包含了 BabyDataset 的 内容而格式非常象一个 sql 表格或者 Excel 的数据表。 让我们看看 df 中的内容。\ndf = pd.DataFrame(bady_data_set,columns=[\u0026#34;names\u0026#34;,\u0026#34;births\u0026#34;]) df[:10] result: 将 dataframe 导出到一个 csv 文件中。 我们将导出文件命名为 births1880.csv。\n导出 csv 文件的函数是 to_csv。 除非你指定了其他的文件目录，否则导出的文件\n将保存在和 notebook 文件相同的位置。\n将 dataframe 导出到一个 csv 文件中。 我们将导出文件命名为 births1880.csv。\n导出 csv 文件的函数是 to_csv。 除非你指定了其他的文件目录，否则导出的文件\n将保存在和 notebook 文件相同的位置。\ndf.to_csv(\u0026#34;births1880.txt\u0026#34;,index=False,header=False) df = pd.read_csv(\u0026#34;./births1880.txt\u0026#34;,names=[\u0026#34;names\u0026#34;,\u0026#34;births\u0026#34;])#可以直接赋予列名，且不会利用到第一列的数据 # df = pd.read_csv(\u0026#34;./births1880.txt\u0026#34;,header=None])#这样可以使得不会将第一行数据作为列名 df.info?#可以返回数据的信息 df.info() 汇总信息:\n数据集里有 1000 条记录\n有一列 names 有 1000 个值\n有一列births 有 1000 个值\n这两列, 一个是numeric(数字型), 另外一个是non numeric(非数字型)\n我们可以使用 head() 这个函数查看一下dataframe中的前 5 条记录。 你也可以传 入一个数字 n 来查看 dataframe 中的前 n 条记录。\ndf.head(6) df.tail() result: import os os.remove(\u0026#34;./births1880.txt\u0026#34;) 我们的数据包含了1880年出生的婴儿及其数量。 我们已经知道我们有1,000条记录\n而且没有缺失值(所有值都是非空 non-null 的)。 我们还可以验证一下 \u0026ldquo;Names\u0026rdquo; 列\n仅包含了5个唯一的名字。\n我们可以使用 dataframe 的 unique 属性找出 \u0026ldquo;Names\u0026rdquo; 列的所有唯一的(unique)的 记录。\n#unique()可以找出指定数据的唯一记录即有哪几种类别 类似于一个set df[\u0026#34;names\u0026#34;].unique() result:\narray([\u0026#39;Mel\u0026#39;, \u0026#39;Jessica\u0026#39;, \u0026#39;Bob\u0026#39;, \u0026#39;John\u0026#39;, \u0026#39;Mary\u0026#39;], dtype=object) [print(_) for _ in df[\u0026#34;names\u0026#34;].unique()] result: #方法2 print(df[\u0026#34;names\u0026#34;].describe()) result: 因为每一个婴儿名字对应有多个值，我们需要把这几个值汇总起来这样一个婴儿名\n字就只出现一次了。 这意味着 1,000 行将变成只有 5 行。 我们使用 groupby 函数\n来完成这个动作。\ndf.groupby? # 创建一个groupby对象 name = df.groupby(\u0026#34;names\u0026#34;) #在group对象上执行求和功能 df = name.sum() df #实质上就是一个分组的过程 跟set类似 result: 要找到最高出生率的婴儿名或者是最热门的婴儿名字，我们可以这么做。\n将 dataframe 排序并且找到第一行\n使用 max() 属性找到最大值\n#方法1 Sorted = df.sort_values([\u0026#34;births\u0026#34;],ascending=False) Sorted.head(1) result: #方法2 df[\u0026#34;births\u0026#34;].max() result:\n105871 df[df[\u0026#34;births\u0026#34;] == df[\u0026#34;births\u0026#34;].max()] result: #绘图 #pd本身有个plot函数绘画 df[\u0026#34;births\u0026#34;].plot.bar() print(\u0026#34;The most popular name\u0026#34;) #sorted_values()的参数by= 是用来指定划分数据依据的列 df.sort_values(by=\u0026#34;births\u0026#34;,ascending=False) result： ","date":"2021-09-21T00:00:00Z","permalink":"https://example.com/p/%E5%AD%A6%E4%B9%A0pandas_%E7%AC%AC%E4%BA%8C%E8%AF%BE/","title":"学习Pandas_第二课"},{"content":" 这里是一个限制决策树层数为4的DecisionTreeClassifier例子。\n#1.导入相关库 from itertools import product#用来相互交叉乘即笛卡尔积 import numpy as np import matplotlib.pyplot as plt from sklearn import datasets from sklearn.tree import DecisionTreeClassifier #2.导入数据 iris = datasets.load_iris()#仍然是使用鸢尾花 X = iris.data[:,[0,2]] X result: code:\ny = iris.target#标签 y result: #使用算法训练模型 iris_decision_tree = DecisionTreeClassifier(max_depth=4) iris_decision_tree.fit(X,y) result:\nDecisionTreeClassifier(max_depth=4) #可视化数据 x_min,x_max = X[:,0].min() - 1, X[:,0].max() + 1#z这个处理是为了调整坐标轴 y_min,y_max = X[:,1].min() - 1, X[:,1].max() + 1 #注意分类图中的x和y #创建坐标轴 xx,yy = np.meshgrid(np.arange(x_min,x_max,0.1),np.arange(y_min,y_max,0.1)) #预测 Z = iris_decision_tree.predict(np.c_[xx.ravel(), yy.ravel()])#np.c_合并array print(Z.shape) print(xx.shape) Z = Z.reshape(xx.shape)#调整维度与坐标一致 #contourf()绘制等高线图xx,yy为z的坐标，这里的z就是算法预测出的结果 和坐标位置不是同一个概念 plt.contourf(xx,yy,Z,alpha=0.4) #c=y会根据y的值给予不同的颜色，相同的值为同一种颜色 plt.scatter(X[:,0],X[:,1],c=y,alpha=0.8) plt.show() result: #可视化决策树 from IPython.display import Image from sklearn import tree import pydotplus iris_dot_data = tree.export_graphviz(iris_decision_tree, out_file=None, # feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True) graph = pydotplus.graph_from_dot_data(iris_dot_data) Image(graph.create_png()) result: iris.feature_names result: ","date":"2021-09-20T00:00:00Z","permalink":"https://example.com/p/wzu_decisiontree/","title":"WZU_DecisionTree"},{"content":"import numpy as np import pandas as pd import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) import matplotlib import matplotlib.pyplot as plt matplotlib.style.use(\u0026#34;ggplot\u0026#34;) plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = \u0026#34;SimHei\u0026#34; plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False from sklearn import datasets,svm from sklearn.svm import SVC from sklearn.datasets import make_moons,make_circles,make_classification 生成一些随机数据来让我们后面去分类，为了数据难一点，我们加入了一些噪音。生成数据的同时把数据归一化\n#make_circles生成月亮形数据 X,y = make_circles(noise=0.2,factor=0.5,random_state=22) #从sklearn.preprocessing导入StandardScaler归一化处理 from sklearn.preprocessing import StandardScaler X = StandardScaler().fit_transform(X) 我们先看看我的数据是什么样子的，这里做一次可视化如下：\nfrom matplotlib.colors import ListedColormap # matplotlib.colors模块用于将颜色或数字参数转换为RGBA或RGB。 #此模块用于将数字映射到颜色或以一维颜色数组(也称为colormap)进行颜色规格转换。 cm = plt.cm.RdBu cm_bright = ListedColormap([\u0026#34;#FF0000\u0026#34;,\u0026#34;#0000FF\u0026#34;]) ax = plt.subplot() ax.set_title(\u0026#34;Input data\u0026#34;) ax.scatter(X[:,0],X[:,1],c=y,cmap=cm_bright) ax.set_xticks(()) ax.set_yticks(()) #会自动调整子图参数，使之填充整个图像区域 plt.tight_layout() plt.show() result: 好了，现在我们要对这个数据集进行SVM RBF分类了，分类时我们使用了网格搜索，在C=(0.1,1,10)和gamma=(1, 0.1, 0.01)形成的9种情况中选择最好的超参数，我们用了4折交叉验证。这里只是一个例子，实际运用中，你可能需要更多的参数组合来进行调参。\nfrom sklearn.model_selection import GridSearchCV\\ #param_grid是以字典形式的 grid = GridSearchCV(SVC(),param_grid={\u0026#34;C\u0026#34;:[0.1,1,10],\u0026#34;gamma\u0026#34;:[1,0.1,0.01]},cv=4) grid.fit(X,y) print(\u0026#34;The best parameters are %s with a score of %0.2f\u0026#34; % (grid.best_params_,grid.best_score_)) result:\nThe best parameters are {\u0026#39;C\u0026#39;: 10, \u0026#39;gamma\u0026#39;: 0.1} with a score of 0.91 最终的输出如下： The best parameters are {\u0026lsquo;C\u0026rsquo;: 10, \u0026lsquo;gamma\u0026rsquo;: 0.1} with a score of 0.91\n也就是说，通过网格搜索，在我们给定的9组超参数中，C=10， Gamma=0.1 分数最高，这就是我们最终的参数候选。\n到这里，我们的调参举例就结束了。不过我们可以看看我们的普通的SVM分类后的可视化。这里我们把这9种组合各个训练后，通过对网格里的点预测来标色，观察分类的效果图。代码如下：\nX[:4] result: code:\ny[:4] result:\narray([0, 0, 1, 0], dtype=int64) code:\nx_min,x_max = X[:,0].min() - 1,X[:,0].max() + 1 #这里的y其实是x2 y_min,y_max = X[:,1].min() - 1,X[:,1].max() + 1 xx,yy = np.meshgrid(np.arange(x_min,x_max,0.02), np.arange(y_min,y_max,0.02)) print(X.shape) print(y.shape) print(xx.shape) print(yy.shape) result:\n(100, 2) (100,) (309, 336) (309, 336) code:\nclf = SVC(C=10,gamma=0.1) clf.fit(X,y) #xx和yy矩阵都变成两个一维数组，然后到np.c_[] 函数组合成一个二维数组 Z = clf.predict(np.c_[xx.ravel(),yy.ravel()]) # Put the result into a color plot print(Z.shape) Z= Z.reshape(xx.shape) print(Z.shape) print(xx.ravel().shape) result:\n(103824,) (309, 336) (103824,) code:\n#记得用（）包起来 for i,C in enumerate((0.1,1,10)): for j,gamma in enumerate((1,0.1,0.01)): plt.subplot() clf = SVC(C=C,gamma=gamma) clf.fit(X,y) #xx和yy矩阵都变成两个一维数组，然后到np.c_[] 函数组合成一个二维数组 Z = clf.predict(np.c_[xx.ravel(),yy.ravel()]) # Put the result into a color plot Z = Z.reshape(xx.shape) #SVM的核函数使得数据变成了三维的 plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8) # Plot also the training points plt.scatter(X[:,0],X[:,1],c=y,cmap=plt.cm.coolwarm) plt.xlim(xx.min(),xx.max()) plt.ylim(yy.min(),yy.max()) plt.xticks(()) plt.yticks(()) plt.xlabel(\u0026#34;gamma\u0026#34;+str(gamma)+\u0026#34;C=\u0026#34;+str(C)) plt.show() result: 以上就是SVM RBF调参的一些总结.\n","date":"2021-09-17T00:00:00Z","permalink":"https://example.com/p/%E5%88%98%E5%BB%BA%E5%B9%B3%E8%80%81%E5%B8%88pinard%E5%8D%9A%E5%AE%A2%E7%9A%84svm_rbf%E5%88%86%E7%B1%BB%E8%B0%83%E5%8F%82%E4%BE%8B%E5%AD%90/","title":"刘建平老师Pinard博客的SVM_RBF分类调参例子"},{"content":" -i 表示忽略大小写（windox下大小写不敏感，但是Linux下敏感）\n","date":"2021-09-15T00:00:00Z","permalink":"https://example.com/p/%E6%AF%8F%E8%8A%82%E9%97%AE%E9%A2%98%E6%95%B4%E7%90%86/","title":"每节问题整理"},{"content":"机器学习练习6 KNN算法 代码修改并注释：黄海广，haiguang2000@wzu.edu.cn\n1．$k$近邻法是基本且简单的分类与回归方法。$k$近邻法的基本做法是：对给定的训练实例点和输入实例点，首先确定输入实例点的$k$个最近邻训练实例点，然后利用这$k$个训练实例点的类的多数来预测输入实例点的类。\n2．$k$近邻模型对应于基于训练数据集对特征空间的一个划分。$k$近邻法中，当训练集、距离度量、$k$值及分类决策规则确定后，其结果唯一确定。\n3．$k$近邻法三要素：距离度量、$k$值的选择和分类决策规则。常用的距离度量是欧氏距离及更一般的pL距离。$k$值小时，$k$近邻模型更复杂；$k$值大时，$k$近邻模型更简单。$k$值的选择反映了对近似误差与估计误差之间的权衡，通常由交叉验证选择最优的$k$。\n常用的分类决策规则是多数表决，对应于经验风险最小化。\n4．$k$近邻法的实现需要考虑如何快速搜索k个最近邻点。kd树是一种便于对k维空间中的数据进行快速检索的数据结构。kd树是二叉树，表示对$k$维空间的一个划分，其每个结点对应于$k$维空间划分中的一个超矩形区域。利用kd树可以省去对大部分数据点的搜索， 从而减少搜索的计算量。\n距离度量 在机器学习算法中，我们经常需要计算样本之间的相似度，通常的做法是计算样本之间的距离。\n设$x$和$y$为两个向量，求它们之间的距离。\n这里用Numpy实现，设和为ndarray \u0026lt;numpy.ndarray\u0026gt;，它们的shape都是(N,)\n$d$为所求的距离，是个浮点数（float）。\nimport numpy as np #注意：运行代码时候需要导入NumPy库 欧氏距离(Euclidean distance) 欧几里得度量(euclidean metric)(也称欧氏距离)是一个通常采用的距离定义，指在$m$维空间中两个点之间的真实距离，或者向量的自然长度(即该点到原点的距离)。在二维和三维空间中的欧氏距离就是两点之间的实际距离。\n距离公式：\n$$ d\\left( x,y \\right) = \\sqrt{\\sum_{i}^{}(x_{i} - y_{i})^{2}} $$ 代码实现：\ndef euclidean(x, y): return np.sqrt(np.sum((x - y)**2)) 曼哈顿距离(Manhattan distance) 想象你在城市道路里，要从一个十字路口开车到另外一个十字路口，驾驶距离是两点间的直线距离吗？显然不是，除非你能穿越大楼。实际驾驶距离就是这个“曼哈顿距离”。而这也是曼哈顿距离名称的来源，曼哈顿距离也称为城市街区距离(City Block distance)。\n距离公式：\n$$ d(x,y) = \\sum_{i}^{}|x_{i} - y_{i}| $$ 代码实现：\ndef manhattan(x, y): return np.sum(np.abs(x - y)) 切比雪夫距离(Chebyshev distance) 在数学中，切比雪夫距离(Chebyshev distance)或是L∞度量，是向量空间中的一种度量，二个点之间的距离定义是其各坐标数值差绝对值的最大值。以数学的观点来看，切比雪夫距离是由一致范数(uniform norm)(或称为上确界范数)所衍生的度量，也是超凸度量(injective metric space)的一种。\n距离公式：\n$$ d\\left( x,y \\right) = \\max_{i}\\left| x_{i} - y_{i} \\right| $$ 若将国际象棋棋盘放在二维直角座标系中，格子的边长定义为1，座标的$x$轴及$y$轴和棋盘方格平行，原点恰落在某一格的中心点，则王从一个位置走到其他位置需要的步数恰为二个位置的切比雪夫距离，因此切比雪夫距离也称为棋盘距离。例如位置F6和位置E2的切比雪夫距离为4。任何一个不在棋盘边缘的位置，和周围八个位置的切比雪夫距离都是1。\n代码实现：\ndef chebyshev(x, y): return np.max(np.abs(x - y)) 闵可夫斯基距离(Minkowski distance) 闵氏空间指狭义相对论中由一个时间维和三个空间维组成的时空，为俄裔德国数学家闵可夫斯基(H.Minkowski,1864-1909)最先表述。他的平坦空间(即假设没有重力，曲率为零的空间)的概念以及表示为特殊距离量的几何学是与狭义相对论的要求相一致的。闵可夫斯基空间不同于牛顿力学的平坦空间。$p$取1或2时的闵氏距离是最为常用的，$p= 2$即为欧氏距离，而$p =1$时则为曼哈顿距离。\n当$p$取无穷时的极限情况下，可以得到切比雪夫距离。\n距离公式：\n$$ d\\left( x,y \\right) = \\left( \\sum_{i}^{}|x_{i} - y_{i}|^{p} \\right)^{\\frac{1}{p}} $$\n代码实现：\ndef minkowski(x, y, p): return np.sum(np.abs(x - y)**p)**(1 / p) 汉明距离(Hamming distance) 汉明距离是使用在数据传输差错控制编码里面的，汉明距离是一个概念，它表示两个(相同长度)字对应位不同的数量，我们以表示两个字,之间的汉明距离。对两个字符串进行异或运算，并统计结果为1的个数，那么这个数就是汉明距离。\n距离公式：\n$$ d\\left( x,y \\right) = \\frac{1}{N}\\sum_{i}^{}1_{x_{i} \\neq y_{i}} $$ 代码实现：\ndef hamming(x, y): return np.sum(x != y) / len(x) 余弦相似度(Cosine Similarity) 余弦相似性通过测量两个向量的夹角的余弦值来度量它们之间的相似性。0度角的余弦值是1，而其他任何角度的余弦值都不大于1；并且其最小值是-1。从而两个向量之间的角度的余弦值确定两个向量是否大致指向相同的方向。两个向量有相同的指向时，余弦相似度的值为1；两个向量夹角为90°时，余弦相似度的值为0；两个向量指向完全相反的方向时，余弦相似度的值为-1。这结果是与向量的长度无关的，仅仅与向量的指向方向相关。余弦相似度通常用于正空间，因此给出的值为0到1之间。 二维空间为例，上图的$a$和$b$是两个向量，我们要计算它们的夹角θ。余弦定理告诉我们，可以用下面的公式求得：\n$$ \\cos\\theta = \\frac{a^{2} + b^{2} - c^{2}}{2ab} $$\n假定$a$向量是$\\left\\lbrack x_{1},y_{1} \\right\\rbrack$，$b$向量是$\\left\\lbrack x_{2},y_{2} \\right\\rbrack$，两个向量间的余弦值可以通过使用欧几里得点积公式求出：\n$$ \\cos\\left( \\theta \\right) = \\frac{A \\cdot B}{\\parallel A \\parallel \\parallel B \\parallel} = \\frac{\\sum_{i = 1}^{n}A_{i} \\times B_{i}}{\\sqrt{\\sum_{i = 1}^{n}(A_{i})^{2} \\times \\sqrt{\\sum_{i = 1}^{n}(B_{i})^{2}}}} $$\n$$ \\cos\\left( \\theta \\right) = \\frac{A \\cdot B}{\\parallel A \\parallel \\parallel B \\parallel} = \\frac{\\left( x_{1},y_{1} \\right) \\cdot \\left( x_{2},y_{2} \\right)}{\\sqrt{x_{1}^{2} + y_{1}^{2}} \\times \\sqrt{x_{2}^{2} + y_{2}^{2}}} = \\frac{x_{1}x_{2} + y_{1}y_{2}}{\\sqrt{x_{1}^{2} + y_{1}^{2}} \\times \\sqrt{x_{2}^{2} + y_{2}^{2}}} $$\n如果向量$a$和$b$不是二维而是$n$维，上述余弦的计算法仍然正确。假定$A$和$B$是两个$n$维向量，$A$是$\\left\\lbrack A_{1},A_{2},\\ldots,A_{n} \\right\\rbrack$，$B$是$\\left\\lbrack B_{1},B_{2},\\ldots,B_{n} \\right\\rbrack$，则$A$与$B$的夹角余弦等于：\n$$ \\cos\\left( \\theta \\right) = \\frac{A \\cdot B}{\\parallel A \\parallel \\parallel B \\parallel} = \\frac{\\sum_{i = 1}^{n}A_{i} \\times B_{i}}{\\sqrt{\\sum_{i = 1}^{n}(A_{i})^{2}} \\times \\sqrt{\\sum_{i = 1}^{n}(B_{i})^{2}}} $$ 代码实现：\nfrom math import * def square_rooted(x): return round(sqrt(sum([a*a for a in x])),3) def cosine_similarity(x, y): numerator = sum(a * b for a, b in zip(x, y)) denominator = square_rooted(x) * square_rooted(y) return round(numerator / float(denominator), 3) print(cosine_similarity([3, 45, 7, 2], [2, 54, 13, 15])) result:\n0.972 KNN算法 1．$k$近邻法是基本且简单的分类与回归方法。$k$近邻法的基本做法是：对给定的训练实例点和输入实例点，首先确定输入实例点的$k$个最近邻训练实例点，然后利用这$k$个训练实例点的类的多数来预测输入实例点的类。\n2．$k$近邻模型对应于基于训练数据集对特征空间的一个划分。$k$近邻法中，当训练集、距离度量、$k$值及分类决策规则确定后，其结果唯一确定。\n3．$k$近邻法三要素：距离度量、$k$值的选择和分类决策规则。常用的距离度量是欧氏距离。$k$值小时，$k$近邻模型更复杂；$k$值大时，$k$近邻模型更简单。$k$值的选择反映了对近似误差与估计误差之间的权衡，通常由交叉验证选择最优的$k$。\n常用的分类决策规则是多数表决，对应于经验风险最小化。\n4．$k$近邻法的实现需要考虑如何快速搜索k个最近邻点。kd树是一种便于对k维空间中的数据进行快速检索的数据结构。kd树是二叉树，表示对$k$维空间的一个划分，其每个结点对应于$k$维空间划分中的一个超矩形区域。利用kd树可以省去对大部分数据点的搜索， 从而减少搜索的计算量。\npython实现，遍历所有数据点，找出$n$个距离最近的点的分类情况，少数服从多数\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from collections import Counter 导入鸢尾花数据集\niris = load_iris() df = pd.DataFrame(iris.data, columns=iris.feature_names) df[\u0026#39;label\u0026#39;] = iris.target df.columns = [\u0026#39;sepal length\u0026#39;, \u0026#39;sepal width\u0026#39;, \u0026#39;petal length\u0026#39;, \u0026#39;petal width\u0026#39;, \u0026#39;label\u0026#39;] df.head() result: 选择长和宽的数据进行可视化\nplt.figure(figsize=(12, 8)) plt.scatter(df[:50][\u0026#39;sepal length\u0026#39;], df[:50][\u0026#39;sepal width\u0026#39;], label=\u0026#39;0\u0026#39;) plt.scatter(df[50:100][\u0026#39;sepal length\u0026#39;], df[50:100][\u0026#39;sepal width\u0026#39;], label=\u0026#39;1\u0026#39;) plt.xlabel(\u0026#39;sepal length\u0026#39;, fontsize=18) plt.ylabel(\u0026#39;sepal width\u0026#39;, fontsize=18) plt.legend() plt.show() result: Numpy实现 class KNN: def __init__(self, X_train, y_train, n_neighbors=3, p=2): \u0026#34;\u0026#34;\u0026#34; parameter: n_neighbors 临近点个数 parameter: p 距离度量 \u0026#34;\u0026#34;\u0026#34; self.n = n_neighbors self.p = p self.X_train = X_train self.y_train = y_train def predict(self, X): # 取出n个点 knn_list = [] for i in range(self.n): dist = np.linalg.norm(X - self.X_train[i], ord=self.p) knn_list.append((dist, self.y_train[i])) for i in range(self.n, len(self.X_train)): max_index = knn_list.index(max(knn_list, key=lambda x: x[0])) dist = np.linalg.norm(X - self.X_train[i], ord=self.p) if knn_list[max_index][0] \u0026gt; dist: knn_list[max_index] = (dist, self.y_train[i]) # 统计 knn = [k[-1] for k in knn_list] count_pairs = Counter(knn) # max_count = sorted(count_pairs, key=lambda x: x)[-1] max_count = sorted(count_pairs.items(), key=lambda x: x[1])[-1][0] return max_count def score(self, X_test, y_test): right_count = 0 n = 10 for X, y in zip(X_test, y_test): label = self.predict(X) if label == y: right_count += 1 return right_count / len(X_test) data = np.array(df.iloc[:150, [0, 1, -1]]) X, y = data[:,:-1], data[:,-1] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) clf = KNN(X_train, y_train) clf.score(X_test, y_test) result:\n0.8 code:\ntest_point = [6.0, 3.0] print(\u0026#39;Test Point: {}\u0026#39;.format(clf.predict(test_point))) result:\nTest Point: 2.0 Scikit-learn实例 sklearn.neighbors.KNeighborsClassifier n_neighbors: 临近点个数，即k的个数，默认是5\np: 距离度量，默认\nalgorithm: 近邻算法，可选{\u0026lsquo;auto\u0026rsquo;, \u0026lsquo;ball_tree\u0026rsquo;, \u0026lsquo;kd_tree\u0026rsquo;, \u0026lsquo;brute\u0026rsquo;}\nweights: 确定近邻的权重\nn_neighbors ： int，optional(default = 5) 默认情况下kneighbors查询使用的邻居数。就是k-NN的k的值，选取最近的k个点。\nweights ： str或callable，可选(默认=‘uniform’) 默认是uniform，参数可以是uniform、distance，也可以是用户自己定义的函数。uniform是均等的权重，就说所有的邻近点的权重都是相等的。distance是不均等的权重，距离近的点比距离远的点的影响大。用户自定义的函数，接收距离的数组，返回一组维数相同的权重。\nalgorithm ： {‘auto’，‘ball_tree’，‘kd_tree’，‘brute’}，可选 快速k近邻搜索算法，默认参数为auto，可以理解为算法自己决定合适的搜索算法。除此之外，用户也可以自己指定搜索算法ball_tree、kd_tree、brute方法进行搜索，brute是蛮力搜索，也就是线性扫描，当训练集很大时，计算非常耗时。kd_tree，构造kd树存储数据以便对其进行快速检索的树形数据结构，kd树也就是数据结构中的二叉树。以中值切分构造的树，每个结点是一个超矩形，在维数小于20时效率高。ball tree是为了克服kd树高纬失效而发明的，其构造过程是以质心C和半径r分割样本空间，每个节点是一个超球体。\nleaf_size ： int，optional(默认值= 30) 默认是30，这个是构造的kd树和ball树的大小。这个值的设置会影响树构建的速度和搜索速度，同样也影响着存储树所需的内存大小。需要根据问题的性质选择最优的大小。\np ： 整数，可选(默认= 2) 距离度量公式。在上小结，我们使用欧氏距离公式进行距离度量。除此之外，还有其他的度量方法，例如曼哈顿距离。这个参数默认为2，也就是默认使用欧式距离公式进行距离度量。也可以设置为1，使用曼哈顿距离公式进行距离度量。\nmetric ： 字符串或可调用，默认为’minkowski’ 用于距离度量，默认度量是minkowski，也就是p=2的欧氏距离(欧几里德度量)。\nmetric_params ： dict，optional(默认=None) 距离公式的其他关键参数，这个可以不管，使用默认的None即可。\nn_jobs ： int或None，可选(默认=None) 并行处理设置。默认为1，临近点搜索并行工作数。如果为-1，那么CPU的所有cores都用于并行工作。\nfrom sklearn.neighbors import KNeighborsClassifier 不同k(n_neighbors)值下的结果：\nclf_sk = KNeighborsClassifier(n_neighbors=3) clf_sk.fit(X_train, y_train) result:\nKNeighborsClassifier(n_neighbors=3) code:\nclf_sk.score(X_test, y_test) result:\n0.8 code:\nclf_sk = KNeighborsClassifier(n_neighbors=4) clf_sk.fit(X_train, y_train) clf_sk.score(X_test, y_test) result:\n0.7333333333333333 code:\nclf_sk = KNeighborsClassifier(n_neighbors=5) clf_sk.fit(X_train, y_train) clf_sk.score(X_test, y_test) result:\n0.7777777777777778 自动调参吧，试试循环，找到最优的k值\nbest_score = 0.0 best_k = -1 for k in range(1, 11): knn_clf = KNeighborsClassifier(n_neighbors=k) knn_clf.fit(X_train, y_train) score = knn_clf.score(X_test, y_test) if score \u0026gt; best_score: best_k = k best_score = score print(\u0026#34;best_k = \u0026#34; + str(best_k)) print(\u0026#34;best_score = \u0026#34; + str(best_score)) result:\nbest_k = 2 best_score = 0.8 KD树的划分和搜索 KD树 KD树(K-Dimension Tree)，，也可称之为$k$维树，可以用更高的效率来对空间进行划分，并且其结构非常适合寻找最近邻居和碰撞检测。KD树是一种便于对$k$维空间中的数据进行快速检索的数据结构。KD树是二叉树，表示对$k$维空间的一个划分，其每个结点对应于$k$维空间划分中的一个超矩形区域。利用KD树可以省去对大部分数据点的搜索，从而减少搜索的计算量。\nKD树是二叉树，表示对𝑘维空间的一个划分(partition)。构造KD树相当于不断地用垂直于坐标轴的超平面将𝑘维空间切分，构成一系列的$k$维超矩形区域。KD树的每个结点对应于一个$k$维超矩形区域。\n构造KD树的方法 构造根结点，使根结点对应于$k$维空间中包含所有实例点的超矩形区域；\n通过下面的递归方法，不断地对$k$维空间进行切分，生成子结点。\n在超矩形区域(结点)上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域(子结点)；\n这时，实例被分到两个子区域。这个过程直到子区域内没有实例时终止(终止时的结点为叶结点)。\n在此过程中，将实例保存在相应的结点上。\n通常，依次选择坐标轴对空间切分，选择训练实例点在选定坐标轴上的中位数(median)为切分点，这样得到的KD树是平衡的。\n注意，平衡的KD树搜索时的效率未必是最优的。\n对于构建过程，有两个优化点：\n选择切分维度 根据数据点在各维度上的分布情况，方差越大，分布越分散从方差大的维度开始切分，有较好的切分效果和平衡性。\n确定中值点 预先对原始数据点在所有维度进行一次排序，存储下来，然后在后续的中值选择中，无须每次都对其子集进行排序，提升了性能。也可以从原始数据点中随机选择固定数目的点，然后对其进行排序，每次从这些样本点中取中值，来作为分割超平面。该方式在实践中被证明可以取得很好性能及很好的平衡性。\nfrom collections import namedtuple from pprint import pformat,pprint #注意下这个命名元组 class Node(namedtuple(\u0026#39;Node\u0026#39;, \u0026#39;location left_child right_child\u0026#39;)): #str_用于为最终用户创建输出，而 _repr_ 主要用于调试和开发。 _repr_ 的目标是明确无误，_str_ 是可读的 def __repr__(self): return pformat(tuple(self)) # kd-tree每个结点中主要包含的数据结构如下 class KdNode(object): def __init__(self, dom_elt, split, left, right): self.dom_elt = dom_elt # k维向量节点(k维空间中的一个样本点) self.split = split # 整数（进行分割维度的序号） self.left = left # 该结点分割超平面左子空间构成的kd-tree self.right = right # 该结点分割超平面右子空间构成的kd-tree # print(\u0026#34;这是dom_elt:\u0026#34;,dom_elt) # print(\u0026#34;这是split:\u0026#34;,split) # print(\u0026#34;这是left:\u0026#34;,left) # print(\u0026#34;这是right:\u0026#34;,right) class KdTreeCreate(object): def __init__(self, data): k = len(data[0]) # 数据维度 其实就是为了获取特征数 n=0 #split是分隔维度的序号 就相当于计算出哪个特征(维度)的方差大得到这个特征所在的列序号，就这根据这个特征的大小划分数据 def CreateNode(split, data_set): # 按第split维划分数据集exset创建KdNode # nonlocal n # n+=1 if not data_set: # 数据集为空 return None # return print(\u0026#34;--------这是第{}次循环-----\u0026#34;.format(n)) # key参数的值为一个函数，此函数只有一个参数且返回一个值用来进行比较 # operator模块提供的itemgetter函数用于获取对象的哪些维的数据，参数为需要获取的数据在对象中的序号 #data_set.sort(key=itemgetter(split)) # 这里就是将dataset进行排序 排序规则按照前面生成的split列来划分数据，从小到大 data_set.sort(key=lambda x: x[split]) split_pos = len(data_set) // 2 # //除法取整 split_pos因为前面已经排序了，所以这里就相当于确定中间值了即节点 # print(data_set) # print(\u0026#34;index:\u0026#34;,split_pos) #nonlocal解决屏蔽向量 # print(\u0026#34;--------这是第{}次循环-----\u0026#34;.format(n)) #注意这里程序的index是从0开始的，数学上是从1开始的，所以前面取整是合理的。 median = data_set[split_pos] # 中位数分割点 6 = data_set[3] split_next = (split + 1) % k # cycle coordinates 循环坐标 感觉这里是针对数据的 这里有点没看懂？ # 比如第一次： split=0 （0+1）%k(2) = 1 # 第二次： split_next=1 (1+1)%k(2) = 0 # 第三次： split_next=0 .....=1 # 递归的创建kd树 return KdNode( median, split, #递归 #不是很清楚上面的那个split_next是怎么一回事 CreateNode(split_next, data_set[:split_pos]), # 创建左子树 CreateNode(split_next, data_set[split_pos + 1:])) # 创建右子树 #这样看来的话好像就是默认第一次时的第一个特征的方差最大 self.root = CreateNode(0, data) # 从第0维分量开始构建kd树,返回根节点 # KDTree的前序遍历 def preorder(root): print(root.dom_elt) if root.left: # 节点不为空 preorder(root.left) if root.right: preorder(root.right) import numpy as np data_md = [(2,3), (5,7), (9,6), (4,5), (6,4), (7,2) ] data_mdL1 = np.array([(2,3), (5,7),(4,5)]) data_mdR1 = np.array([(9,6),(7,2)]) data_md_np = np.array(data_md) print(data_md_np) print(data_mdR1.var(axis=0)) result: #这里就为什么要用到namedtuple的原因了，这样分类的x[0]才能取到第一列 如果是[[7,2],[3,6]]的话就是第一个元素的了[7,2] X = [(7,2),(3,6)] print(sorted(X,key=lambda x:x[0])) result:\n[(3, 6), (7, 2)] 确定中值点 预先对原始数据点在所有维度进行一次排序，存储下来，然后在后续的中值选择中，无须每次都对其子集进行排序，提升了性能。也可以从原始数据点中随机选择固定数目的点，然后对其进行排序，每次从这些样本点中取中值，来作为分割超平面。该方式在实践中被证明可以取得很好性能及很好的平衡性。\n#好好根据这段话理解下面的例子结果 kd_create_test = KdTreeCreate(data_md ) #用md的例子看看先 #第一次循环是: 执行预先对原始数据点在所有维度（其实就是第一个维度）进行一次排序 同时也取到了中值的index， #第二次循环是: 因为第一次循环知道了中值index，那么就可以符合KD树创建来划分了，这次先取出小于第一次循环得出的中值 #第三次循环是: 同理因为第二次循环知道了上一小组的index 这次先取出小于第二次循环得出的中值，即构建了（2，3）这个最左下的点 #第四次循环是： 空 本来过程也是同二三的，但是发现第三次取出后，后面没有数据了所以就返回这次空 #第五次 空 #第六次 ：返回最左下的节点（2，3） 同时因为第二次循环中得出这一次的中值划分，那么这次就取右边的点即[5,7] #第七次 空 #第八次 空 连续两个空代表这个节点下已经没有左右点了 #第九次 ：因为第一次中我们知道了中指，那么上面的循环取中值左边的点，那么这次我们开始划分第一次中值右边的点 同时返回最下面的第二个节点的信息 # 并且第二次循环的数已经没了，就可以返回节点的信息了 # 同时判断下剩下中值记得先看看那个维度的方差大 #第十次 ：取出一个节点，后面的过程和前面是类似的 result: # 对构建好的kd树进行搜索，寻找与目标点最近的样本点： from math import sqrt from collections import namedtuple # 定义一个namedtuple,分别存放最近坐标点nearest_point、最近距离、和访问过的节点数nodes_visited result = namedtuple(\u0026#34;Result_tuple\u0026#34;, \u0026#34;nearest_point nearest_dist nodes_visited\u0026#34;) def find_nearest(tree, point): #以为point(4,4)为例子 k = len(point) # 数据维度即数据特征数 def travel(kd_node, target, max_dist): if kd_node is None: return result([0] * k, float(\u0026#34;inf\u0026#34;), 0) # python中用float(\u0026#34;inf\u0026#34;)和float(\u0026#34;-inf\u0026#34;)表示正负无穷 nodes_visited = 1 s = kd_node.split # 进行分割的维度 pivot = kd_node.dom_elt # 进行分割的“轴” if target[s] \u0026lt;= pivot[s]: # 如果目标点第s维小于分割轴的对应值(目标离左子树更近) nearer_node = kd_node.left # 下一个访问节点为左子树根节点 further_node = kd_node.right # 同时记录下右子树 else: # 目标离右子树更近 nearer_node = kd_node.right # 下一个访问节点为右子树根节点 further_node = kd_node.left temp1 = travel(nearer_node, target, max_dist) # 进行遍历找到包含目标点的区域 nearest = temp1.nearest_point # 以此叶结点作为“当前最近点” dist = temp1.nearest_dist # 更新最近距离 nodes_visited += temp1.nodes_visited if dist \u0026lt; max_dist: max_dist = dist # 最近点将在以目标点为球心，max_dist为半径的超球体内 temp_dist = abs(pivot[s] - target[s]) # 第s维上目标点与分割超平面的距离 if max_dist \u0026lt; temp_dist: # 判断超球体是否与超平面相交 return result(nearest, dist, nodes_visited) # 不相交则可以直接返回，不用继续判断 #---------------------------------------------------------------------- # 计算目标点与分割点的欧氏距离 temp_dist = sqrt(sum((p1 - p2)**2 for p1, p2 in zip(pivot, target))) if temp_dist \u0026lt; dist: # 如果“更近” nearest = pivot # 更新最近点 dist = temp_dist # 更新最近距离 max_dist = dist # 更新超球体半径 # 检查另一个子结点对应的区域是否有更近的点 temp2 = travel(further_node, target, max_dist) nodes_visited += temp2.nodes_visited if temp2.nearest_dist \u0026lt; dist: # 如果另一个子结点内存在更近距离 nearest = temp2.nearest_point # 更新最近点 dist = temp2.nearest_dist # 更新最近距离 return result(nearest, dist, nodes_visited) return travel(tree.root, point, float(\u0026#34;inf\u0026#34;)) # 从根节点开始递归 #先用我们自己举的例子来看看 data_md = [(2,3), (5,7), (9,6), (4,5), (6,4), (7,2) ] kd_create_test = KdTreeCreate(data_md ) kd_create_test_result = find_nearest(kd_create_test,[4,4]) print(kd_create_test_result) #结果没问题 result:\nResult_tuple(nearest_point=(4, 5), nearest_dist=1.0, nodes_visited=4) code:\nfrom time import process_time from random import random # 产生一个k维随机向量，每维分量值在0~1之间 def random_point(k): return [random() for _ in range(k)] # 产生n个k维随机向量 def random_points(k, n): return [random_point(k) for _ in range(n)] N = 400000 t0 = process_time() kd2 = KdTreeCreate(random_points(3, N)) # 构建包含四十万个3维空间样本点的kd树 ret2 = find_nearest(kd2, [0.1, 0.5, 0.8]) # 四十万个样本点中寻找离目标最近的点 t1 = process_time() print(\u0026#34;time: \u0026#34;, t1 - t0, \u0026#34;s\u0026#34;) print(ret2) result: KD树的绘图代码 from operator import itemgetter def kdtree(point_list, depth=0): if len(point_list) == 0: return None # 选择“基于深度的轴”，以便轴在所有有效值之间循环 # 只支持二维 axis = depth % 2 # Sort point list and choose median as pivot element point_list.sort(key=itemgetter(axis)) median = len(point_list) // 2 # 选择中值点 # 创建节点并构造子树 return Node( location = point_list[median], left_child = kdtree(point_list[:median], depth + 1), right_child = kdtree(point_list[median + 1:], depth + 1) ) import matplotlib.pyplot as plt # KD树的线宽 line_width = [4., 3.5, 3., 2.5, 2., 1.5, 1., .5, 0.3] def plot_tree(tree, min_x, max_x, min_y, max_y, prev_node, branch, depth=0): \u0026#34;\u0026#34;\u0026#34; plot K-D tree :param tree input tree to be plotted :param min_x :param max_x :param min_y :param max_y :param prev_node parent\u0026#39;s node :param branch True if left, False if right :param depth tree\u0026#39;s depth :return tree node \u0026#34;\u0026#34;\u0026#34; cur_node = tree.location # 当前树节点 left_branch = tree.left_child # 左分支 right_branch = tree.right_child # 右分支 #根据树的深度设置线条的宽度 if depth \u0026gt; len(line_width) - 1: ln_width = line_width[len(line_width) - 1] else: ln_width = line_width[depth] k = len(cur_node) axis = depth % k # 画垂直分割线 if axis == 0: if branch is not None and prev_node is not None: if branch: max_y = prev_node[1] else: min_y = prev_node[1] plt.plot([cur_node[0], cur_node[0]], [min_y, max_y], linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;red\u0026#39;, linewidth=ln_width) # 画水平分割线 elif axis == 1: if branch is not None and prev_node is not None: if branch: max_x = prev_node[0] else: min_x = prev_node[0] plt.plot([min_x, max_x], [cur_node[1], cur_node[1]], linestyle=\u0026#39;-\u0026#39;, color=\u0026#39;blue\u0026#39;, linewidth=ln_width) # 画当前节点 plt.plot(cur_node[0], cur_node[1], \u0026#39;ko\u0026#39;) # 绘制当前节点的左分支和右分支 if left_branch is not None: plot_tree(left_branch, min_x, max_x, min_y, max_y, cur_node, True, depth + 1) if right_branch is not None: plot_tree(right_branch, min_x, max_x, min_y, max_y, cur_node, False, depth + 1) def create_diagram(tree, width, height, min_val, max_val, delta): plt.figure(\u0026#34;Kd Tree\u0026#34;, figsize=(width, height)) plt.axis( [min_val - delta, max_val + delta, min_val - delta, max_val + delta]) plt.grid(b=True, which=\u0026#39;major\u0026#39;, color=\u0026#39;0.75\u0026#39;, linestyle=\u0026#39;--\u0026#39;) plt.xticks([i for i in range(min_val - delta, max_val + delta, 1)]) plt.yticks([i for i in range(min_val - delta, max_val + delta, 1)]) # 画出树 plot_tree(tree, min_val - delta, max_val + delta, min_val - delta, max_val + delta, None, None) plt.title(\u0026#39;KD Tree\u0026#39;) def label_nodes(node, i): loc = node.location plt.text(loc[0] + 0.15, loc[1] + 0.15, str(i), fontsize=10) if node.left_child: i = label_nodes(node.left_child, i + 1) if node.right_child: i = label_nodes(node.right_child, i + 1) return i def draw_target(point, radius): plt.plot(point[0], point[1], marker=\u0026#39;o\u0026#39;, color=\u0026#39;#ff007f\u0026#39;) circle = plt.Circle(point, 0.3, facecolor=\u0026#39;#ff007f\u0026#39;, edgecolor=\u0026#39;#ff007f\u0026#39;, alpha=0.5) plt.gca().add_patch(circle) # 围绕目标点绘制超球体 circle = plt.Circle(point, radius, facecolor=\u0026#39;#ffd83d\u0026#39;, edgecolor=\u0026#39;#ffd83d\u0026#39;, alpha=0.5) plt.gca().add_patch(circle) def draw_neighbors(point_list): for point in point_list: # 画出找到的最近的邻居 plt.plot(point[0], point[1], \u0026#39;go\u0026#39;) circle = plt.Circle(point, 0.3, facecolor=\u0026#39;#33cc00\u0026#39;, edgecolor=\u0026#39;#33cc00\u0026#39;, alpha=0.5) plt.gca().add_patch(circle) from graphviz import Digraph def add_node(dot, node, parent_id=None, i=0, edge_label=\u0026#39;\u0026#39;): loc = node.location node_id = str(i) dot.node(node_id, f\u0026#34;{i}\\n({loc[0]},{loc[1]})\u0026#34;) if parent_id: dot.edge(parent_id, node_id, label=edge_label) if node.left_child: i = add_node(dot, node.left_child, node_id, i + 1, \u0026#39;l\u0026#39;) if node.right_child: i = add_node(dot, node.right_child, node_id, i + 1, \u0026#39;r\u0026#39;) return i def create_graph(tree): dot = Digraph(comment=\u0026#39;Kd-tree\u0026#39;) dot.attr(\u0026#39;node\u0026#39;, fontsize=\u0026#39;20\u0026#39;, shape=\u0026#39;circle\u0026#39;, width=\u0026#39;1\u0026#39;, fixedsize=\u0026#39;true\u0026#39;) dot.attr(\u0026#39;edge\u0026#39;, arrowsize=\u0026#39;0.7\u0026#39;) add_node(dot, tree) return dot # point_list = [[2,3],[5,7],[9,6],[4,5],[6,4],[7,2]] point_list1 = [(2,3),(5,7),(9,6),(4,5),(6,4),(7,2)] tree = kdtree(point_list1) print(tree) create_graph(tree) result: max_int = 10000000 min_int = -max_int - 1 max_float = float(\u0026#39;inf\u0026#39;) def get_val_range(point_list): min_val = max_int max_val = -max_int - 1 for point in point_list: min_v = min(point) if min_v \u0026lt; min_val: min_val = min_v max_v = max(point) if max_v \u0026gt; max_val: max_val = max_v return (min_val, max_val) min_val, max_val=get_val_range(point_list1) create_diagram(tree, 8., 8., min_val, max_val, 1) label_nodes(tree, 0) plt.show() 参考 Prof. Andrew Ng. Machine Learning. Stanford University 李航，《统计学习方法》 ","date":"2021-09-14T00:00:00Z","permalink":"https://example.com/p/wzu_knn%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/","title":"WZU_KNN代码学习记录"},{"content":" 这是刘建平Pinard老师博客上KNN的例子，略做了修改,https://www.cnblogs.com/nolonely/p/6980160.html\n%matplotlib inline import numpy as np import pandas as pd import matplotlib.pyplot as plt # from sklearn.datasets.samples_generator import make_classification from sklearn.datasets._samples_generator import make_classification 这里再讲下sklearn.datasets._sample_generator(旧写法sklearn.datasets.sample_generator) 是用来生成数据集的：可以用来分类任务，可以用来回归任务，可以用来聚类任务，用于流形学习的，用于因子分解任务的,用于分类任务和聚类任务的：这些函数产生样本特征向量矩阵以及对应的类别标签集合\nmake_blobs：多类单标签数据集，为每个类分配一个或多个正太分布的点集\nmake_classification：多类单标签数据集，为每个类分配一个或多个正太分布的点集，提供了为数据添加噪声的方式，包括维度相关性，无效特征以及冗余特征等\nmake_gaussian-quantiles：将一个单高斯分布的点集划分为两个数量均等的点集，作为两类\nmake_hastie-10-2：产生一个相似的二元分类数据集，有10个维度\nmake_circle和make_moom产生二维二元分类数据集来测试某些算法的性能，可以为数据集添加噪声，可以为二元分类器产生一些球形判决界面的数据,X为样本特征，Y为样本类别输出， 共1000个样本，每个样本2个特征，输出有3个类别，没有冗余特征，每个类别一个簇\ncode:\n#n_samples 样本数 n_features特征数 n_classes样本y即类别数 n_clusters_per_class 每个类别的簇数 (质心) 暂时没搞懂这个簇数有什么影响 X, Y = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_clusters_per_class=1, n_classes=3) X[:10] result: code:\nY[:10] result:\narray([1, 0, 1, 2, 1, 1, 2, 1, 2, 0]) code:\nplt.scatter(X[:, 0], X[:, 1], marker=\u0026#39;o\u0026#39;, c=Y) #参数c就是color，赋值为可迭代参数对象，长度与x，y相同，根据值的不同使得（x,y）参数对表现为不同的颜色。 # 简单地说，按x,y值其中某一个值来区分颜色就好，比如上边想按照y值来区分，所以直接c=y就可以了， #这里就是根据类取划分颜色 plt.show() result: 接着我们用KNN来拟合模型，我们选择K=15，权重为距离远近 from sklearn import neighbors clf = neighbors.KNeighborsClassifier(n_neighbors=15,weights=\u0026#34;distance\u0026#34;) clf.fit(X,Y) result:\nKNeighborsClassifier(n_neighbors=15, weights=\u0026#39;distance\u0026#39;) code:\nfrom matplotlib.colors import ListedColormap cmap_light = ListedColormap([\u0026#39;#FFAAAA\u0026#39;, \u0026#39;#AAFFAA\u0026#39;, \u0026#39;#AAAAFF\u0026#39;]) cmap_bold = ListedColormap([\u0026#39;#FF0000\u0026#39;, \u0026#39;#00FF00\u0026#39;, \u0026#39;#0000FF\u0026#39;]) #确认训练集的边界 x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 print(x_min, x_max,y_min, y_max) #生成随机数据来做测试集，然后作预测 xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02), np.arange(y_min, y_max, 0.02)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) # 画出测试集数据 Z = Z.reshape(xx.shape) plt.figure() # plt.pcolormesh的作用在于能够直观表现出分类边界 #传入x和y轴的刻度采样点xx和yy;Z为要划分分解线的数据（包含x和y的数据），camp中的颜色要跟数据的类别数量对应 plt.pcolormesh(xx, yy, Z, cmap=cmap_light) # 也画出所有的训练集数据 plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=cmap_bold) plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.title(\u0026#34;3-Class classification (k = 15, weights = \u0026#39;distance\u0026#39;)\u0026#34; ) result: 看plt.pcolormesh生成的分类边界，可以看到大多数数据拟合不错，仅有少量的异常点不在范围内。\n","date":"2021-09-14T00:00:00Z","permalink":"https://example.com/p/%E5%88%98%E5%BB%BA%E5%B9%B3%E8%80%81%E5%B8%88pinard%E5%8D%9A%E5%AE%A2%E7%9A%84knn%E7%AE%97%E6%B3%95%E4%BE%8B%E5%AD%90/","title":"刘建平老师Pinard博客的KNN算法例子"},{"content":" 这里是用sklearn的KDtree来实现WZU对应的纯手写的那一部分，因为纯手写太麻烦了，不过里面提到的排序的思路值得一学！！ 顺便说一下，WZU的KNN的那个KD绘图，我还没看\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt from sklearn import neighbors #sklearn中的knn是有kd树和限定半径近邻，我们这里用的是kd树 from matplotlib.colors import ListedColormap#方便可视化时，使得相同的类颜色一致 在这次数据中没有意义了 import random from time import process_time#获取当前的时间 import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = [\u0026#34;SimHei\u0026#34;] plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False 在次之前先让我们看看md中数据来熟悉sklearn中KDtree的使用\nfrom sklearn import neighbors data_md = [(2,3), (5,7), (9,6), (4,5), (6,4), (7,2) ] data_md_tree = neighbors.KDTree(data_md) #Get data and node arrays. data_md_tree.get_arrays() #Arrays for storing tree data, index, node data and node bounds. result: code:\ndata_md_tree.query([(4,4)])#论上是与(4,5)最近的 result:\n(array([[1.]]), array([[3]], dtype=int64)) code:\ndata_md_tree.get_n_calls() result:\n6 code:\ndata_md_tree.get_tree_stats() result:\n(0, 1, 0) 数据 #这里随机创造一组数据 # 产生一个k维(列)随机向量，每维分量值在0~1之间 def random_point(k): return [random.random() for _ in range(k)] # 产生n(行)个k维随机向量 def random_points(k,n): return [random_point(k) for _ in range(n)] dataset = random_points(3,400000) dataset = pd.DataFrame(dataset) dataset.head() result: code:\ndataset.describe() result: code:\ndataset.isnull().sum() result: code:\ndataset.shape result:\n(400000, 3) KNN算法模型 from sklearn import neighbors #创建KD树 #改变leaf_size不会改变查询结果，但是会显著影响查询速度（其实应该也包含训练速度吧）和存储内存。 KD_Tree_model = neighbors.KDTree(np.array(dataset),leaf_size=40) KD_Tree_model.get_arrays() result: code:\nKD_Tree_model.query([[0.1,0.5,0.8]]) result:\n(array([[0.01414242]]), array([[39321]], dtype=int64)) code:\nKD_Tree_model.get_tree_stats() result:\n(25, 3, 27) code:\ntree.get_tree_stats() result:\n(0, 4, 3) ","date":"2021-09-12T00:00:00Z","permalink":"https://example.com/p/my_knn_code/","title":"my_KNN_code"},{"content":" copy过来的，略作修改\n1．朴素贝叶斯法是典型的生成学习方法。生成方法由训练数据学习联合概率分布 $P(X,Y)$，然后求得后验概率分布$P(Y|X)$。具体来说，利用训练数据学习$P(X|Y)$和$P(Y)$的估计，得到联合概率分布：\n$$P(X,Y)＝P(Y)P(X|Y)$$\n概率估计方法可以是极大似然估计或贝叶斯估计。\n2．朴素贝叶斯法的基本假设是条件独立性，\n$$\\begin{aligned} P(X\u0026amp;=x | Y=c_{k} )=P\\left(X^{(1)}=x^{(1)}, \\cdots, X^{(n)}=x^{(n)} | Y=c_{k}\\right) \\ \u0026amp;=\\prod_{j=1}^{n} P\\left(X^{(j)}=x^{(j)} | Y=c_{k}\\right) \\end{aligned}$$\n这是一个较强的假设。由于这一假设，模型包含的条件概率的数量大为减少，朴素贝叶斯法的学习与预测大为简化。因而朴素贝叶斯法高效，且易于实现。其缺点是分类的性能不一定很高。\n3．朴素贝叶斯法利用贝叶斯定理与学到的联合概率模型进行分类预测。\n$$P(Y | X)=\\frac{P(X, Y)}{P(X)}=\\frac{P(Y) P(X | Y)}{\\sum_{Y} P(Y) P(X | Y)}$$\n将输入$x$分到后验概率最大的类$y$。\n$$y=\\arg \\max {c{k}} P\\left(Y=c_{k}\\right) \\prod_{j=1}^{n} P\\left(X_{j}=x^{(j)} | Y=c_{k}\\right)$$\n后验概率最大等价于0-1损失函数时的期望风险最小化。（可能会用到拉普拉斯平滑）\n模型：\n高斯模型 多项式模型 伯努利模型 自定义一组数据用来看看 import numpy as np import pandas as pd import math from collections import Counter #这里用的是sklearn上自带的数据集Iris 鸢尾花 #https://www.cnblogs.com/nolonely/p/6980160.html #http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html from sklearn.datasets import load_iris #从model_selection模块中导入train_test_split划分数据用 from sklearn.model_selection import train_test_split 先来初步了解上这个dataset https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html\n这是data即数据集的数据样本不包含结果和列名\ndata{ndarray, dataframe} of shape (150, 4)\nThe data matrix. If as_frame=True, data will be a pandas DataFrame.\n这是数据列名,即特征名\nfeature_names: list\nThe names of the dataset columns.\n这是数据的结果名称，即出现了什么结果，是一个集合的概念\ntarget_names: list\nThe names of target classes.\n这是每一个样本的结果\ntarget: {ndarray, Series} of shape (150,)\nThe classification target. If as_frame=True, target will be a pandas Series.\niris = load_iris()#加载iris对象 #用pd.DataFrame()创建一个dataframe的对象，参数有data，index，columns等 df = pd.DataFrame(iris.data,columns=iris.feature_names) df.head() result: 可以看出数据中没有标签结果即分类的结果，所以要加上去看看\ndf[\u0026#39;label\u0026#39;] = iris.target df[1:100] result: 再仔细观察可以发现列名有个单位，把他去掉先\n#重命名列名 df.columns = [\u0026#39;sepal length\u0026#39;,\u0026#39;sepal width\u0026#39;,\u0026#39;petal length\u0026#39;,\u0026#39;petal width\u0026#39;,\u0026#39;label\u0026#39;] df.head() result: code:\ndf.isnull().sum() result: 把dataframe转为np数据，方便后面训练\ndataset = np.array(df.iloc[:100,:]) dataset.shape result:\n(100, 5) 划分数据集 X = dataset[:,:4]#dataset[:,:-1] Y = dataset[:,4] X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.4,random_state=42) #train_test_split依次返回X训练，X测试，Y训练，Y测试的结果 X_train[0],Y_train[0] result:\n(array([5. , 3.3, 1.4, 0.2]), 0.0) 算法 在徒手写之前先用sklearn实现一遍，因为比较简单\n#导入NB模型 高斯模型 伯努利模型 多项式模型 from sklearn.naive_bayes import GaussianNB,BernoulliNB,MultinomialNB #加载模型 G_model = GaussianNB() B_model = BernoulliNB() M_model = MultinomialNB() #训练数据 G_model.fit(X_train,Y_train) B_model.fit(X_train,Y_train) M_model.fit(X_train,Y_train) result:\nMultinomialNB() code:\n#查看score，其实我这里还没搞清sklearn中的score到底是什么 #用测试集测试score print(G_model.score(X_test,Y_test)) print(B_model.score(X_test,Y_test)) print(M_model.score(X_test,Y_test)) result：\n1.0 0.425 1.0 code:\n#预测一下 #随便输入一个样本 print(G_model.predict([[3.1, 4.4, 2.1,0.3]])) print(B_model.predict([[3.1, 4.4, 2.1,0.3]])) print(M_model.predict([[3.1, 4.4, 2.1,0.3]])) result:\n[0.] [1.] [0.] 在sklearn中可以看出Gaussian和Multinomial的准确性是一致的，Burnoulli的准确性低,注意了是针对iris这一数据而言的\n徒手写实现一遍 参考：https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/\nGaussianNB 高斯朴素贝叶斯 特征的可能性被假设为高斯\n概率密度函数： $$P(x_i | y_k)=\\frac{1}{\\sqrt{2\\pi\\sigma^2_{yk}}}exp(-\\frac{(x_i-\\mu_{yk})^2}{2\\sigma^2_{yk}})$$\n数学期望(mean)：$\\mu$\n方差：$\\sigma^2=\\frac{\\sum(X-\\mu)^2}{N}$\n实际上是计算每个数据处于不同类别中的概率，取大的\n#封装一个类 #注意下类的继承性 class NaiveBayes: #这一步可以将其理解为固定的init自身写法 def __init__(self): print(\u0026#34;NB\u0026#34;) self.model = None #后面的方法中都要记得传入self #mean数学期望均值 def mean(self,x): mean_result = sum(x) / float(len(x)) return mean_result #方差 def stdev(self,x): #这里self可以理解为因为要在类中调用类中已经定义了的方法，所以要使用self引出 average = self.mean(x) #pow(i-average,2) for i in x]注意下列表推导式的运用 stdev_result = math.sqrt(sum([math.pow(i-average,2) for i in x]) / float(len(x))) return stdev_result #概率密度函数 def Gaussian_pro(self,x,mean,stdev): #这里的x是X_test或者要被预测的数据 #exponent exp_part = math.exp(-(math.pow(x-mean,2) / (2*math.pow(stdev,2)))) Gaussian_result = exp_part / (math.sqrt(2*math.pi*math.pow(stdev,2))) return Gaussian_result #处理训练样本,获取每个样本数据的mean和stdev，回头看下概率密度函数，就知道了 #https://www.cnblogs.com/sddai/p/14303453.html #理解下参数前面的*号，其解包作用即将list/turple中的元素逐一取出；字典的话用** #这个summarize到底有什么用啊？？？ #我知道这个summarize的用处了，看下面fit函数就明白了，它是用来返回每个类别的mean和stdev的 def summarize(self,X_train): summarize_result = [(self.mean(i), self.stdev(i)) for i in zip(*X_train)] #返回每个数据的mean和stdev return summarize_result #分类分别求出训练样本的数学期望和标准差 def fit(self,X,Y): #先看看标签的类别 labels = list(set(Y)) #把相同类别的数据放在一起，用字典 data = {label: [] for label in labels} for i,label in zip(X,Y): data[label].append(i) self.model = { #dictionary.items返回可遍历的(键, 值) 元组数组 label: self.summarize(value) for label,value in data.items() } #print(self.model) #这里self.model的结果就是每个类别下的X的均值和方差了 return \u0026#34;训练结束啦\u0026#34; #计算概率 def calculate_pro(self,input_data): #首先清楚一点:预测数据的类别，就是看它在哪种类别下的概率大 pro = {} #这个for间接体现出了用class的好处 for label,value in self.model.items(): #初始化每个类别的概率为1 pro[label]=1 #len(value)相当于判断有多个特征，因为要每个特征与自己对应的mean和stdev做运算 for i in range(len(value)): mean,stdev = value[i] #这里就相当于计算改类别下的特征概率了 注意独立性所以总概率变成相乘了 pro[label] *= self.Gaussian_pro(input_data[i],mean,stdev) return pro #预测类别 def predict(self,X_test): #后面的[]是根据实际数据读取的label label = sorted(self.calculate_pro(X_test).items(),key=lambda x:x[-1])[-1][0] return label #打分 def score(self,X_test,Y_test): right = 0 for X,Y in zip(X_test,Y_test): label = self.predict(X) if label == Y: right += 1 return right / float(len(X_test)) model_1 = NaiveBayes() result:\nNB model_1.fit(X_train,Y_train) result: code:\nmodel_1.predict([4.5,4.4,2.0,0.4]) result:\n0.0 code:\nmodel_1.score(X_test,Y_test) result:\n1.0 code:\ndata = { \u0026#39;a\u0026#39;:[1,2,3], \u0026#39;b\u0026#39;:[232,333] } for label,value in data.items(): print(label,value) result:\na [1, 2, 3] b [232, 333] 以下是关于sklearn结果输出的区别\n#https://www.cnblogs.com/pinard/p/6074222.html import numpy as np X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]]) Y = np.array([1, 1, 1, 2, 2, 2]) from sklearn.naive_bayes import GaussianNB clf = GaussianNB() #拟合数据 clf.fit(X, Y) print(\u0026#34;==Predict result by predict==\u0026#34;)#返回最佳的label print(clf.predict([[-0.8, -1]])) print(\u0026#34;==Predict result by predict_proba==\u0026#34;)#返回label的概率 print(clf.predict_proba([[-0.8, -1]])) print(\u0026#34;==Predict result by predict_log_proba==\u0026#34;)#将最终概率结果值对数转换 print(clf.predict_log_proba([[-0.8, -1]])) result: 总结 一般采用pd读取数据，后面训练时再转化为np格式 徒手写的model_1逻辑还是有点问题，回头再看看，感觉上是没有理解怎么bayes的原理 看3已解决 2022/1/05 看了这篇博客发现自己有个地方理解错了：https://www.cnblogs.com/pinard/p/6069267.html ","date":"2021-09-11T00:00:00Z","permalink":"https://example.com/p/mynbcode/","title":"myNBcode"},{"content":" 这是系统性学习一遍Linxu的笔记 看的教程：https://www.bilibili.com/video/BV1CQ4y127LQ?spm_id_from=333.999.0.0\nls 1 列的第一个字符可能是 d/l/- -表示这个文件是一个二进制文件 d表示是一个目录文件 l表示一个软连接文件；第二个字符到第九个字符代表权限\n2 列 表示文件的数量 （如目录的多个文件也会显示出来）\n3 列/4列 分别是文件的所有者/所有组 配合权限使用\n5 列是文件的大小 单位为btype（字节） ls -alh可以转换为M/kb的形式\n6 列 文件上一次被修改的时间 （当年的会具体到h）\n7 文件名 （软连接则指向真实路径）\n以.点开头的是隐藏文件，ls -a 可以看出\nls -al /cd /pwd/ls -alh Linux文件系统 bin ： 放的都是命令文件\nsbin： 放的是命令文件 权限等级高于bin\nboot : 存放的是启动系统所需的东西\ndev ： 存放Linux的设备文件\netc ： 存放配置/应用 文件\nhome： 存放用户的身份目录 root比较特殊单独成一个\nrun：系统启动和程序启动时运行产生的文件\nusr: 存放应用程序的文件\ntmp：存放临时文件 /dev/sda2 理解为window的D盘访问\ncp / mv cp cp -r 可以拷贝目录中的所有文件（目录） mv 用法和cp一样 touch / mkdir / rm touch 1112310630.35 ：11年12月10号6点30分35秒\nmkdir rm rm -rf * 删除所有文件\ncat cat -n file1 \u0026gt; file2 将file1内容以及行号到file2中 cat file | grep word 将file中包含word的行抓出来出来 （|是通道符） head / tail tail -f 是是实时更新的，可以ctrl+C退出\nmore history | more ： history是Linux中的命令，也就是命令的返回结果可以用more搭配阅读\nless vi 交互式是指 可以使用一些操作 vi命令扩展 复制粘贴时可以使用ctrl + v进入快速模式 权限修改 chmod / chown /chgrp 通配符 模糊搜索 find -mtime -3 表示三天内修改过的文件 -mtime 3 表示刚好三天的文件 -mtime +3 表示三天前修改过的文件\nfind / -name test.* 表示从全局查找test.开头的文件 locate man man 命令 可以查找这个命令的说明\nhelp 命令 \u0026ndash;help 可以查找这个命令的说明\nBash shell的快捷键 ctrl + a/e 可以快速跳到命令行首/尾 ctrl + u/k 从当前删除到命令开头/末尾 Esc + . 可以快速输入上一个命令中的参数 Tab 则是用来补全命令的 / 补充文件名也行 ctrl + r 搜索以前使用过的命令 方向键的上下 可以快速调用你使用过的命令\n重定向 / \u0026gt; ll.txt 相当于把ll.txt清空 echo 也可以重定向 网络命令\nping ping 域名 的话会先将域名解析为IP再ping （window下加个 -t才表示长ping） ttl：生成时间 随跳数增多减少 time：是响应时间 （36ms合理，超过200ms会有影响） icmp_seq： 若数值出现不连续，则出现丢包 rrt min/avg/max/mdev = 36.630/36.660/36.721/0.123 ms 代表最小响应时间/平均响应时间/最大/方差（越小网络情况越好） ifconfig route H主机路由 ip ip address （ip a / ip addr）功能等同于ifconfig 添加/删除路由 ip maddress （ip m）可以用来返回主机的MAC地址 ip配置持久化 Linux 下一切皆文件 添加辅组网卡eth1 vi ifcfg-eth1 I eth1是指定从eth1网卡进去\n执行：systemctl restart network\nnetstat l:指的是 正在监听的端口（服务） 可以用来查看服务是否有成功对外使用 n：则会将netstat识别的协议用数字形式显示 nslookup host dig traceroute telnet nmap 可以查看服务器对外开放的端口 \u0026ndash; top -port 10 : 扫描10个最常用的端口 -p 0-100: 扫描 0-100的端口状态 wget curl 注意是大写I\n软件安装、进程与服务\nrpm rpm安装会存在依懒包的问题\nyum yum会自动安装存在依懒包的问题 systemctl 进程 ps top kill pstree 防火墙 firewalld SELinux 硬件资源管理 lscpu 查看cpu规格信息 uptime 查看cpu负载均衡 lsmem 查看内存规格\nfree df /dev/sda1 挂载到了boot目录下 du fdisk 对新的磁盘进行分区以及格式化 mkfs 磁盘挂载持久化 磁盘管理 iostat iotop 硬件资源管小结 系统日志 rsyslog grep -v “^#” /etc/rsyslog.conf | grep -v \u0026ldquo;^$\u0026rdquo;\n-v 表示过滤 日志轮转 logrotate logger journalctl 通配符| 其实就是为了串起多个命令的作用\n","date":"2021-09-10T00:00:00Z","permalink":"https://example.com/p/linux%E5%AD%A6%E4%B9%A0%E5%AD%97%E5%85%B8/","title":"Linux学习字典"},{"content":" 这次来练习下逻辑回归,感觉过程和线性回归很类似，只是加了个sigmoid函数,dataset分别是ex2data1.txt / ex2data2.txt\ndataset1 在训练的初始阶段，我们将要构建一个逻辑回归模型来预测，某个学生是否被大学录取。设想你是大学相关部分的管理者，想通过申请学生两次测试的评分，来决定他们是否被录取。现在你拥有之前申请学生的可以用于训练逻辑回归的训练样本集。对于每一个训练样本，你有他们两次测试的评分和最后是被录取的结果。为了完成这个预测任务，我们准备构建一个可以基于两次测试评分来评估录取可能性的分类模型。上面的话是copy过来的\n分析数据 import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns sns.set_style(\u0026#39;white\u0026#39;) import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) #为了美观，当然是不影响结果的前提下 plt.rcParams[\u0026#39;font.sans-serif\u0026#39;]=[\u0026#39;SimHei\u0026#39;] #正常显示中文 plt.rcParams[\u0026#39;axes.unicode_minus\u0026#39;]=False #正常显示非负号 dataset1 = pd.read_csv(\u0026#39;./ex2data1.txt\u0026#39;,header=None,names=[\u0026#39;Exam1\u0026#39;,\u0026#39;Exam2\u0026#39;,\u0026#39;Admitted\u0026#39;]) dataset1.head() result: code:\ndataset1.describe() result: code:\ndataset1.shape result:\n(100, 3) code:\ndataset1.isnull().sum() result:\nExam1 0 Exam2 0 Admitted 0 dtype: int64 #可视化下数据 f,axes = plt.subplots(figsize=(9,9)) dataset1_corr = dataset1.corr() print(dataset1_corr) sns.heatmap(dataset1_corr,annot=True) plt.xticks(range(len(dataset1_corr.columns)),dataset1_corr.columns) plt.yticks(range(len(dataset1_corr.columns)),dataset1_corr.columns) plt.show() result: f,axes = plt.subplots(figsize=(12,8)) #最终纳取为1即正，0为未被录取 #录取和未录取的分开 yes_admitted = dataset1[dataset1[\u0026#39;Admitted\u0026#39;].isin([1])] no_admitted = dataset1[dataset1[\u0026#39;Admitted\u0026#39;].isin([0])] # axes.scatter(yes_admitted[\u0026#39;Exam1\u0026#39;], # yes_admitted[\u0026#39;Exam2\u0026#39;], # s=50, # c=\u0026#39;b\u0026#39;, # marker=\u0026#39;o\u0026#39;, # label=\u0026#39;Admitted\u0026#39;) # axes.scatter(no_admitted[\u0026#39;Exam1\u0026#39;], # no_admitted[\u0026#39;Exam2\u0026#39;], # s=50, # c=\u0026#39;r\u0026#39;, # marker=\u0026#39;x\u0026#39;, # label=\u0026#39;Not Admitted\u0026#39;) #被录取的数据 axes.scatter(yes_admitted[\u0026#39;Exam1\u0026#39;],yes_admitted[\u0026#39;Exam2\u0026#39;],c=\u0026#39;r\u0026#39;,marker=\u0026#39;o\u0026#39;,label=\u0026#39;Admitted\u0026#39;) #未被录取的 axes.scatter(no_admitted[\u0026#39;Exam1\u0026#39;],no_admitted[\u0026#39;Exam2\u0026#39;],c=\u0026#39;b\u0026#39;,marker=\u0026#39;x\u0026#39;,label=\u0026#39;unAdmitted\u0026#39;) axes.legend() axes.set_xlabel(\u0026#39;Exam1\u0026#39;) axes.set_ylabel(\u0026#39;Exam2\u0026#39;) plt.show() result: #可以看出是有一条比较理想的决策边界的 yes_admitted = dataset1[dataset1[\u0026#39;Admitted\u0026#39;].isin([1])] yes_admitted.head() result: 处理数据 dataset1.insert(0,\u0026#34;Ones\u0026#34;,1)#参数：第几列，新列名，新列值 #提取特征 X = np.array(dataset1.iloc[:,0:3].values)#记得由dataframe转为np数组 Y = np.array(dataset1.iloc[:,3:].values) #初始化权重) W = np.zeros(3) X.shape,Y.shape,W.shape # 注意下数据的维度，可以这样理解：因为W是要和X矩阵相乘的，所以行列符合a*b b*c 而X和Y是要相减的，所以符合行相等，而且Y必然只有一列 result:\n((100, 3), (100, 1), (3,)) 算法 采用sklearn看看先 from sklearn.linear_model import LogisticRegression slearn1_logis = LogisticRegression(penalty=\u0026#39;l2\u0026#39;,C=1.0)#加个l2正则化Redge，正则化系数为1.0 slearn1_logis.fit(X,Y) result:\nLogisticRegression() slearn1_logis.score(X,Y) result:\n0.89 接下来手写\nSigmoid 函数 $g$ 代表一个常用的逻辑函数（logistic function）为$S$形函数（Sigmoid function），公式为： $$g\\left( z \\right)=\\frac{1}{1+{{e}^{-z}}}$$ 合起来，我们得到逻辑回归模型的假设函数： $${{h}}\\left( x \\right)=\\frac{1}{1+{{e}^{-{{w }^{T}}x}}}$$ code:\ndef Sigmoid(x): return 1/(1+np.exp(-x)) #检查是否有定义错Sigmoid nums = np.arange(-10,10,step=1)#等差数列 f,axes = plt.subplots(figsize=(12,8)) axes.plot(nums,Sigmoid(nums),c=\u0026#39;b\u0026#39;) plt.show() result: 代价函数： $J\\left(w\\right)=-\\frac{1}{m}\\sum\\limits_{i=1}^{m}{({{y}^{(i)}}\\log \\left( {h}\\left( {{x}^{(i)}} \\right) \\right)+\\left( 1-{{y}^{(i)}} \\right)\\log \\left( 1-{h}\\left( {{x}^{(i)}} \\right) \\right))}$ code:\n#定义代价函数 def cost(W,X,Y): X=np.matrix(X)#记得转为np矩阵,方便后面使用multiply Y=np.matrix(Y) W=np.matrix(W) first_part = np.multiply(Y,np.log(Sigmoid(X*W.T))) second_part = np.multiply(1-Y,np.log(1-Sigmoid(X*W.T))) return -np.sum(first_part+second_part)/(len(X)) cost(W,X,Y) result:\n0.6931471805599453 gradient descent(梯度下降) 这是批量梯度下降（batch gradient descent） 转化为向量化计算： $\\frac{1}{m} X^T( Sigmoid(XW) - y )$ $$\\frac{\\partial J\\left( w \\right)}{\\partial {{w }{j}}}=\\frac{1}{m}\\sum\\limits{i=1}^{m}{({{h}}\\left( {{x}^{(i)}} \\right)-{{y}^{(i)}})x_{_{j}}^{(i)}}$$ W.ravel()#ravel变成一维即1*3 W.shape result:\n(3,) code:\n#定义BDG def BDG(W,X,Y): X=np.matrix(X)#记得转为np矩阵,方便后面使用multiply Y=np.matrix(Y) W=np.matrix(W) param = int(W.ravel().shape[1]) grad = np.zeros(param) error = Sigmoid(X*W.T)-Y for i in range(param): grad[i] = np.sum(np.multiply(error,X[:,i]))/len(X) return grad BDG(W,X,Y) result:\narray([ -0.1 , -12.00921659, -11.26284221]) 注意，我们实际上没有在这个函数中执行梯度下降，我们仅仅在计算一个梯度步长。在练习中， 一个称为“fminunc”的Octave函数是用来优化函数来计算成本和梯度参数。由于我们使用Python， 我们可以用SciPy的“optimize”命名空间来做同样的事情。\n#采用ciPy\u0026#39;s truncated newton（TNC）实现寻找最优参数。用这个时传入func和fprime的函数的w要在前面 import scipy.optimize as opt result = opt.fmin_tnc(func=cost,x0=W,fprime=BDG,args=(X,Y)) #要把 #func：优化的目标函数 # x0：初值 # fprime：提供优化函数func的梯度函数，不然优化函数func必须返回函数值和梯度，或者设置approx_grad=True # approx_grad :如果设置为True，会给出近似梯度 # args：元组，是传递给优化函数的参数 result result:\n(array([-25.16131863, 0.20623159, 0.20147149]), 36, 0) code:\nresult[0] result:\narray([-25.16131863, 0.20623159, 0.20147149]) code:\ncost(result[0],X,Y) result:\n0.20349770158947458 接下来，我们需要编写一个函数，用我们所学的参数w来为数据集X输出预测。然后，我们可以使用这个函数来给我们的分类器的训练精度打分。 逻辑回归模型的假设函数： $${{h}}\\left( x \\right)=\\frac{1}{1+{{e}^{-{{w }^{T}}X}}}$$ 当${{h}}$大于等于0.5时，预测 y=1\n当${{h}}$小于0.5时，预测 y=0 。\n#定义预测函数 def predict(W,X): probability = Sigmoid(X*W.T) return [1 if x \u0026gt;=0.5 else 0 for x in probability]#注意这种写法 #使用预测函数预测原视数据，评估准确性 w_new = np.matrix(result[0])#用函数前记得把数据变成矩阵 predictions = predict(w_new,X) correct = [ 1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a,b) in zip(predictions,Y) #zip函数会将不同行的数据装成新的一行，再换行，装下列 ] accuracy = (sum(map(int,correct)) % len(correct)) #map的语法https://www.runoob.com/python/python-func-map.html,第一个参数为方法，第二个为可迭代的对象 print(\u0026#34;accuracy:{0}%\u0026#34;.format(accuracy)) result:\naccuracy:89% 准确性达到了89，还不错的样子,和sklearn的结果一致\ndataset2： 在训练的第二部分，我们将要通过加入正则项提升逻辑回归算法。如果你对正则化有点眼生，或者喜欢这一节的方程的背景， 请参考在\u0026quot;exercises\u0026quot;文件夹中的\u0026quot;ex2.pdf\u0026quot;。简而言之，正则化是成本函数中的一个术语，它使算法更倾向于“更简单”的模型 （在这种情况下，模型将更小的系数）。这个理论助于减少过拟合，提高模型的泛化能力。这样，我们开始吧。 设想你是工厂的生产主管，你有一些芯片在两次测试中的测试结果。对于这两次测试，你想决定是否芯片要被接受或抛弃。 为了帮助你做出艰难的决定，你拥有过去芯片的测试数据集，从其中你可以构建一个逻辑回归模型。\n数据 dataset2 = pd.read_csv(\u0026#34;./ex2data2.txt\u0026#34;,header=None,names=[\u0026#39;TT1\u0026#39;,\u0026#39;TT2\u0026#39;,\u0026#39;Accepted\u0026#39;])#注意数据的第一行是不是标题，不是的话记得header=None，再赋予列名 dataset2.head() result: dataset2.describe() result: code:\ndataset2.isnull().sum() result: y_accept = dataset2[dataset2[\u0026#39;Accepted\u0026#39;].isin([1])] n_accept = dataset2[dataset2[\u0026#39;Accepted\u0026#39;].isin([0])] # y_accept.head f,axes = plt.subplots(figsize=(12,8)) axes.scatter(y_accept[\u0026#39;TT1\u0026#39;],y_accept[\u0026#39;TT2\u0026#39;],c=\u0026#39;r\u0026#39;,label=\u0026#39;Accepted\u0026#39;) axes.scatter(n_accept[\u0026#39;TT1\u0026#39;],n_accept[\u0026#39;TT2\u0026#39;],c=\u0026#39;b\u0026#39;,label=\u0026#39;UnAccepted\u0026#39;) axes.legend() axes.set_xlabel(\u0026#34;TT1\u0026#34;) axes.set_ylabel(\u0026#34;TT2\u0026#34;) plt.show() result: 可以看出决策边界也挺明显的，不过感觉上会出现过拟合的问题，所以加个L2正则化即Ridge 这个数据看起来可比前一次的复杂得多。特别地，你会注意到其中没有线性决策界限，来良好的分开两类数据。一个方法是用像逻辑回归这样的线性技术来构造从原始特征的多项式中得到的特征。可以考虑创建一组多项式特征入手。\n这里构造多项式特征当时没看懂\n其实就是：由上面那个图我们可以得出绝对没有办法通过线性函数去拟合，所以只能通过非线性去拟合，就要用到多项式特征化了\ncode:\n#假设degree=5看看 degree = 5 X2_1 = dataset2[\u0026#39;TT1\u0026#39;] X2_2 = dataset2[\u0026#39;TT2\u0026#39;] dataset2.insert(3,\u0026#39;One\u0026#39;,1) #多项式处理 for i in range (1,degree): for j in range(0,i): dataset2[\u0026#39;F\u0026#39; + str(i) + str(j)] = np.power(X2_1,i-j)*np.power(X2_2,j) #pandas的drop函数是删除数据，默认为axis=0，即删除行数据；axis=1删除指定的列数据 # inplace是否在原对象基础上进行修改;inplace = True：不创建新的对象，直接对原始对象进行修改； # inplace = False：对数据进行修改，创建并返回新的对象承载其修改结果。 dataset2.drop(\u0026#39;TT1\u0026#39;,axis=1,inplace=True) dataset2.drop(\u0026#39;TT2\u0026#39;,axis=1,inplace=True) dataset2.head() result: 算法修改\nregularized cost（正则化代价函数） $$J\\left( w \\right)=\\frac{1}{m}\\sum\\limits_{i=1}^{m}{[-{{y}^{(i)}}\\log \\left( {{h}}\\left( {{x}^{(i)}} \\right) \\right)-\\left( 1-{{y}^{(i)}} \\right)\\log \\left( 1-{{h}}\\left( {{x}^{(i)}} \\right) \\right)]}+\\frac{\\lambda }{2m}\\sum\\limits_{j=1}^{n}{w _{j}^{2}}$$\n# a是学习率 def COSTreg(W,X,Y,a): # 记得转为np矩阵,方便后面使用multiply W = np.matrix(W) X = np.matrix(X) Y = np.matrix(Y) first_part = np.multiply(-Y,np.log(Sigmoid(X*W.T))) second_part = np.multiply(-(1-Y),np.log(1-Sigmoid(X*W.T))) W_2 = np.power(W[:,1:W.shape[1]],2)#这里有点没弄明白为什么是1:W.shape[1]，不要第一列？还是说因为第一列是W0即偏差，单独拿出来了？ reg_part = (a/(2*len(X)))*np.sum(W_2) result = np.sum((first_part + second_part))/len(X) + reg_part return result 如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对${{w }_{0}}$ 进行正则化，所以梯度下降算法将分两种情形： 对上面的算法中 j=1,2,\u0026hellip;,n 时的更新式子进行调整可得： ${{w }{j}}:={{w }{j}}(1-a\\frac{\\lambda }{m})-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{({{h}{w }}\\left( {{x}^{(i)}} \\right)-{{y}^{(i)}})x{j}^{(i)}}$\n所以上面的代价函数是可以的\n在此之前，先看看np的ravel\narr = np.arange(12).reshape(3,4) arr result:\narray([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]]) code\narr_ravel = arr.ravel() arr_ravel result:\narray([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]) arr_ravel.shape result:\n(12,) arr_ravel.shape[0] result:\n12 其实就是将数组全部数据放在了同一行中，所以变成了一维的，比如(m,n) -\u0026gt; (1,m*n)\n如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对${{w }_{0}}$ 进行正则化，所以梯度下降算法将分两种情形： 对上面的算法中 j=1,2,\u0026hellip;,n 时的更新式子进行调整可得：\n${{w }{j}}:={{w }{j}}(1-a\\frac{\\lambda }{m})-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{({{h}{w }}\\left( {{x}^{(i)}} \\right)-{{y}^{(i)}})x{j}^{(i)}}$\n所以上面的代价函数是可以的\ndef BGDreg(W,X,Y,a): # 记得转为np矩阵,方便后面使用multiply W = np.matrix(W) X = np.matrix(X) Y = np.matrix(Y) param = int(W.ravel().shape[1])#这个是针对求和的那个x的 print(param) grad = np.zeros(param) error = Sigmoid(X*W.T)-Y #注意理解这个循环了，它是针对每个特征的w的 for i in range(param): term = np.multiply(error,X[:,i])#求和的那部分 if(i==0): grad[i] = np.sum(term) / len(X) else: grad[i] = (np.sum(term) / len(X)) + ((a/len(X))*W[:,i]) return grad 回顾下现在的dataset2长什么样\ndataset2我们前面多项式处理了\ndataset2.head() result: code:\n#初始化变量 columns = dataset2.shape[1]#获取特征数即行数 #特征分离 X2 = dataset2.iloc[:,1:columns] Y2 = dataset2.iloc[:,0:1] #转化为np数组 X2 = np.array(X2.values) Y2 = np.array(Y2.values) #初始化W2，考虑了One的 W2 = np.zeros(11) #初始化学习率 a = 1 现在，让我们尝试调用新的默认为0的$w$的正则化函数，以确保计算工作正常。\nCOSTreg(W2,X2,Y2,a) result:\n0.6931471805599454 BGDreg(W2,X2,Y2,a) result: 优化函数处理 采用ciPy\u0026rsquo;s truncated newton（TNC）实现寻找最优参数。用这个时传入func和fprime的函数的w要在前面\n要把\nfunc：优化的目标函数\nx0：初值\nfprime：提供优化函数func的梯度函数，不然优化函数func必须返回函数值和梯度，或者设置approx_grad=True\napprox_grad :如果设置为True，会给出近似梯度\nargs：元组，是传递给优化函数的参数\nimport scipy.optimize as opt result2 = opt.fmin_tnc(func=COSTreg,x0=W2,fprime=BGDreg,args=(X2,Y2,a)) result2 result: 这里再提一嘴scipy的优化，其实本质上就是一种算法的训练了，得到了最优的W2\n同样看看准确率，可以采用dataset1的预测函数\nCOSTreg(result2[0],X2,Y2,a) result:\n0.6226824388287262 W2_WIN = np.matrix(result2[0])#同样记得转化为矩阵 prediction2 = predict(W2_WIN,X2) correct2 = [ 1 if ((a==1 and b==1) or(a==0 and b==0)) else 0 for (a,b) in zip(prediction2,Y2) #zip函数会将不同行的数据装成新的一行，再换行，装下列 ] #把correct2相加，0相加还是为0，所以不影响准确率的判断 accuracy2 = (sum(map(int,correct2)) % len(correct2)) print(\u0026#39;accuracy2:{0}%\u0026#39;.format(accuracy2)) #准确率略低 result:\naccuracy2:78% 看下sklearn的结果\n# 导入模型 from sklearn.linear_model import LogisticRegression model2 = LogisticRegression(penalty=\u0026#39;l2\u0026#39;,C=1.0) # 训练数据 model2.fit(X2,Y2.ravel()) result:\nLogisticRegression() code:\nmodel2.score(X2,Y2) result:\n0.6610169491525424 准确率明显比我们之前写的低，可能是参数的问题，毕竟没有指定的话使用的参数值是默认的，可以后期进行调整\n","date":"2021-09-07T00:00:00Z","permalink":"https://example.com/p/mylogicregresscode/","title":"mylogicRegresscode"},{"content":" 放这里用来随时随地看\n在处理自行车数据时，我需要温度和降水数据，来弄清楚人们下雨时是否喜欢骑自 行车。 所以我访问了加拿大历史天气数据的网站，并想出如何自动获得它们。 这里我们将获取 201 年 3 月的数据，并清理它们。 以下是可用于在蒙特利尔获取数据的网址模板。\nimport pandas as pd import numpy as np import matplotlib import matplotlib.pyplot as plt matplotlib.style.use(\u0026#34;ggplot\u0026#34;) plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = \u0026#34;SimHei\u0026#34; plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) df = pd.read_csv(\u0026#34;./comptagevelo2012.csv\u0026#34;,index_col=\u0026#34;Date\u0026#34;,sep=\u0026#34;,\u0026#34;, encoding=\u0026#34;latin1\u0026#34;,parse_dates=[\u0026#34;Date\u0026#34;],dayfirst=True) df result: ","date":"2021-09-07T00:00:00Z","permalink":"https://example.com/p/pandas%E7%A7%98%E7%B1%8D_%E7%AC%AC%E4%BA%94%E7%AB%A0/","title":"Pandas秘籍_第五章"},{"content":"机器学习练习7 决策树 代码修改并注释：黄海广，haiguang2000@wzu.edu.cn\n1．分类决策树模型是表示基于特征对实例进行分类的树形结构。决策树可以转换成一个if-then规则的集合，也可以看作是定义在特征空间划分上的类的条件概率分布。\n2．决策树学习旨在构建一个与训练数据拟合很好，并且复杂度小的决策树。因为从可能的决策树中直接选取最优决策树是NP完全问题。现实中采用启发式方法学习次优的决策树。\n决策树学习算法包括3部分：特征选择、树的生成和树的剪枝。常用的算法有ID3、 C4.5和CART。\n3．特征选择的目的在于选取对训练数据能够分类的特征。特征选择的关键是其准则。常用的准则如下：\n（1）样本集合$D$对特征$A$的信息增益（ID3）\n$$g(D, A)=H(D)-H(D|A)$$\n$$H(D)=-\\sum_{k=1}^{K} \\frac{\\left|C_{k}\\right|}{|D|} \\log {2} \\frac{\\left|C{k}\\right|}{|D|}$$\n$$H(D | A)=\\sum_{i=1}^{n} \\frac{\\left|D_{i}\\right|}{|D|} H\\left(D_{i}\\right)$$\n其中，$H(D)$是数据集$D$的熵，$H(D_i)$是数据集$D_i$的熵，$H(D|A)$是数据集$D$对特征$A$的条件熵。\t$D_i$是$D$中特征$A$取第$i$个值的样本子集，$C_k$是$D$中属于第$k$类的样本子集。$n$是特征$A$取 值的个数，$K$是类的个数。\n（2）样本集合$D$对特征$A$的信息增益比（C4.5）\n$$g_{R}(D, A)=\\frac{g(D, A)}{H(D)}$$\n其中，$g(D,A)$是信息增益，$H(D)$是数据集$D$的熵。\n（3）样本集合$D$的基尼指数（CART）\n$$\\operatorname{Gini}(D)=1-\\sum_{k=1}^{K}\\left(\\frac{\\left|C_{k}\\right|}{|D|}\\right)^{2}$$\n特征$A$条件下集合$D$的基尼指数：\n$$\\operatorname{Gini}(D, A)=\\frac{\\left|D_{1}\\right|}{|D|} \\operatorname{Gini}\\left(D_{1}\\right)+\\frac{\\left|D_{2}\\right|}{|D|} \\operatorname{Gini}\\left(D_{2}\\right)$$\n4．决策树的生成。通常使用信息增益最大、信息增益比最大或基尼指数最小作为特征选择的准则。决策树的生成往往通过计算信息增益或其他指标，从根结点开始，递归地产生决策树。这相当于用信息增益或其他准则不断地选取局部最优的特征，或将训练集分割为能够基本正确分类的子集。\n5．决策树的剪枝。由于生成的决策树存在过拟合问题，需要对它进行剪枝，以简化学到的决策树。决策树的剪枝，往往从已生成的树上剪掉一些叶结点或叶结点以上的子树，并将其父结点或根结点作为新的叶结点，从而简化生成的决策树。\nimport numpy as np import pandas as pd import math from math import log 创建数据 def create_data(): datasets = [[\u0026#39;青年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;一般\u0026#39;, \u0026#39;否\u0026#39;], [\u0026#39;青年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;好\u0026#39;, \u0026#39;否\u0026#39;], [\u0026#39;青年\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;好\u0026#39;, \u0026#39;是\u0026#39;], [\u0026#39;青年\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;一般\u0026#39;, \u0026#39;是\u0026#39;], [\u0026#39;青年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;一般\u0026#39;, \u0026#39;否\u0026#39;], [\u0026#39;中年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;一般\u0026#39;, \u0026#39;否\u0026#39;], [\u0026#39;中年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;好\u0026#39;, \u0026#39;否\u0026#39;], [\u0026#39;中年\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;好\u0026#39;, \u0026#39;是\u0026#39;], [\u0026#39;中年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;非常好\u0026#39;, \u0026#39;是\u0026#39;], [\u0026#39;中年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;非常好\u0026#39;, \u0026#39;是\u0026#39;], [\u0026#39;老年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;非常好\u0026#39;, \u0026#39;是\u0026#39;], [\u0026#39;老年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;好\u0026#39;, \u0026#39;是\u0026#39;], [\u0026#39;老年\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;好\u0026#39;, \u0026#39;是\u0026#39;], [\u0026#39;老年\u0026#39;, \u0026#39;是\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;非常好\u0026#39;, \u0026#39;是\u0026#39;], [\u0026#39;老年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;一般\u0026#39;, \u0026#39;否\u0026#39;], ] labels = [u\u0026#39;年龄\u0026#39;, u\u0026#39;有工作\u0026#39;, u\u0026#39;有自己的房子\u0026#39;, u\u0026#39;信贷情况\u0026#39;, u\u0026#39;类别\u0026#39;] # 返回数据集和每个维度的名称 return datasets, labels datasets, labels = create_data() train_data = pd.DataFrame(datasets, columns=labels) train_data result: 熵 def calc_ent(datasets): data_length = len(datasets) label_count = {} #这里for优化可以直接用dict的setdefault功能 for i in range(data_length): label = datasets[i][-1] if label not in label_count: label_count[label] = 0 label_count[label] += 1 ent = -sum([(p / data_length) * log(p / data_length, 2) for p in label_count.values()]) return ent 条件熵 def cond_ent(datasets, axis=0): data_length = len(datasets) feature_sets = {} for i in range(data_length): feature = datasets[i][axis] if feature not in feature_sets: feature_sets[feature] = [] feature_sets[feature].append(datasets[i]) cond_ent = sum([(len(p) / data_length) * calc_ent(p) for p in feature_sets.values()]) return cond_ent calc_ent(datasets) result:\n0.9709505944546686 信息增益 def info_gain(ent, cond_ent): return ent - cond_ent def info_gain_train(datasets): count = len(datasets[0]) - 1 ent = calc_ent(datasets) best_feature = [] for c in range(count): c_info_gain = info_gain(ent, cond_ent(datasets, axis=c)) best_feature.append((c, c_info_gain)) print(\u0026#39;特征({}) 的信息增益为： {:.3f}\u0026#39;.format(labels[c], c_info_gain)) # 比较大小 best_ = max(best_feature, key=lambda x: x[-1]) return \u0026#39;特征({})的信息增益最大，选择为根节点特征\u0026#39;.format(labels[best_[0]]) info_gain_train(np.array(datasets)) result: 利用ID3算法生成决策树 # 定义节点类 二叉树 class Node: def __init__(self, root=True, label=None, feature_name=None, feature=None): self.root = root self.label = label self.feature_name = feature_name self.feature = feature self.tree = {} self.result = { \u0026#39;label:\u0026#39;: self.label, \u0026#39;feature\u0026#39;: self.feature, \u0026#39;tree\u0026#39;: self.tree } def __repr__(self): return \u0026#39;{}\u0026#39;.format(self.result) def add_node(self, val, node): self.tree[val] = node def predict(self, features): if self.root is True: return self.label return self.tree[features[self.feature]].predict(features) class DTree: def __init__(self, epsilon=0.1): self.epsilon = epsilon self._tree = {} # 熵 @staticmethod def calc_ent(datasets): data_length = len(datasets) label_count = {} for i in range(data_length): label = datasets[i][-1] if label not in label_count: label_count[label] = 0 label_count[label] += 1 ent = -sum([(p / data_length) * log(p / data_length, 2) for p in label_count.values()]) return ent # 经验条件熵 def cond_ent(self, datasets, axis=0): data_length = len(datasets) feature_sets = {} for i in range(data_length): feature = datasets[i][axis] if feature not in feature_sets: feature_sets[feature] = [] feature_sets[feature].append(datasets[i]) cond_ent = sum([(len(p) / data_length) * self.calc_ent(p) for p in feature_sets.values()]) return cond_ent # 信息增益 @staticmethod def info_gain(ent, cond_ent): return ent - cond_ent def info_gain_train(self, datasets): count = len(datasets[0]) - 1 ent = self.calc_ent(datasets) best_feature = [] for c in range(count): c_info_gain = self.info_gain(ent, self.cond_ent(datasets, axis=c)) best_feature.append((c, c_info_gain)) # 比较大小 best_ = max(best_feature, key=lambda x: x[-1]) return best_ def train(self, train_data): \u0026#34;\u0026#34;\u0026#34; input:数据集D(DataFrame格式)，特征集A，阈值eta output:决策树T \u0026#34;\u0026#34;\u0026#34; _, y_train, features = train_data.iloc[:, : -1], train_data.iloc[:, -1], train_data.columns[: -1] # 1,若D中实例属于同一类Ck，则T为单节点树，并将类Ck作为结点的类标记，返回T if len(y_train.value_counts()) == 1: return Node(root=True, label=y_train.iloc[0]) # 2, 若A为空，则T为单节点树，将D中实例树最大的类Ck作为该节点的类标记，返回T if len(features) == 0: return Node( root=True, label=y_train.value_counts().sort_values( ascending=False).index[0]) # 3,计算最大信息增益 同5.1,Ag为信息增益最大的特征 max_feature, max_info_gain = self.info_gain_train(np.array(train_data)) max_feature_name = features[max_feature] # 4,Ag的信息增益小于阈值eta,则置T为单节点树，并将D中是实例数最大的类Ck作为该节点的类标记，返回T if max_info_gain \u0026lt; self.epsilon: return Node( root=True, label=y_train.value_counts().sort_values( ascending=False).index[0]) # 5,构建Ag子集 node_tree = Node( root=False, feature_name=max_feature_name, feature=max_feature) feature_list = train_data[max_feature_name].value_counts().index for f in feature_list: sub_train_df = train_data.loc[train_data[max_feature_name] == f].drop([max_feature_name], axis=1) # 6, 递归生成树 sub_tree = self.train(sub_train_df) node_tree.add_node(f, sub_tree) # pprint.pprint(node_tree.tree) return node_tree def fit(self, train_data): self._tree = self.train(train_data) return self._tree def predict(self, X_test): return self._tree.predict(X_test) datasets, labels = create_data() data_df = pd.DataFrame(datasets, columns=labels) dt = DTree() tree = dt.fit(data_df) tree result: code:\ndt.predict([\u0026#39;老年\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;否\u0026#39;, \u0026#39;一般\u0026#39;]) result:\n否 Scikit-learn实例 from sklearn.datasets import load_iris from sklearn.model_selection import train_test_split from collections import Counter 使用Iris数据集，我们可以构建如下树：\n# data def create_data(): iris = load_iris() df = pd.DataFrame(iris.data, columns=iris.feature_names) df[\u0026#39;label\u0026#39;] = iris.target df.columns = [ \u0026#39;sepal length\u0026#39;, \u0026#39;sepal width\u0026#39;, \u0026#39;petal length\u0026#39;, \u0026#39;petal width\u0026#39;, \u0026#39;label\u0026#39; ] data = np.array(df.iloc[:100, [0, 1, -1]]) # print(data) return data[:, :2], data[:, -1],iris.feature_names[0:2] X, y,feature_name= create_data() X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3) 决策树分类 from sklearn.tree import DecisionTreeClassifier from sklearn.tree import export_graphviz import graphviz from sklearn import tree clf = DecisionTreeClassifier() clf.fit(X_train, y_train,) clf.score(X_test, y_test) result:\n0.9666666666666667 一旦经过训练，就可以用 plot_tree函数绘制树：\ntree.plot_tree(clf) result: 也可以导出树\ntree_pic = export_graphviz(clf, out_file=\u0026#34;mytree.pdf\u0026#34;) with open(\u0026#39;mytree.pdf\u0026#39;) as f: dot_graph = f.read() graphviz.Source(dot_graph) 或者，还可以使用函数 export_text以文本格式导出树。此方法不需要安装外部库，而且更紧凑：\nfrom sklearn.tree import export_text r = export_text(clf,feature_name) print(r) 决策树回归 import numpy as np from sklearn.tree import DecisionTreeRegressor import matplotlib.pyplot as plt # Create a random dataset rng = np.random.RandomState(1) X = np.sort(5 * rng.rand(80, 1), axis=0) y = np.sin(X).ravel() y[::5] += 3 * (0.5 - rng.rand(16)) # Fit regression model regr_1 = DecisionTreeRegressor(max_depth=2) regr_2 = DecisionTreeRegressor(max_depth=5) regr_1.fit(X, y) regr_2.fit(X, y) # Predict X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis] y_1 = regr_1.predict(X_test) y_2 = regr_2.predict(X_test) # Plot the results plt.figure() plt.scatter(X, y, s=20, edgecolor=\u0026#34;black\u0026#34;, c=\u0026#34;darkorange\u0026#34;, label=\u0026#34;data\u0026#34;) plt.plot(X_test, y_1, color=\u0026#34;cornflowerblue\u0026#34;, label=\u0026#34;max_depth=2\u0026#34;, linewidth=2) plt.plot(X_test, y_2, color=\u0026#34;yellowgreen\u0026#34;, label=\u0026#34;max_depth=5\u0026#34;, linewidth=2) plt.xlabel(\u0026#34;data\u0026#34;) plt.ylabel(\u0026#34;target\u0026#34;) plt.title(\u0026#34;Decision Tree Regression\u0026#34;) plt.legend() plt.show() result: Scikit-learn 的决策树参数 DecisionTreeClassifier(criterion=\u0026#34;gini\u0026#34;, splitter=\u0026#34;best\u0026#34;, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0., max_features=None, random_state=None, max_leaf_nodes=None, min_impurity_decrease=0., min_impurity_split=None, class_weight=None, presort=False) 参数含义： 1.criterion:string, optional (default=\u0026#34;gini\u0026#34;) (1).criterion=\u0026#39;gini\u0026#39;,分裂节点时评价准则是Gini指数。 (2).criterion=\u0026#39;entropy\u0026#39;,分裂节点时的评价指标是信息增益。 2.max_depth:int or None, optional (default=None)。指定树的最大深度。 如果为None，表示树的深度不限。直到所有的叶子节点都是纯净的，即叶子节点 中所有的样本点都属于同一个类别。或者每个叶子节点包含的样本数小于min_samples_split。 3.splitter:string, optional (default=\u0026#34;best\u0026#34;)。指定分裂节点时的策略。 (1).splitter=\u0026#39;best\u0026#39;,表示选择最优的分裂策略。 (2).splitter=\u0026#39;random\u0026#39;,表示选择最好的随机切分策略。 4.min_samples_split:int, float, optional (default=2)。表示分裂一个内部节点需要的做少样本数。 (1).如果为整数，则min_samples_split就是最少样本数。 (2).如果为浮点数(0到1之间)，则每次分裂最少样本数为ceil(min_samples_split * n_samples) 5.min_samples_leaf: int, float, optional (default=1)。指定每个叶子节点需要的最少样本数。 (1).如果为整数，则min_samples_split就是最少样本数。 (2).如果为浮点数(0到1之间)，则每个叶子节点最少样本数为ceil(min_samples_leaf * n_samples) 6.min_weight_fraction_leaf:float, optional (default=0.) 指定叶子节点中样本的最小权重。 7.max_features:int, float, string or None, optional (default=None). 搜寻最佳划分的时候考虑的特征数量。 (1).如果为整数，每次分裂只考虑max_features个特征。 (2).如果为浮点数(0到1之间)，每次切分只考虑int(max_features * n_features)个特征。 (3).如果为\u0026#39;auto\u0026#39;或者\u0026#39;sqrt\u0026#39;,则每次切分只考虑sqrt(n_features)个特征 (4).如果为\u0026#39;log2\u0026#39;,则每次切分只考虑log2(n_features)个特征。 (5).如果为None,则每次切分考虑n_features个特征。 (6).如果已经考虑了max_features个特征，但还是没有找到一个有效的切分，那么还会继续寻找 下一个特征，直到找到一个有效的切分为止。 8.random_state:int, RandomState instance or None, optional (default=None) (1).如果为整数，则它指定了随机数生成器的种子。 (2).如果为RandomState实例，则指定了随机数生成器。 (3).如果为None，则使用默认的随机数生成器。 9.max_leaf_nodes: int or None, optional (default=None)。指定了叶子节点的最大数量。 (1).如果为None,叶子节点数量不限。 (2).如果为整数，则max_depth被忽略。 10.min_impurity_decrease:float, optional (default=0.) 如果节点的分裂导致不纯度的减少(分裂后样本比分裂前更加纯净)大于或等于min_impurity_decrease，则分裂该节点。 加权不纯度的减少量计算公式为： min_impurity_decrease=N_t / N * (impurity - N_t_R / N_t * right_impurity - N_t_L / N_t * left_impurity) 其中N是样本的总数，N_t是当前节点的样本数，N_t_L是分裂后左子节点的样本数， N_t_R是分裂后右子节点的样本数。impurity指当前节点的基尼指数，right_impurity指 分裂后右子节点的基尼指数。left_impurity指分裂后左子节点的基尼指数。 11.min_impurity_split:float 树生长过程中早停止的阈值。如果当前节点的不纯度高于阈值，节点将分裂，否则它是叶子节点。 这个参数已经被弃用。用min_impurity_decrease代替了min_impurity_split。 12.class_weight:dict, list of dicts, \u0026#34;balanced\u0026#34; or None, default=None 类别权重的形式为{class_label: weight} (1).如果没有给出每个类别的权重，则每个类别的权重都为1。 (2).如果class_weight=\u0026#39;balanced\u0026#39;，则分类的权重与样本中每个类别出现的频率成反比。 计算公式为：n_samples / (n_classes * np.bincount(y)) (3).如果sample_weight提供了样本权重(由fit方法提供)，则这些权重都会乘以sample_weight。 13.presort:bool, optional (default=False) 指定是否需要提前排序数据从而加速训练中寻找最优切分的过程。设置为True时，对于大数据集 会减慢总体的训练过程；但是对于一个小数据集或者设定了最大深度的情况下，会加速训练过程。 决策树调参 # 导入库 from sklearn.tree import DecisionTreeClassifier from sklearn import datasets from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt from sklearn.model_selection import GridSearchCV from sklearn.tree import DecisionTreeRegressor from sklearn import metrics # 导入数据集 X = datasets.load_iris() # 以全部字典形式返回,有data,target,target_names三个键 data = X.data target = X.target name = X.target_names x, y = datasets.load_iris(return_X_y=True) # 能一次性取前2个 print(x.shape, y.shape) result:\n(150, 4) (150,) # 数据分为训练集和测试集 x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=100) # 用GridSearchCV寻找最优参数（字典） param = { \u0026#39;criterion\u0026#39;: [\u0026#39;gini\u0026#39;], \u0026#39;max_depth\u0026#39;: [30, 50, 60, 100], \u0026#39;min_samples_leaf\u0026#39;: [2, 3, 5, 10], \u0026#39;min_impurity_decrease\u0026#39;: [0.1, 0.2, 0.5] } grid = GridSearchCV(DecisionTreeClassifier(), param_grid=param, cv=6) grid.fit(x_train, y_train) print(\u0026#39;最优分类器:\u0026#39;, grid.best_params_, \u0026#39;最优分数:\u0026#39;, grid.best_score_) # 得到最优的参数和分值 result:\n最优分类器: {\u0026#39;criterion\u0026#39;: \u0026#39;gini\u0026#39;, \u0026#39;max_depth\u0026#39;: 30, \u0026#39;min_impurity_decrease\u0026#39;: 0.2, \u0026#39;min_samples_leaf\u0026#39;: 5} 最优分数: 0.9416666666666665 参考 https://github.com/fengdu78/lihang-code 《统计学习方法》，清华大学出版社，李航著，2019年出版 https://scikit-learn.org ","date":"2021-09-07T00:00:00Z","permalink":"https://example.com/p/wzu_%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/","title":"WZU_决策树算法代码学习记录"},{"content":"机器学习练习 3 - 逻辑回归\n在这一次练习中，我们将要实现逻辑回归并且应用到一个分类任务。我们还将通过将正则化加入训练算法，来提高算法的鲁棒性，并用更复杂的情形来测试它。\n代码修改并注释：黄海广，haiguang2000@wzu.edu.cn\n逻辑回归 在训练的初始阶段，我们将要构建一个逻辑回归模型来预测，某个学生是否被大学录取。设想你是大学相关部分的管理者，想通过申请学生两次测试的评分，来决定他们是否被录取。现在你拥有之前申请学生的可以用于训练逻辑回归的训练样本集。对于每一个训练样本，你有他们两次测试的评分和最后是被录取的结果。为了完成这个预测任务，我们准备构建一个可以基于两次测试评分来评估录取可能性的分类模型。\n让我们从检查数据开始。\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt path = \u0026#39;ex2data1.txt\u0026#39; data = pd.read_csv(path, header=None, names=[\u0026#39;Exam 1\u0026#39;, \u0026#39;Exam 2\u0026#39;, \u0026#39;Admitted\u0026#39;]) data.head() result: code:\ndata.shape result:\n(100, 3) 让我们创建两个分数的散点图，并使用颜色编码来可视化，如果样本是正的（被接纳）或负的（未被接纳）。\npositive = data[data[\u0026#39;Admitted\u0026#39;].isin([1])] negative = data[data[\u0026#39;Admitted\u0026#39;].isin([0])] fig, ax = plt.subplots(figsize=(12, 8)) ax.scatter(positive[\u0026#39;Exam 1\u0026#39;], positive[\u0026#39;Exam 2\u0026#39;], s=50, c=\u0026#39;b\u0026#39;, marker=\u0026#39;o\u0026#39;, label=\u0026#39;Admitted\u0026#39;) ax.scatter(negative[\u0026#39;Exam 1\u0026#39;], negative[\u0026#39;Exam 2\u0026#39;], s=50, c=\u0026#39;r\u0026#39;, marker=\u0026#39;x\u0026#39;, label=\u0026#39;Not Admitted\u0026#39;) ax.legend() ax.set_xlabel(\u0026#39;Exam 1 Score\u0026#39;) ax.set_ylabel(\u0026#39;Exam 2 Score\u0026#39;) plt.show() result: 看起来在两类间，有一个清晰的决策边界。现在我们需要实现逻辑回归，那样就可以训练一个模型来预测结果。\nSigmoid 函数 $g$ 代表一个常用的逻辑函数（logistic function）为$S$形函数（Sigmoid function），公式为：$$ g\\left( z \\right)=\\frac{1}{1+{{e}^{-z}}}$$ 合起来，我们得到逻辑回归模型的假设函数： $${{h}}\\left( x \\right)=\\frac{1}{1+{{e}^{-{{w }^{T}}x}}}$$\ncode:\ndef sigmoid(z): return 1 / (1 + np.exp(-z)) 让我们做一个快速的检查，来确保它可以工作。\nnums = np.arange(-10, 10, step=1) fig, ax = plt.subplots(figsize=(12, 8)) ax.plot(nums, sigmoid(nums), \u0026#39;r\u0026#39;) plt.show() result: 棒极了！现在，我们需要编写代价函数来评估结果。 代价函数： $J\\left(w\\right)=-\\frac{1}{m}\\sum\\limits_{i=1}^{m}{({{y}^{(i)}}\\log \\left( {h}\\left( {{x}^{(i)}} \\right) \\right)+\\left( 1-{{y}^{(i)}} \\right)\\log \\left( 1-{h}\\left( {{x}^{(i)}} \\right) \\right))}$\ndef cost(w, X, y): w = np.matrix(w) X = np.matrix(X) y = np.matrix(y) first = np.multiply(-y, np.log(sigmoid(X * w.T))) second = np.multiply((1 - y), np.log(1 - sigmoid(X * w.T))) return np.sum(first - second) / (len(X)) 现在，我们要做一些设置，和我们在练习1在线性回归的练习很相似。\n# add a ones column - this makes the matrix multiplication work out easier data.insert(0, \u0026#39;Ones\u0026#39;, 1) # set X (training data) and y (target variable) cols = data.shape[1] X = data.iloc[:, 0:cols - 1] y = data.iloc[:, cols - 1:cols] # convert to numpy arrays and initalize the parameter array w X = np.array(X.values) y = np.array(y.values) w = np.zeros(3) 让我们来检查矩阵的维度来确保一切良好。\nX.shape, w.shape, y.shape result:\n((100, 3), (3,), (100, 1)) code:\ndata.iloc[:, cols - 1:cols] result: 让我们计算初始化参数的代价函数($w$为0)。\ncode:\ncost(w, X, y) result:\n0.6931471805599453 看起来不错，接下来，我们需要一个函数来计算我们的训练数据、标签和一些参数$w$的梯度。\ngradient descent(梯度下降) 这是批量梯度下降（batch gradient descent） 转化为向量化计算： $\\frac{1}{m} X^T( Sigmoid(XW) - y )$ $$\\frac{\\partial J\\left( w \\right)}{\\partial {{w }{j}}}=\\frac{1}{m}\\sum\\limits{i=1}^{m}{({{h}}\\left( {{x}^{(i)}} \\right)-{{y}^{(i)}})x_{_{j}}^{(i)}}$$ def gradient(w, X, y): w = np.matrix(w) X = np.matrix(X) y = np.matrix(y) parameters = int(w.ravel().shape[1]) grad = np.zeros(parameters) error = sigmoid(X * w.T) - y for i in range(parameters): term = np.multiply(error, X[:, i]) grad[i] = np.sum(term) / len(X) return grad 注意，我们实际上没有在这个函数中执行梯度下降，我们仅仅在计算一个梯度步长。在练习中，一个称为“fminunc”的Octave函数是用来优化函数来计算成本和梯度参数。由于我们使用Python，我们可以用SciPy的“optimize”命名空间来做同样的事情。\n我们看看用我们的数据和初始参数为0的梯度下降法的结果。\ngradient(w, X, y) result:\narray([ -0.1 , -12.00921659, -11.26284221]) 现在可以用SciPy\u0026rsquo;s truncated newton（TNC）实现寻找最优参数。\ncode:\nimport scipy.optimize as opt result = opt.fmin_tnc(func=cost, x0=w, fprime=gradient, args=(X, y)) result result:\n(array([-25.16131872, 0.20623159, 0.20147149]), 36, 0) 让我们看看在这个结论下代价函数计算结果是什么个样子\ncost(result[0], X, y) result:\n0.20349770158947425 接下来，我们需要编写一个函数，用我们所学的参数w来为数据集X输出预测。然后，我们可以使用这个函数来给我们的分类器的训练精度打分。 逻辑回归模型的假设函数： $${{h}}\\left( x \\right)=\\frac{1}{1+{{e}^{-{{w }^{T}}X}}}$$ 当${{h}}$大于等于0.5时，预测 y=1\n当${{h}}$小于0.5时，预测 y=0 。\ndef predict(w, X): probability = sigmoid(X * w.T) return [1 if x \u0026gt;= 0.5 else 0 for x in probability] w_min = np.matrix(result[0]) predictions = predict(w_min, X) correct = [ 1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y) ] accuracy = (sum(map(int, correct)) % len(correct)) print(\u0026#39;accuracy = {0}%\u0026#39;.format(accuracy)) result:\naccuracy = 89% 我们的逻辑回归分类器预测正确，如果一个学生被录取或没有录取，达到89%的精确度。不坏！记住，这是训练集的准确性。我们没有保持住了设置或使用交叉验证得到的真实逼近，所以这个数字有可能高于其真实值（这个话题将在以后说明）。\n正则化逻辑回归 在训练的第二部分，我们将要通过加入正则项提升逻辑回归算法。如果你对正则化有点眼生，或者喜欢这一节的方程的背景，请参考在\u0026quot;exercises\u0026quot;文件夹中的\u0026quot;ex2.pdf\u0026quot;。简而言之，正则化是成本函数中的一个术语，它使算法更倾向于“更简单”的模型（在这种情况下，模型将更小的系数）。这个理论助于减少过拟合，提高模型的泛化能力。这样，我们开始吧。\n设想你是工厂的生产主管，你有一些芯片在两次测试中的测试结果。对于这两次测试，你想决定是否芯片要被接受或抛弃。为了帮助你做出艰难的决定，你拥有过去芯片的测试数据集，从其中你可以构建一个逻辑回归模型。\n和第一部分很像，从数据可视化开始吧！\npath = \u0026#39;ex2data2.txt\u0026#39; data2 = pd.read_csv(path, header=None, names=[\u0026#39;Test 1\u0026#39;, \u0026#39;Test 2\u0026#39;, \u0026#39;Accepted\u0026#39;]) data2.head() result: code:\npositive = data2[data2[\u0026#39;Accepted\u0026#39;].isin([1])] negative = data2[data2[\u0026#39;Accepted\u0026#39;].isin([0])] fig, ax = plt.subplots(figsize=(12, 8)) ax.scatter(positive[\u0026#39;Test 1\u0026#39;], positive[\u0026#39;Test 2\u0026#39;], s=50, c=\u0026#39;b\u0026#39;, marker=\u0026#39;o\u0026#39;, label=\u0026#39;Accepted\u0026#39;) ax.scatter(negative[\u0026#39;Test 1\u0026#39;], negative[\u0026#39;Test 2\u0026#39;], s=50, c=\u0026#39;r\u0026#39;, marker=\u0026#39;x\u0026#39;, label=\u0026#39;Rejected\u0026#39;) ax.legend() ax.set_xlabel(\u0026#39;Test 1 Score\u0026#39;) ax.set_ylabel(\u0026#39;Test 2 Score\u0026#39;) plt.show() result: 这个数据看起来可比前一次的复杂得多。特别地，你会注意到其中没有线性决策界限，来良好的分开两类数据。一个方法是用像逻辑回归这样的线性技术来构造从原始特征的多项式中得到的特征。让我们通过创建一组多项式特征入手吧。\ncode:\ndegree = 5 x1 = data2[\u0026#39;Test 1\u0026#39;] x2 = data2[\u0026#39;Test 2\u0026#39;] data2.insert(3, \u0026#39;Ones\u0026#39;, 1) for i in range(1, degree): for j in range(0, i): data2[\u0026#39;F\u0026#39; + str(i) + str(j)] = np.power(x1, i-j) * np.power(x2, j) data2.drop(\u0026#39;Test 1\u0026#39;, axis=1, inplace=True) data2.drop(\u0026#39;Test 2\u0026#39;, axis=1, inplace=True) data2.head() result: 现在，我们需要修改第1部分的成本和梯度函数，包括正则化项。首先是成本函数：\nregularized cost（正则化代价函数） $$J\\left( w \\right)=\\frac{1}{m}\\sum\\limits_{i=1}^{m}{[-{{y}^{(i)}}\\log \\left( {{h}}\\left( {{x}^{(i)}} \\right) \\right)-\\left( 1-{{y}^{(i)}} \\right)\\log \\left( 1-{{h}}\\left( {{x}^{(i)}} \\right) \\right)]}+\\frac{\\lambda }{2m}\\sum\\limits_{j=1}^{n}{w _{j}^{2}}$$\ndef costReg(w, X, y, learningRate): w = np.matrix(w) X = np.matrix(X) y = np.matrix(y) first = np.multiply(-y, np.log(sigmoid(X * w.T))) second = np.multiply((1 - y), np.log(1 - sigmoid(X * w.T))) reg = (learningRate / (2 * len(X))) * np.sum(np.power(w[:, 1:w.shape[1]], 2)) return np.sum(first - second) / len(X) + reg 请注意等式中的\u0026quot;reg\u0026quot; 项。还注意到另外的一个“学习率”参数。这是一种超参数，用来控制正则化项。现在我们需要添加正则化梯度函数：\n如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对${{w }_{0}}$ 进行正则化，所以梯度下降算法将分两种情形：\n对上面的算法中 j=1,2,\u0026hellip;,n 时的更新式子进行调整可得： ${{w }{j}}:={{w }{j}}(1-a\\frac{\\lambda }{m})-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{({{h}{w }}\\left( {{x}^{(i)}} \\right)-{{y}^{(i)}})x{j}^{(i)}}$\ndef gradientReg(w, X, y, learningRate): w = np.matrix(w) X = np.matrix(X) y = np.matrix(y) parameters = int(w.ravel().shape[1]) grad = np.zeros(parameters) error = sigmoid(X * w.T) - y for i in range(parameters): term = np.multiply(error, X[:, i]) if (i == 0): grad[i] = np.sum(term) / len(X) else: grad[i] = (np.sum(term) / len(X)) + ( (learningRate / len(X)) * w[:, i]) return grad 就像在第一部分中做的一样，初始化变量。\n# set X and y (remember from above that we moved the label to column 0) cols = data2.shape[1] X2 = data2.iloc[:,1:cols] y2 = data2.iloc[:,0:1] # convert to numpy arrays and initalize the parameter array w X2 = np.array(X2.values) y2 = np.array(y2.values) w2 = np.zeros(11) 让我们初始学习率到一个合理值。如果有必要的话（即如果惩罚太强或不够强）,我们可以之后再折腾这个。\nlearningRate = 1 现在，让我们尝试调用新的默认为0的$w$的正则化函数，以确保计算工作正常。\ncostReg(w2, X2, y2, learningRate) result:\n0.6931471805599454 code:\ngradientReg(w2, X2, y2, learningRate) result: 现在我们可以使用和第一部分相同的优化函数来计算优化后的结果。 code:\nresult2 = opt.fmin_tnc(func=costReg, x0=w2, fprime=gradientReg, args=(X2, y2, learningRate)) result2 result: 最后，我们可以使用第1部分中的预测函数来查看我们的方案在训练数据上的准确度。\nw_min = np.matrix(result2[0]) predictions = predict(w_min, X2) correct = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, y2)] accuracy = (sum(map(int, correct)) % len(correct)) print (\u0026#39;accuracy = {0}%\u0026#39;.format(accuracy)) result:\naccuracy = 78% 虽然我们实现了这些算法，值得注意的是，我们还可以使用高级Python库像scikit-learn来解决这个问题。\nfrom sklearn import linear_model#调用sklearn的线性回归包 model = linear_model.LogisticRegression(penalty=\u0026#39;l2\u0026#39;, C=1.0) model.fit(X2, y2.ravel()) result:\nLogisticRegression() code:\nmodel.score(X2, y2) result:\n0.6610169491525424 这个准确度和我们刚刚实现的差了好多，不过请记住这个结果可以使用默认参数下计算的结果。我们可能需要做一些参数的调整来获得和我们之前结果相同的精确度。\n参考 Prof. Andrew Ng. Machine Learning. Stanford University ","date":"2021-09-07T00:00:00Z","permalink":"https://example.com/p/wzu_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/","title":"WZU_逻辑回归代码学习记录"},{"content":" 放这里用来随时随地看\nimport pandas as pd import numpy as np import matplotlib import matplotlib.pyplot as plt matplotlib.style.use(\u0026#34;ggplot\u0026#34;)#这要放在plt后 import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = \u0026#34;SimHei\u0026#34; plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15,5) %matplotlib inline complaints = pd.read_csv(\u0026#34;./311_Service_Requests_from_2010_to_Present.csv\u0026#34;) 我想知道哪个区有最多的噪音投诉。 首先，我们来看看数据，看看它是什么样子：\ncomplaints[:5] result: 为了得到噪音投诉，我们需要找到 Complaint Type 列为 Noise - Street/Sidewalk 的行。\n#其实就是列用了个判断去取值 noise_complaints = complaints[complaints[\u0026#34;Complaint Type\u0026#34;] == \u0026#34;Noise - Street/Sidewalk\u0026#34;] noise_complaints[:3] result: 您还可以将多个条件与 \u0026amp; 运算符组合，如下所示:\nis_noise = complaints[\u0026#34;Complaint Type\u0026#34;] == \u0026#34;Noise - Street/Sidewalk\u0026#34; is_noise[:6] result: code:\nin_brooklyn = complaints[\u0026#39;Borough\u0026#39;] == \u0026#34;BROOKLYN\u0026#34; complaints[is_noise \u0026amp; in_brooklyn][:5] #就是可以同时用多个判断 result: 或者如果我们只需要几列：\ncomplaints[is_noise \u0026amp; in_brooklyn][[\u0026#34;Complaint Type\u0026#34;,\u0026#34;Borough\u0026#34;,\u0026#34;Created Date\u0026#34;]] numpy 数组的注解\n在内部，列的类型是 pd.Series 。\npd.Series([1,2,3]) 而且 pandas.Series 的内部是 numpy 数组。 如果将 .values 添加到任 何 Series 的末尾，你将得到它的内部 numpy 数组。\n就是说对于Series内部来说，它每个元素对象实际上还是np的数据格式 所以这个二进制数组选择的操作，实际上适用于任何 NumPy 数组： 所以，哪个区的噪音投诉最多？\n#可以用value_counts noise_complaints = complaints[complaints[\u0026#34;Complaint Type\u0026#34;] == \u0026#34;Noise - Street/Sidewalk\u0026#34;] noise_complaints[\u0026#34;Borough\u0026#34;].value_counts() result: 这是曼哈顿！ 但是，如果我们想要除以总投诉数量，以使它有点更有意义？ 这也 很容易：\nnoise_complaint_counts = noise_complaints[\u0026#34;Borough\u0026#34;].value_counts() complaint_counts = complaints[\u0026#34;Borough\u0026#34;].value_counts() noise_complaint_counts/complaint_counts #每个一对一相除 code:\n(noise_complaint_counts/complaint_counts).plot(kind=\u0026#34;bar\u0026#34;) result: ","date":"2021-09-06T00:00:00Z","permalink":"https://example.com/p/pandas%E7%A7%98%E7%B1%8D_%E7%AC%AC%E4%B8%89%E7%AB%A0/","title":"Pandas秘籍_第三章"},{"content":" 放这里用来随时随地看\nimport pandas as pd import matplotlib import matplotlib.pyplot as plt matplotlib.style.use(\u0026#34;ggplot\u0026#34;) matplotlib.line_width = 5000#行宽 matplotlib.max_columns = 60 plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = \u0026#34;SimHei\u0026#34; plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15,5) complaints = pd.read_csv()#读取数据csv complaints#查看数据有什么 选择列和行 为了选择一列，使用列名称作为索引，像这样：\ncomplaints[\u0026#34;Complaint Type\u0026#34;] 要获得 DataFrame 的前 5 行，我们可以使用切片： df [:5]\ncomplaints[:5] 我们可以组合它们来获得一列的前五行\ncomplaints[\u0026#34;Compaint Type\u0026#34;][:5] #等同于 complaints[:5][\u0026#34;Complaint Type\u0026#34;] 选择多列 如果我们只关心投诉类型和区，但不关心其余的信息怎么办？ Pandas 使它很容易 选择列的一个子集：只需将所需列的列表用作索引。\n#记得用一个[]装起来 complaints[[\u0026#34;Complaint Type\u0026#34;,\u0026#34;Borough\u0026#34;]] 这会向我们展示总结，我们可以获取前 10 列：\ncomplaints[[\u0026#34;Complaint Type\u0026#34;,\u0026#34;Borough\u0026#34;]][:10] value_counts() 方法计算类别 什么是最常见的投诉类型？\n这是个易于回答的问题，我们可以调用 .value_counts() 方法：\n这个方法可以计算类别\ncomplaints[\u0026#34;Complaint Type\u0026#34;].value_counts() 如果我们想要最常见的 10 个投诉类型，我们可以这样：\ncomplaint_counts = complaint[\u0026#34;Complaint Type\u0026#34;].value_counts() complaint_counts[:10] 但是还可以更好，我们可以绘制出来!\n#plot()中kind参数可以指定图的类型，如bar为柱状图，否则默认为折线图 complaint_counts[:10].plot(kind=\u0026#34;bar\u0026#34;) ","date":"2021-09-05T00:00:00Z","permalink":"https://example.com/p/pandas%E7%A7%98%E7%B1%8D_%E7%AC%AC%E4%BA%8C%E7%AB%A0/","title":"Pandas秘籍_第二章"},{"content":" 放这里用来随时随地看\nimport pandas as pd import numpy as np import matplotlib import matplotlib.pyplot as plt matplotlib.style.use(\u0026#34;ggplot\u0026#34;) plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = \u0026#34;SimHei\u0026#34; plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15,5) import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) 好的！ 我们将在这里回顾我们的自行车道数据集。 我住在蒙特利尔，我很好奇我 们是一个通勤城市，还是以骑自行车为乐趣的城市 - 人们在周末还是工作日骑自行车？\ndf = pd.read_csv(\u0026#34;./comptagevelo2012.csv\u0026#34;,index_col=\u0026#34;Date\u0026#34;,sep=\u0026#34;,\u0026#34;, encoding=\u0026#34;latin1\u0026#34;,parse_dates=[\u0026#34;Date\u0026#34;],dayfirst=True) df result: code:\ndf[\u0026#34;Berri1\u0026#34;].plot() result: 接下来，我们只是看看 Berri 自行车道。 Berri 是蒙特利尔的一条街道，是一个相当 重要的自行车道。 现在我习惯走这条路去图书馆，但我在旧蒙特利尔工作时，我习 惯于走这条路去上班。 所以我们要创建一个只有 Berri 自行车道的 DataFrame 。\nb_df = df[[\u0026#34;Berri1\u0026#34;]]#[[\u0026#34;xxx\u0026#34;]]这样可以使得赋予赋值的b_df是DataFrame对象 b_df[:5] result: code:\nb_df_2 = df[\u0026#34;Berri1\u0026#34;] b_df_2[:5] ressult: 接下来，我们需要添加一列 weekday 。 首先，我们可以从索引得到星期。 我们还 没有谈到索引，但索引在上面的 DataFrame 中是左边的东西，在 Date 下面。 它 基本上是一年中的所有日子。\nb_df.index Pandas 有一堆非常棒的时间序列功能，所以如果我们想得到每一行的月份中的日 期，我们可以这样做：\n#这个多read_csv时注意要对Date的调整 b_df.index.day result: 我们实际上想要星期：\nb_df.index.weekday result: 这是周中的日期，其中 0 是星期一。我通过查询日历得到 0 是星期\n现在我们知道了如何获取星期，我们可以将其添加到我们的 DataFrame 中作为一 列\nb_df[\u0026#34;weekday\u0026#34;] = b_df.index.weekday b_df[:5] result: 按星期统计骑手 这很易于实现！ Dataframe 有一个类似于 SQL groupby 的 .groupby() 方法，如果你熟悉的话。\n在这种情况下， berri_bikes.groupby(\u0026lsquo;weekday\u0026rsquo;) .aggregate(sum)`意味着“按 星期对行分组，然后将星期相同的所有值相加”。\naggregate意味着聚合\nweekday_counts = b_df.groupby(\u0026#34;weekday\u0026#34;).aggregate(sum) weekday_counts result: 很难记住 0, 1, 2, 3, 4, 5, 6 是什么，所以让我们修复它并绘制出来：\n#就是可以直接赋值index而修改index weekday_counts.index = [\u0026#39;Monday\u0026#39;, \u0026#39;Tuesday\u0026#39;, \u0026#39;Wednesday\u0026#39;, \u0026#39;Thursday\u0026#39;, \u0026#39;Friday\u0026#39;, \u0026#39;Saturday\u0026#39;, \u0026#39;Sunday\u0026#39;] weekday_counts result: weekday_counts.plot(kind=\u0026#34;bar\u0026#34;) result: ","date":"2021-09-04T00:00:00Z","permalink":"https://example.com/p/pandas%E7%A7%98%E7%B1%8D_%E7%AC%AC%E5%9B%9B%E7%AB%A0/","title":"Pandas秘籍_第四章"},{"content":" 放这里用来随时随地看\n读取文件 import pandas as pd import numpy as np import matplotlib.pyplot as plt import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = \u0026#34;SimHei\u0026#34; plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False 您可以使用 read_csv 函数从CSV文件读取数据。 默认情况下，它假定字段以逗 号分隔。\n这个数据集是一个列表，蒙特利尔的 7 个不同的自行车道上每天有多少人。\ndf = pd.read_csv(\u0026#34;./comptagevelo2012.csv\u0026#34;,index_col=\u0026#34;Date\u0026#34;) df.tail() result: 你可以看到这完全损坏了。 read_csv 拥有一堆选项能够让我们修复它，在这里我 们：\n将列分隔符改成 ; sep=\u0026quot;,\u0026quot; 将编码改为 latin1 （默认为 utf-8 ） encoding=\u0026ldquo;latin1\u0026rdquo; 解析 Date 列中的日期 parse_dates=[\u0026ldquo;Date\u0026rdquo;] 告诉它我们的日期将日放在前面，而不是月 dayfirst = True 将索引设置为 Date index_col = \u0026ldquo;Date\u0026rdquo; df = pd.read_csv(\u0026#34;./comptagevelo2012.csv\u0026#34;,index_col=\u0026#34;Date\u0026#34;,sep=\u0026#34;,\u0026#34;, encoding=\u0026#34;latin1\u0026#34;,parse_dates=[\u0026#34;Date\u0026#34;],dayfirst=True) df.head() result: 选择一列 当你读取 CSV 时，你会得到一种称为 DataFrame 的对象，它由行和列组成。 您 从数据框架中获取列的方式与从字典中获取元素的方式相同。\ncode:\ndf[\u0026#34;Berri1\u0026#34;] result: 绘制一列 只需要在末尾添加 .plot() ，再容易不过了。\n#使图表漂亮一些 #这个很有用！！！！mark了 import matplotlib plt.rcParams[\u0026#34;figure.figsize\u0026#34;] = (15,5) matplotlib.style.use(\u0026#34;ggplot\u0026#34;) df[\u0026#34;Berri1\u0026#34;].plot() result: 我们也可以很容易地绘制所有的列。 我们会让它更大一点。 你可以看到它挤在一 起，但所有的自行车道基本表现相同 - 如果对骑自行车的人来说是一个糟糕的一 天，任意地方都是糟糕的一天。\ncode:\ndf.plot(figsize=(15,10)) result: ","date":"2021-09-03T00:00:00Z","permalink":"https://example.com/p/pandas%E7%A7%98%E7%B1%8D_%E7%AC%AC%E4%B8%80%E7%AB%A0/","title":"Pandas秘籍_第一章"},{"content":" 这是针对吴恩达老师课程的线性回归的课后练习 dataset:regress_data1.csv/regress_data2.csv\n采用手写算法，初期不调用sklearn库\n收集数据 数据由外部提供\n分析数据 import pandas as pd import numpy as np import matplotlib.pyplot as plt dataset1 = pd.read_csv(\u0026#34;./regress_data1.csv\u0026#34;) print(dataset1.head()) print(dataset1.describe()) result: 可以看出只有一个特征属于单变量的线性回归\n#可视化数据 plt.rcParams[\u0026#39;font.sans-serif\u0026#39;]=[\u0026#39;SimHei\u0026#39;]#显示中文 plt.rcParams[\u0026#39;axes.unicode_minus\u0026#39;]=False#显示负号 dataset1.plot(kind=\u0026#39;scatter\u0026#39;,x=\u0026#39;人口\u0026#39;,y=\u0026#39;收益\u0026#39;,figsize=(12,8)) plt.xlabel(\u0026#39;人口\u0026#39;,fontsize=18) plt.ylabel(\u0026#39;收益\u0026#39;,fontsize=18)#可以添加rotationx=0使得收益转为来 plt.show() result: 处理数据 #插入一列恒为1的列 dataset1.insert(0,\u0026#39;Ones\u0026#39;,1)#在第零列插入列名为Ones，值为1 的一列 dataset1 result: #分开特征和目标 X = dataset1.iloc[:,:2] Y = dataset1.iloc[:,2] print(X.head()) print(Y.head()) print(Y.shape) result: code:\nX.shape result:\n(97, 2) 训练算法 #编写cost函数，方便起见写成np数组，并初始化w和alpha X = np.matrix(X.values) Y = np.matrix(Y.values).T w = np.matrix(np.array([0,0]))#因为从dataset1中可以看出只有两个特征，所以初始化w为（1，2）的0矩阵就好了 print(X.shape,Y.shape,w.shape)#注意矩阵的数据的行列 result:\n(97, 2) (97, 1) (1, 2) 参数$w$为特征函数的代价函数 $$J\\left( w \\right)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}{{{\\left( {{h}}\\left( {{x}^{(i)}} \\right)-{{y}^{(i)}} \\right)}^{2}}}$$ 其中：$${{h}}\\left( x \\right)={{w}^{T}}X={{w }{0}}{{x}{0}}+{{w }{1}}{{x}{1}}+{{w }{2}}{{x}{2}}+\u0026hellip;+{{w }{n}}{{x}{n}}$$ code:\n#编写代价函数 def Cost(X,Y,w): cost_value = (1/(2*len(X)))*(np.sum(np.power((X*w.T-Y),2))) return cost_value Cost(X,Y,w) result:\n32.072733877455676 Batch Gradient Decent（批量梯度下降） $${{w }{j}}:={{w }{j}}- \\alpha \\frac{1}{m}\\sum\\limits_{i=1}^m \\frac{\\partial }{\\partial {{w}_{j}}}J\\left( w \\right)$$\ndef BGD(X,Y,w,alpha,iters): temp = np.matrix(np.zeros(w.shape))#初始化w矩阵存放空间 1*2 param = int(w.ravel().shape[1])#ravel在多维数组编程一维数组即一行,这个是针对X(j)的 cost = np.zeros(iters)#初始化空间存放每次迭代的cost for i in range(iters): error = X*w.T-Y for j in range(param): temp[0,j] = w[0,j]-(alpha/len(X))*np.sum(np.multiply(error, X[:,j])) w = temp cost[i] = Cost(X,Y,w) return w, cost g, cost = BGD(X,Y,w,0.01,1000) print(g)#迭代1000次后的w Cost(X,Y,g)#计算此时的w的误差 result: #可视化拟合效果 fig, axes = plt.subplots(1,2,figsize=(14,4)) x = np.linspace(dataset1[\u0026#39;人口\u0026#39;].min(),dataset1[\u0026#39;人口\u0026#39;].max(),100) Y_pred = g[0,0] + g[0,1]*x axes[0].plot(x,Y_pred,\u0026#39;r\u0026#39;) axes[0].scatter(dataset1[\u0026#39;人口\u0026#39;],dataset1[\u0026#39;收益\u0026#39;]) axes[0].set_xlabel(\u0026#39;人口\u0026#39;,fontsize=14) axes[0].set_ylabel(\u0026#39;收益\u0026#39;,fontsize=14) # ases[0].yaxis.tick_left() iters = 1000 axes[1].plot(np.arange(iters),cost) axes[1].set_xlabel(\u0026#39;iters\u0026#39;,fontsize=14) axes[1].set_ylabel(\u0026#39;cost\u0026#39;,fontsize=14) axes[1].set_title(\u0026#39;迭代与代价\u0026#39;,fontsize=18) axes[1].yaxis.tick_right() plt.show() 第二份数据regress_data2.csv 分析数据 dataset2 = pd.read_csv(\u0026#39;./regress_data2.csv\u0026#39;) dataset2.head() result: code:\ndataset2.describe() result: 可以看出有两个特征且特征之间的数值跨度过大，打算用Z-SCore标准化\ndataset2 = (dataset2 - dataset2.mean())/dataset2.std() dataset2.head() result: 重复dataset1的处理方法\n# dataset2.insert(0,\u0026#39;One\u0026#39;,1) dataset2.head() result: X2 = np.matrix((dataset2.iloc[:,:3]).values) Y2 = np.matrix(((dataset2.iloc[:,-1])).values) Y2 = Y2.T w2 = np.matrix(np.array([0,0,0])) print(X2.shape,Y2.shape,w2.shape) result:\n(47, 3) (47, 1) (1, 3) code:\ng2,cost2 = BGD(X2,Y2,w2,0.01,1000) Cost(X2,Y2,g2) result:\n0.13070336960771892 可视化结构 f,axes = plt.subplots(figsize=(12,8)) iters = 1000 axes.plot(np.arange(iters),cost2) axes.set_xlabel(\u0026#39;iters\u0026#39;,fontsize=14) axes.set_ylabel(\u0026#39;cost\u0026#39;,fontsize=14) axes.set_title(\u0026#39;迭代与代价\u0026#39;,fontsize=18) axes.yaxis.tick_right() plt.show() 偷个鸡用sklearn实现一遍，针对regress_data1 from sklearn.linear_model import LinearRegression model = LinearRegression(normalize=False)#加载模型 model.fit(X, Y)#训练模型 #补充下其实coef_就是w1.。。。。wn；而intercept是wo，即截距 Ys_pred = model.predict(X).flatten() Xs = np.array(X[:,1]) f,axes = plt.subplots(figsize=(14,8)) axes.plot(Xs,Ys_pred,\u0026#39;r\u0026#39;,label=\u0026#39;sklearn预测\u0026#39;) axes.scatter(dataset1[\u0026#39;人口\u0026#39;],dataset1[\u0026#39;收益\u0026#39;],label=\u0026#39;实际数据\u0026#39;) plt.show() result: Ridge Regression 岭回归/L2正则化 $L_2$正则化 $J ( { w } ) = \\frac { 1 } { 2 } \\sum _ { i = 1 } ^ { m } ( h _ { w} ( x ^ { ( i ) } ) - y ^ { ( i ) } ) ^ { 2 } + \\lambda \\sum _ { j = 1 } ^ { n } w_ { j } ^ { 2 }$，此时称作Ridge Regression：\nfrom sklearn.linear_model import Ridge model_Ridge = Ridge() model.fit(X,Y) Ys_Ridge_pred = model.predict(X).flatten() Xs_Ridge = np.array(X[:,1]) f,axes = plt.subplots(figsize=(14,8)) axes.plot(Xs_Ridge,Ys_Ridge_pred,\u0026#39;r\u0026#39;,label=\u0026#39;sklearn预测\u0026#39;) axes.scatter(dataset1[\u0026#39;人口\u0026#39;],dataset1[\u0026#39;收益\u0026#39;],label=\u0026#39;实际数据\u0026#39;) plt.show() result: Lasso回归/ $L_1$正则化： $J ( {w } ) = \\frac { 1 } { 2 } \\sum _ { i = 1 } ^ { m } ( h _ { w} ( x ^ { ( i ) } ) - y ^ { ( i ) } ) ^ { 2 } + \\lambda \\sum _ { j = 1 } ^ { n } | w _ { j } |$，此时称作Lasso Regression\nfrom sklearn.linear_model import Lasso Lasso_model = Lasso() Lasso_model.fit(X,Y) Ys_Lasso_pred = model.predict(X).flatten() Xs_Lasso = np.array(X[:,1]) f,axes = plt.subplots(figsize=(14,8)) axes.plot(Xs_Lasso,Ys_Lasso_pred,\u0026#39;r\u0026#39;,label=\u0026#39;sklearn预测\u0026#39;) axes.scatter(dataset1[\u0026#39;人口\u0026#39;],dataset1[\u0026#39;收益\u0026#39;],label=\u0026#39;实际数据\u0026#39;) plt.show() result: 看看学习率对误差的影响 #采用交叉验证，cv=10 from sklearn.model_selection import cross_val_score test_scores = [] alphas = np.logspace(-3,2,50) for alpha in alphas: clf = Ridge(alpha) test_score = np.sqrt(-cross_val_score(clf,X,Y,cv=10,scoring=\u0026#39;neg_mean_squared_error\u0026#39;)) test_scores.append(np.mean(test_score)) plt.plot(alphas, test_scores) plt.title(\u0026#34;Alpha vs CV Error\u0026#34;); plt.show() 最小二乘法(LSM)： 最小二乘法的需要求解最优参数$w^{*}$：\n已知：目标函数\n$J\\left( w \\right)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}{{{\\left( {h}\\left( {x^{(i)}} \\right)-{y^{(i)}} \\right)}^{2}}}$\n其中：${h}\\left( x \\right)={w^{T}}X={w_{0}}{x_{0}}+{w_{1}}{x_{1}}+{w_{2}}{x_{2}}+\u0026hellip;+{w_{n}}{x_{n}}$\n将向量表达形式转为矩阵表达形式，则有$J(w )=\\frac{1}{2}{{\\left( Xw -y\\right)}^{2}}$ ，其中$X$为$m$行$n+1$列的矩阵（$m$为样本个数，$n$为特征个数），$w$为$n+1$行1列的矩阵(包含了$w_0$)，$y$为$m$行1列的矩阵，则可以求得最优参数$w^{*} ={{\\left( {X^{T}}X \\right)}^{-1}}{X^{T}}y$\n梯度下降与最小二乘法的比较：\n梯度下降：需要选择学习率$\\alpha$，需要多次迭代，当特征数量$n$大时也能较好适用，适用于各种类型的模型\n最小二乘法：不需要选择学习率$\\alpha$，一次计算得出，需要计算${{\\left( {{X}^{T}}X \\right)}^{-1}}$，如果特征数量$n$较大则运算代价大，因为矩阵逆的计算时间复杂度为$O(n^3)$，通常来说当$n$小于10000 时还是可以接受的，只适用于线性模型，不适合逻辑回归模型等其他模型\ndef LSM(X,Y): w = np.linalg.inv(X.T@X)@X.T@Y#np.linalg.inv()函数矩阵求逆 return w LSM_w = LSM(X,Y) print(\u0026#39;这是用LSM算的w\u0026#39;,LSM_w) print(\u0026#39;这是用BGD算的\u0026#39;,g) result: 差别有点大\n","date":"2021-09-01T00:00:00Z","permalink":"https://example.com/p/myregressioncode1/","title":"myRegressioncode1"},{"content":"这次练习采用sklearn来实现预测,dataset：ToyotaCorolla,这里不详细探究调参，后期返回来再摸索参数对训练的影响,date 2021/10/1\n收集数据 分析数据 import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt sns.set_style(\u0026#39;white\u0026#39;) dataset = pd.read_csv(\u0026#39;ToyotaCorolla.csv\u0026#39;) dataset.head()#最好加上（）输出的结构结构比较好看 result: dataset.describe() result: code:\nlen(dataset)#dataset.count()也行 result:\n1436 code:\ndataset.isnull().sum()#数据样本看来不用做null的处理了，没有null值~~~太好了 result: code:\n#采用和seaborn可视化数据,用一下热图吧 #首先，先看看相关性 dataset_corr = dataset.corr() print(dataset_corr.shape) #corr是pandas的函数之一，计算列与列之间的相关系数，返回相关系数矩阵，相关系数的取值范围为[-1, 1],当接近1时，表示两者具有强烈的正相关性，比如‘s’和‘x’；当接近-1时，表示有强烈的的负相关性，比如‘s’和‘c’，而若值接近0，则表示相关性很低. f,axes = plt.subplots(figsize=(10,10)) sns.heatmap(dataset_corr,annot=True,fmt=\u0026#39;.3f\u0026#39;) length = dataset_corr.columns plt.yticks(range(len(length)),dataset_corr.columns) plt.xticks(range(len(length)),dataset_corr.columns) plt.show() result: code:\ndataset_corr = dataset.corr() length = dataset_corr.columns print(length) result: 由上面的热图可以看出price和Age、KM呈负相关系数较大，和HP、Weight呈正相关的系数较大;注意热图中没有显示FuelType的数据，因为它是文本数据\n画个线性的图看看 f,axes = plt.subplots(2,2,figsize=(14,8)) #负相关的两个 sns.regplot(x=\u0026#39;Price\u0026#39;,y=\u0026#39;Age\u0026#39;,data=dataset,scatter_kws={\u0026#39;alpha\u0026#39;:0.5},ax=axes[0,0])#这里的x和y一定要与dataset中的列名一致 axes[0,0].set_xlabel(\u0026#39;Price\u0026#39;,fontsize=14) axes[0,0].set_ylabel(\u0026#39;Age\u0026#39;,fontsize=14) axes[0,0].yaxis.tick_left() sns.regplot(x=\u0026#39;Price\u0026#39;,y=\u0026#39;KM\u0026#39;,data=dataset,scatter_kws={\u0026#39;alpha\u0026#39;:0.5},ax=axes[0,1])#这里的x和y一定要与dataset中的列名一致 axes[0,1].set_xlabel(\u0026#39;Price\u0026#39;,fontsize=14) axes[0,1].set_ylabel(\u0026#39;KM\u0026#39;,fontsize=14) axes[0,1].yaxis.tick_right() sns.regplot(x=\u0026#39;Price\u0026#39;,y=\u0026#39;HP\u0026#39;,data=dataset,scatter_kws={\u0026#39;alpha\u0026#39;:0.5},ax=axes[1,0])#这里的x和y一定要与dataset中的列名一致 axes[1,0].set_xlabel(\u0026#39;Price\u0026#39;,fontsize=14) axes[1,0].set_ylabel(\u0026#39;HP\u0026#39;,fontsize=14) axes[1,0].yaxis.tick_left() sns.regplot(x=\u0026#39;Price\u0026#39;,y=\u0026#39;Weight\u0026#39;,data=dataset,scatter_kws={\u0026#39;alpha\u0026#39;:0.5},ax=axes[1,1])#这里的x和y一定要与dataset中的列名一致 axes[1,1].set_xlabel(\u0026#39;Price\u0026#39;,fontsize=14) axes[1,1].set_ylabel(\u0026#39;Weight\u0026#39;,fontsize=14) axes[1,1].yaxis.tick_right() result: 似乎KM的泛化能力强点\nimport warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) f,axes = plt.subplots(1,2,figsize=(14,4)) sns.distplot(dataset[\u0026#39;KM\u0026#39;],ax=axes[0])#distplot就是分布图的一种了，即hint axes[0].set_xlabel(\u0026#39;KM\u0026#39;,fontsize=14) axes[0].set_ylabel(\u0026#39;COUNT\u0026#39;,fontsize=14) axes[0].yaxis.tick_left() sns.scatterplot(x=\u0026#39;Price\u0026#39;,y=\u0026#39;KM\u0026#39;,data=dataset,ax=axes[1])#displot就是分布图的一种了，即hist axes[1].set_xlabel(\u0026#39;Price\u0026#39;,fontsize=14) axes[1].set_ylabel(\u0026#39;KM\u0026#39;,fontsize=14) axes[1].yaxis.tick_right() axes[1].yaxis.set_label_position(\u0026#34;right\u0026#34;) plt.show() result: Furltype的可视化 文本出现的次数分布图\nfrom collections import Counter fuel_count_list = Counter(dataset[\u0026#39;FuelType\u0026#39;]) fuel_count_list#返回的是一个字典 result:\nCounter({\u0026#39;Diesel\u0026#39;: 155, \u0026#39;Petrol\u0026#39;: 1264, \u0026#39;CNG\u0026#39;: 17}) code\nf,axes = plt.subplots(6,2,figsize=(14,24)) #KM的 sns.countplot(dataset[\u0026#39;FuelType\u0026#39;], ax = axes[0,0])#sns.countplot会自身统计次数，只需指定列即可 axes[0,0].set_xlabel(\u0026#39;Fuel Type\u0026#39;,fontsize=14) axes[0,0].set_ylabel(\u0026#39;Count\u0026#39;,fontsize=14) axes[0,0].yaxis.tick_left() sns.violinplot(x=\u0026#39;FuelType\u0026#39;,y=\u0026#39;Price\u0026#39;,data=dataset,ax=axes[0,1]) axes[0,1].set_xlabel(\u0026#39;FuelType\u0026#39;,fontsize=14) axes[0,1].set_ylabel(\u0026#39;Price\u0026#39;,fontsize=14) axes[0,1].yaxis.tick_right() axes[0,1].yaxis.set_label_position(\u0026#39;right\u0026#39;) #HP的 sns.distplot(dataset[\u0026#39;HP\u0026#39;],ax=axes[1,0]) axes[1,0].set_xlabel(\u0026#39;HP\u0026#39;,fontsize=14) axes[1,0].set_ylabel(\u0026#39;Count\u0026#39;,fontsize=14) axes[1,0].yaxis.tick_left() sns.scatterplot(x=\u0026#39;HP\u0026#39;,y=\u0026#39;Price\u0026#39;,ax=axes[1,1],data=dataset) axes[1,1].set_xlabel(\u0026#39;HP\u0026#39;,fontsize=14) axes[1,1].set_ylabel(\u0026#39;Count\u0026#39;,fontsize=14) axes[1,1].yaxis.tick_right() axes[1,1].yaxis.set_label_position(\u0026#39;right\u0026#39;) #MetColor sns.distplot(dataset[\u0026#39;MetColor\u0026#39;],ax=axes[2,0]) axes[2,0].set_xlabel(\u0026#39;MetColor\u0026#39;,fontsize=14) axes[2,0].set_ylabel(\u0026#39;Count\u0026#39;,fontsize=14) axes[2,0].yaxis.tick_left() sns.boxplot(x=\u0026#39;MetColor\u0026#39;,y=\u0026#39;Price\u0026#39;,ax=axes[2,1],data=dataset) axes[2,1].set_xlabel(\u0026#39;HP\u0026#39;,fontsize=14) axes[2,1].set_ylabel(\u0026#39;Count\u0026#39;,fontsize=14) axes[2,1].yaxis.tick_right() axes[2,1].yaxis.set_label_position(\u0026#39;right\u0026#39;) #Automatic的 sns.distplot(dataset[\u0026#39;Automatic\u0026#39;],ax=axes[3,0]) axes[3,0].set_xlabel(\u0026#39;Automatic\u0026#39;,fontsize=14) axes[3,0].set_ylabel(\u0026#39;Count\u0026#39;,fontsize=14) axes[3,0].yaxis.tick_left() sns.boxenplot(x=\u0026#39;Automatic\u0026#39;,y=\u0026#39;Price\u0026#39;,ax=axes[3,1],data=dataset) axes[3,1].set_xlabel(\u0026#39;Automatic\u0026#39;,fontsize=14) axes[3,1].set_ylabel(\u0026#39;Price\u0026#39;,fontsize=14) axes[3,1].yaxis.tick_right() axes[3,1].yaxis.set_label_position(\u0026#39;right\u0026#39;) #Doors sns.distplot(dataset[\u0026#39;Doors\u0026#39;],ax=axes[4,0]) axes[4,0].set_xlabel(\u0026#39;Doors\u0026#39;,fontsize=14) axes[4,0].set_ylabel(\u0026#39;Count\u0026#39;,fontsize=14) axes[4,0].yaxis.tick_left() sns.boxplot(x=\u0026#39;Doors\u0026#39;,y=\u0026#39;Price\u0026#39;,ax=axes[4,1],data=dataset) axes[4,1].set_xlabel(\u0026#39;Doors\u0026#39;,fontsize=14) axes[4,1].set_ylabel(\u0026#39;Price\u0026#39;,fontsize=14) axes[4,1].yaxis.tick_right() axes[4,1].yaxis.set_label_position(\u0026#39;right\u0026#39;) #CC sns.distplot(dataset[\u0026#39;CC\u0026#39;],ax=axes[5,0]) axes[5,0].set_xlabel(\u0026#39;CC\u0026#39;,fontsize=14) axes[5,0].set_ylabel(\u0026#39;Count\u0026#39;,fontsize=14) axes[5,0].yaxis.tick_left() sns.boxplot(x=\u0026#39;CC\u0026#39;,y=\u0026#39;Price\u0026#39;,ax=axes[5,1],data=dataset) axes[5,1].set_xlabel(\u0026#39;CC\u0026#39;,fontsize=14) axes[5,1].set_ylabel(\u0026#39;Price\u0026#39;,fontsize=14) axes[5,1].yaxis.tick_right() axes[5,1].yaxis.set_label_position(\u0026#39;right\u0026#39;) plt.show() 处理数据 搞不懂one—hot的必要性\n#one-hot下 dataset = pd.get_dummies(dataset) dataset.head() result: code:\n#提取特征训练 X = dataset.iloc[:,1:].values Y = dataset.iloc[:,0].values print(X.shape,Y.shape) result:\n(1436, 11) (1436,) code:\nY = Y.reshape(-1,1)#-1代表任意行 Y.shape result:\n(1436, 1) code:\n#划分训练集和测试集 from sklearn.model_selection import train_test_split X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3,random_state=42) #ranom_state是为了保证下次运行划分出来的trainset和testset一致 X_train.shape,X_test.shape,Y_train.shape,Y_test.shape result:\n((1005, 11), (431, 11), (1005, 1), (431, 1)) 回归拟合 from sklearn.model_selection import cross_val_score#交叉验证 from sklearn.metrics import mean_squared_error #导入MSE评价函数,均方误差 from sklearn.metrics import r2_score from sklearn.linear_model import LinearRegression from sklearn.linear_model import Ridge from sklearn.linear_model import Lasso 线性回归 Regre_linear = LinearRegression() Regre_linear.fit(X_train,Y_train) # Regre_cv_linear = cross_val_score(estimator=Regre_linear,X=X_train,y=Y_train,cv=10) result:\nLinearRegression() code:\nY_Regre_Train_predict = Regre_linear.predict(X_train) Y_Regre_Test_predict = Regre_linear.predict(X_test) #训练集的误差 r2_Regre_Train = r2_score(Y_train,Y_Regre_Train_predict) #测试集的误差 r2_Regre_Test = r2_score(Y_test,Y_Regre_Test_predict) #均方根误差 Regre_Test_RMSE = np.sqrt(mean_squared_error(Y_test,Y_Regre_Test_predict)) print(\u0026#39;R2 of train:\u0026#39;,r2_Regre_Train) print(\u0026#39;R2 of test:\u0026#39;,r2_Regre_Test) print(\u0026#39;RMSE of test:\u0026#39;,Regre_Test_RMSE) result:\nR2 of train: 0.8699125610462864 R2 of test: 0.8646349374190103 RMSE of test: 1372.796968700995 Regre_linear = LinearRegression() Regre_linear.fit(X_train,Y_train) Regre_cv_linear = cross_val_score(estimator=Regre_linear,X=X_train,y=Y_train,cv=5) Y_Regre_Train_predict = Regre_linear.predict(X_train) Y_Regre_Test_predict = Regre_linear.predict(X_test) #训练集的误差 r2_Regre_Train = r2_score(Y_train,Y_Regre_Train_predict) #测试集的误差 r2_Regre_Test = r2_score(Y_test,Y_Regre_Test_predict) #均方根误差 Regre_Test_RMSE = np.sqrt(mean_squared_error(Y_test,Y_Regre_Test_predict)) print(\u0026#39;R2 of train:\u0026#39;,r2_Regre_Train) print(\u0026#39;R2 of test:\u0026#39;,r2_Regre_Test) print(\u0026#39;RMSF of test:\u0026#39;,Regre_Test_RMSE) result:\nR2 of train: 0.8699125610462864 R2 of test: 0.8646349374190103 RMSF of test: 1372.796968700995 发现了个问题自己对交叉验证好像有误解，因为上面弄了cv的和没有cv的误差没区别,回过头再解决\n二阶回归 from sklearn.preprocessing import PolynomialFeatures two_Regre = PolynomialFeatures(degree=2) X_two_train = two_Regre.fit_transform(X_train)#预处理数据 two_Regre.fit(X_two_train,Y_train)#这是对输入数据的fit，并非真正的训练 two_Regression = LinearRegression() two_Regression.fit(X_two_train,Y_train) #就是说每次训练都要调用two_Regre处理下数据，再用two_Regression训练 result:\nLinearRegression() code:\nY2_Regre_Train_pred = two_Regression.predict(two_Regre.fit_transform(X_train)) Y2_Regre_Test_pred = two_Regression.predict(two_Regre.fit_transform(X_test)) R2_Regre_Train = r2_score(Y_train,Y2_Regre_Train_pred) R2_Regre_Test = r2_score(Y_test,Y2_Regre_Test_pred) Regre2_Test_RMSE = np.sqrt(mean_squared_error(Y_test,Y2_Regre_Test_pred)) print(\u0026#39;R2 of train:\u0026#39;,R2_Regre_Train) print(\u0026#39;R2 of test:\u0026#39;,R2_Regre_Test) print(\u0026#39;RMSF of test:\u0026#39;,Regre2_Test_RMSE) result:\nR2 of train: 0.9164400386560799 R2 of test: 0.8040200419182072 RMSF of test: 1651.8052707170002 可以看出对于训练集的准确率提高了，但是测试集的准确率下降了，可能是与训练样本过于拟合了，泛化能力下降了，后面加个惩罚看看即正则化\nRidge 回归/L2正则化 from sklearn.linear_model import Ridge from sklearn.preprocessing import PolynomialFeatures from sklearn.preprocessing import StandardScaler #用来标准化数据 from sklearn.pipeline import Pipeline #算法流，这个在后期一些的模型选择grid比较有用,简单来说就是可以将一个机器学习的过程封装到一个容器里 pipe_steps = [ (\u0026#39;scalar\u0026#39;,StandardScaler()), (\u0026#39;poly\u0026#39;,PolynomialFeatures(degree=3)), (\u0026#39;model\u0026#39;,Ridge(alpha=1500,fit_intercept=True))#alpha随便调一个 ] ridge = Pipeline(pipe_steps) ridge.fit(X_train,Y_train) result: code\nY_Ridge_Train_pred = ridge.predict(X_train) Y_Ridge_Test_pred = ridge.predict(X_test) R2_Ridge_Train = r2_score(Y_train,Y_Ridge_Train_pred) R2_Ridge_Test = r2_score(Y_test,Y_Ridge_Test_pred) YRidge_Test_RMSE = np.sqrt(mean_squared_error(Y_test,Y_Ridge_Test_pred)) print(\u0026#39;R2 of train:\u0026#39;,R2_Ridge_Train) print(\u0026#39;R2 of test:\u0026#39;,R2_Ridge_Test) print(\u0026#39;RMSF of test:\u0026#39;,YRidge_Test_RMSE) result: 可以看出相比简单的线性回归，测试和训练的准确都提高了，相比二次回归测试的准确提高了，泛化能力提升了一点，接下来看看L1正则化的效果\nL1正则化/Lasso回归 from sklearn.linear_model import Lasso Lasso_pipe_steps = [ (\u0026#39;scalar\u0026#39;,StandardScaler()), (\u0026#39;poly\u0026#39;,PolynomialFeatures(degree=3)), (\u0026#39;model\u0026#39;,Lasso(alpha=2.2,fit_intercept=True,tol=0.01))#alpha随便调一个,tol收敛也是 ] Lasso_pipe = Pipeline(Lasso_pipe_steps) Lasso_pipe.fit(X_train,Y_train) result: code:\nY_Lasso_Train_pred = Lasso_pipe.predict(X_train) Y_Lasso_Test_pred = Lasso_pipe.predict(X_test) R2_Lasso_Train = r2_score(Y_train,Y_Lasso_Train_pred) R2_Lasso_Test = r2_score(Y_test,Y_Lasso_Test_pred) YLasso_Test_RMSE = np.sqrt(mean_squared_error(Y_test,Y_Lasso_Test_pred)) print(\u0026#39;R2 of train:\u0026#39;,R2_Lasso_Train) print(\u0026#39;R2 of test:\u0026#39;,R2_Lasso_Test) print(\u0026#39;RMSF of test:\u0026#39;,YLasso_Test_RMSE) result: 可以看出两者的准确率都提高了，还算可以的\n模型评估可视化 DataFrame_model = [ (\u0026#39;Linear Regression\u0026#39;,r2_Regre_Train,r2_Regre_Test,Regre_Test_RMSE), (\u0026#39;2Poly Regression\u0026#39;,R2_Regre_Train,R2_Regre_Test,Regre2_Test_RMSE), (\u0026#39;Ridge Regression\u0026#39;,R2_Ridge_Train,R2_Ridge_Test,YRidge_Test_RMSE), (\u0026#39;Lasso Regression\u0026#39;,R2_Lasso_Train,R2_Lasso_Test,YLasso_Test_RMSE) ] model_data_show = pd.DataFrame(data=DataFrame_model,columns=[\u0026#39;Model\u0026#39;, \u0026#39;R2_Score(training)\u0026#39;, \u0026#39;R2_Score(test)\u0026#39;,\u0026#39;RMSE\u0026#39;]) model_data_show result: code:\nf,axes = plt.subplots(1,1,figsize=(20,8)) model_data_show.sort_values(by=[\u0026#39;RMSE\u0026#39;])#pandas的sort_values sns.barplot(x=\u0026#39;RMSE\u0026#39;,y=\u0026#39;Model\u0026#39;,data=model_data_show,ax=axes) axes.set_xlabel(\u0026#39;RMSE\u0026#39;,fontsize=18) axes.set_ylabel(\u0026#39;Model\u0026#39;,fontsize=18) plt.show() result: 懒得写了结论了，问题还蛮大的，感觉还是云里雾里的，继续炼吧\n","date":"2021-09-01T00:00:00Z","permalink":"https://example.com/p/myregressioncode2/","title":"myRegressioncode2"},{"content":"机器学习练习 - 线性回归 代码修改并注释：黄海广，haiguang2000@wzu.edu.cn\n单变量线性回归 import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib.pyplot as plt plt.rcParams[\u0026#39;font.sans-serif\u0026#39;]=[\u0026#39;SimHei\u0026#39;] #用来正常显示中文标签 plt.rcParams[\u0026#39;axes.unicode_minus\u0026#39;]=False #用来正常显示负号 path = \u0026#39;data/regress_data1.csv\u0026#39; data = pd.read_csv(path) data.head() result: code:\ndata.describe() result: 看下数据长什么样子\ncode:\ndata.plot(kind=\u0026#39;scatter\u0026#39;, x=\u0026#39;人口\u0026#39;, y=\u0026#39;收益\u0026#39;, figsize=(12,8)) plt.xlabel(\u0026#39;人口\u0026#39;, fontsize=18) plt.ylabel(\u0026#39;收益\u0026#39;, rotation=0, fontsize=18) plt.show() result: 现在让我们使用梯度下降来实现线性回归，以最小化代价函数。\n首先，我们将创建一个以参数$w$为特征函数的代价函数 $$J\\left( w \\right)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}{{{\\left( {{h}}\\left( {{x}^{(i)}} \\right)-{{y}^{(i)}} \\right)}^{2}}}$$ 其中：$${{h}}\\left( x \\right)={{w}^{T}}X={{w }{0}}{{x}{0}}+{{w }{1}}{{x}{1}}+{{w }{2}}{{x}{2}}+\u0026hellip;+{{w }{n}}{{x}{n}}$$ code:\ndef computeCost(X, y, w): inner = np.power(((X * w.T) - y), 2)# (m,n) @ (n, 1) -\u0026gt; (n, 1) # return np.sum(inner) / (2 * len(X)) return np.sum(inner) / (2 * X.shape[0]) 让我们在训练集中添加一列，以便我们可以使用向量化的解决方案来计算代价和梯度。\ndata.insert(0, \u0026#39;Ones\u0026#39;, 1) data result: 现在我们来做一些变量初始化。\n# set X (training data) and y (target variable) cols = data.shape[1] X = data.iloc[:,:cols-1]#X是所有行，去掉最后一列 y = data.iloc[:,cols-1:]#X是所有行，最后一列 观察下 X (训练集) and y (目标变量)是否正确.\ncode:\nX.head()#head()是观察前5行 result: code:\ny.head() result: 代价函数是应该是numpy矩阵，所以我们需要转换X和Y，然后才能使用它们。 我们还需要初始化w。\nX = np.matrix(X.values) y = np.matrix(y.values) w = np.matrix(np.array([0,0])) w 是一个(1,2)矩阵\ncode:\nw result:\nmatrix([[0, 0]]) 看下维度\ncode:\nX.shape, w.shape, y.shape result:\n((97, 2), (1, 2), (97, 1)) 计算代价函数 (theta初始值为0).\ncomputeCost(X, y, w) result:\n32.072733877455676 Batch Gradient Decent（批量梯度下降） $${{w }{j}}:={{w }{j}}- \\alpha \\frac{1}{m}\\sum\\limits_{i=1}^m \\frac{\\partial }{\\partial {{w}_{j}}}J\\left( w \\right)$$\ndef batch_gradientDescent(X, y, w, alpha, iters): temp = np.matrix(np.zeros(w.shape)) parameters = int(w.ravel().shape[1]) cost = np.zeros(iters) for i in range(iters): error = (X * w.T) - y for j in range(parameters): term = np.multiply(error, X[:, j]) temp[0, j] = w[0, j] - ((alpha / len(X)) * np.sum(term)) w = temp cost[i] = computeCost(X, y, w) return w, cost 初始化一些附加变量 - 学习速率α和要执行的迭代次数。\nalpha = 0.01 iters = 1000 现在让我们运行梯度下降算法来将我们的参数θ适合于训练集。\ng, cost = batch_gradientDescent(X, y, w, alpha, iters) g result:\nmatrix([[-3.24140214, 1.1272942 ]]) 最后，我们可以使用我们拟合的参数计算训练模型的代价函数（误差）。\ncomputeCost(X, y, g) result:\n4.515955503078914 现在我们来绘制线性模型以及数据，直观地看出它的拟合。\nx = np.linspace(data[\u0026#39;人口\u0026#39;].min(), data[\u0026#39;人口\u0026#39;].max(), 100) f = g[0, 0] + (g[0, 1] * x) fig, ax = plt.subplots(figsize=(12, 8)) ax.plot(x, f, \u0026#39;r\u0026#39;, label=\u0026#39;预测值\u0026#39;) ax.scatter(data[\u0026#39;人口\u0026#39;], data[\u0026#39;收益\u0026#39;], label=\u0026#39;训练数据\u0026#39;) ax.legend(loc=2) ax.set_xlabel(\u0026#39;人口\u0026#39;, fontsize=18) ax.set_ylabel(\u0026#39;收益\u0026#39;, rotation=0, fontsize=18) ax.set_title(\u0026#39;预测收益和人口规模\u0026#39;, fontsize=18) plt.show() result: 由于梯度方程式函数也在每个训练迭代中输出一个代价的向量，所以我们也可以绘制。 请注意，代价总是降低 - 这是凸优化问题的一个例子。\nfig, ax = plt.subplots(figsize=(12, 8)) ax.plot(np.arange(iters), cost, \u0026#39;r\u0026#39;) ax.set_xlabel(\u0026#39;迭代次数\u0026#39;, fontsize=18) ax.set_ylabel(\u0026#39;代价\u0026#39;, rotation=0, fontsize=18) ax.set_title(\u0026#39;误差和训练Epoch数\u0026#39;, fontsize=18) plt.show() 多变量线性回归 练习还包括一个房屋价格数据集，其中有2个变量（房子的大小，卧室的数量）和目标（房子的价格）。 我们使用我们已经应用的技术来分析数据集。\ncode:\npath = \u0026#39;data/regress_data2.csv\u0026#39; data2 = pd.read_csv(path) data2.head() result: 对于此任务，我们添加了另一个预处理步骤 - 特征归一化。 这个对于pandas来说很简单\ndata2 = (data2 - data2.mean()) / data2.std() data2.head() result: 现在我们重复第1部分的预处理步骤，并对新数据集运行线性回归程序。\n# add ones column data2.insert(0, \u0026#39;Ones\u0026#39;, 1) # set X (training data) and y (target variable) cols = data2.shape[1] X2 = data2.iloc[:,0:cols-1] y2 = data2.iloc[:,cols-1:cols] # convert to matrices and initialize theta X2 = np.matrix(X2.values) y2 = np.matrix(y2.values) w2 = np.matrix(np.array([0,0,0])) # perform linear regression on the data set g2, cost2 = batch_gradientDescent(X2, y2, w2, alpha, iters) # get the cost (error) of the model computeCost(X2, y2, g2) result:\n0.13070336960771892 我们也可以快速查看这一个的训练进程。\nfig, ax = plt.subplots(figsize=(12,8)) ax.plot(np.arange(iters), cost2, \u0026#39;r\u0026#39;) ax.set_xlabel(\u0026#39;迭代次数\u0026#39;, fontsize=18) ax.set_ylabel(\u0026#39;代价\u0026#39;, rotation=0, fontsize=18) ax.set_title(\u0026#39;误差和训练Epoch数\u0026#39;, fontsize=18) plt.show() result: 我们也可以使用scikit-learn的线性回归函数，而不是从头开始实现这些算法。 我们将scikit-learn的线性回归算法应用于第1部分的数据，并看看它的表现。 code:\nfrom sklearn.linear_model import LinearRegression model = LinearRegression() model.fit(X, y) result:\nLinearRegression() scikit-learn model的预测表现\nx = np.array(X[:, 1].A1) f = model.predict(X).flatten() fig, ax = plt.subplots(figsize=(12, 8)) ax.plot(x, f, \u0026#39;r\u0026#39;, label=\u0026#39;预测值\u0026#39;) ax.scatter(data[\u0026#39;人口\u0026#39;], data[\u0026#39;收益\u0026#39;], label=\u0026#39;训练数据\u0026#39;) ax.legend(loc=2, fontsize=18) ax.set_xlabel(\u0026#39;人口\u0026#39;, fontsize=18) ax.set_ylabel(\u0026#39;收益\u0026#39;, rotation=0, fontsize=18) ax.set_title(\u0026#39;预测收益和人口规模\u0026#39;, fontsize=18) plt.show() result: $L_2$正则化 $J ( { w } ) = \\frac { 1 } { 2 } \\sum _ { i = 1 } ^ { m } ( h _ { w} ( x ^ { ( i ) } ) - y ^ { ( i ) } ) ^ { 2 } + \\lambda \\sum _ { j = 1 } ^ { n } w_ { j } ^ { 2 }$，此时称作Ridge Regression：\nfrom sklearn.linear_model import Ridge model = Ridge() model.fit(X, y) result:\nRidge() code\nx2 = np.array(X[:, 1].A1) f2 = model.predict(X).flatten() fig, ax = plt.subplots(figsize=(12, 8)) ax.plot(x2, f2, \u0026#39;r\u0026#39;, label=\u0026#39;预测值Ridge\u0026#39;) ax.scatter(data[\u0026#39;人口\u0026#39;], data[\u0026#39;收益\u0026#39;], label=\u0026#39;训练数据\u0026#39;) ax.legend(loc=2, fontsize=18) ax.set_xlabel(\u0026#39;人口\u0026#39;, fontsize=18) ax.set_ylabel(\u0026#39;收益\u0026#39;, rotation=0, fontsize=18) ax.set_title(\u0026#39;预测收益和人口规模\u0026#39;, fontsize=18) plt.show() result: $L_1$正则化： $J ( {w } ) = \\frac { 1 } { 2 } \\sum _ { i = 1 } ^ { m } ( h _ { w} ( x ^ { ( i ) } ) - y ^ { ( i ) } ) ^ { 2 } + \\lambda \\sum _ { j = 1 } ^ { n } | w _ { j } |$，此时称作Lasso Regression\ncode:\nfrom sklearn.linear_model import Lasso model = Lasso() model.fit(X, y) result:\nLasso() code:\nx3= np.array(X[:, 1].A1) f3 = model.predict(X).flatten() fig, ax = plt.subplots(figsize=(12, 8)) ax.plot(x3, f3, \u0026#39;r\u0026#39;, label=\u0026#39;预测值Lasso\u0026#39;) ax.scatter(data[\u0026#39;人口\u0026#39;], data[\u0026#39;收益\u0026#39;], label=\u0026#39;训练数据\u0026#39;) ax.legend(loc=2, fontsize=18) ax.set_xlabel(\u0026#39;人口\u0026#39;, fontsize=18) ax.set_ylabel(\u0026#39;收益\u0026#39;, rotation=0, fontsize=18) ax.set_title(\u0026#39;预测收益和人口规模\u0026#39;, fontsize=18) plt.show() result: 调参 from sklearn.model_selection import cross_val_score alphas = np.logspace(-3, 2, 50) test_scores = [] for alpha in alphas: clf = Ridge(alpha) test_score = np.sqrt(-cross_val_score(clf, X, y, cv=5, scoring=\u0026#39;neg_mean_squared_error\u0026#39;)) test_scores.append(np.mean(test_score)) import matplotlib.pyplot as plt plt.plot(alphas, test_scores) plt.title(\u0026#34;Alpha vs CV Error\u0026#34;); plt.show() result: 最小二乘法(LSM)： 最小二乘法的需要求解最优参数$w^{*}$：\n已知：目标函数\n$J\\left( w \\right)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}{{{\\left( {h}\\left( {x^{(i)}} \\right)-{y^{(i)}} \\right)}^{2}}}$\n其中：${h}\\left( x \\right)={w^{T}}X={w_{0}}{x_{0}}+{w_{1}}{x_{1}}+{w_{2}}{x_{2}}+\u0026hellip;+{w_{n}}{x_{n}}$\n将向量表达形式转为矩阵表达形式，则有$J(w )=\\frac{1}{2}{{\\left( Xw -y\\right)}^{2}}$ ，其中$X$为$m$行$n+1$列的矩阵（$m$为样本个数，$n$为特征个数），$w$为$n+1$行1列的矩阵(包含了$w_0$)，$y$为$m$行1列的矩阵，则可以求得最优参数$w^{*} ={{\\left( {X^{T}}X \\right)}^{-1}}{X^{T}}y$\n梯度下降与最小二乘法的比较：\n梯度下降：需要选择学习率$\\alpha$，需要多次迭代，当特征数量$n$大时也能较好适用，适用于各种类型的模型\n最小二乘法：不需要选择学习率$\\alpha$，一次计算得出，需要计算${{\\left( {{X}^{T}}X \\right)}^{-1}}$，如果特征数量$n$较大则运算代价大，因为矩阵逆的计算时间复杂度为$O(n^3)$，通常来说当$n$小于10000 时还是可以接受的，只适用于线性模型，不适合逻辑回归模型等其他模型\ndef LSM(X, y): w = np.linalg.inv(X.T@X)@X.T@y#X.T@X等价于X.T.dot(X) return w final_w2=LSM(X, y)#感觉和批量梯度下降的theta的值有点差距 final_w2 result:\nmatrix([[-3.89578088], [ 1.19303364]]) #梯度下降得到的结果是matrix([[-3.24140214, 1.1272942 ]]) 参考 机器学习，吴恩达 《统计学习方法》，李航 ","date":"2021-09-01T00:00:00Z","permalink":"https://example.com/p/wzu_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/","title":"WZU_线性回归代码学习记录"},{"content":"数据读取与训练集测试集划分 import pandas as pd import numpy as np import matplotlib import matplotlib.pyplot as plt import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) matplotlib.style.use(\u0026#34;ggplot\u0026#34;) from sklearn.linear_model import LinearRegression from sklearn import datasets data = pd.read_csv(\u0026#34;./CCPP/Folds5x2_pp.csv\u0026#34;) data.head() result: code:\nfrom sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=22) print(X_train.shape) print(X_test.shape) print(y_train.shape) print(y_test.shape) result:\n(7176, 4) (2392, 4) (7176, 1) (2392, 1) 用sklearn运行Ridge回归 要运行Ridge回归，我们必须要指定超参数α。你也许会问：“我也不知道超参数是多少啊？” 我也不知道，那么我们随机指定一个(比如1)，后面我们会讲到用交叉验证从多个输入超参数α中快速选择最优超参数的办法。\nfrom sklearn.linear_model import Ridge ridge = Ridge(alpha=1) ridge.fit(X_train,y_train) result:\nRidge(alpha=1) code:\nprint(ridge.intercept_) print(ridge.coef_) result:\n[450.17908215] [[-1.95982132 -0.23677544 0.06592191 -0.15350214]] 也就是说我们得到的模型是：\nPE=447.05552892−1.97373209∗AT−0.2323016∗V+0.06935852∗AP−0.15806479∗RH\n但是这样还没有完？为什么呢，因为我们假设了超参数α为1， 实际上我们并不知道超参数α取多少最好，实际研究是需要在多组自选的α中选择一个最优的。\n那么我们是不是要把上面这段程序在N种α的值情况下，跑N遍，然后再比较结果的优劣程度呢？ 可以这么做，但是scikit-learn提供了另外一个交叉验证选择最优α的API，下面我们就用这个API来选择α。\n用scikit-learn选择Ridge回归超参数α from sklearn.linear_model import RidgeCV #就是说用RidgeCV可以传入多个alpha的值 ridgecv = RidgeCV(alphas=[0.01,0.1,0.5,1,3,5,6,10,20,100]) ridgecv.fit(X_train,y_train) ridgecv.alpha_ result:\n6.0 输出结果为：6.0，说明在我们给定的这组超参数中， 6是最优的α值。\n用scikit-learn研究超参数α和回归系数θ的关系 通过Ridge回归的损失函数表达式可以看到，α越大，那么正则项惩罚的就越厉害，得到回归系数θ就越小，最终趋近与0。而如果α越小，即正则化项越小，那么回归系数θ就越来越接近于普通的线性回归系数。\n这里我们用scikit-learn来研究这种Ridge回归的变化，例子参考了scikit-learn的官网例子。\n我们自己生成一个10x10的矩阵X，表示一组有10个样本，每个样本有10个特征的数据。生成一个10x1的向量y代表样本输出。\ncode:\nX = 1. / (np.arange(1,11) + np.arange(0,10)[:,np.newaxis]) y = np.ones(10) X[:10] result: 这样我们的数据有了，接着就是准备超参数α了。我们准备了200个超参数，来分别跑 Ridge回归。准备这么多的目的是为了后面画图看α和θ的关系`\ncode:\nn_alphas = 200 #logspace 对数等比数列 alphas = np.logspace(-10,-2,n_alphas) 有了这200个超参数α，我们做200次循环，分别求出各个超参数对应的θ(10个维度)，存起来后面画图用。\nfrom sklearn.linear_model import Ridge clf = Ridge(fit_intercept=False) coefs = [] for i in alphas: #设置本次循环的超参数 clf.set_params(alpha=i) #针对每个alpha做ridge回归 clf.fit(X,y) # 把每一个超参数alpha对应的theta存下来 coefs.append(clf.coef_) 好了，有了200个超参数α，以及对应的θ，我们可以画图了。我们的图是以α为x轴，θ的10个维度为y轴画的。代码如下：\nax = plt.gca() ax.plot(alphas,coefs) #将alpha的值取对数便于画图 set_xscale设置x轴比例 ax.set_xscale(\u0026#34;log\u0026#34;) #翻转x轴的大小方向，让alpha从大到小显示 ax.set_xlim(ax.get_xlim()[::-1]) plt.xlabel(\u0026#34;alpha\u0026#34;) plt.ylabel(\u0026#34;weights\u0026#34;) plt.title(\u0026#34;Ridge coefficients as a function of the regularization\u0026#34;) # plt.axis(\u0026#34;tight\u0026#34;) #按照图形的内容自动收紧坐标轴，不留空白区域 ax.autoscale(tight=True) plt.show() result: 从图上也可以看出，当α比较大，接近于10−2的时候，θ的10个维度都趋于0。而当α比较小，接近于10−10的时候，θ的10个维度都趋于线性回归的回归系数。\n","date":"2021-09-01T00:00:00Z","permalink":"https://example.com/p/%E7%94%A8scikit-learn%E5%92%8Cpandas%E5%AD%A6%E4%B9%A0ridge%E5%9B%9E%E5%BD%92/","title":"用scikit-learn和pandas学习Ridge回归"},{"content":"pandas来读取数据 import pandas as pd import numpy as np import matplotlib import matplotlib.pyplot as plt matplotlib.style.use(\u0026#34;ggplot\u0026#34;) plt.rcParams[\u0026#34;font.sans-serif\u0026#34;] = \u0026#34;SimHei\u0026#34; plt.rcParams[\u0026#34;axes.unicode_minus\u0026#34;] = False from sklearn import datasets,linear_model import warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) data = pd.read_csv(\u0026#34;./CCPP/Folds5x2_pp.csv\u0026#34;) data.head() result: 准备运行算法的数据 data.shape result:\n(9568, 5) 结果是(9568, 5)。说明我们有9568个样本，每个样本有5列。\n现在我们开始准备样本特征X，我们用AT， V，AP和RH这4个列作为样本特征。\ncode:\nX = data[[\u0026#34;AT\u0026#34;,\u0026#34;V\u0026#34;,\u0026#34;AP\u0026#34;,\u0026#34;RH\u0026#34;]] X.head() result: code:\ny = data[[\u0026#34;PE\u0026#34;]] y.head result: 划分训练集和测试集 from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1) print(X_train.shape) print(X_test.shape) print(y_train.shape) print(y_test.shape) #可以看到75%的样本数据被作为训练集，25%的样本被作为测试集。 result: 运行scikit-learn的线性模型 scikit-learn的线性回归算法使用的是最小二乘法来实现的。\nfrom sklearn.linear_model import LinearRegression linreg = LinearRegression() linreg.fit(X_train,y_train) result:\nLinearRegression() 拟合完毕后，我们看看我们的需要的模型系数结果：\nprint(linreg.intercept_) print(linreg.coef_) result:\n[447.06297099] [[-1.97376045 -0.23229086 0.0693515 -0.15806957]] 这样我们就得到了在步骤1里面需要求得的5个值。也就是说PE和其他4个变量的关系如下：\n模型评估 我们需要评估我们的模型的好坏程度，对于线性回归来说，我们一般用均方差（Mean Squared Error, MSE）或者均方根差(Root Mean Squared Error, RMSE)在测试集上的表现来评价模型的好坏\n#模型拟合测试集 y_pred = linreg.predict(X_test) from sklearn import metrics #用sklearn计算MSE print(\u0026#34;MSE:\u0026#34;,metrics.mean_squared_error(y_test,y_pred)) #用sklearn计算RMSE import numpy as np print(\u0026#34;RMSE:\u0026#34;,np.sqrt(metrics.mean_squared_error(y_test,y_pred))) result:\nMSE: 20.08040120207389 RMSE: 4.481116066570235 得到了MSE或者RMSE，如果我们用其他方法得到了不同的系数，需要选择模型时，就用MSE小的时候对应的参数。\n比如这次我们用AT， V，AP这3个列作为样本特征。不要RH， 输出仍然是PE\ndata.head() result: code:\nX = data[[\u0026#34;AT\u0026#34;,\u0026#34;V\u0026#34;,\u0026#34;AP\u0026#34;]] y = data[\u0026#34;PE\u0026#34;] X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=22) linreg2 = LinearRegression() linreg2.fit(X_train,y_train) y_pred = linreg2.predict(X_test) print(\u0026#34;MSE:\u0026#34;,metrics.mean_squared_error(y_pred,y_test)) print(\u0026#34;RMSE:\u0026#34;,np.sqrt(metrics.mean_squared_error(y_test,y_pred))) result:\nMSE: 24.05110701434769 RMSE: 4.9041927994673795 可以看出，去掉RH后，模型拟合的没有加上RH的好，MSE变大了。\n交叉验证 我们可以通过交叉验证来持续优化模型，代码如下，我们采用10折交叉验证，即cross_val_predict中的cv参数为10：\nX = data[[\u0026#34;AT\u0026#34;,\u0026#34;V\u0026#34;,\u0026#34;AP\u0026#34;,\u0026#34;RH\u0026#34;]] y = data[[\u0026#34;PE\u0026#34;]] from sklearn.model_selection import cross_val_predict #cross_val_predict 返回所有的预测结果 predicted = cross_val_predict(linreg,X,y,cv=10) predicted[:10] result: code:\npredicted.shape result:\n(9568, 1) code:\ny.shape result:\n(9568, 1) code:\nprint(\u0026#34;MSE:\u0026#34;,metrics.mean_squared_error(y,predicted)) print(\u0026#34;RMSE:\u0026#34;,np.sqrt(metrics.mean_squared_error(y,predicted))) result:\nMSE: 20.795597461943103 RMSE: 4.560219014690315 可以看出，采用交叉验证模型的MSE比第6节的大，主要原因是我们这里是对所有折的样本做测试集对应的预测值的MSE，而第6节仅仅对25%的测试集做了MSE。两者的先决条件并不同。\n画图观察结果 这里画图真实值和预测值的变化关系，离中间的直线y=x直接越近的点代表预测损失越低\nfig,ax = plt.subplots() ax.scatter(y,predicted) ax.plot([y.min(),y.max()],[predicted.min(),predicted.max()],\u0026#34;k--\u0026#34;,lw=5) ax.set_xlabel(\u0026#34;Measured\u0026#34;) ax.set_ylabel(\u0026#34;Predicted\u0026#34;) plt.figure(figsize=(15,5)) plt.show() result: ","date":"2021-09-01T00:00:00Z","permalink":"https://example.com/p/%E7%94%A8scikit-learn%E5%92%8Cpandas%E5%AD%A6%E4%B9%A0%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/","title":"用scikit-learn和pandas学习线性回归"},{"content":"biopython官方地址：https://biopython.org/\ngithub地址：https://github.com/biopython/biopython/blob/master/\n中文版教程：https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html\nbiopython包的说明（具体到每个模块了）：https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html\n解析器的设计 过去很多Biopython解析器都是根据面向事件设计出来的，包括Scanner和Consumer。\nScanners是将输入的数据源进行逐行分析，只要识别出数据中的信息就会发送一个事件。 例如，如果数据中包含物种名信息，Scanner只要读到某行包含名称信息时就会产生一个 organism_name 事件。\nConsumers是用来接收Scanners所发出事件的对象。 接着上面的例子，当Consumer收到了 organism_name 事件，在当前应用程序中无论以何种方式都会运行。\n这是一个非常灵活的构架，如果你想要将一个文件解析成多种其他格式的，这将会很有优势。 例如， Bio.GenBank 模块可以运用这种方式构建 SeqRecord 或者其他独特的文件格式记录对象。\n最近，很多添加了 Bio.SeqIO 和 Bio.AlignIO 的解析器使用了一种更为简单的方法， 但是只能产生单一形式的文件格式（分别是 SeqRecord and MultipleSeqAlignment ）。 在某些情况，Bio.SeqIO 解析器实际上包含了另一种Biopython解析器 - 例如， Bio.SwissProt 解析器产生了特定的SwissProt格式对象，又转换成了 SeqRecord 格式对象。\n替换矩阵 SubsMat 这个模块提供了一个类和一些固定的方法来产生替换矩阵，类似于BLOSUM或者PAM矩阵，但是是基于用户提供的数据。 此外，你还可以从已建立的替换矩阵集合MatrixInfo.py中选择一个矩阵。 SeqMat 类来自于一个字典（dictionary）:\nclass SeqMat(dict) 这个字典的格式是 {(i1,j1):n1, (i1,j2):n2,\u0026hellip;,(ik,jk):nk} ， i和j是字母集，而n是一个值。\n属性 self.alphabet: Bio.Alphabet中定义的一个类 self.ab_list: 排列好的字母列表。主要是内部需求。 方法 __init__(self,data=None,alphabet=None, mat_name=\u0026#39;\u0026#39;, build_later=0): data: 可以是一个字典，也可以是另一个SeqMat实例。 alphabet: 一个Bio.Alphabet的实例。如果没有提供，将从数据构建一个alphabet。 mat_name: 矩阵名，例如 BLOSUM62 或者 PAM250 build_later: 默认值为false。如果为true，用户应该只提供alphabet和空字典。如果想要之后再构建矩阵，这样会跳过alphabet大小和矩阵大小的检查。 entropy(self,obs_freq_mat) obs_freq_mat: 一个观测频率矩阵。基于“obs_freq_mat”的频率返回矩阵的熵值。矩阵实例须为LO或者SUBS。 sum(self) 计算矩阵的字母表中每个字母值的总和，返回值是字典的形式 {i1: s1, i2: s2,\u0026hellip;,in:sn}, 其中:\ni: 一个字母; s: 半矩阵中某个字母值的总和; n: alphabet中字母的个数。 print_mat(self,f,format=\u0026#34;%4d\u0026#34;,bottomformat=\u0026#34;%4s\u0026#34;,alphabet=None) 将矩阵打印到文件句柄f。 format 是矩阵值的格式； bottomformat 是底部行（矩阵字母）的格式。下面是一个3字母矩阵的例子：\nA 23 B 12 34 C 7 22 27 A B C alphabet 可选参数是alphabet中所有字符的一个字符串。如果用户提供了数据，则轴上字母的顺序是根据字符串中的顺序，而不是字母表的顺序。\n用法\n安排下面这部分是因为大多数读者希望知道如何产生一个对数机率矩阵（log-odds matrix）。 当然，也可以生成和研究过渡矩阵（log-odds matrix）。 但是大部分的人只是想要一个对数机率矩阵，仅此而已。\n产生一个可接受的替代矩阵首先，你应该从数据中产生出一个可接受替代矩阵（accepted replacement matrix，ARM）。 ARM中的数值是根据你的数据中替换的个数决定的。数据可以是一对或者多对的序列比对结果。 例如，丙氨酸被半胱氨酸替换了10次，而半胱氨酸被丙氨酸替换了12次，其相对应的ARM为： (\u0026#39;A\u0026#39;,\u0026#39;C\u0026#39;): 10, (\u0026#39;C\u0026#39;,\u0026#39;A\u0026#39;): 12 由于顺序并不重要，用户也可以只用一个输入:\n(\u0026#39;A\u0026#39;,\u0026#39;C\u0026#39;): 22 一个SeqMat实例的初始化可以用全矩阵（第一种计数方法：10,12），也可以用半矩阵（后一种方法，22）。 一个蛋白字母全矩阵的大小应该是20x20 = 400。而一个这样的半矩阵大小是20x20/2 + 20/2 = 210。 这是因为相同字母的输入并没有改变（矩阵的对角线）。如果一个大小为N的alphabet：\n全矩阵大小: N*N 半矩阵大小: N(N+1)/2 如果传递的是全矩阵，SeqMat的构造函数会自动产生半矩阵。 如果传递的是半矩阵，则键的字母将按照字母表顺序排列(‘A’,’C’)，而不是(‘C’,’A’)。\n讲到这里，如果你想知道的仅仅只是怎样产生一个对数机率矩阵的话，请直接看用法示例那个章节。对于想要更加深入地知道核苷酸/氨基酸频率数据的读者，接下来要讲的是内部函数的细节。 生成观测频率矩阵(observed frequency matrix，OFM) 用法： OFM = SubsMat._build_obs_freq_mat(ARM) OFM是由ARM产生的，只是将替换的个数换成了替换频率。 3. 生成期望频率矩阵(expected frequency matrix，EFM) 用法\nEFM = SubsMat._build_exp_freq_mat(OFM,exp_freq_table) exp_freq_table: 为一个FreqTable的实例。有关FreqTable更多信息请见第 20.2.2 节。简单地说，期望频率表表示字母表中每个元素显示的频率。这个表相当于一个字典，字母是键，字母对应的频率是值。总和为1。 期望频率表可以（理论上说也应该可以）从OFM得到。所以大多数情况你可以用下面的代码产生 exp_freq_table: \u0026gt;\u0026gt;\u0026gt; exp_freq_table = SubsMat._exp_freq_table_from_obs_freq(OFM) \u0026gt;\u0026gt;\u0026gt; EFM = SubsMat._build_exp_freq_mat(OFM,exp_freq_table) 如果需要，你也可以使用自己提供的 exp_freq_table 。 4. 生成替换频率矩阵(substitution frequency matrix，SFM) 用法:\nSFM = SubsMat._build_subs_mat(OFM,EFM) 使用观察频率矩阵(OFM)和期望频率矩阵(EFM)，得到相应值的除法结果。 5. 生成对数机率矩阵(log-odds matrix， LOM) 用法:\nLOM=SubsMat._build_log_odds_mat(SFM[,logbase=10,factor=10.0,round_digit=1]) 使用替换频率矩阵(SFM)。 logbase: 用来产生对数机率值的对数的底。 factor: 对数机率值的乘数因子。 每个数通过log(LOM[key])*factor产生，如果需要，还可以四舍五入到 round_digit 指定的小数点位数。 用法实例 因为大部分人都想用最简单的方法产生对数机率矩阵（LOM），SubsMat提供了一个可以完成所有需求的函数：\nmake_log_odds_matrix(acc_rep_mat,exp_freq_table=None,logbase=10, factor=10.0,round_digit=0): acc_rep_mat: 用户提供可接受替代矩阵（ARM） exp_freq_table: 期望频率表。如果用户没有提供，就从 acc_rep_mat 产生。 logbase: LOM的对数的底。默认为10。 round_digit: 四舍五入的小数点位数。默认为0。 FreqTable FreqTable.FreqTable(UserDict.UserDict) 属性: alphabet: 一个Bio.Alphabet实例。 data: 频率字典 count: 计数字典(如果有计数的话)。 功能: read_count(f): 从f读入一个计数文件。然后将其转换成频率。 read_freq(f): 从f读入一个频率数据文件。当然，我们不用计数，我们感兴趣的是字母频率。 用法示例: 文件中有残基的个数，用空格分格，形式如下（以3个字母为例）： A 35 B 65 C 100 用 FreqTable.read_count(file_handle) 函数读入。\n一个等价的频率文件:\nA 0.175 B 0.325 C 0.5 反之，残基频率或者计数也可以作为字典输入。 一个计数字典的例子（3个字母）：\n{\u0026#39;A\u0026#39;: 35, \u0026#39;B\u0026#39;: 65, \u0026#39;C\u0026#39;: 100} 这也意味着’C’的频率是0.5，’B’的频率是0.325，’A’的频率是0.175。A、B、C的总和为200。\n一个相同数据的频率字典如下：\n{\u0026#39;A\u0026#39;: 0.175, \u0026#39;B\u0026#39;: 0.325, \u0026#39;C\u0026#39;: 0.5} 总和为1。\n当传入一个字典数据作为参数，应该指出这是一个计数还是频率的字典。因此FreqTable类的构造函数需要两个参数：字典本身和FreqTable.COUNT或者FreqTable.FREQ，分别代表计数或者频率。\n读入期望的计数，readCount会产生频率。下面的任意一个都可以用来产生频率表（ftab）：\n\u0026gt;\u0026gt;\u0026gt; from SubsMat import * \u0026gt;\u0026gt;\u0026gt; ftab = FreqTable.FreqTable(my_frequency_dictionary,FreqTable.FREQ) \u0026gt;\u0026gt;\u0026gt; ftab = FreqTable.FreqTable(my_count_dictionary,FreqTable.COUNT) \u0026gt;\u0026gt;\u0026gt; ftab = FreqTable.read_count(open(\u0026#39;myCountFile\u0026#39;)) \u0026gt;\u0026gt;\u0026gt; ftab = FreqTable.read_frequency(open(\u0026#39;myFrequencyFile\u0026#39;)) ","date":"2021-08-19T00:00:00Z","permalink":"https://example.com/p/ch20_%E9%AB%98%E7%BA%A7/","title":"ch20_高级"},{"content":"biopython官方地址：https://biopython.org/\ngithub地址：https://github.com/biopython/biopython/blob/master/\n中文版教程：https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html\nbiopython包的说明（具体到每个模块了）：https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html\nBiopython具有一个基于Python标准单元测试框架 unittesthttp://docs.python.org/library/unittest.html 的回归测试框架（文件 run_tests.py）。而为模块提供全面测试， 是确保Biopython代码在使用期内尽可能无bug的一个最重要的方面。 也经常是最被轻视的方面之一。本章旨在使运行Biopython测试和编 写测试代码尽可能容易。理想情况下，进入Biopython的每个模块都 应该有一个测试（还应该有对应文档！）。强烈鼓励我们所有开发 者，以及任何从源码安装Biopython的人运行单元测试。\n运行测试 在你下载Biopython源码或者从我们的源码仓库签出时，你会发现一 个子目录调用 Tests。 这包括关键脚本 run_tests.py、 名为 test_XXX.py 的很多独立脚本、一个叫 output 的子目录和 很多其他包含测试套件输入文件的子目录。\n作为构建和安装Biopython的一部分，你通常会在命令行上从Biopython 源码顶层目录运行整个测试套件如下：\npython setup.py test 这事实上等价于转到 Tests 子目录，并运行：\npython run_tests.py #你通常会想要只运行测试的一部分，这可以如下来操作： python run_tests.py test_SeqIO.py test_AlignIO.py #当给出测试列表时， .py 扩展名是可选的，所以你可以只需打字： python run_tests.py test_SeqIO test_AlignIO #要运行 docstring 测试（见 19.3 节）的话， 你可以用 python run_tests.py doctest 缺省情况下， run_tests.py 运行所有测试，包括docstring测试。\n如果一单个测试失败了，你还可以尝试直接运行它，它会给出更多信息。\n重要的是，要注意单个单元测试有两类作用：\n简单打印和比较脚本。 这些单元测试本质上是简短的 Python 示例 程序，它们会打印出各种输出文本。对于一个名为 test_XXX.py 的测试文件，在 output 子目录（包含期望的输出）下会有一个 叫做 test_XXX 的匹配文本文件。测试框架所做的全部就是运行 脚本并检查输出的一致性。 基于 unittest 的标准测试。 这些会 import unittest ，然 后定义 unittest.TestCase 类，这些类的每一个都带有一或多个 像以 test_ 开头、检查代码的某些方面的方法那样的子测试。这 些测试不应该直接打印任何输出。 目前，大约一半的 Biopython 测试是 unittest 风格的测试，另一半 是 print-and-compare 测试。\n直接运行一个简单的 print-and-compare 测试通常会在屏幕上给出大量 输出，但是并不检查输出是否跟期望的一样。如果测试以一个意外的错误 而失败，那么应该很容易精确定位脚本失败的位置。例如，对于一个print-and-compare 测试，试一下： python test_SeqIO.py 基于 unittest 的测试反倒是精确地显示你测试的哪一个小块失败了。 例如\npython test_Cluster.py 编写测试 假如说你想为一个叫做 Biospam 的模块写一些测试。这可以是你写的 一个模块，或者是一个还没有任何测试的现存模块。在下面的例子中，我们 假设 Biospam 是一个做简单数学的模块。\n每个 Biopython 测试都可以有三个重要的文件和相关目录：\ntest_Biospam.py – 关于你的模块的真正测试代码。 Biospam [optional]– 一个包含任何必要输入文件的目录。任何会 生成的输出文件也应该写在这里（并且最好在测试结束后打扫干净)以防 堵塞主 Tests 目录。 output/Biospam – [只针对 print-and-compare 测试] 这个文件 包括运行 test_Biospam.py 的期望输出。这个文件对于 unittest 风格的测试不是必须的，因为测试脚本 test_Biospam.py 会自己做验证。 你要自己决定你是想编写一个 print-and-compare 测试脚本还是一个 unittest 风格的测试脚本。重要的是你不能把这两种风格混合在一个 测试脚本中。尤其是，不要在一个 print-and-compare 测试中使用unittest 特征。\nTests 目录中任何具有 test_ 前缀的脚本都会被 run_tests.py 找到并运行。下面，我们展示一个示例测试脚本 test_Biospam.py ，针对 一个 print-and-compare 测试和一个基于 unittest 的测试。如果你把这个 脚本放进 Biopython的 Tests 目录，那么 run_tests.py 就会找到它并 执行其中包含的测试： $ python run_tests.py test_Ace ... ok test_AlignIO ... ok test_BioSQL ... ok test_BioSQL_SeqIO ... ok test_Biospam ... ok test_CAPS ... ok test_Clustalw ... ok ... ---------------------------------------------------------------------- Ran 107 tests in 86.127 seconds 编写一个print-and-compare测试 一个 print-and-compare 风格的测试对于初学者和新手来说是很容易写的- 本质上它只是一个使用你的模块的示例脚本。\n为了写一个关于 Biospam 的 print-and-compare 测试，这是你应该 做的：\n编写一个叫 test_Biospam.py 的脚本 这个脚本应该位于 Tests 目录 脚本应该测试模块的所有重要功能（当然，你测试的越多、你的测试就 越好！）。 尽量避免任何平台特异的东西，例如打印浮点数而不用显式格式字符串 来避免有太多小数位（不同的平台会给出稍微不同的值）。 如果脚本需要文件来进行测试，这些应转到目录 Tests/Biospam 中进行 （如果你只需一些通用的东西，像一个 FASTA 序列文件，或者一条 GenBank 记录，试着用一个现存的样品输入文件来代替）。 写出测试输出并验证输出是正确的 有两种方法可以做到这一点： 长期方法： 运行脚本并将输出写到一个文件中。在 UNIX （包括 Linux 和 Mac OS X ）机器上，你可以这样做： python test_Biospam.py \u0026gt; test_Biospam 这会把输出写到文件 test_Biospam 中。 手动查看文件 test_Biospam 来确保输出正确。当你确定都没问 题、没有bug后，你需要快速编辑 test_Biospam 文件使其首行为： ‘test_Biospam’ （不含引号）。 复制文件 test_Biospam 到目录 Tests/output 中。 快速方法: 运行 python run_tests.py -g test_Biospam.py 。回归测试框架 很聪明的会以他喜欢的方式把输出放在恰当的地方。 转到输出（应该在 Tests/output/test_Biospam）并复查输出以确 保其完全正确。 现在改换到 Tests 目录并运行 python run_tests.py 进行回归测试。 这会运行所有测试，而你会看到你的测试也在运行（并通过）。 好了！这样你就得到了可用于签入或提交到Biopython的、关于你的模块的 一个友好的测试。恭喜你！ 例如，测试 Biospam 模块中的 addition 和 multiplication 功 能的测试脚本 test_Biospam.py 也许看起来是下面这个样子：\nfrom Bio import Biospam print \u0026#34;2 + 3 =\u0026#34;, Biospam.addition(2, 3) print \u0026#34;9 - 1 =\u0026#34;, Biospam.addition(9, -1) print \u0026#34;2 * 3 =\u0026#34;, Biospam.multiplication(2, 3) print \u0026#34;9 * (- 1) =\u0026#34;, Biospam.multiplication(9, -1) 我们用 python run_tests.py -g test_Biospam.py 来生成对应的输出， 并检查输出文件 output/test_Biospam ：\ntest_Biospam 2 + 3 = 5 9 - 1 = 8 2 * 3 = 6 9 * (- 1) = -9 通常，更大的 print-and-compare 测试的困难在于追踪输出行与测试脚本 命令之间的对应关系。为此，打印出一些标记是很重要的，这些标记帮助你 把输入脚本按行和产生的输出匹配起来。\n编写一个基于unittest的测试 我们想要Biopython中的所有模块都具有单元测试，并且一个简单的 print-and-compare 测试比一点儿测试都没有要好。不过，尽管有一个陡峭的 学习曲线，使用 unittest 框架能给出一个更结构化的结果，并且如果有 一个测试失败，这能够清晰准确地指出测试的哪部分出了问题。子测试也可以 单独运行，这对于测试和排错很有帮助。\n从2.1版开始 unittest 框架就包含在Python中了，并且存档在 Python Library Reference （就是所推荐的你的枕边书）。也有 关于unittest 的在线文档。如果你 熟悉 unittest 系统（或类似于某些噪音测试框架的东西），你应该不会有 什么麻烦。你也许发现，寻找Biopython中的现成例子很有帮助。\n这是关于 Biospam 的一个 unittest 风格的极小测试脚本，你可以 复制粘贴过去运行它：\nimport unittest from Bio import Biospam class BiospamTestAddition(unittest.TestCase): def test_addition1(self): result = Biospam.addition(2, 3) self.assertEqual(result, 5) def test_addition2(self): result = Biospam.addition(9, -1) self.assertEqual(result, 8) class BiospamTestDivision(unittest.TestCase): def test_division1(self): result = Biospam.division(3.0, 2.0) self.assertAlmostEqual(result, 1.5) def test_division2(self): result = Biospam.division(10.0, -2.0) self.assertAlmostEqual(result, -5.0) if __name__ == \u0026#34;__main__\u0026#34;: runner = unittest.TextTestRunner(verbosity = 2) unittest.main(testRunner=runner) 在分割测试中，我们使用 assertAlmostEqual 而不是 assertEqual 以免因舍入误差造成的测试失败；详情以及 unittest 中的其他可用功能 参见Python文档中的 unittest 章节（在线参考）。\n这里是基于 unittest 的测试的一些关键点：\n测试实例存储在 unittest.TestCase 的子类中并涵盖了你的代码 的一个基本方面。 对于任何在每个测试方法前后都要运行的重复代码，你可以使用方法 setUp 和 tearDown 。例如 setUp 方法可用于创建你正在 测试的对象的实例，或打开一个文件句柄。 tearDown 可做任何整理， 例如关闭文件句柄。 测试以 test_ 为前缀并且每项测试应覆盖你所想要测试的内容的一个 具体部分。一个类中你想包含多少个测试都行。 在测试脚本的末尾，你可以用 if __name__ == \u0026#34;__main__\u0026#34;: runner = unittest.TextTestRunner(verbosity = 2) unittest.main(testRunner=runner) 来执行测试脚本，当脚本是从 自己运行（而不是从 run_tests.py 导入）时。 如果你运行该脚本，那么你会见到类似下面的东西:\n$ python test_BiospamMyModule.py test_addition1 (__main__.TestAddition) ... ok test_addition2 (__main__.TestAddition) ... ok test_division1 (__main__.TestDivision) ... ok test_division2 (__main__.TestDivision) ... ok ---------------------------------------------------------------------- Ran 4 tests in 0.059s OK 为了更清晰地表明每个测试都干了什么，你可以给每个测试加上 docstrings 。 它们会在运行测试的时候显示出来，如果一个测试失败这会是有用的信息。\nimport unittest from Bio import Biospam class BiospamTestAddition(unittest.TestCase): def test_addition1(self): \u0026#34;\u0026#34;\u0026#34;An addition test\u0026#34;\u0026#34;\u0026#34; result = Biospam.addition(2, 3) self.assertEqual(result, 5) def test_addition2(self): \u0026#34;\u0026#34;\u0026#34;A second addition test\u0026#34;\u0026#34;\u0026#34; result = Biospam.addition(9, -1) self.assertEqual(result, 8) class BiospamTestDivision(unittest.TestCase): def test_division1(self): \u0026#34;\u0026#34;\u0026#34;Now let\u0026#39;s check division\u0026#34;\u0026#34;\u0026#34; result = Biospam.division(3.0, 2.0) self.assertAlmostEqual(result, 1.5) def test_division2(self): \u0026#34;\u0026#34;\u0026#34;A second division test\u0026#34;\u0026#34;\u0026#34; result = Biospam.division(10.0, -2.0) self.assertAlmostEqual(result, -5.0) if __name__ == \u0026#34;__main__\u0026#34;: runner = unittest.TextTestRunner(verbosity = 2) unittest.main(testRunner=runner) 运行脚本你就会看到：\n$ python test_BiospamMyModule.py An addition test ... ok A second addition test ... ok Now let\u0026#39;s check division ... ok A second division test ... ok ---------------------------------------------------------------------- Ran 4 tests in 0.001s OK 如果你的模块包含 docstring 测试（见 19.3 小节）， 你也许想在要运行的测试中包含这些。你可以修改 if name == \u0026ldquo;main\u0026rdquo;: 下面的代码如下面这样：\nif __name__ == \u0026#34;__main__\u0026#34;: unittest_suite = unittest.TestLoader().loadTestsFromName(\u0026#34;test_Biospam\u0026#34;) doctest_suite = doctest.DocTestSuite(Biospam) suite = unittest.TestSuite((unittest_suite, doctest_suite)) runner = unittest.TextTestRunner(sys.stdout, verbosity = 2) runner.run(suite) 这只与你执行 python test_Biospam.py 时是否想要运行 docstring 测试 有关；运行 python run_tests.py ，docstring 测试会自动运行（假设他们 被包含在 run_tests.py 中的 docstring 测试列表中，见下面的小节）。\n编写doctests Python 模块、类和函数支持使用 docstrings 创建文档。 doctest 框架 （包含在Python中） 允许开发者将工作例子嵌入在 docstrings 中，并自动测试这些例子。\n目前只有一小部分 Biopython 包含 doctests 。 run_tests.py 脚本 看护着 doctests 的运行。为此， run_tests.py 脚本开头是要测试 的模块的一个手动编译列表，该列表允许我们跳过那些可能没有安装可选 外部依赖库的模块（例如 Reportlab 和 NumPy 库）。所以如果你在 Biopython 模块中加一些针对 dostrings 的 doctests ，为了把它们包含在 Biopython 套件中，你必须更新 run_tests.py 以包含你的模块。现在， run_tests.py 的相关部分看起来像下面这样：\n# This is the list of modules containing docstring tests. # If you develop docstring tests for other modules, please add # those modules here. DOCTEST_MODULES = [\u0026#34;Bio.Seq\u0026#34;, \u0026#34;Bio.SeqRecord\u0026#34;, \u0026#34;Bio.SeqIO\u0026#34;, \u0026#34;...\u0026#34;, ] #Silently ignore any doctests for modules requiring numpy! try: import numpy DOCTEST_MODULES.extend([\u0026#34;Bio.Statistics.lowess\u0026#34;]) except ImportError: pass 注意我们首先把 doctests 看做文档，所以你应该坚持典型用法。通常处理错 误条件等诸如此类的复杂例子最好留给一个专门的单元测试。\n注意，如果你想编写涉及文件解析的 doctests ，定义文件位置复杂性是很要 紧的。理想情况下，假设代码会从 Tests 目录运行，使用相对路径即可， 关于这一点的一个例子参见 Bio.SeqIO doctests 。\n要想只运行 docstring 测试，使用\n$ python run_tests.py doctest ","date":"2021-08-18T00:00:00Z","permalink":"https://example.com/p/ch19_biopython%E6%B5%8B%E8%AF%95%E6%A1%86%E6%9E%B6/","title":"ch19_biopython测试框架"},{"content":"biopython官方地址：https://biopython.org/\ngithub地址：https://github.com/biopython/biopython/blob/master/\n中文版教程：https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html\nbiopython包的说明（具体到每个模块了）：https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html\n操作序列文件 过滤文件中的序列 通常你会拥有一个包含许多序列的大文件（例如，FASTA基因文件，或者FASTQ或SFF读长文件），和一个包含你所感兴趣的序列的ID列表，而你希望创建一个由这一ID列表里的序列构成的文件。\n让我们假设这个ID列表在一个简单的文本文件中，作为每一行的第一个词。这可能是一个表格文件，其第一列是序列ID。尝试下面的代码：\nfrom Bio import SeqIO input_file = \u0026#34;big_file.sff\u0026#34; id_file = \u0026#34;short_list.txt\u0026#34; output_file = \u0026#34;short_list.sff\u0026#34; wanted = set(line.rstrip(\u0026#34;\\n\u0026#34;).split(None,1)[0] for line in open(id_file)) print \u0026#34;Found %i unique identifiers in %s\u0026#34; % (len(wanted), id_file) records = (r for r in SeqIO.parse(input_file, \u0026#34;sff\u0026#34;) if r.id in wanted) count = SeqIO.write(records, output_file, \u0026#34;sff\u0026#34;) print \u0026#34;Saved %i records from %s to %s\u0026#34; % (count, input_file, output_file) if count \u0026lt; len(wanted): print \u0026#34;Warning %i IDs not found in %s\u0026#34; % (len(wanted)-count, input_file) 注意，我们使用Python的 set 类型而不是 list，这会使得检测成员关系更快。\n生成随机基因组 假设你在检视一个基因组序列，寻找一些序列特征——或许是极端局部GC含量偏差，或者可能的限制性酶切位点。一旦你使你的Python代码在真实的基因组上运行后，尝试在相同的随机化版本基因组上运行，并进行统计分析或许是明智的选择（毕竟，任何你发现的“特性”都可能只是偶然事件）。\n在这一讨论中，我们将使用来自 Yersinia pestis biovar Microtus 的pPCP1质粒的GenBank文件。该文件包含在Biopython单元测试的GenBank文件夹中，或者你可以从我们的网站上得到， NC_005816.gb. 该文件仅有一个记录，所以我们能用 Bio.SeqIO.read() 函数把它当做 SeqRecord 读入：\n\u0026gt;\u0026gt;\u0026gt; from Bio import SeqIO \u0026gt;\u0026gt;\u0026gt; original_rec = SeqIO.read(\u0026#34;NC_005816.gb\u0026#34;,\u0026#34;genbank\u0026#34;) 那么，我们怎么生成一个原始序列重排后的版本能？我会使用Python内置的 random 模块来做这个，特别是 random.shuffle 函数——但是这个只作用于Python列表。我们的序列是一个 Seq 对象，所以为了重排它，我们需要将它转换为一个列表：\n\u0026gt;\u0026gt;\u0026gt; import random \u0026gt;\u0026gt;\u0026gt; nuc_list = list(original_rec.seq) \u0026gt;\u0026gt;\u0026gt; random.shuffle(nuc_list) #acts in situ! 现在，为了使用 Bio.SeqIO 输出重排的序列，我们需要使用重排后的列表重新创建包含一个新的 SeqRecord 包含随即化后的 Seq 。要实现这个，我们需要将核苷酸（单字母字符串）列表转换为长字符串——在Python中，一般使用字符串对象的join方法来实现它。\n\u0026gt;\u0026gt;\u0026gt; from Bio.Seq import Seq \u0026gt;\u0026gt;\u0026gt; from Bio.SeqRecord import SeqRecord \u0026gt;\u0026gt;\u0026gt; shuffled_rec = SeqRecord(Seq(\u0026#34;\u0026#34;.join(nuc_list), original_rec.seq.alphabet), ... id=\u0026#34;Shuffled\u0026#34;, description=\u0026#34;Based on %s\u0026#34; % original_rec.id) 让我们将所有这些片段放到一起来组成一个完整的Python脚本，这个脚本将生成一个FASTA序列文件，其包含30个原始序列的随机重排版本。\n第一个版本只是使用一个大的for循环，并一个一个的输出记录（使用章节 5.5.4 描述的SeqRecord 的格式化方法）：\nimport random from Bio.Seq import Seq from Bio.SeqRecord import SeqRecord from Bio import SeqIO original_rec = SeqIO.read(\u0026#34;NC_005816.gb\u0026#34;,\u0026#34;genbank\u0026#34;) handle = open(\u0026#34;shuffled.fasta\u0026#34;, \u0026#34;w\u0026#34;) for i in range(30): nuc_list = list(original_rec.seq) random.shuffle(nuc_list) shuffled_rec = SeqRecord(Seq(\u0026#34;\u0026#34;.join(nuc_list), original_rec.seq.alphabet), \\ id=\u0026#34;Shuffled%i\u0026#34; % (i+1), \\ description=\u0026#34;Based on %s\u0026#34; % original_rec.id) handle.write(shuffled_rec.format(\u0026#34;fasta\u0026#34;)) handle.close() 我个人更喜欢下面的版本（不使用for循环），而使用一个函数来重排记录以及一个生成表达式：\nimport random from Bio.Seq import Seq from Bio.SeqRecord import SeqRecord from Bio import SeqIO def make_shuffle_record(record, new_id): nuc_list = list(record.seq) random.shuffle(nuc_list) return SeqRecord(Seq(\u0026#34;\u0026#34;.join(nuc_list), record.seq.alphabet), \\ id=new_id, description=\u0026#34;Based on %s\u0026#34; % original_rec.id) original_rec = SeqIO.read(\u0026#34;NC_005816.gb\u0026#34;,\u0026#34;genbank\u0026#34;) shuffled_recs = (make_shuffle_record(original_rec, \u0026#34;Shuffled%i\u0026#34; % (i+1)) \\ for i in range(30)) handle = open(\u0026#34;shuffled.fasta\u0026#34;, \u0026#34;w\u0026#34;) SeqIO.write(shuffled_recs, handle, \u0026#34;fasta\u0026#34;) handle.close() 翻译CDS条目为FASTA文件 假设你有一个包含某个物种的CDS条目作为输入文件，你想生成一个由它们的蛋白序列组成的FASTA文件。也就是，从原始文件中取出每一个核苷酸序列，并翻译它。回到章节 3.9 我们了解了怎么使用 Seq 对象的 translate 方法，和可选的 cds 参数来使得不同的起始密码子能正确翻译。\n就像章节 5.5.3 中反向互补的例子中展示的那样，我们可以用 Bio.SeqIO 将与翻译步骤结合起来。对于每一个核苷酸 SeqRecord ，我们需要创建一个蛋白的 SeqRecord —— 并对它命名。\n你能编写自己的函数来做这个事情，为你的序列选择合适的蛋白标识和恰当的密码表。在本例中，我们仅使用默认的密码表，并给序列ID加一个前缀。\nfrom Bio.SeqRecord import SeqRecord def make_protein_record(nuc_record): \u0026#34;\u0026#34;\u0026#34;Returns a new SeqRecord with the translated sequence (default table).\u0026#34;\u0026#34;\u0026#34; return SeqRecord(seq = nuc_record.seq.translate(cds=True), \\ id = \u0026#34;trans_\u0026#34; + nuc_record.id, \\ description = \u0026#34;translation of CDS, using default table\u0026#34;) 我们能用这个函数将核苷酸记录转换为蛋白记录，作为输出。一个优雅且内存高效的方式是使用生成表达式（generator expression）：\nfrom Bio import SeqIO proteins = (make_protein_record(nuc_rec) for nuc_rec in \\ SeqIO.parse(\u0026#34;coding_sequences.fasta\u0026#34;, \u0026#34;fasta\u0026#34;)) SeqIO.write(proteins, \u0026#34;translations.fasta\u0026#34;, \u0026#34;fasta\u0026#34;) 以上代码适用于完整编码序列的FASTA文件。如果你使用部分编码序列，你可能希望在以上的例子中使用 nuc_record.seq.translate(to_stop=True) ，这会告诉Biopython不检查起始密码的有效性，等等。\n将FASTA文件中的序列变为大写 通常你会从合作者那里得到FASTA文件的数据，有时候这些序列可能是大小写混合的。在某些情况下，这些可能是有意为之的（例如，小写的作为低质量的区域），但通常大小写并不重要。你可能希望编辑这个文件以使所有的序列都变得一致（如，都为大写），你可以使用 SeqRecord 对象的 upper() 方法轻易的实现（Biopython 1.55中引入）：\nfrom Bio import SeqIO records = (rec.upper() for rec in SeqIO.parse(\u0026#34;mixed.fas\u0026#34;, \u0026#34;fasta\u0026#34;)) count = SeqIO.write(records, \u0026#34;upper.fas\u0026#34;, \u0026#34;fasta\u0026#34;) print \u0026#34;Converted %i records to upper case\u0026#34; % count 这是怎么工作的呢？第一行只是导入 Bio.SeqIO 模块。第二行是最有趣的——这是一个Python生成器表达式，它提供 mixed.fas 里每个记录的大写版本。第三行中，我们把这个生成器表达式传给 Bio.SeqIO.write() 函数，它会把大写的序列写出到 upper.fas 输出文件。\n我们使用生成器（而不是一个列表或列表解析式）的原因是，前一方式每次仅有一个记录保存在内存中。当你在处理包含成千上万的条目的文件时，这可能非常重要。\n对序列文件排序 假设你想对一个序列文件按序列长度排序（例如，一个序列拼接的重叠群(contig)集合），而你工作的文件格式可能是像FASTA或FASTQ这样 Bio.SeqIO 能读写（和索引）的格式。\n如果文件足够小，你能将它都一次读入内存为一个 SeqRecord 对象列表，对列表进行排序，并保存它：\nfrom Bio import SeqIO records = list(SeqIO.parse(\u0026#34;ls_orchid.fasta\u0026#34;,\u0026#34;fasta\u0026#34;)) records.sort(cmp=lambda x,y: cmp(len(x),len(y))) SeqIO.write(records, \u0026#34;sorted_orchids.fasta\u0026#34;, \u0026#34;fasta\u0026#34;) 唯一巧妙的地方是指明一个比较函数来说明怎样对记录进行排序（这里我们按长度对他们排序）。如果你希望最长的记录在第一个，你可以交换比对，或者使用reverse参数：\nfrom Bio import SeqIO records = list(SeqIO.parse(\u0026#34;ls_orchid.fasta\u0026#34;,\u0026#34;fasta\u0026#34;)) records.sort(cmp=lambda x,y: cmp(len(y),len(x))) SeqIO.write(records, \u0026#34;sorted_orchids.fasta\u0026#34;, \u0026#34;fasta\u0026#34;) 现在这一实现是非常直接的——但是如果你的文件非常大，你不能像这样把它整个加载到内存中应该怎么办呢？例如，你可能有一些二代测序的读长要根据长度排序。这时你可以使用 Bio.SeqIO.index() 函数解决。\nfrom Bio import SeqIO #Get the lengths and ids, and sort on length len_and_ids = sorted((len(rec), rec.id) for rec in \\ SeqIO.parse(\u0026#34;ls_orchid.fasta\u0026#34;,\u0026#34;fasta\u0026#34;)) ids = reversed([id for (length, id) in len_and_ids]) del len_and_ids #free this memory record_index = SeqIO.index(\u0026#34;ls_orchid.fasta\u0026#34;, \u0026#34;fasta\u0026#34;) records = (record_index[id] for id in ids) SeqIO.write(records, \u0026#34;sorted.fasta\u0026#34;, \u0026#34;fasta\u0026#34;) 首先我们使用 Bio.SeqIO.parse() 来将整个文件扫描一遍，并将所有记录的标识和它们的长度保存在一个元组（tuple）中。接着我们对这个元组按照序列长度进行排序，并舍弃这些长度。有了这一排列后的标识列表， Bio.SeqIO.index() 允许我们一个一个获取这些记录，我们把它们传给 Bio.SeqIO.write() 输出。\n这些例子都使用 Bio.SeqIO 来解析记录为 SeqRecord 对象，并通过 Bio.SeqIO.write() 输出。当你想排序的文件格式 Bio.SeqIO.write() 不支持应该怎么办呢？如纯文本的SwissProt格式。这里有一个额外的解决方法——使用在 Biopython 1.54 (见 5.4.2.2 )中 Bio.SeqIO.index() 添加的 get_raw() 方法。\nfrom Bio import SeqIO #Get the lengths and ids, and sort on length len_and_ids = sorted((len(rec), rec.id) for rec in \\ SeqIO.parse(\u0026#34;ls_orchid.fasta\u0026#34;,\u0026#34;fasta\u0026#34;)) ids = reversed([id for (length, id) in len_and_ids]) del len_and_ids #free this memory record_index = SeqIO.index(\u0026#34;ls_orchid.fasta\u0026#34;, \u0026#34;fasta\u0026#34;) handle = open(\u0026#34;sorted.fasta\u0026#34;, \u0026#34;w\u0026#34;) for id in ids: handle.write(record_index.get_raw(id)) handle.close() 作为一个奖励，由于以上例子不重复将数据解析为 SeqRecord 对象，所以它会更快。\nFASTAQ文件的简单质量过滤 一个常见的工作是输入一个大的测序读长集合，并根据它们的质量分数过滤它们（或修剪它们）。下面的例子非常简单，然而足以展示处理 SeqRecord 对象中质量数据的基本用法。我们所有要做的事情是读入一个FASTQ文件，过滤并取出那些PHRED质量分数在某个阈值（这里为20）以上的序列。\n在这个例子中，我们将使用从ENA序列读长存档下载的真实数据， ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR020/SRR020192/SRR020192.fastq.gz (2MB) ，解压后为19MB的文件 SRR020192.fastq 。这是在Roche 454 GS FLX测序平台生成的感染加利福利亚海狮的病毒单端数据（参见 http://www.ebi.ac.uk/ena/data/view/SRS004476 ）。\n首先，让我们来统计reads的数目：\nfrom Bio import SeqIO count = 0 for rec in SeqIO.parse(\u0026#34;SRR020192.fastq\u0026#34;, \u0026#34;fastq\u0026#34;): count += 1 print \u0026#34;%i reads\u0026#34; % count 现在让我们做一个简单的过滤，PHRED质量不小于20：\nfrom Bio import SeqIO good_reads = (rec for rec in \\ SeqIO.parse(\u0026#34;SRR020192.fastq\u0026#34;, \u0026#34;fastq\u0026#34;) \\ if min(rec.letter_annotations[\u0026#34;phred_quality\u0026#34;]) \u0026gt;= 20) count = SeqIO.write(good_reads, \u0026#34;good_quality.fastq\u0026#34;, \u0026#34;fastq\u0026#34;) print \u0026#34;Saved %i reads\u0026#34; % count 这只取出了41892条读长中的14580条。一个更有意义的做法是根据质量来裁剪reads，但是这里只是作为一个例子。\nFASTQ文件可以包含上百万的记录，所以最好避免一次全部加载它们到内存。这个例子使用一个生成器表达式，这意味着每次只有内存里只有一个 SeqRecord 被创建 —— 避免内存限制。\n切除引物序列 在这个例子中，假设我们需要寻找一个FASTQ数据中以 GATGACGGTGT 为5’端的引物序列的reads。同上面的例子一样，我们将使用从ENA下载的 SRR020192.fastq 文件（ ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR020/SRR020192/SRR020192.fastq.gz ）。该方式同样适用于任何其他Biopython支持的格式（例如FASTA文件）。\n这个代码使用 Bio.SeqIO 和一个生成器表达式（避免一次加载所有的序列到内存中），以及 Seq 对象的 startswith 方法来检查读长是否以引物序列开始：\nfrom Bio import SeqIO primer_reads = (rec for rec in \\ SeqIO.parse(\u0026#34;SRR020192.fastq\u0026#34;, \u0026#34;fastq\u0026#34;) \\ if rec.seq.startswith(\u0026#34;GATGACGGTGT\u0026#34;)) count = SeqIO.write(primer_reads, \u0026#34;with_primer.fastq\u0026#34;, \u0026#34;fastq\u0026#34;) print \u0026#34;Saved %i reads\u0026#34; % count 这将从 SRR014849.fastq 找到13819条读长记录，并保存为一个新的FASTQ文件——with_primer.fastq。\n现在，假设你希望创建一个包含这些读长，但去除了所有引物序列的FASTQ文件。只需要很小的修改，我们就能对 SeqRecord 进行切片（参见章节 4.6 ）以移除前11个字母（我们的引物长度）：\nfrom Bio import SeqIO trimmed_primer_reads = (rec[11:] for rec in \\ SeqIO.parse(\u0026#34;SRR020192.fastq\u0026#34;, \u0026#34;fastq\u0026#34;) \\ if rec.seq.startswith(\u0026#34;GATGACGGTGT\u0026#34;)) count = SeqIO.write(trimmed_primer_reads, \u0026#34;with_primer_trimmed.fastq\u0026#34;, \u0026#34;fastq\u0026#34;) print \u0026#34;Saved %i reads\u0026#34; % count 这也将从 SRR020192.fastq 取出13819条读长，但是移除了前十个字符，并将它们保存为另一个新的FASTQ文件， with_primer_trimmed.fastq 。\n最后，假设你想移除部分reads中的引物并创建一个新的FASTQ文件，而其他的reads保持不变。如果我们仍然希望使用生成器表达式，声明我们自己的修剪（trim）函数可能更加清楚：\nfrom Bio import SeqIO def trim_primer(record, primer): if record.seq.startswith(primer): return record[len(primer):] else: return record trimmed_reads = (trim_primer(record, \u0026#34;GATGACGGTGT\u0026#34;) for record in \\ SeqIO.parse(\u0026#34;SRR020192.fastq\u0026#34;, \u0026#34;fastq\u0026#34;)) count = SeqIO.write(trimmed_reads, \u0026#34;trimmed.fastq\u0026#34;, \u0026#34;fastq\u0026#34;) print \u0026#34;Saved %i reads\u0026#34; % count 以上代码会运行较长的时间，因为这次输出文件包含所有41892个reads。再次，我们将使用生成器表达式来避免内存问题。你也可以使用一个生成器函数来替代生成器表达式。\nfrom Bio import SeqIO def trim_primers(records, primer): \u0026#34;\u0026#34;\u0026#34;Removes perfect primer sequences at start of reads. This is a generator function, the records argument should be a list or iterator returning SeqRecord objects. \u0026#34;\u0026#34;\u0026#34; len_primer = len(primer) #cache this for later for record in records: if record.seq.startswith(primer): yield record[len_primer:] else: yield record original_reads = SeqIO.parse(\u0026#34;SRR020192.fastq\u0026#34;, \u0026#34;fastq\u0026#34;) trimmed_reads = trim_primers(original_reads, \u0026#34;GATGACGGTGT\u0026#34;) count = SeqIO.write(trimmed_reads, \u0026#34;trimmed.fastq\u0026#34;, \u0026#34;fastq\u0026#34;) print \u0026#34;Saved %i reads\u0026#34; % count 这种形式非常灵活，如果你想做一些更复杂的事情，譬如只保留部分记录 —— 像下一个例子中展示的那样。\n切除接头序列 这实际上是前面例子的一个简单扩展。我们将假设 GATGACGGTGT 是某个FASTQ格式数据的一个接头序列，并再次使用来自NCBI的 SRR020192.fastq 文件 （ ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR020/SRR020192/SRR020192.fastq.gz ）。\n然而在本例中，我们将在读长的 任何位置 查找序列，不仅仅是最开始：\nfrom Bio import SeqIO def trim_adaptors(records, adaptor): \u0026#34;\u0026#34;\u0026#34;Trims perfect adaptor sequences. This is a generator function, the records argument should be a list or iterator returning SeqRecord objects. \u0026#34;\u0026#34;\u0026#34; len_adaptor = len(adaptor) #cache this for later for record in records: index = record.seq.find(adaptor) if index == -1: #adaptor not found, so won\u0026#39;t trim yield record else: #trim off the adaptor yield record[index+len_adaptor:] original_reads = SeqIO.parse(\u0026#34;SRR020192.fastq\u0026#34;, \u0026#34;fastq\u0026#34;) trimmed_reads = trim_adaptors(original_reads, \u0026#34;GATGACGGTGT\u0026#34;) count = SeqIO.write(trimmed_reads, \u0026#34;trimmed.fastq\u0026#34;, \u0026#34;fastq\u0026#34;) print \u0026#34;Saved %i reads\u0026#34; % count 因为我们在这个例子中使用的是FASTQ文件， SeqRecord 对象包括reads质量分数的单字母注释（per-letter-annotation）。我们可以通过对具有一定质量分数的 SeqRecord 对象进行切片，并将返回的结果保存到一个FASTQ文件。\n和上面的例子（只在每个读长的开始查找引物/接头）相比，你会发现有些reads剪切后非常短（例如，如果接头序列在读长的中部发现，而不是开始附近）。所以，让我们再加入一个最低长度要求：\nfrom Bio import SeqIO def trim_adaptors(records, adaptor, min_len): \u0026#34;\u0026#34;\u0026#34;Trims perfect adaptor sequences, checks read length. This is a generator function, the records argument should be a list or iterator returning SeqRecord objects. \u0026#34;\u0026#34;\u0026#34; len_adaptor = len(adaptor) #cache this for later for record in records: len_record = len(record) #cache this for later if len(record) \u0026lt; min_len: #Too short to keep continue index = record.seq.find(adaptor) if index == -1: #adaptor not found, so won\u0026#39;t trim yield record elif len_record - index - len_adaptor \u0026gt;= min_len: #after trimming this will still be long enough yield record[index+len_adaptor:] original_reads = SeqIO.parse(\u0026#34;SRR020192.fastq\u0026#34;, \u0026#34;fastq\u0026#34;) trimmed_reads = trim_adaptors(original_reads, \u0026#34;GATGACGGTGT\u0026#34;, 100) count = SeqIO.write(trimmed_reads, \u0026#34;trimmed.fastq\u0026#34;, \u0026#34;fastq\u0026#34;) print \u0026#34;Saved %i reads\u0026#34; % count 通过改变格式名称，你也可以将这个应用于FASTA文件。该代码也可以扩展为模糊匹配，而非绝对匹配（或许用一个两两比对，或者考虑读长的质量分数），但是这会使代码变得更慢。\n转换FASTQ文件 回到章节 5.5.2 ，我们展示了怎样使用 Bio.SeqIO 来实现两个文件格式间的转换。这里，我们将更进一步探讨二代DNA测序中使用的FASTQ文件。更加详细的介绍可以参加 Cock et al. (2009) [7] 。FASTQ文件同时存储DNA序列（以Python字符串的形式）和相应的读长质量。\nPHRED分数（在大多数FASTQ文件中使用，也存在于QUAL、ACE和SFF文件中）已经成为一个用来表示某个给定碱基测序错误概率（这里用 P__e 表示）的 实际 标准（使用一个以10为底的对数转换）：\nQPHRED=−10×log10(Pe)QPHRED=−10×log10(Pe)\n这意味着一个错误的读长（ P__e = 1 ）得到的PHRED质量为0，而一个非常好的 P__e = 0.00001 的读长得到的PHRED质量为50。在实际的测序数据中，质量比这个要高的非常稀少，通过后期处理，如读长映射（mapping）和组装，PHRED质量到达90是可能的（确实，MAQ工具允许PHRED分数在0到93之间）。\nFASTQ格式有潜力成为以单文件纯文本方式存储测序读长的字符和质量分数的 实际 的标准。 唯一的美中不足是，目前至少有三个FASTQ格式版本，它们相互并不兼容，且难以区分…\n原始的Sanger FASTQ格式将PHRED质量分数和33个ASCII字符偏移进行编码。NCBI目前在它们的Short Read Archive中使用这种格式。我们在 Bio.SeqIO 中称之为 fastq （或 fastq-sanger ）格式。 Solexa（后来由Illumina收购）引入了他们自己的版本，使用Solexa质量分数和64个ASCII字符偏移进行编码。我们叫做 fastq-solexa 格式。 Illumina工作流1.3进一步推出了PHRED质量分数（更为一致的版本）的FASTQ文件，但是却以64个ASCII字符偏移编码。我们叫做 fastq-illumina 格式。 Solexa质量分数采用一种不同的对数转换：\n由于Solexa/Illumina目前在他们的1.3版本的工作流程中已迁移到使用PHRED分数，Solexa质量分数将逐渐淡出使用。如果你将错误估值取等号（ P__e ），这两个等式允许在两个评分系统之间进行转换 —— Biopython在 Bio.SeqIO.QualityIO 模块中有函数可以实现。这一模块在使用 Bio.SeqIO 进行从Solexa/Illumina老文件格式到标准Sanger FASTQ文件格式转换时被调用： from Bio import SeqIO SeqIO.convert(\u0026#34;solexa.fastq\u0026#34;, \u0026#34;fastq-solexa\u0026#34;, \u0026#34;standard.fastq\u0026#34;, \u0026#34;fastq\u0026#34;) 如果你想转换新的Illumina 1.3+ FASTQ文件，改变只会导致ASCII码的整体偏移。因为尽管编码不同，所有的质量分数都是PHRED分数：\nfrom Bio import SeqIO SeqIO.convert(\u0026#34;illumina.fastq\u0026#34;, \u0026#34;fastq-illumina\u0026#34;, \u0026#34;standard.fastq\u0026#34;, \u0026#34;fastq\u0026#34;) 注意，像这样使用 Bio.SeqIO.convert() 会比 Bio.SeqIO.parse() 和 Bio.SeqIO.write() 组合快得 多 ，因为转换FASTQ（包括FASTQ到FASTA的转换）的代码经过优化。\n对于质量好的读长，PHRED和Solexa分数几乎相等，这意味着，因为 fasta-solexa 和 fastq-illumina 都使用64个ASCII字符偏移，它们的文件几乎相同。这是Illumina有意设计的，也意味着使用老版本 fasta-solexa 格式文件的应用或许也能接受新版本 fastq-illumina 格式文件（在高质量的数据上）。当然，两个版本和原始的，由Sanger、NCBI和其他地方使用的FASTQ标准有很大不同（格式名为 fastq 或 fastq-sanger ）。\n\u0026gt;\u0026gt;\u0026gt; from Bio.SeqIO import QualityIO \u0026gt;\u0026gt;\u0026gt; help(QualityIO) ... 转换FASTA和QUAL文件为FASTQ文件 FASTQ 同时 包含序列和他们的质量信息字符串。FASTA文件 只 包含序列，而QUAL文件 只 包含质量。因此，一个单独的FASTQ文件可以转换为 成对的 FASTA和QUAL文件，FASTQ文件也可以由成对的FASTA和QUAL文件生成。\n#从FASTQ到FASTA很简单： from Bio import SeqIO SeqIO.convert(\u0026#34;example.fastq\u0026#34;, \u0026#34;fastq\u0026#34;, \u0026#34;example.fasta\u0026#34;, \u0026#34;fasta\u0026#34;) #从FASTQ到QUAL也很简单： from Bio import SeqIO SeqIO.convert(\u0026#34;example.fastq\u0026#34;, \u0026#34;fastq\u0026#34;, \u0026#34;example.qual\u0026#34;, \u0026#34;qual\u0026#34;) 然而，反向则有一点复杂。你可以使用 Bio.SeqIO.parse() 迭代一个 单独 文件中的所有记录，但是这里我们有两个输入文件。有几个可能的策略，然而这里假设两个文件是真的完全匹配的，最内存高效的方式是同时循环两个文件。代码有些巧妙，所以在 Bio.SeqIO.QualityIO 模块中我们提供一个函数来实现，叫做 PairedFastaQualIterator。它接受两个句柄（FASTA文件和QUAL文件）并返回一个 SeqRecord 迭代器：\nfrom Bio.SeqIO.QualityIO import PairedFastaQualIterator for record in PairedFastaQualIterator(open(\u0026#34;example.fasta\u0026#34;), open(\u0026#34;example.qual\u0026#34;)): print record 这个函数将检查FASTA和QUAL文件是否一致（例如，记录顺序是相同的，并且序列长度一致）。你可以和 Bio.SeqIO.write() 函数结合使用，转换一对FASTA和QUAL文件为单独的FASTQ文件：\nfrom Bio import SeqIO from Bio.SeqIO.QualityIO import PairedFastaQualIterator handle = open(\u0026#34;temp.fastq\u0026#34;, \u0026#34;w\u0026#34;) #w=write records = PairedFastaQualIterator(open(\u0026#34;example.fasta\u0026#34;), open(\u0026#34;example.qual\u0026#34;)) count = SeqIO.write(records, handle, \u0026#34;fastq\u0026#34;) handle.close() print \u0026#34;Converted %i records\u0026#34; % count 索引FASTQ文件 FASTQ文件通常非常大，包含上百万的读长。由于数据量的原因，你不能一次将所有的记录加载到内存中。这就是为什么上面的例子（过滤和剪切）以迭代的方式遍历整个文件，每次只查看一个 SeqRecord 。\n然而，有时候你不能使用一个大的循环或迭代器 —— 你或许需要随机获取读长。这里 Bio.SeqIO.index() 函数被证明非常有用，它允许你使用名字获取FASTQ中的任何读长（参见章节 5.4.2 ）。\n我们将再次使用来自 ENA (ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR020/SRR020192/SRR020192.fastq.gz) 的文件 SRR020192.fastq ，尽管这是一个非常小的FASTQ文件，只有不到50,000读长：\n\u0026gt;\u0026gt;\u0026gt; from Bio import SeqIO \u0026gt;\u0026gt;\u0026gt; fq_dict = SeqIO.index(\u0026#34;SRR020192.fastq\u0026#34;, \u0026#34;fastq\u0026#34;) \u0026gt;\u0026gt;\u0026gt; len(fq_dict) 41892 \u0026gt;\u0026gt;\u0026gt; fq_dict.keys()[:4] [\u0026#39;SRR020192.38240\u0026#39;, \u0026#39;SRR020192.23181\u0026#39;, \u0026#39;SRR020192.40568\u0026#39;, \u0026#39;SRR020192.23186\u0026#39;] \u0026gt;\u0026gt;\u0026gt; fq_dict[\u0026#34;SRR020192.23186\u0026#34;].seq Seq(\u0026#39;GTCCCAGTATTCGGATTTGTCTGCCAAAACAATGAAATTGACACAGTTTACAAC...CCG\u0026#39;, SingleLetterAlphabet()) 当在包含7百万读长的FASTQ文件上测试时，索引大概需要花费1分钟，然而获取记录几乎是瞬间完成的。\n章节 18.1.5 的例子展示了如何使用 Bio.SeqIO.index() 函数来对FASTA文件进行排序 —— 这也可以用在FASTQ文件上。\n转换SFF文件 如果你处理454(Roche)序列数据，你可能会接触Standard Flowgram Format (SFF)原始数据。这包括序列读长（called bases）、质量分数和原始流信息。\n一个最常见的工作是转换SFF文件为一对FASTA和QUAL文件，或者一个单独的FASTQ文件。这可以使用 Bio.SeqIO.convert() 来轻松实现（参见 5.5.2 ）：\n\u0026gt;\u0026gt;\u0026gt; from Bio import SeqIO \u0026gt;\u0026gt;\u0026gt; SeqIO.convert(\u0026#34;E3MFGYR02_random_10_reads.sff\u0026#34;, \u0026#34;sff\u0026#34;, \u0026#34;reads.fasta\u0026#34;, \u0026#34;fasta\u0026#34;) 10 \u0026gt;\u0026gt;\u0026gt; SeqIO.convert(\u0026#34;E3MFGYR02_random_10_reads.sff\u0026#34;, \u0026#34;sff\u0026#34;, \u0026#34;reads.qual\u0026#34;, \u0026#34;qual\u0026#34;) 10 \u0026gt;\u0026gt;\u0026gt; SeqIO.convert(\u0026#34;E3MFGYR02_random_10_reads.sff\u0026#34;, \u0026#34;sff\u0026#34;, \u0026#34;reads.fastq\u0026#34;, \u0026#34;fastq\u0026#34;) 10 注意这个转换函数返回记录的条数，在这个例子中为10。这将给你 未裁剪 的读长，其中先导和跟随链中低质量的序列，或接头序列将以小写字母显示。如果你希望得到 裁剪 后的读长（使用SFF文件中的剪切信息），可以使用下面的代码：\n\u0026gt;\u0026gt;\u0026gt; from Bio import SeqIO \u0026gt;\u0026gt;\u0026gt; SeqIO.convert(\u0026#34;E3MFGYR02_random_10_reads.sff\u0026#34;, \u0026#34;sff-trim\u0026#34;, \u0026#34;trimmed.fasta\u0026#34;, \u0026#34;fasta\u0026#34;) 10 \u0026gt;\u0026gt;\u0026gt; SeqIO.convert(\u0026#34;E3MFGYR02_random_10_reads.sff\u0026#34;, \u0026#34;sff-trim\u0026#34;, \u0026#34;trimmed.qual\u0026#34;, \u0026#34;qual\u0026#34;) 10 \u0026gt;\u0026gt;\u0026gt; SeqIO.convert(\u0026#34;E3MFGYR02_random_10_reads.sff\u0026#34;, \u0026#34;sff-trim\u0026#34;, \u0026#34;trimmed.fastq\u0026#34;, \u0026#34;fastq\u0026#34;) 10 如果你使用Linux，你可以向Roche请求一份“脱离仪器（off instrument）”的工具（通常叫做Newbler工具）。它提供了另一种的方式来在命令行实现SFF到FASTA或QUAL的转换（但并不支持FASTQ输出）。\n$ sffinfo -seq -notrim E3MFGYR02_random_10_reads.sff \u0026gt; reads.fasta $ sffinfo -qual -notrim E3MFGYR02_random_10_reads.sff \u0026gt; reads.qual $ sffinfo -seq -trim E3MFGYR02_random_10_reads.sff \u0026gt; trimmed.fasta $ sffinfo -qual -trim E3MFGYR02_random_10_reads.sff \u0026gt; trimmed.qual Biopython以大小写混合的方式来表示剪切位点，这是有意模拟Roche工具的做法。\n要获得Biopython对SFF支持的更多信息，请参考内部帮助：\n\u0026gt;\u0026gt;\u0026gt; from Bio.SeqIO import SffIO \u0026gt;\u0026gt;\u0026gt; help(SffIO) ... 在识别可能的基因中一个非常简单的第一步是寻找开放读码框（Open Reading Frame，ORF）。这里我们的意思是寻找六个编码框中所有的没有终止密码子的长区域 —— 一个ORF是一个不包含任何框内终止密码子的核苷酸区域。\n当然，为了发现基因，你也需要确定起始密码子、可能的启动子的位置 —— 而且在真核生物中，你也需要关心内含子。然而，这种方法在病毒和原核生物中仍然有效。\n为了展示怎样用Biopython实现这个目的，我们首先需要一个序列来查找。作为例子，我们再次使用细菌的质粒 —— 尽管这次我们将以没有任何基因标记的纯文本FASTA文件开始： NC_005816.fna 。这是一个细菌序列，所以我们需要使用NCBI密码子表11。\n\u0026gt;\u0026gt;\u0026gt; from Bio import SeqIO \u0026gt;\u0026gt;\u0026gt; record = SeqIO.read(\u0026#34;NC_005816.fna\u0026#34;,\u0026#34;fasta\u0026#34;) \u0026gt;\u0026gt;\u0026gt; table = 11 \u0026gt;\u0026gt;\u0026gt; min_pro_len = 100 这里有一个巧妙的技巧，使用 Seq 对象的 split 方法获得一个包含六个读码框中所有可能的ORF翻译的列表：\n\u0026gt;\u0026gt;\u0026gt; for strand, nuc in [(+1, record.seq), (-1, record.seq.reverse_complement())]: ... for frame in range(3): ... length = 3 * ((len(record)-frame) // 3) #Multiple of three ... for pro in nuc[frame:frame+length].translate(table).split(\u0026#34;*\u0026#34;): ... if len(pro) \u0026gt;= min_pro_len: ... print \u0026#34;%s...%s - length %i, strand %i, frame %i\u0026#34; \\ ... % (pro[:30], pro[-3:], len(pro), strand, frame) GCLMKKSSIVATIITILSGSANAASSQLIP...YRF - length 315, strand 1, frame 0 KSGELRQTPPASSTLHLRLILQRSGVMMEL...NPE - length 285, strand 1, frame 1 GLNCSFFSICNWKFIDYINRLFQIIYLCKN...YYH - length 176, strand 1, frame 1 VKKILYIKALFLCTVIKLRRFIFSVNNMKF...DLP - length 165, strand 1, frame 1 NQIQGVICSPDSGEFMVTFETVMEIKILHK...GVA - length 355, strand 1, frame 2 RRKEHVSKKRRPQKRPRRRRFFHRLRPPDE...PTR - length 128, strand 1, frame 2 TGKQNSCQMSAIWQLRQNTATKTRQNRARI...AIK - length 100, strand 1, frame 2 QGSGYAFPHASILSGIAMSHFYFLVLHAVK...CSD - length 114, strand -1, frame 0 IYSTSEHTGEQVMRTLDEVIASRSPESQTR...FHV - length 111, strand -1, frame 0 WGKLQVIGLSMWMVLFSQRFDDWLNEQEDA...ESK - length 125, strand -1, frame 1 RGIFMSDTMVVNGSGGVPAFLFSGSTLSSY...LLK - length 361, strand -1, frame 1 WDVKTVTGVLHHPFHLTFSLCPEGATQSGR...VKR - length 111, strand -1, frame 1 LSHTVTDFTDQMAQVGLCQCVNVFLDEVTG...KAA - length 107, strand -1, frame 2 RALTGLSAPGIRSQTSCDRLRELRYVPVSL...PLQ - length 119, strand -1, frame 2 意，这里我们从 每条 序列的5’末（起始）端开始计算读码框。对 正向 链一直从5’末（起始）端开始计算有时更加容易。\n你可以轻易编辑上面的循环代码，来创建一个待选蛋白列表，或者将它转换为一个列表解析。现在，这个代码所不能做的一个事情是记录蛋白的位置信息。\n你可以用几种方式来处理。例如，下面的代码以蛋白计数的方式记录位置信息，并通过乘以三倍来转换为父序列（parent sequence），并记录编码框和链的信息：\nfrom Bio import SeqIO record = SeqIO.read(\u0026#34;NC_005816.gb\u0026#34;,\u0026#34;genbank\u0026#34;) table = 11 min_pro_len = 100 def find_orfs_with_trans(seq, trans_table, min_protein_length): answer = [] seq_len = len(seq) for strand, nuc in [(+1, seq), (-1, seq.reverse_complement())]: for frame in range(3): trans = str(nuc[frame:].translate(trans_table)) trans_len = len(trans) aa_start = 0 aa_end = 0 while aa_start \u0026lt; trans_len: aa_end = trans.find(\u0026#34;*\u0026#34;, aa_start) if aa_end == -1: aa_end = trans_len if aa_end-aa_start \u0026gt;= min_protein_length: if strand == 1: start = frame+aa_start*3 end = min(seq_len,frame+aa_end*3+3) else: start = seq_len-frame-aa_end*3-3 end = seq_len-frame-aa_start*3 answer.append((start, end, strand, trans[aa_start:aa_end])) aa_start = aa_end+1 answer.sort() return answer orf_list = find_orfs_with_trans(record.seq, table, min_pro_len) for start, end, strand, pro in orf_list: print \u0026#34;%s...%s - length %i, strand %i, %i:%i\u0026#34; \\ % (pro[:30], pro[-3:], len(pro), strand, start, end) 输出是：\nNQIQGVICSPDSGEFMVTFETVMEIKILHK...GVA - length 355, strand 1, 41:1109 WDVKTVTGVLHHPFHLTFSLCPEGATQSGR...VKR - length 111, strand -1, 491:827 KSGELRQTPPASSTLHLRLILQRSGVMMEL...NPE - length 285, strand 1, 1030:1888 RALTGLSAPGIRSQTSCDRLRELRYVPVSL...PLQ - length 119, strand -1, 2830:3190 RRKEHVSKKRRPQKRPRRRRFFHRLRPPDE...PTR - length 128, strand 1, 3470:3857 GLNCSFFSICNWKFIDYINRLFQIIYLCKN...YYH - length 176, strand 1, 4249:4780 RGIFMSDTMVVNGSGGVPAFLFSGSTLSSY...LLK - length 361, strand -1, 4814:5900 VKKILYIKALFLCTVIKLRRFIFSVNNMKF...DLP - length 165, strand 1, 5923:6421 LSHTVTDFTDQMAQVGLCQCVNVFLDEVTG...KAA - length 107, strand -1, 5974:6298 GCLMKKSSIVATIITILSGSANAASSQLIP...YRF - length 315, strand 1, 6654:7602 IYSTSEHTGEQVMRTLDEVIASRSPESQTR...FHV - length 111, strand -1, 7788:8124 WGKLQVIGLSMWMVLFSQRFDDWLNEQEDA...ESK - length 125, strand -1, 8087:8465 TGKQNSCQMSAIWQLRQNTATKTRQNRARI...AIK - length 100, strand 1, 8741:9044 QGSGYAFPHASILSGIAMSHFYFLVLHAVK...CSD - length 114, strand -1, 9264:9609 如果你注释掉排序语句，那么蛋白序列将和之前显示的顺序一样，所以你能确定这是在做相同的事情。这里，我们可以按位置进行排序，使得和GenBank文件中的实际注释相比对较更加容易（就像章节 17.1.9 中显示的那样）。\n然而，如果你想要的只是所有开放读码框的位置，翻译每一个可能的密码子将是很浪费时间的，包括转换和查找反向互补链。那么，你所要做的所有事情是查找可能的终止密码子(和他们反向互补)。使用正则表达式是一个很直接的方式（参见Python中的 re 模块）。这是描述查找字符串的一个非常强大的模块（然而非常复杂），也被许多编程语言和命令行工具，如 grep，所支持。\n序列解析与简单作图 这一部分展示更多使用第 5 章介绍的 Bio.SeqIO 模块进行序列解析的例子，以及Python类库matplotlib中 pylab 的作图接口（参见 matplotlib 主页的教程 ）。注意，跟随这些例子，你需要安装matplotlib - 但是即使没有它，你依然可以尝试数据的解析的内容。\n序列长度柱状图 在很多时候，你可能想要将某个数据集中的序列长度分布进行可视化 —— 例如，基因组组装项目中的contig的大小范围。在这个例子中，我们将再次使用我们的兰花FASTA文件 ls_orchid.fasta ，它只包含94条序列。\n首先，我们使用 Bio.SeqIO 来解析这个FASTA文件，并创建一个序列长度的列表。你可以用一个for循环来实现，然而我觉得列表解析（list comprehension）更简洁：\n\u0026gt;\u0026gt;\u0026gt; from Bio import SeqIO \u0026gt;\u0026gt;\u0026gt; sizes = [len(rec) for rec in SeqIO.parse(\u0026#34;ls_orchid.fasta\u0026#34;, \u0026#34;fasta\u0026#34;)] \u0026gt;\u0026gt;\u0026gt; len(sizes), min(sizes), max(sizes) (94, 572, 789) \u0026gt;\u0026gt;\u0026gt; sizes [740, 753, 748, 744, 733, 718, 730, 704, 740, 709, 700, 726, ..., 592] 现在我们得到了所有基因的长度（以整数列表的形式），我们可以用matplotlib的柱状图功能来显示它。\nfrom Bio import SeqIO sizes = [len(rec) for rec in SeqIO.parse(\u0026#34;ls_orchid.fasta\u0026#34;, \u0026#34;fasta\u0026#34;)] import pylab pylab.hist(sizes, bins=20) pylab.title(\u0026#34;%i orchid sequences\\nLengths %i to %i\u0026#34; \\ % (len(sizes),min(sizes),max(sizes))) pylab.xlabel(\u0026#34;Sequence length (bp)\u0026#34;) pylab.ylabel(\u0026#34;Count\u0026#34;) pylab.show() 这将弹出一个包含如下图形的新的窗口：\n注意，这些兰花序列的长度大多数大约在740bp左右，这里有可能有两个不同长度的序列分类，其中包含一个较短的序列子集。\n提示： 除了使用 pylab.show() 在窗口中显示图像以外，你也可以使用 pylab.savefig(\u0026hellip;) 来将图像保存为图像文件中（例如PNG或PDF文件）。\n序列CG%含量作图 对于核酸序列，另一个经常计算的值是GC含量。例如，你可能想要查看一个细菌基因组中所有基因的GC%，并研究任何离群值来确定可能最近通过基因水平转移而获得的基因。同样，对于这个例子，我们再次使用兰花FASTA文件 ls_orchid.fasta 。\n首先，我们使用 Bio.SeqIO 解析这个FASTA文件并创建一个GC百分含量的列表。你可以使用for循环，但我更喜欢这样：\nfrom Bio import SeqIO from Bio.SeqUtils import GC gc_values = sorted(GC(rec.seq) for rec in SeqIO.parse(\u0026#34;ls_orchid.fasta\u0026#34;, \u0026#34;fasta\u0026#34;)) 读取完每个序列并计算了GC百分比，我们接着将它们按升序排列。现在，我们用matplotlib来对这个浮点数列表进行可视化：\nimport pylab pylab.plot(gc_values) pylab.title(\u0026#34;%i orchid sequences\\nGC%% %0.1f to %0.1f\u0026#34; \\ % (len(gc_values),min(gc_values),max(gc_values))) pylab.xlabel(\u0026#34;Genes\u0026#34;) pylab.ylabel(\u0026#34;GC%\u0026#34;) pylab.show() 像之前的例子一样，弹出一个窗口中将包含如下图形：\n如果你使用的是一个物种中的所有基因集，你可能会得到一个更加平滑的图\n核苷酸点线图 点线图是可视化比较两条核苷酸序列的相似性的一种方式。采用一个滑动窗来相互比较较短的子序列（比较通常根据一个不匹配阈值来实现）。为了简单起见，此处我们将只查找完全匹配（如下图黑色所示）。\n我们需要两条序列开始。为了论证，我们只取兰花FASTA文件中的前两条序列。ls_orchid.fasta:\nfrom Bio import SeqIO handle = open(\u0026#34;ls_orchid.fasta\u0026#34;) record_iterator = SeqIO.parse(handle, \u0026#34;fasta\u0026#34;) rec_one = record_iterator.next() rec_two = record_iterator.next() handle.close() 我们将展示两种方式。首先，一个简单的实现，它将所有滑动窗大小的子序列相互比较，并生产一个相似性矩阵。你可以创建一个矩阵或数组对象，而在这儿，我们只用一个用嵌套的列表解析生成的布尔值列表的列表。\nwindow = 7 seq_one = str(rec_one.seq).upper() seq_two = str(rec_two.seq).upper() data = [[(seq_one[i:i+window] \u0026lt;\u0026gt; seq_two[j:j+window]) \\ for j in range(len(seq_one)-window)] \\ for i in range(len(seq_two)-window)] 注意，我们在这里并 没有 检查反向的互补匹配。现在我们将使用matplotlib的 pylab.imshow() 函数来显示这个数据，首先启用灰度模式，以保证这是在黑白颜色下完成的：\nimport pylab pylab.gray() pylab.imshow(data) pylab.xlabel(\u0026#34;%s (length %i bp)\u0026#34; % (rec_one.id, len(rec_one))) pylab.ylabel(\u0026#34;%s (length %i bp)\u0026#34; % (rec_two.id, len(rec_two))) pylab.title(\u0026#34;Dot plot using window size %i\\n(allowing no mis-matches)\u0026#34; % window) pylab.show() 这将弹出一个新的窗口，包含类似这样的图形：\n可能如您所料，这两条序列非常相似，图中部分滑动窗大小的线沿着对角线匹配。图中没有对角线外的点，这意味着序列中并没有倒位或其他有趣的偏离对角线匹配。\n上面的代码在小的例子中工作得很好，但是应用到大的序列时，这里有两个问题。首先，这种以穷举地方式进行所有可能的两两比对非常缓慢。作为替代，我们将创建一个词典来映射所有滑动窗大小的子序列的位置，然后取两者的交集来获得两条序列中都发现的子序列。这将占用更多的内存，然而速度 更 快。另外， pylab.imshow( ) 函数只能显示较小的矩阵。作为替代，我们将使用 pylab.scatter( ) 函数。\n我们从创建，从滑动窗大小的子序列到其位置的字典映射开始：\nwindow = 7 dict_one = {} dict_two = {} for (seq, section_dict) in [(str(rec_one.seq).upper(), dict_one), (str(rec_two.seq).upper(), dict_two)]: for i in range(len(seq)-window): section = seq[i:i+window] try: section_dict[section].append(i) except KeyError: section_dict[section] = [i] #Now find any sub-sequences found in both sequences #(Python 2.3 would require slightly different code here) matches = set(dict_one).intersection(dict_two) print \u0026#34;%i unique matches\u0026#34; % len(matches) 为了使用 pylab.scatter() 函数，我们需要两个分别对应 x 和 y 轴的列表：\n#Create lists of x and y co-ordinates for scatter plot x = [] y = [] for section in matches: for i in dict_one[section]: for j in dict_two[section]: x.append(i) y.append(j) 现在我们能以散点图的形式画出优化后的点线图：\nimport pylab pylab.cla() #clear any prior graph pylab.gray() pylab.scatter(x,y) pylab.xlim(0, len(rec_one)-window) pylab.ylim(0, len(rec_two)-window) pylab.xlabel(\u0026#34;%s (length %i bp)\u0026#34; % (rec_one.id, len(rec_one))) pylab.ylabel(\u0026#34;%s (length %i bp)\u0026#34; % (rec_two.id, len(rec_two))) pylab.title(\u0026#34;Dot plot using window size %i\\n(allowing no mis-matches)\u0026#34; % window) pylab.show() 这将弹出一个新的窗口，包含如下图形：\n我个人认为第二个图更加易读！再次注意，我们在这里 没有 检查反向互补匹配 —— 你可以扩展这个例子来实现它，或许可以以一种颜色显示正向匹配，另一种显示反向匹配。\n绘制序列读长数据的质量图 如果你在处理二代测序数据，你可能希望绘制数据的质量图。这里使用两个包含双末端（paired end）读长的FASTQ文件作为例子，其中 SRR001666_1.fastq 为正向读长， SRR001666_2.fastq 为反向读长。它们可以从ENA序列读长档案的FTP站点下载（ ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR001/SRR001666/SRR001666_1.fastq.gz 和 ftp://ftp.sra.ebi.ac.uk/vol1/fastq/SRR001/SRR001666/SRR001666_2.fastq.gz ）， 且来自 E. coli —— 参见 http://www.ebi.ac.uk/ena/data/view/SRR001666 的详细介绍。在下面的代码中， pylab.subplot(\u0026hellip;) 函数被用来在两个子图中展示正向和反向的质量。这里也有少量的代码来保证仅仅展示前50个读长的质量。\nimport pylab from Bio import SeqIO for subfigure in [1,2]: filename = \u0026#34;SRR001666_%i.fastq\u0026#34; % subfigure pylab.subplot(1, 2, subfigure) for i,record in enumerate(SeqIO.parse(filename, \u0026#34;fastq\u0026#34;)): if i \u0026gt;= 50 : break #trick! pylab.plot(record.letter_annotations[\u0026#34;phred_quality\u0026#34;]) pylab.ylim(0,45) pylab.ylabel(\u0026#34;PHRED quality score\u0026#34;) pylab.xlabel(\u0026#34;Position\u0026#34;) pylab.savefig(\u0026#34;SRR001666.png\u0026#34;) print \u0026#34;Done\u0026#34; 你应该注意到，这里我们使用了 Bio.SeqIO 的格式名称 fastq ，因为NCBI使用标准Sanger FASTQ和PHRED分数的存储这些读长。然而，你可能从读长的长度中猜到，这些数据来自Illumina Genome Analyzer，而且可能最初是以Solexa/Illumina FASTQ两种格式变种中的一种存在。\n这个例子使用 pylab.savefig(\u0026hellip;) 函数，而不是pylab.show(…) ，然而就像前面提到的一样，它们两者都非常有用。下面是得到的结果：\n处理序列比对 计算摘要信息 一旦你有一个比对，你很可能希望找出关于它的一些信息。我们尽力将这些功能分离到单独的能作用于比对对象的类中，而不是将所有的能生成比对信息的函数都放入比对对象本身。\n准备计算比对对象的摘要信息非常快捷。假设我们已经得到了一个比对对象 alignment ，例如由在第 6 章介绍的 Bio.AlignIO.read(\u0026hellip;) 读入。我们获得该对象的摘要信息所要做的所有事情是：\nfrom Bio.Align import AlignInfo summary_align = AlignInfo.SummaryInfo(alignment) summary_align 对象非常有用，它将帮你做以下巧妙的事情：\n计算一个快速一致序列 – 参见章节 18.3.2 获取一个针对该比对的位点特异性打分矩阵 – 参见章节 18.3.3 计算比对的信息量 – 参见章节 18.3.4 生成该比对中的替换信息 – 章节 18.4 详细描述了使用该方法生成一个替换矩阵 计算一个快速一致序列 在章节 18.3.1 中描述的 SummaryInfo 对象提供了一个可以快速计算比对的保守（consensus）序列的功能。假设我们有一个 SummaryInfo 对象，叫做 summary_align，我们能通过下面的方法计算一个保守序列\nconsensus = summary_align.dumb_consensus() 就行名字显示的那样，这是一个非常简单的保守序列计算器，它将只是在保守序列中累加每个位点的所有残基，如果最普遍的残基数大于某个阈值时，这个最普遍的残基将被添加到保守序列中。如果它没有到达这个阈值，将添加一个“不确定字符”。最终返回的保守序列对象是一个Seq对象，它的字母表是从组成保守序列所有序列的字母表中推断出来的。所以使用 print consensus 将给出如下信息：\nconsensus Seq(\u0026#39;TATACATNAAAGNAGGGGGATGCGGATAAATGGAAAGGCGAAAGAAAGAAAAAAATGAAT ...\u0026#39;, IUPACAmbiguousDNA()) the threshold\n这是用来设定某个残基在某个位点出现的频率超过一定阈值，才将其添加到保守序列。默认为0.7（即70%）。\nthe ambiguous character\n指定保守序列中的不确定字符。默认为’N’。\nthe consensus alphabet\n指定保守序列的字母表。如果没有提供，我们将从比对序列的字母表基础上推断该字母表。\n位点特异性评分矩阵 位点特异性评分矩阵（Position specific score matrices，PSSMs）以另一种总结比对信息的方式（与刚才介绍的保守序列不同），这或许在某些情况下更为有用。简单来说，PSSM是一个计数矩阵。对于比对中的每一列，将所有可能出现的字母进行计数并加和。这些加和值将和一个代表序列（默认为比对中的第一条序列）一起显示出来。这个序列可能是保守序列，但也可以是比对中的任何序列。例如，对于比对，\nGTATC AT--C CTGTC 它的PSSM是：\nG A T C G 1 1 0 1 T 0 0 3 0 A 1 1 0 0 T 0 0 2 0 C 0 0 0 3 假设我们有一个比对对象叫做 c_align ，为了获得PSSM和保守序列，我们首先得到一个摘要对象（summary object），并计算一致序列：\nsummary_align = AlignInfo.SummaryInfo(c_align) consensus = summary_align.dumb_consensus() 现在，我们想创建PSSM，但是在计算中忽略任何 N 不确定残基：\nmy_pssm = summary_align.pos_specific_score_matrix(consensus, chars_to_ignore = [\u0026#39;N\u0026#39;]) 为了维持字母表的严格性，你可以在PSSM的顶部显示比对对象字母表中规定的字符。空白字符（Gaps）并不包含在PSSM的顶轴中。 传入并显示在左侧轴的序列可以不是保守序列。例如，你如果想要在PSSM左边显示比对中的第二条序列，你只需要： second_seq = alignment.get_seq_by_num(1) my_pssm = summary_align.pos_specific_score_matrix(second_seq chars_to_ignore = [\u0026#39;N\u0026#39;]) 以上的命令将返回一个 PSSM 对象。为了显示出PSSM，我们只需 print my_pssm，结果如下：\nA C G T T 0.0 0.0 0.0 7.0 A 7.0 0.0 0.0 0.0 T 0.0 0.0 0.0 7.0 A 7.0 0.0 0.0 0.0 C 0.0 7.0 0.0 0.0 A 7.0 0.0 0.0 0.0 T 0.0 0.0 0.0 7.0 T 1.0 0.0 0.0 6.0 ... 你可以用 your_pssm[sequence_number][residue_count_name] 获得任何PSSM的元素。例如，获取上面PSSM中第二个元素的‘A’残基的计数，你可以：\n\u0026gt;\u0026gt;\u0026gt; print my_pssm[1][\u0026#34;A\u0026#34;] 7.0 PSSM类的结构有望使得获取元素和打印漂亮的矩阵都很方便\n信息量 一个潜在而有用的衡量进化保守性的测度是序列的信息量。\n一个有用的针对分子生物学家的信息论的介绍可以在这里找到： http://www.lecb.ncifcrf.gov/~toms/paper/primer/ 。对于我们的目地，我们将查看保守序列或其部分序列的信息量。我们使用下面的公式计算多序列比对中某个特定的列的信息量：\n其中：\nIC__j – 比对中第 j 列的信息量。 N__a – 字母表中字母的个数。 P__ij – 第 j 列的某个特定字母 i 的频率（即，如果G在包含有6个序列的比对中有3次出现，则该列G的信息量为0.5） Q__i – 字母 i 的期望频率。这是一个可选参数，由用户自行决定使用。默认情况下，它被自动赋值为0.05 = 1/20，若为蛋白字母表；或0.25 = 1/4 ，若为核酸字母表。这是在没有先验分布假设的情况下计算信息量。而在假设先验分布或使用非标准字母表时，你需要提供 Q__i 的值。 好了，现在我们知道Biopython如何计算了序列比对的信息量，让我们看看怎么对部分比对区域进行计算。\n首先，我们需要使用我们的比对来获得一个比对摘要对象，我们假设它叫做 summary_align （参见章节 18.3.1 来了解怎样得到它）。一旦我们得到这个对象，计算某个区域的信息量就像下面一样简单：\ninfo_content = summary_align.information_content(5, 30, chars_to_ignore = [\u0026#39;N\u0026#39;]) 哇哦，这比上面的公式看起来要简单多了！变量 info_content 现在含有一个浮点数来表示指定区域（比对中的5到30）的信息量。我们在计算信息量时特意忽略了不确定残基’N’，因为这个值没有包括在我们的字母表中（因而我们不必要关心它！）。\n像上面提到的一样，我们同样能通过提供期望频率计算相对信息量：\nexpect_freq = { \u0026#39;A\u0026#39; : .3, \u0026#39;G\u0026#39; : .2, \u0026#39;T\u0026#39; : .3, \u0026#39;C\u0026#39; : .2} 期望值不能以原始的字典传入，而需要作为 SubsMat.FreqTable 对象传入（参见章节 20.2.2 以获得关于FreqTables的更多信息）。FreqTable对象提供了一个关联字典和字母表的标准，这和Biopython中Seq类的工作方式类似。\n要从频率字典创建一个FreqTable对象，你只需要：\nfrom Bio.Alphabet import IUPAC from Bio.SubsMat import FreqTable e_freq_table = FreqTable.FreqTable(expect_freq, FreqTable.FREQ, IUPAC.unambiguous_dna) 现在我们得到了它，计算我们比对区域的相对信息量就像下面一样简单：\ninfo_content = summary_align.information_content(5, 30, e_freq_table = e_freq_table, chars_to_ignore = [\u0026#39;N\u0026#39;]) 现在，info_content 将包含与期望频率相关的该区域的相对信息量。\n返回值是按上面的公式以2为对底数计算的。你可以通过传入 log_base 参数来改变成你想要的底数：\ninfo_content = summary_align.information_content(5, 30, log_base = 10, chars_to_ignore = [\u0026#39;N\u0026#39;]) 好了，现在你已经知道怎么计算信息量了。如果你想要在实际的生命科学问题中应用它，最好找一些关于 信息量的文献钻研，以了解它是怎样用的。希望你的钻研不会发现任何有关这个函数的编码错误。\n替换矩阵 替换矩阵是每天的生物信息学工作中的极端重要的一部分。它们提供决定两个不同的残基有多少相互替换的可能性的得分规则。这在序列比较中必不可少。Durbin等的“Biological Biological Sequence Analysis” 一书中提供了对替换矩阵以及它们的用法的非常好的介绍。一些非常有名的替换矩阵是PAM和BLOSUM系列矩阵。\nBiopython提供了大量的常见替换矩阵，也提供了创建你自己的替换矩阵的功能。\n使用常见替换矩阵 从序列比对创建你自己的替换矩阵 使用替换矩阵类能轻易做出的一个非常酷的事情，是从序列比对创建出你自己的替换矩阵。实际中，通常是使用蛋白比对来做。在这个例子中，我们将首先得到一个Biopython比对对象，然后得到一个摘要对象来计算关于这个比对的相关信息。文件 protein.aln （也可在 这里 获取）包含Clustalw格式的比对输出。\n\u0026gt;\u0026gt;\u0026gt; from Bio import AlignIO \u0026gt;\u0026gt;\u0026gt; from Bio import Alphabet \u0026gt;\u0026gt;\u0026gt; from Bio.Alphabet import IUPAC \u0026gt;\u0026gt;\u0026gt; from Bio.Align import AlignInfo \u0026gt;\u0026gt;\u0026gt; filename = \u0026#34;protein.aln\u0026#34; \u0026gt;\u0026gt;\u0026gt; alpha = Alphabet.Gapped(IUPAC.protein) \u0026gt;\u0026gt;\u0026gt; c_align = AlignIO.read(filename, \u0026#34;clustal\u0026#34;, alphabet=alpha) \u0026gt;\u0026gt;\u0026gt; summary_align = AlignInfo.SummaryInfo(c_align) 现在我们得到了我们的 summary_align 对象，我们想使用它来找出不同的残基相互替换的次数。为了使例子的可读性更强，我们将只关注那些有极性电荷侧链的氨基酸。幸运的是，这能在生成替代字典时轻松实现，通过传入所有需要被忽略的字符。这样我们将能创建一个只包含带电荷的极性氨基酸的替代字典：\n\u0026gt;\u0026gt;\u0026gt; replace_info = summary_align.replacement_dictionary([\u0026#34;G\u0026#34;, \u0026#34;A\u0026#34;, \u0026#34;V\u0026#34;, \u0026#34;L\u0026#34;, \u0026#34;I\u0026#34;, ... \u0026#34;M\u0026#34;, \u0026#34;P\u0026#34;, \u0026#34;F\u0026#34;, \u0026#34;W\u0026#34;, \u0026#34;S\u0026#34;, ... \u0026#34;T\u0026#34;, \u0026#34;N\u0026#34;, \u0026#34;Q\u0026#34;, \u0026#34;Y\u0026#34;, \u0026#34;C\u0026#34;]) 这个关于氨基酸替代的信息以python字典的形式展示出来将会像如下的样子（顺序可能有所差异）：\n{(\u0026#39;R\u0026#39;, \u0026#39;R\u0026#39;): 2079.0, (\u0026#39;R\u0026#39;, \u0026#39;H\u0026#39;): 17.0, (\u0026#39;R\u0026#39;, \u0026#39;K\u0026#39;): 103.0, (\u0026#39;R\u0026#39;, \u0026#39;E\u0026#39;): 2.0, (\u0026#39;R\u0026#39;, \u0026#39;D\u0026#39;): 2.0, (\u0026#39;H\u0026#39;, \u0026#39;R\u0026#39;): 0, (\u0026#39;D\u0026#39;, \u0026#39;H\u0026#39;): 15.0, (\u0026#39;K\u0026#39;, \u0026#39;K\u0026#39;): 3218.0, (\u0026#39;K\u0026#39;, \u0026#39;H\u0026#39;): 24.0, (\u0026#39;H\u0026#39;, \u0026#39;K\u0026#39;): 8.0, (\u0026#39;E\u0026#39;, \u0026#39;H\u0026#39;): 15.0, (\u0026#39;H\u0026#39;, \u0026#39;H\u0026#39;): 1235.0, (\u0026#39;H\u0026#39;, \u0026#39;E\u0026#39;): 18.0, (\u0026#39;H\u0026#39;, \u0026#39;D\u0026#39;): 0, (\u0026#39;K\u0026#39;, \u0026#39;D\u0026#39;): 0, (\u0026#39;K\u0026#39;, \u0026#39;E\u0026#39;): 9.0, (\u0026#39;D\u0026#39;, \u0026#39;R\u0026#39;): 48.0, (\u0026#39;E\u0026#39;, \u0026#39;R\u0026#39;): 2.0, (\u0026#39;D\u0026#39;, \u0026#39;K\u0026#39;): 1.0, (\u0026#39;E\u0026#39;, \u0026#39;K\u0026#39;): 45.0, (\u0026#39;K\u0026#39;, \u0026#39;R\u0026#39;): 130.0, (\u0026#39;E\u0026#39;, \u0026#39;D\u0026#39;): 241.0, (\u0026#39;E\u0026#39;, \u0026#39;E\u0026#39;): 3305.0, (\u0026#39;D\u0026#39;, \u0026#39;E\u0026#39;): 270.0, (\u0026#39;D\u0026#39;, \u0026#39;D\u0026#39;): 2360.0} 这个信息提供了我们所需要的替换次数，或者说我们期望的不同的事情相互替换有多么频繁。事实上，（你可能会感到惊奇）这就是我们继续创建替代矩阵所需要的全部信息。首先，我们使用替代字典信息创建一个“接受替换矩阵”（Accepted Replacement Matrix，ARM）：\n\u0026gt;\u0026gt;\u0026gt; from Bio import SubsMat \u0026gt;\u0026gt;\u0026gt; my_arm = SubsMat.SeqMat(replace_info) 使用这个“接受替换矩阵”，我们能继续创建我们的对数矩阵（即一个标准类型的替换举证）：\n\u0026gt;\u0026gt;\u0026gt; my_lom = SubsMat.make_log_odds_matrix(my_arm) 在创建的这个对数矩阵时有以下可选参数：\nexp_freq_table – 你可以传入一个每个字母的期望频率的表格。如果提供，在计算期望替换时，这将替代传入的“接收替换矩阵”。 logbase - 用来创建对数奇数矩阵的对数底数。默认为10。 factor - 用来乘以每个矩阵元素的因数。默认为10，这样通常可以使得矩阵的数据容易处理。 round_digit - 矩阵中四舍五入所取的小数位数，默认为0（即没有小数）。 一旦你获得了你的对数矩阵，你可以使用函数 print_mat 很漂亮的显示出来。使用我们创建的矩阵可以得到： \u0026gt;\u0026gt;\u0026gt; my_lom.print_mat() D 2 E -1 1 H -5 -4 3 K -10 -5 -4 1 R -4 -8 -4 -2 2 D E H K R 很好。我们现在得到了自己的替换矩阵！\nBioSQL-存储序列到关系数据库中 BioSQL 是 OBF 多个项目（BioPerl、 BioJava等）为了支持共享的存储序列数据的数据库架构而共同努力的结果。理论上，你可以用BioPerl加载GenBank文件到数据库中，然后用Biopython从数据库中提取出来为一个包含Feature的Record对象 —— 并获得或多或少和直接用 Bio.SeqIO （第 5 章）加载GenBank文件为SeqRecord相同的东西。\nBiopython中BioSQL模块的文档目前放在 http://biopython.org/wiki/BioSQL ，是我们维基页面的一部分。\nNextPrevious\n","date":"2021-08-17T00:00:00Z","permalink":"https://example.com/p/ch18_coobook/","title":"ch18_coobook"},{"content":" ","date":"2021-08-16T00:00:00Z","permalink":"https://example.com/p/python%E7%94%9F%E7%89%A9%E4%BF%A1%E6%81%AF%E5%AD%A6%E6%95%B0%E6%8D%AE%E7%AE%A1%E7%90%86/","title":"《PYTHON生物信息学数据管理》"},{"content":"biopython官方地址：https://biopython.org/\ngithub地址：https://github.com/biopython/biopython/blob/master/\n中文版教程：https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html\nbiopython包的说明（具体到每个模块了）：https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html\n注意本章介绍的所有监督学习方法都需要先安装Numerical Python （numpy）。\nLogistic回归模型 背景和目的 Logistic回归是一种监督学习方法，通过若干预测变量 x__i 的加权和来尝试将样本划分为 K 个不同类别。Logistic回归模型可用来计算预测变量的权重 β_i_ 。在Biopython中，logistic回归模型目前只实现了二类别（ K = 2 ）分类，而预测变量的数量没有限制。\n作为一个例子，我们试着预测细菌中的操纵子结构。一个操纵子是在一条DNA链上许多相邻基因组成的一个集合，可以被共同转录为一条mRNA分子。这条mRNA分子经翻译后产生多个不同的蛋白质。我们将以枯草芽孢杆菌的操纵子数据进行说明，它的一个操纵子平均包含2.4个基因。\n作为理解细菌的基因调节的第一步，我们需要知道其操纵子的结构。枯草芽孢杆菌大约10%的基因操纵子结构已经通过实验获知。剩下的90%的基因操纵子结构可以通过一种监督学习方法来预测。\n在这种监督学习方法中，我们需要选择某些与操纵子结构有关的容易度量的预测变量 x__i 。例如可以选择基因间碱基对距离来来作为其中一个预测变量。同一个操纵子中的相邻基因往往距离相对较近，而位于不同操纵子的相邻基因间通常具有更大的空间来容纳启动子和终止子序列。另一个预测变量可以基于基因表达量度。根据操纵子的定义，属于同一个操纵子的基因有相同的基因表达谱，而不同操纵子的两个基因的表达谱也不相同。在实际操作中，由于存在测量误差，对相同操纵子的基因表达轮廓的测量不会完全一致。为了测量基因表达轮廓的相似性，我们假设测量误差服从正态分布，然后计算对应的对数似然分值。\n现在我们有了两个预测变量，可以据此预测在同一条DNA链上两个相邻基因是否属于相同的操纵子： - _x_1 ：两基因间的碱基对数； - _x_2 ：两基因表达谱的相似度。\n在logistic回归模型中，我们使用这两个预测变量的加权和来计算一个联合得分 S：\nS=β0+β1x1+β2x2.S=β0+β1x1+β2x2.\n根据下面两组示例基因，logistic回归模型对参数 β0 ， β1, β2 给出合适的值： - OP: 相邻基因，相同DNA链，属于相同操纵子； - NOP: 相邻基因，相同DNA链，属于不同操纵子。\n在logistic回归模型中，属于某个类别的概率依赖于通过logistic函数得出的分数。对于这两类OP和NOP，相应概率可如下表述：\n使用一组已知是否属于相同操纵子（OP类别）或不同操纵子（NOP类别）的基因对，通过最大化相应概率函数的对数似然值，我们可以计算权重 β0, β1, β2 。\n训练logistic回归模型 已知类别(OP or NOP)的相邻基因对.如果两个基因相重叠，其基因间距离为负值\n基因对\t基因间距离 (x1)\t基因表达得分 (x2)\t类别 cotJA — cotJB\t-53\t-200.78\tOP yesK — yesL\t117\t-267.14\tOP lplA — lplB\t57\t-163.47\tOP lplB — lplC\t16\t-190.30\tOP lplC — lplD\t11\t-220.94\tOP lplD — yetF\t85\t-193.94\tOP yfmT — yfmS\t16\t-182.71\tOP yfmF — yfmE\t15\t-180.41\tOP citS — citT\t-26\t-181.73\tOP citM — yflN\t58\t-259.87\tOP yfiI — yfiJ\t126\t-414.53\tNOP lipB — yfiQ\t191\t-249.57\tNOP yfiU — yfiV\t113\t-265.28\tNOP yfhH — yfhI\t145\t-312.99\tNOP cotY — cotX\t154\t-213.83\tNOP yjoB — rapA\t147\t-380.85\tNOP ptsI — splA\t93\t-291.13\tNO 列出了枯草芽孢杆菌的一些基因对，这些基因的操纵子结构已知。让我们根据表中的这些数据来计算其logistic回归模型：\n\u0026gt;\u0026gt;\u0026gt; from Bio import LogisticRegression \u0026gt;\u0026gt;\u0026gt; xs = [[-53, -200.78], [117, -267.14], [57, -163.47], [16, -190.30], [11, -220.94], [85, -193.94], [16, -182.71], [15, -180.41], [-26, -181.73], [58, -259.87], [126, -414.53], [191, -249.57], [113, -265.28], [145, -312.99], [154, -213.83], [147, -380.85], [93, -291.13]] \u0026gt;\u0026gt;\u0026gt; ys = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0] \u0026gt;\u0026gt;\u0026gt; model = LogisticRegression.train(xs, ys) 这里， xs 和 ys 是训练数据： xs 包含每个基因对的预测变量， ys 指定是否这个基因对属于相同操纵子（ 1 ，类别OP）或不同操纵子（0，类别NOP）。Logistic回归模型结果存储在 model 中，包含权重 β0, β1, and β2:\n\u0026gt;\u0026gt;\u0026gt; model.beta [8.9830290157144681, -0.035968960444850887, 0.02181395662983519] 注意 β1 是负的，这是因为具有更短基因间距离的基因对有更高的概率属于相同操纵子（类别OP）。另一方面， β2 为正，因为属于相同操纵子的基因对通常有更高的基因表达谱相似性得分。参数 β0 是正值是因为在这个训练数据中操纵子基因对占据大多数。\n函数 train 有两个可选参数： update_fn 和 typecode 。 update_fn 可用来指定一个回调函数，以迭代数和对数似然值做参数。在这个例子中，我们可以使用这个回调函数追踪模型计算（使用Newton-Raphson迭代来最大化logistic回归模型的对数似然函数）进度：\n\u0026gt;\u0026gt;\u0026gt; def show_progress(iteration, loglikelihood): print \u0026#34;Iteration:\u0026#34;, iteration, \u0026#34;Log-likelihood function:\u0026#34;, loglikelihood \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; model = LogisticRegression.train(xs, ys, update_fn=show_progress) Iteration: 0 Log-likelihood function: -11.7835020695 Iteration: 1 Log-likelihood function: -7.15886767672 Iteration: 2 Log-likelihood function: -5.76877209868 Iteration: 3 Log-likelihood function: -5.11362294338 Iteration: 4 Log-likelihood function: -4.74870642433 Iteration: 5 Log-likelihood function: -4.50026077146 Iteration: 6 Log-likelihood function: -4.31127773737 Iteration: 7 Log-likelihood function: -4.16015043396 Iteration: 8 Log-likelihood function: -4.03561719785 Iteration: 9 Log-likelihood function: -3.93073282192 Iteration: 10 Log-likelihood function: -3.84087660929 Iteration: 11 Log-likelihood function: -3.76282560605 Iteration: 12 Log-likelihood function: -3.69425027154 Iteration: 13 Log-likelihood function: -3.6334178602 Iteration: 14 Log-likelihood function: -3.57900855837 Iteration: 15 Log-likelihood function: -3.52999671386 Iteration: 16 Log-likelihood function: -3.48557145163 Iteration: 17 Log-likelihood function: -3.44508206139 Iteration: 18 Log-likelihood function: -3.40799948447 Iteration: 19 Log-likelihood function: -3.3738885624 Iteration: 20 Log-likelihood function: -3.3423876581 Iteration: 21 Log-likelihood function: -3.31319343769 Iteration: 22 Log-likelihood function: -3.2860493346 Iteration: 23 Log-likelihood function: -3.2607366863 Iteration: 24 Log-likelihood function: -3.23706784091 Iteration: 25 Log-likelihood function: -3.21488073614 Iteration: 26 Log-likelihood function: -3.19403459259 Iteration: 27 Log-likelihood function: -3.17440646052 Iteration: 28 Log-likelihood function: -3.15588842703 Iteration: 29 Log-likelihood function: -3.13838533947 Iteration: 30 Log-likelihood function: -3.12181293595 Iteration: 31 Log-likelihood function: -3.10609629966 Iteration: 32 Log-likelihood function: -3.09116857282 Iteration: 33 Log-likelihood function: -3.07696988017 Iteration: 34 Log-likelihood function: -3.06344642288 Iteration: 35 Log-likelihood function: -3.05054971191 Iteration: 36 Log-likelihood function: -3.03823591619 Iteration: 37 Log-likelihood function: -3.02646530573 Iteration: 38 Log-likelihood function: -3.01520177394 Iteration: 39 Log-likelihood function: -3.00441242601 Iteration: 40 Log-likelihood function: -2.99406722296 Iteration: 41 Log-likelihood function: -2.98413867259 一旦对数似然函数得分增加值小于0.01，迭代将终止。如果在500次迭代后还没有到达收敛， train 函数返回并抛出一个 AssertionError 。\n可选的关键字 typecode 几乎可以一直忽略。这个关键字允许用户选择要使用的数值矩阵类型。当为了避免大数据计算的内存问题时，可能有必要使用单精度浮点数（Float8，Float16等等）而不是默认的double型。\n使用logistic回归模型进行分类 调用 classify 函数可以进行分类。给定一个logistic回归模型和 _x_1 和 _x_2 的值（例如，操纵子结构未知的基因对）， classify 函数返回 1 或 0 ，分别对应类别OP和NOP。例如，考虑基因对 yxcE ， yxcD 和 yxiB ， yxiA ：\n操纵子状态未知的相邻基因对\n基因对\t基因间距离 x1\t基因表达得分 x2 yxcE — yxcD\t6\t-173.143442352 yxiB — yxiA\t309\t-271.005880394 Logistic回归模型预测 yxcE ， yxcD 属于相同操纵子（类别OP），而 yxiB ， yxiA 属于不同操纵子:\n\u0026gt;\u0026gt;\u0026gt; print \u0026#34;yxcE, yxcD:\u0026#34;, LogisticRegression.classify(model, [6,-173.143442352]) yxcE, yxcD: 1 \u0026gt;\u0026gt;\u0026gt; print \u0026#34;yxiB, yxiA:\u0026#34;, LogisticRegression.classify(model, [309, -271.005880394]) yxiB, yxiA: 0 这个结果和生物学文献报道的一致）。\n为了确定这个预测的可信度，我们可以调用 calculate 函数来获得类别OP和NOP的概率公式。对于 yxcE, yxcD 我们发现\n\u0026gt;\u0026gt;\u0026gt; q, p = LogisticRegression.calculate(model, [6,-173.143442352]) \u0026gt;\u0026gt;\u0026gt; print \u0026#34;class OP: probability =\u0026#34;, p, \u0026#34;class NOP: probability =\u0026#34;, q class OP: probability = 0.993242163503 class NOP: probability = 0.00675783649744 对于 yxiB ， yxiA\n\u0026gt;\u0026gt;\u0026gt; q, p = LogisticRegression.calculate(model, [309, -271.005880394]) \u0026gt;\u0026gt;\u0026gt; print \u0026#34;class OP: probability =\u0026#34;, p, \u0026#34;class NOP: probability =\u0026#34;, q class OP: probability = 0.000321211251817 class NOP: probability = 0.999678788748 为了确定回归模型的预测精确性，我们可以把模型应用到训练数据上：\n\u0026gt;\u0026gt;\u0026gt; for i in range(len(ys)): print \u0026#34;True:\u0026#34;, ys[i], \u0026#34;Predicted:\u0026#34;, LogisticRegression.classify(model, xs[i]) True: 1 Predicted: 1 True: 1 Predicted: 0 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 0 Predicted: 0 True: 0 Predicted: 0 True: 0 Predicted: 0 True: 0 Predicted: 0 True: 0 Predicted: 0 True: 0 Predicted: 0 True: 0 Predicted: 0 这表示除一个基因对外其他所有预测都是正确的。Leave-one-out分析可以对预测精确性给出一个更可信的估计。Leave-one-out是指从训练数据中移除要预测的基因重新计算模型，再用该模型进行预测比对：\n\u0026gt;\u0026gt;\u0026gt; for i in range(len(ys)): model = LogisticRegression.train(xs[:i]+xs[i+1:], ys[:i]+ys[i+1:]) print \u0026#34;True:\u0026#34;, ys[i], \u0026#34;Predicted:\u0026#34;, LogisticRegression.classify(model, xs[i]) True: 1 Predicted: 1 True: 1 Predicted: 0 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 0 Predicted: 0 True: 0 Predicted: 0 True: 0 Predicted: 0 True: 0 Predicted: 0 True: 0 Predicted: 1 True: 0 Predicted: 0 True: 0 Predicted: 0 Leave-one-out分析显示这个logistic回归模型的预测只对两个基因对不正确，对应预测精确度为88%。\nLogistic回归，线性判别分析和支持向量机 Logistic回归模型类似于线性判别分析。在线性判别分析中，类别概率同样可由方程(16.2) 和 (16.3) 给出。但是，不是直接估计系数β，我们首先对预测变量 x 拟合一个正态分布。然后通过这个正态分布的平均值和方差计算系数β。如果 x 的分布确实是正态的，线性判别分析将比logistic回归模型有更好的性能。另一方面，logistic回归模型对于偏态到正态的广泛分布更加强健。\n另一个相似的方法是应用线性核函数的支持向量机。这样的SVM也使用一个预测变量的线性组合，但是是从靠近类别之间的边界区域的预测变量 x 来估计系数β。如果logistic回归模型(公式 (16.2) 和 (16.3) )很好的描述了远离边界区域的 x ，我们可以期望logistic回归模型优于线性核函数SVM，因为它应用了更多数据。如果不是，SVM可能更好。\nTrevor Hastie, Robert Tibshirani, and Jerome Friedman: The Elements of Statistical Learning. Data Mining, Inference, and Prediction.(统计学习基础:数据挖掘、推理与预测) Springer Series in Statistics, 2001. 4.4章.\nK-最近邻居法KNN 背景和目的 最近邻居法是一种不需要将数据拟合到一个模型的监督学习算法。数据点是基于训练数据集的 k 个最近邻居类别进行分类的。\n在Biopython中， KNN 方法可在 Bio.KNN 中获得。我们使用 16.1 同样的操纵子数据集来说明Biopython中 KNN 方法的用法\n初始化一个KNN模型 我们创建和初始化一个_KNN_模型：\n\u0026gt;\u0026gt;\u0026gt; from Bio import kNN \u0026gt;\u0026gt;\u0026gt; k = 3 \u0026gt;\u0026gt;\u0026gt; model = kNN.train(xs, ys, k) 这里 xs 和 ys 和 前面的相同。 k 是分类中的邻居数 k 。对于二分类，为 k 选择一个奇数可以避免tied votes。函数名 train 在这里有点不合适，因为就没有训练模型：这个函数仅仅是用来存储模型变量 xs ， ys 和 k 。\n使用KNN模型来分类 应用 KNN 模型对新数据进行分类，我们使用 classify 函数。这个函数以一个数据点(_x_1,_x_2)为参数并在训练数据集 xs 中寻找 k -最近邻居。然后基于在这 k 个邻居中出现最多的类别（ ys ）来对数据点(_x_1,_x_2)进行分类。\n对于基因对 yxcE 、 yxcD 和 yxiB 、 yxiA 的例子，我们发现\n\u0026gt;\u0026gt;\u0026gt; x = [6, -173.143442352] \u0026gt;\u0026gt;\u0026gt; print \u0026#34;yxcE, yxcD:\u0026#34;, kNN.classify(model, x) yxcE, yxcD: 1 \u0026gt;\u0026gt;\u0026gt; x = [309, -271.005880394] \u0026gt;\u0026gt;\u0026gt; print \u0026#34;yxiB, yxiA:\u0026#34;, kNN.classify(model, x) yxiB, yxiA: 0 和logistic回归模型一致，yxcE,_yxcD_被归为一类（类别OP），yxiB,_yxiA_属于不同操纵子。\n函数 classify 可以指定距离函数和权重函数作为可选参数。距离函数影响作为最近邻居的 k 个类别的选择，因为这些到查询点（ x ， y ）有最小距离的类别被定义为邻居。默认使用欧几里德距离。另外，我们也可以如示例中的使用曼哈顿距离：\n\u0026gt;\u0026gt;\u0026gt; def cityblock(x1, x2): ... assert len(x1)==2 ... assert len(x2)==2 ... distance = abs(x1[0]-x2[0]) + abs(x1[1]-x2[1]) ... return distance ... \u0026gt;\u0026gt;\u0026gt; x = [6, -173.143442352] \u0026gt;\u0026gt;\u0026gt; print \u0026#34;yxcE, yxcD:\u0026#34;, kNN.classify(model, x, distance_fn = cityblock) yxcE, yxcD: 1 权重函数可以用于权重投票。例如，相比于相邻较远的邻居，我们可能想给更近的邻居一个更高的权重：\n\u0026gt;\u0026gt;\u0026gt; def weight(x1, x2): ... assert len(x1)==2 ... assert len(x2)==2 ... return exp(-abs(x1[0]-x2[0]) - abs(x1[1]-x2[1])) ... \u0026gt;\u0026gt;\u0026gt; x = [6, -173.143442352] \u0026gt;\u0026gt;\u0026gt; print \u0026#34;yxcE, yxcD:\u0026#34;, kNN.classify(model, x, weight_fn = weight) yxcE, yxcD: 1 默认所有邻居有相同权重。\n为了确定这些预测的置信度，我们可以调用函数 calculate 来计算分配到类别OP和NOP的总权重。对于默认的加权方案，这样减少了每个分类的邻居数量。对于 yxcE ， yxcD ， 我们发现\n\u0026gt;\u0026gt;\u0026gt; x = [6, -173.143442352] \u0026gt;\u0026gt;\u0026gt; weight = kNN.calculate(model, x) \u0026gt;\u0026gt;\u0026gt; print \u0026#34;class OP: weight =\u0026#34;, weight[0], \u0026#34;class NOP: weight =\u0026#34;, weight[1] class OP: weight = 0.0 class NOP: weight = 3.0 这意味着 x1 ， x2 的所有三个邻居都属于NOP类别。对另一个例子 yesK ， yesL 我们发现\n\u0026gt;\u0026gt;\u0026gt; x = [117, -267.14] \u0026gt;\u0026gt;\u0026gt; weight = kNN.calculate(model, x) \u0026gt;\u0026gt;\u0026gt; print \u0026#34;class OP: weight =\u0026#34;, weight[0], \u0026#34;class NOP: weight =\u0026#34;, weight[1] class OP: weight = 2.0 class NOP: weight = 1.0 这意思是两个邻居是操纵子对，另一个是非操纵子对\n对于_KNN_方法的预测精确性，我们对训练数据应用模型：\n\u0026gt;\u0026gt; for i in range(len(ys)): print \u0026#34;True:\u0026#34;, ys[i], \u0026#34;Predicted:\u0026#34;, kNN.classify(model, xs[i]) True: 1 Predicted: 1 True: 1 Predicted: 0 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 0 True: 0 Predicted: 0 True: 0 Predicted: 0 True: 0 Predicted: 0 True: 0 Predicted: 0 True: 0 Predicted: 0 True: 0 Predicted: 0 True: 0 Predicted: 0 显示除了两个基因对所有预测都是正确的。Leave-one-out分析可以对预测精确性给出一个更可信的估计，这是通过从训练数据中移除要预测的基因，再重新计算模型实现：\n\u0026gt;\u0026gt;\u0026gt; for i in range(len(ys)): model = kNN.train(xs[:i]+xs[i+1:], ys[:i]+ys[i+1:]) print \u0026#34;True:\u0026#34;, ys[i], \u0026#34;Predicted:\u0026#34;, kNN.classify(model, xs[i]) True: 1 Predicted: 1 True: 1 Predicted: 0 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 1 True: 1 Predicted: 0 True: 0 Predicted: 0 True: 0 Predicted: 0 True: 0 Predicted: 1 True: 0 Predicted: 0 True: 0 Predicted: 0 True: 0 Predicted: 0 True: 0 Predicted: 1 Leave-one-out分析显示这个 KNN 模型的预测正确17个基因对中的13个，对应预测精确度为76%。\nNaive贝叶斯 这部分将描述模块 Bio.NaiveBayes\n最大熵 这部分将描述模块 Bio.MaximumEntropy\n马尔科夫模型 这部分将描述模块 Bio.MarkovModel 和/或 Bio.HMM.MarkovModel .\n","date":"2021-08-16T00:00:00Z","permalink":"https://example.com/p/ch16_%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/","title":"ch16_监督学习方法"},{"content":"biopython官方地址：https://biopython.org/\ngithub地址：https://github.com/biopython/biopython/blob/master/\n中文版教程：https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html\nbiopython包的说明（具体到每个模块了）：https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html\n聚类分析是根据元素相似度，进行分组的过程。在生物信息学中，聚类分析广泛 用于基因表达数据分析，用来对具有相似表达谱的基因归类；从而鉴定功能相关的基 因，或预测未知基因的功能。\nBiopython中的 Bio.Cluster 模块提供了常用的聚类算法。虽然Bio.Cluster被设计用于 基因表达数据，它也可用于其他类型数据的聚类。 Bio.Cluster 和其使用的C聚类库的说明见De Hoon et al. [14].\nBio.Cluster 包含了以下四种聚类算法：\n系统聚类（成对重心法，最短距离，最大距离和平均连锁法); k-means, k-medians, 和 k-medoids 聚类; 自组织映射（Self-Organizing Maps）; 主成分分析 数据表示法\n用于聚类的输入为一个 n x m 的Python 数值矩阵 data。在基因表达数据聚类中， 每一行表示不同的基因，每一列表示不同的实验条件。 Bio.Cluster 既可以 针对每行（基因），也可以针对每列（实验条件）进行聚类。\n缺失值\n在芯片实验中，经常会有些缺失值，可以用一个额外的 n × m Numerical Python 整型矩阵 mask 表示。 例如 mask[i,j] ,表示 data[i,j] 是个缺失值， 并且在分析中被忽略。\n随机数据生成器\nk-means/medians/medoids 聚类和 Self-Organizing Maps (SOMs) 需要调用随机数生成器。在 Bio.Cluster 中，正态分布随机数 生成器的算法是基于L’Ecuyer [25] ，二项分布的随机数 生成器算法是基于Kachitvichyanukul and Schmeiser [23] 开发的BTPE算法。随机数生成器在调用时会首先进行初始化。由于随机数生成器使用了 两个乘同余发生器（multiplicative linear congruential generators），所以初始化时需要两个整型的 种子。这两个种子可以调用系统提供的 rand （C标准库）函数生成。在 Bio.Cluster 中， 我们首先调用 srand 使用以秒为单位的时间戳的值初始值，再用 rand 随机产生两 个随机数作为种子来产生正态分布的随机数。 距离函数 为了对元素根据相似度进行聚类，第一步需要定义相似度。Bio.Cluster 提供了八种不同 的距离函数来衡量相似度或者距离，分别用不同的字母代表：\n\u0026rsquo;e\u0026rsquo;: Euclidean 距离; \u0026lsquo;b\u0026rsquo;: City-block 距离. \u0026lsquo;c\u0026rsquo;: Pearson 相关系数; \u0026lsquo;a\u0026rsquo;: Pearson相关系数的绝对值; \u0026lsquo;u\u0026rsquo;: Uncentered Pearson correlation （相当于两个数据向量的夹角余弦值） \u0026lsquo;x\u0026rsquo;: uncentered Pearson correlation的绝对值; \u0026rsquo;s\u0026rsquo;: Spearman’s 秩相关系数; \u0026lsquo;k\u0026rsquo;: Kendall’s τ. Euclidean距离 City-block distance pearson相关系数 Absolute Pearson correlation Uncentered correlation (夹角余弦) Absolute uncentered correlation Spearman rank correlation Kendall\u0026rsquo;s τ Weighting 对于 Bio.Cluster 中大部分距离函数，都可以使用加权向量。加权向量包含着 数据集中每个元素的权重。如果元素 i 的权重为 w__i，那么将会认为该元素 出现了 w__i 次 。权重值可以不为整数。对于 Spearman 秩相关系数 和Kendall’s τ, 权重没有太大的意义，因此不适用于这两个函数。\n计算距离矩阵 距离矩阵是 data 中，所有元素的两两间的距离的平方矩阵，可以用 Bio.Cluster 模块中 distancematrix 函数计算：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Cluster import distancematrix \u0026gt;\u0026gt;\u0026gt; matrix = distancematrix(data) 其中，包含以下参数：\n**data (必选)**包含所有元素的矩阵 **mask (默认: None)**缺失数据矩阵。若 mask[i,j]0, 则 data[i,j] 缺失。若 maskNone, 表明没有缺失数据。 **weight (默认: None)**权重矩阵。若 weight==None, 则假设所有的数据使用相同的权重。 **transpose (默认: 0)**选择使用 data 中的行 (transpose0), 或者列 (transpose1)来计算距离. dist (默认: \u0026rsquo;e\u0026rsquo;, Euclidean distance) 为了节省内存，函数返回的距离矩阵是一个一维数组的列表。每行的列数等于行号。 因此，第一行有0个元素。例如： [array([]), array([1.]), array([7., 3.]), array([4., 2., 6.])] 计算类的相关性质 计算类中心 类中心可以定义为该类中在每个维度上所有元素的平均值或者中值，可以用 Bio.Cluster 中的 clustercentroids 函数计算：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Cluster import clustercentroids \u0026gt;\u0026gt;\u0026gt; cdata, cmask = clustercentroids(data) 包含了以下参数:\n**data (必选)**包含所有元素的矩阵。 **mask (默认: None)**缺失数据矩阵。若 mask[i,j]0, 则 data[i,j] 缺失。若 maskNone, 则明没有缺失数据。 **clusterid (默认: None)**一个表示每个元素的所属类的整型向量。如果 clusterid 是 None, 表明所有的元素属于相同的类。 **method (默认: \u0026lsquo;a\u0026rsquo;)**指定使用算术平方根 (method\u0026rsquo;a\u0026rsquo;) 或者中值(method\u0026rsquo;m\u0026rsquo;) 来计算类中心。 **transpose (默认: 0)**选择使用 data 中的行 (transpose0), 或者列 (transpose1) 来计算类中心. 这个函数返回值为元组 (cdata, cmask)。 类中心的数据存储在一个二维的Numerical Python 数组 cdata 中, 缺失值的结果存储在二维的Numerical Python整型数组 cmask 中。 当 transpose = 0 时， 这两个数组的维度是（类数，列数），当 transpose = 1 时，数组的长度为 （行数，类数）。 其中每一行（当 transpose = 0) 或者 每一列（当 transpose = 1 ） 包含着对应每类对应的数据的平均值。 计算类间距离 根据每个 items 的距离函数，我们可以计算出两个 clusters 的距离。两个类别的 算术平均值之间的距离通常用于重心法聚类和 k-means 聚类，而 k-medoids 聚类中，通常利用两类的中值进行计算。最短距离法利用的是两类间最近的元素之间的距离， 而最大距离法利用最长的元素之间的距离。在两两平均连锁聚类法中， 类间的距离定义为类内所有对应元素两两间距离的平均值。\n为了计算两类之间的距离，可以利用:\n\u0026gt;\u0026gt;\u0026gt; from Bio.Cluster import clusterdistance \u0026gt;\u0026gt;\u0026gt; distance = clusterdistance(data) 其中，包含的参数有：\n**data (必选)**包含所有元素的矩阵。 **mask (默认: None)**缺失数据矩阵。若 mask[i,j]0, 则 data[i,j] 缺失。若 maskNone, 则明没有缺失数据。 **weight (默认: None)**权重矩阵。若 weight等于None, 则假设所有的数据使用相同的权重。 **index1 (默认: 0)**第一个类所包含的元素索引的列表。如果一个类别只包含一个元素 i ，则数据类型 可以为一个列表 [i], 或者整数 i. **index2 (默认: 0)**第二个类所包含的元素的列表。如果一个类别只包含一个元素 i ，则数据类型 可以为一个列表 [i], 或者整数 i. **method (默认: \u0026lsquo;a\u0026rsquo;)**选择计算类别间距离的方法: \u0026lsquo;a\u0026rsquo;: 使用两个类中心的距离 (算术平均值); \u0026rsquo;m\u0026rsquo;: 使用两个类中心的距离 (中值); \u0026rsquo;s\u0026rsquo;: 使用两类中最短的两个元素之间的距离; \u0026lsquo;x\u0026rsquo;: 使用两类中最长的两个元素之间的距离; \u0026lsquo;v\u0026rsquo;: 使用两类中对应元素间的距离的平均值作为距离。 **dist (默认: \u0026rsquo;e\u0026rsquo;, Euclidean distance)**选择距离函数 . **transpose (默认: 0)**选择使用 data 中的行 (transpose0), 或者列 (transpose1)来计算距离. 划分算法 划分算法依据所有元素到各自聚类中心距离之和最小化原则， 将元素分为 k 类。类的个数 k 由用户定义。 Bio.Cluster 提供了三种不同 的算法:\nk-means 聚类 k-medians 聚类 k-medoids 聚类 这些算法的区别在于如何定义聚类中心。在 k-means 中, 聚类中心定义为该类中所有 元素的平均值。 在 k-medians 聚类中， 利用每个维度的中间值来计算。 最后， k-medoids 聚类中，聚类中心定义为该类中，距离其他所有元素距离之和最小的元素所在的位置。 这个方法适用于已知距离矩阵，但是原始数据矩阵未知的情况，例如根据结构相似度对蛋白进行聚类\nexpectation-maximization (EM) 算法通常用于将数据分成 k 组。在 EM算法的起始阶段, 随机的把元素分配到不同的组。为了保证所有的类都包含元素，可以利用二项分布的方法随机 为每类挑选元素。然后，随机的对分组进行排列，保证每个元素有相同的概率被分到任何一个类别。 最终，保证每类中至少含有一个元素。\n之后进行迭代: 利用均值，中值或者medoid计算每类的中心; 计算每类的元素离各自中心的距离; 对于每个元素，判别其离哪个聚类中心最近; 将元素重新分配到最近的聚类，当不能进行调整时，迭代终止。 为了避免迭代中产生空的类别，在 k-means 和 k-medians 聚类中，算法始终记录着每类中元素的 个数，并且阻止最后一个元素被分到其他的类别中。对于 k-medoids 聚类, 这种检查就是没有必要的， 因为当只剩最后一个元素时，它离中心的距离为0，所以不会被分配到其他的类别中。\n由于起始阶段的每类中的元素分配是随机的，而通常当EM算法执行时，可能产生不同的聚类结果。为了找到最优的聚类结果， 可以对进行 k-means 算法重复多次，每次都以不同的随机分配作为起始。每次运行后，都会保存所有元素距离 其中心距离之和，并且选择总距离最小的运行结果最为最终的结果。\nEM算法运行的次数取决于需要聚类元素的多少。一般而言，我们可以根据最优解被发现的次数来选择。 这个次数会作为划分算法的返回值。如果最优解被多次返回，那么不太可能存在比这个 更优的解。然后，如果最优解只被发现一次，那么可能存在着距离更小的解。但是，如果需要聚类的 元素过多的话（多余几百），那么很难找到一个全局最优解。\nEM算法会在不能进行任何分配的时候停止。我们注意到，在某些随机的起始分配中，由于 相同的解会在迭代中周期性的重复，从而导致EM算法的失败。因此，我们在迭代中也会 检查是否有周期性出现的解存在。首先，在给定数目的迭代后，当前的聚类结果会保存作为一个参考。之后 继续迭代一定次数，比较该结果同之前保存的结果，可以确定之前的结果是否重复出现。 如果有重复出现，迭代会终止。如果没有出现，那么再次迭代后的结果会保存作为新的参考。 通常，会首先重复10次迭代，再保存结果为新的参考。之后，迭代的次数会翻倍，保证在长的周期中也可以 检测到该解。 K-means and k-medians k-means 和 k-medians 算法可以利用 Bio.Cluster中的 kcluster 实现:\n\u0026gt;\u0026gt;\u0026gt; from Bio.Cluster import kcluster \u0026gt;\u0026gt;\u0026gt; clusterid, error, nfound = kcluster(data) 其中，包含的参数有：\n**data (必选)**包含所有元素的矩阵。 **nclusters (默认: 2)**期望的类的数目 k. **mask (默认: None)**缺失数据矩阵。若 mask[i,j]0, 则 data[i,j] 缺失。若 maskNone, 则明没有缺失数据。 **weight (默认: None)**权重矩阵。若 weight=None, 则假设所有的数据使用相同的权重。 **transpose (默认: 0)**选择使用 data 中的行 (transpose0), 或者列 (transpose1)来计算距离. - npass (默认: 1) k-means/-medians 聚类算法运行的次数，每次运行使用不同的随机的起始值。 如果指定了 initialid , 程序会忽略npass 的值，并且聚类算法只会运行一次。 **method (默认: a)**指定聚类中心计算方法:当指定 method 使用其他值时，算法会采用算数平均值。 method=\u0026lsquo;a\u0026rsquo;: 算数平均值 (k-means clustering); method=\u0026rsquo;m\u0026rsquo;: 中值 (k-medians clustering). **dist (默认: \u0026rsquo;e\u0026rsquo;, Euclidean distance)**选择距离函数 (具体见 15.1 ). 尽管八种距离都可以用于 kcluster 计算, 但从经验上来讲，Euclidean 距离适合 k-means 算法, city-block 距离适合 k-medians. **initialid (默认: None)**指定EM算法运行初始的聚类类别。如果 initialid=None, 那么每运行一次EM算法时， 都会采取不同的随机初始聚类，总共运行的次数由 npass 决定。如果 initialid 不是 None, 那么它应该为一个长度为类别数的1维数组，每类中至少含有一个元素。通常当初始分类确定后，EM算法的结果也就确定了。 这个函数的返回值为一个包含 (clusterid, error, nfound) 的元组，其中 clusterid 是 一个整型矩阵，为每行或列所在的类。 error 是最优聚类解中，每类内距离的总和， nfound 指的是最优解出现的次数。 k-medoids聚类 kmedoids 函数根据提供的距离矩阵和聚类数，来运行 k-medoids 聚类：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Cluster import kmedoids \u0026gt;\u0026gt;\u0026gt; clusterid, error, nfound = kmedoids(distance) 其中，包含的参数有: , nclusters=2, npass=1, initialid=None)|\n**distance (必选)**两两元素间的距离矩阵，可以通过三种不同的方法提供： 提供一个2D的 Numerical Python 数组 (函数只会使用矩阵里左下角数据): distance = array([[0.0, 1.1, 2.3], [1.1, 0.0, 4.5], [2.3, 4.5, 0.0]]) 输入一个一维的 Numerical Python 数组，包含了距离矩阵左下角的数据：\ndistance = array([1.1, 2.3, 4.5]) 输入一个列表，包含距离矩阵左下角的数据：\ndistance = [array([]|, array([1.1]), array([2.3, 4.5]) ] 三种方法对应着同样的距离矩阵。\n**nclusters (默认: 2)**期望的类的数目 k. npass (默认: 1)k-medoids 聚类算法运行的次数，每次运行使用不同的随机的起始值。 如果指定了 initialid , npass 的值会忽略，并且聚类算法只会运行一次。 **initialid (默认: None)**指定EM算法运行初始的聚类类别。如果 initialid=None, 那么每运行一次EM算法时， 都会采取不同的随机初始聚类，总共运行的次数由 npass 决定。如果 initialid 不是 None, 那么它应该为一个长度为类别数的1维数组，每类中至少含有一个元素。通常当初始分类确定后，EM算法的结果也就确定了 函数返回值为一个 包含 (clusterid, error, nfound) 的元组, 其中 clusterid 一个整型矩阵，为每行或列类所在的类。error 是在最优解中，类内距离的总和， nfound 指的是最优解出现的次数。需要注意的是， clusterid 中的类号是指的是代表聚类中心的元素号 系统聚类 系统聚类同 k-means 聚类有本质的不同。在系统聚类中，基因间或者实验条件间的相似度是通过 树的形式展现出来的。由于可以利用Treeview或者Java Treeview来查看这些树的结构，因此系统聚类在基因表达谱数据中得到普遍应用。\n系统聚类的第一步是计算所有元素间的距离矩阵。之后，融合两个最近的元素成为一个节点。然后，不断的 通过融合相近的元素或者节点来形成新的节点，直到所有的元素都属于同一个节点。在追溯元素和节点融合 的过程的同时形成了树的结构。不同于 k-means 使用的EM算法，系统聚类的过程是固定的。\n系统聚类也存在着几个不同的方法，他们区别在于如何计算子节点间的距离。在 Bio.Cluster 中，提供了最短距离法（ pairwise single）,最长距离法（maximum）, 类平均法（average）, 和重心法（centroid linkage）。\n在最短距离法中，节点间的距离被定义两个节点最近样品间距离。 在最短距离法中，节点间的距离被定义两个节点最远样品间距离。 在类平均法中，节点间的距离被定义为所有样品对之间的平均距离。 在重心法中，节点间的距离被定义为两个节点重心间的距离。重心的计算是通过对 每类中所有元素进行计算的。由于每次都要计算新的节点与 其他元素和已存在节点的距离， 因此重心法的运行时间比其他系统聚类的方法更长。该方法另外一个特性是，当聚类树的 长大的时候，距离并不会增加，有时候反而减少。这是由于使用Pearson相关系数作为距离时， 对重心的计算和距离的计算不一致产生:因为Pearson相关系数在计算距离时会对数据进行有效归一化，， 但是重心的计算不会存在该种归一化。 对于最短距离法，最长距离法和类平均法时，两个节点之间的距离是直接对类别里的元素计算得到的。 因此，聚类的算法在得到距离矩阵后，不一定需要提供最开始的基因表达数据。而对于重心法而言， 新生成的节点的中心必须依靠原始的数据，而不是仅仅依靠距离矩阵。\n最短距离法的实现是根据 SLINK algorithm (R. Sibson, 1973), 这个算法具有快速和高效的特点。 并且这个方法聚类的结果同传统的方法结果一致。并且该算法，也可以有效的运用于大量的数据，而传统的 算法则需要大量的内存需求和运行时间。 展示系统聚类的结果 系统聚类的结果是用树的结构展示所有节点，每个节点包含两个元素或者子节点。通常，我们既关心那个元素 或者哪个子节点互相融合，也关心二者之间的距离（或者相似度）。我们可以调用 Bio.Cluster 中的 Node 类，来存储聚类树的一个节点。 Node 的实例包含以下三个属性：\nleft right distance 其中, left 和 right 是合并到该节点两个元素或子节点的编号。 distance 指的是二者间的距离。其中元素的编号是从0到（元素数目-1）， 而聚类的组别是从-1到-（元素数目-1）。请注意，节点的数目比元素的数目少一。\n为了创建一个新的 Node 对象,我们需要指定 left 和 right; distance 是可选的。 \u0026gt;\u0026gt;\u0026gt; from Bio.Cluster import Node \u0026gt;\u0026gt;\u0026gt; Node(2,3) (2, 3): 0 \u0026gt;\u0026gt;\u0026gt; Node(2,3,0.91) (2, 3): 0.91 对于已存在 Node 对象的 left, right, 和 distance 都是可以直接修改的：\n\u0026gt;\u0026gt;\u0026gt; node = Node(4,5) \u0026gt;\u0026gt;\u0026gt; node.left = 6 \u0026gt;\u0026gt;\u0026gt; node.right = 2 \u0026gt;\u0026gt;\u0026gt; node.distance = 0.73 \u0026gt;\u0026gt;\u0026gt; node (6, 2): 0.73 当 left 和 right 不是整数的时候，或者 distance 不能被转化成浮点值，会抛出错误。\nPython的类 Tree 包含着整个系统聚类的结果。 Tree 的对象可以通过 一个 Node 的列表创建:\n\u0026gt;\u0026gt;\u0026gt; from Bio.Cluster import Node, Tree \u0026gt;\u0026gt;\u0026gt; nodes = [Node(1,2,0.2), Node(0,3,0.5), Node(-2,4,0.6), Node(-1,-3,0.9)] \u0026gt;\u0026gt;\u0026gt; tree = Tree(nodes) \u0026gt;\u0026gt;\u0026gt; print tree (1, 2): 0.2 (0, 3): 0.5 (-2, 4): 0.6 (-1, -3): 0.9 #Tree 的初始器会检查包含节点的列表是否是一个正确的系统聚类树的结果: \u0026gt;\u0026gt;\u0026gt; nodes = [Node(1,2,0.2), Node(0,2,0.5)] \u0026gt;\u0026gt;\u0026gt; Tree(nodes) Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in ? ValueError: Inconsistent tree 也可以使用中括号来对 Tree 对象进行检索：\n\u0026gt;\u0026gt;\u0026gt; nodes = [Node(1,2,0.2), Node(0,-1,0.5)] \u0026gt;\u0026gt;\u0026gt; tree = Tree(nodes) \u0026gt;\u0026gt;\u0026gt; tree[0] (1, 2): 0.2 \u0026gt;\u0026gt;\u0026gt; tree[1] (0, -1): 0.5 \u0026gt;\u0026gt;\u0026gt; tree[-1] (0, -1): 0.5 因为 Tree 对象是只读的，我们不能对 Tree 对象中任何一个节点进行改变。然而，我们可以将其 转换成一个节点的列表，对列表进行操作，最后创建新的树。\n\u0026gt;\u0026gt;\u0026gt; tree = Tree([Node(1,2,0.1), Node(0,-1,0.5), Node(-2,3,0.9)]) \u0026gt;\u0026gt;\u0026gt; print tree (1, 2): 0.1 (0, -1): 0.5 (-2, 3): 0.9 \u0026gt;\u0026gt;\u0026gt; nodes = tree[:] \u0026gt;\u0026gt;\u0026gt; nodes[0] = Node(0,1,0.2) \u0026gt;\u0026gt;\u0026gt; nodes[1].left = 2 \u0026gt;\u0026gt;\u0026gt; tree = Tree(nodes) \u0026gt;\u0026gt;\u0026gt; print tree (0, 1): 0.2 (2, -1): 0.5 (-2, 3): 0.9 这个性质保证了Tree 结果的正确性。\n为了利用可视化工具，例如Java Treeview，来查看系统聚类树，最好对所有节点的距离进行标准化， 使其位于0和1之间。可以通过对 Tree 对象调用 scale 方法来实现这个功能：\n\u0026gt;\u0026gt;\u0026gt; tree.scale() 这个方法不需要任何参数，返回值是 None.\n经过系统聚类后，可以对 Tree 对象进行剪接，将所有的元素分为 k 类：\n\u0026gt;\u0026gt;\u0026gt; clusterid = tree.cut(nclusters=1) 其中 nclusters (默认是 1) 是期望的类别数 k。这个方法会忽略树结构里面的 最高的 _k_−1 节点，最终形成 k 个独立的类别。对于 k 必须为正数，并且小于或者等于 元素的数目。这个方法会返回一个数组 clusterid ,包含着每个元素对应的类\n运行系统聚类 为了进行系统聚类，可以用 Bio.Cluster 中的 treecluster 函数。\n\u0026gt;\u0026gt;\u0026gt; from Bio.Cluster import treecluster \u0026gt;\u0026gt;\u0026gt; tree = treecluster(data) 包括以下参数:\ndata包含所有元素的矩阵。 **mask (默认: None)**缺失数据矩阵。若 mask[i,j]0, 则 data[i,j] 缺失。若 maskNone, 则明没有缺失数据。 **weight (默认: None)**权重矩阵。若 weight=None, 则假设所有的数据使用相同的权重。 **transpose (默认: 0)**选择使用 data 中的行 (transpose0), 或者列 (transpose1)来计算距离. **method (默认: \u0026rsquo;m\u0026rsquo;)**选择节点间距离计算方法: method=\u0026rsquo;s\u0026rsquo;: 最小距离法 method=\u0026rsquo;m\u0026rsquo;: 最大距离法 method=\u0026lsquo;c\u0026rsquo;: 重心法 method=\u0026lsquo;a\u0026rsquo;: 类平均法 **dist (默认: \u0026rsquo;e\u0026rsquo;, Euclidean distance)**选择距离函数. 为了对距离矩阵进行系统聚类，可以在调用 treecluster 时， 用 distancematrix 参数来代替 data 参数： \u0026gt;\u0026gt;\u0026gt; from Bio.Cluster import treecluster \u0026gt;\u0026gt;\u0026gt; tree = treecluster(distancematrix=distance)``` 这种情况下，需要定义下列参数： - **distancematrix**元素两两间的距离矩阵，可以通过三种不同的方法提供： - 提供一个2D的 Numerical Python 数组 (函数只会使用矩阵里左下角数据): - ```python distance = array([[0.0, 1.1, 2.3], [1.1, 0.0, 4.5], [2.3, 4.5, 0.0]]) 输入一个一维的 Numerical Python 数组，包含了距离矩阵左下角的数据： distance = array([1.1, 2.3, 4.5]) 输入一个列表，包含距离矩阵左下角的数据： distance = [array([]), array([1.1]), array([2.3, 4.5]) 三种方法对应着同样的距离矩阵。由于 treecluster 会对距离矩阵中的值进行随机洗牌， 如果后面需要调用这个距离矩阵，请在调用 treecluster 之情，事先存到一个新的变量 method选择节点间距离计算方法:其中，最小距离法、最大距离法和类平均法可以只通过距离矩阵计算，而重心法却不行。 method=\u0026rsquo;s\u0026rsquo;: 最小距离法 method=\u0026rsquo;m\u0026rsquo;: 最大距离法 method=\u0026lsquo;a\u0026rsquo;: 类平均法 当调用 treecluster时, data 或者 distancematrix 总有一个必须为 None。\n函数返回一个 Tree 对象，该对象包含着 (元素数目-1）个节点，当选择行作为聚类时，元素的 数目同行数一致；当使用列作为聚类时，元素的数目同列数一致。每个节点都意味着一对相邻连锁的 事件，其中节点的性质 left 和 right 包含着每个合并的元素或者子节点的编号， distance 是两个合并元素或者子节点的距离。元素编号是从 0 到 (元素数目 − 1) , 而类别是从 -1 到 −(元素 数目 -1 ） Self-Organizing Maps Self-Organizing Maps (SOMs) 是由 Kohonen 在描述神经网络的时候发明的 (see for instance Kohonen, 1997 [24] ). Tamayo (1999) 第一次讲 Self-Organizing Maps 应用到基因表达数据上。 [30].\nSOMs 根据某种拓扑结果将元素进行分类。通常选用的是矩形的拓扑结构。在SOMs生成的类别中，相邻的 两个类的拓扑结构相似度高于他们对其他的相似度。\n计算SOM的第一步是随机分配数据向量到每个类别中，如果使用行进行聚类，那么每个数据向量中的元素 个数等于列数。\n一个SOM 会一次读入一行，并且找到该向量最近的拓扑聚类结构。之后利用找到的数据向量对 这个类别的数据向量和相邻的类别的数据向量进行调整。调整如下\n其中 (N__x, N__y) 是定义拓扑结构的矩形维度。\n函数 somcluster 可以用来在一个矩形的网格里计算 Self-Organizing Map。 首先，初始化一个随机数产生器。利用随机化产生器来对节点数据进行初始化。在SOM中， 基因或者芯片的调整顺序同样是随机的。用户可以定义总的SOM迭代的次数。\n运行 somcluster, 例如：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Cluster import somcluster \u0026gt;\u0026gt;\u0026gt; clusterid, celldata = somcluster(data) 其中，可以定义一下参数:\n**data (required)**包含所有元素的矩阵。 **mask (默认: None)**缺失数据矩阵。若 mask[i,j]0, 则 data[i,j] 缺失。若 maskNone, 则明没有缺失数据。 **weight (默认: None)**权重矩阵。若 weight=None, 则假设所有的数据使用相同的权重。 **transpose (默认: 0)**选择使用 data 中的行 (transpose0), 或者列 (transpose1)来聚类. **nxgrid, nygrid (默认: 2, 1)**当Self-Organizing Map计算的时候，矩形的网格所包含的横向和纵向的格子。 **inittau (默认: 0.02)**SOM算法中，参数 τ 的初始值，默认是 0.02。 这个初始值同Michael Eisen’s Cluster/TreeView 一致。 **niter (默认: 1)**迭代运行的次数。 **dist (默认: \u0026rsquo;e\u0026rsquo;, Euclidean distance)**选择距离函数 (具体见 15.1 ). 这个函数返回的是一个元组 (clusterid, celldata): **clusterid:**一个两列的数组，行的数目等于待聚类元素的个数。每行包含着在矩形SOM网格中，将每个元素分配到的 格子的 x 和 y 的坐标。 **celldata:**当以行进行聚类时，生成的矩阵维度为 (nxgrid, nygrid, number of columns)； 当以列进行聚类时，生成的矩阵维度为 (nxgrid, nygrid, number of rows)。 在这个矩阵里， [ix][iy] 表示着一个一维向量，其中用于计算该类中心的这基因的表达谱数据. PCA主成分分析 主成分分析 (PCA) 被广泛的用于分析多维数据，一个将主成分分析应用于表达谱数据的请见 Yeung and Ruzzo (2001) [33].\n简而言之，PCA是一种坐标转换的方法，转换后的基础向量成为主成分，变换前的每行可以用主成分的 线性关系显示。主成分的选择是基于是残差尽可能的小的原则。例如，一个 n × 3 的数据矩阵可以表示为三维 空间内的一个椭圆球形的点的云。第一主成分是这个椭圆球形的最长轴，第二主成分是次长轴，第三主成分 是最短的轴。矩阵中，每一行都可以用主成分的线性关系展示。一般而言，为了对数据进行降维，只保留最 重要的几个主成分。剩余的残差认为是不可解释的方差。\n可以通过计算数据的协方差矩阵的特征向量来得到主成分。每个主成分对应的特征值决定了 其在数据中代表的方差的大小。\n在进行主成分分析前，矩阵的数据每一列都要减去其平均值。在上面椭圆球形云的例子中，数据在3D 空间中，围绕着其中心分布，而主成分则显示着每个点对其中心的变化。\n函数 pca 首先使用奇异值分解（singular value decomposition）来计算矩阵的特征值和 特征向量。奇异值分解使用的是Algol写的C语言的 svd [16] , 利用的是 Householder bidiagonalization 和 QR 算法的变异。主成分，每个数据在主成分上的坐标和主成分 对应的特征值都会被计算出来，并按照特征值的降序排列。如果需要数据中心，则需要在调用 pca 前，对每列数据减去其平均值。\n将主成分分析应用于二维矩阵 data,可以：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Cluster import pca \u0026gt;\u0026gt;\u0026gt; columnmean, coordinates, components, eigenvalues = pca(data) 函数会返回一个元组 columnmean, coordinates, components, eigenvalues :\ncolumnmean包含 data 每列均值的数组 . coordinatesdata 中每行数据在主成分上对应的坐标。 components主成分 eigenvalues每个主成分对应的特征值 原始的数据 data 可以通过计算 columnmean + dot(coordinates, components) 得到。 处理Cluster/TreeView-type文件 Cluster/TreeView 是一个对基因表达数据可视化的工具。他们最初由 Michael Eisen 在 Stanford University 完成。Bio.Cluster 包含着读写 Cluster/TreeView 对应的文件格式的函数。因此，将结果保存为该格式后， 可以用Treeview对结果进行直接的查看。我们推荐使用 Alok Saldanha 的 http://jtreeview.sourceforge.net/Java TreeView 程序。这个软件可以显示系统聚类和 k-means 聚类的结果\n类 Record 的一个对象包含着一个 Cluster/TreeView-type数据文件需要的所有信息。 为了将结果保存到一个 Record 对象中，首先需要打开一个文件，并读取：\n\u0026gt;\u0026gt;\u0026gt; from Bio import Cluster \u0026gt;\u0026gt;\u0026gt; handle = open(\u0026#34;mydatafile.txt\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = Cluster.read(handle) \u0026gt;\u0026gt;\u0026gt; handle.close() 两步操作使得你可以较灵活地操作不同来源的数据，例如：\n\u0026gt;\u0026gt;\u0026gt; import gzip # Python standard library \u0026gt;\u0026gt;\u0026gt; handle = gzip.open(\u0026#34;mydatafile.txt.gz\u0026#34;) 来打开一个gzipped文件，或者利用\n\u0026gt;\u0026gt;\u0026gt; import urllib # Python standard library \u0026gt;\u0026gt;\u0026gt; handle = urllib.urlopen(\u0026#34;http://somewhere.org/mydatafile.txt\u0026#34;) 来打开一个网络文件，然后调用 read.\nread 命令会读取一个由制表符分割的文本文件 mydatafile.txt，文件包含着 符合Michael Eisen’s Cluster/TreeView格式的基因表达数据。具体的格式说明，可以参见 Cluster/TreeView手册，链接见 Michael Eisen’s lab website 或者 our website.\n一个 Record 对象有以下的性质:\ndata包含基因表达数据的矩阵，每行为基因，每列为芯片。 mask缺失值的整型数组。如果 mask[i,j]0, 则 data[i,j] 是缺失的. 如果 maskNone, 那么没有数据缺失。 geneid包含每个基因的独特说明的列表 (例如 ORF 数目). genename包含每个基因说明的列表（例如基因名）。如果文件中不包含该数据， 那么 genename 被设为 None. gweight计算表达谱数据中，基因间的距离使用的权重。如果文件中不含该信息，则 gweight 为 None. gorder期望输出文件中基因的排列的顺序。如果文件中不含该信息，则 gorder 为None. expid包含每个芯片说明的列表，例如实验条件。 eweight计算表达谱数据中，不同芯片间的距离使用的权重。如果文件中不含该信息，则 eweight 为 None. eorder期望输出文件中基因的排列的顺序。如果文件中不含该信息，则 eorder 为 None. uniqid用于代替文件中 UNIQID 的字符串. 在载入 Record 对象后，上述的每个性质可以直接读取和修改。例如，可以对 record.data 直接取对数来对数据进行log转换。 计算距离矩阵 为了计算record中存储元素的距离矩阵，可以用：\n\u0026gt;\u0026gt;\u0026gt; matrix = record.distancematrix() 其中，包含以下参数：\n**transpose (默认: 0)**选择对 data 的行 (transpose0), 或者列 (transpose1)计算距离。 **dist (默认: \u0026rsquo;e\u0026rsquo;, Euclidean distance)**选择合适的元素距离算法 (见 15.1 ). 函数会返回一个距离矩阵，每行的列数等于行数。 计算聚类中心 为了计算存储在record中的元素的聚类中心，利用：\n\u0026gt;\u0026gt;\u0026gt; cdata, cmask = record.clustercentroids() **clusterid (默认: None)**展示每个元素所属类的整型向量。如果缺少 clusterid,默认所有的元素属于同一类。 **method (默认: \u0026lsquo;a\u0026rsquo;)**选择使用算术平均值 (method\u0026rsquo;a\u0026rsquo;) 或者中值 (method\u0026rsquo;m\u0026rsquo;)来计算聚类中心。 **transpose (默认: 0)**选择计算data 的行 (transpose0), 或者列 (transpose1)计算中心。 函数返回元组 cdata, cmask ; 计算两类间的距离 为了计算存储在record中的两类的距离，利用：\n\u0026gt;\u0026gt;\u0026gt; distance = record.clusterdistance() 其中，包含以下参数：\n**index1 (默认: 0)**第一个类别所包含的元素的列表。如果一个类别只包含一个元素 i 可以为一个列表 [i], 或者整数 i. **index2 (默认: 0)**第二个类别所包含的元素的列表。如果一个类别只包含一个元素 i 可以为一个列表 [i], 或者整数 i. **method (默认: \u0026lsquo;a\u0026rsquo;)**选择计算类别间距离的方法: \u0026lsquo;a\u0026rsquo;: 使用两个聚类中心的距离 (算术平均值); \u0026rsquo;m\u0026rsquo;: 使用两个聚类中心的距离 (中值); \u0026rsquo;s\u0026rsquo;: 使用两类中最短的两个元素之间的距离; \u0026lsquo;x\u0026rsquo;: 使用两类中最长的两个元素之间的距离; \u0026lsquo;v\u0026rsquo;: 使用两类中两两元素距离的平均值作为距离。 **dist (默认: \u0026rsquo;e\u0026rsquo;, Euclidean distance)**选择使用的距离函数 (见 15.1 ). **transpose (默认: 0)**选择 使用 data 的行 ( transpose0 ), 或者列 ( transpose1 )计算距离。 进行系统聚类 为了对存储在record中的数据进行系统聚类，利用：\n\u0026gt;\u0026gt;\u0026gt; tree = record.treecluster() 包含以下参数:\n**transpose (默认: 0)**选择使用行 ( transpose0 ) 或者列 ( transpose1 ) 用于聚类 **method (默认: \u0026rsquo;m\u0026rsquo;)**选择合适的节点距离计算方法: method=\u0026rsquo;s\u0026rsquo;: 最小距离法 method=\u0026rsquo;m\u0026rsquo;: 最大距离法 method=\u0026lsquo;c\u0026rsquo;: 重心法 method=\u0026lsquo;a\u0026rsquo;: 类平均法 **dist (默认: \u0026rsquo;e\u0026rsquo;, Euclidean distance)**选择使用的距离函数(见 15.1 ). transpose选择使用基因或者芯片进行聚类，如果是 transpose0 , 则使用基因 (行) 进行聚类，如果使用 transpose1, 芯片 (列) 用于聚类. 函数返回 Tree 对象。对象包含 (元素数目 − 1） 节点, 如果使用行进行聚类时，元素数目为总行数； 当使用列进行聚类时，元素数目为总列数。每个节点描述着一对节点连接，然而节点的性质 left 和 right 包含着相邻节点所有的元素和子节点数， distance 显示着左右节点的距离。 元素从 0 到 (元素数目 − 1) 进行索引, 而类别从 -1 to −(元素数目−1)进行索引。 进行k-means or k-medians聚类 为了对存储在record中的元素进行 k-means 或者 k-medians 聚类，可以使用：\n\u0026gt;\u0026gt;\u0026gt; clusterid, error, nfound = record.kcluster() 包含以下参数:\n**nclusters (默认: 2)**类的数目 k. **transpose (默认: 0)**选择 使用 data 的行 ( transpose0 ), 或者列 ( transpose1 )计算距离。 npass (默认: 1)k-means/-medians 聚类算法运行的次数，每次运行使用不同的随机的起始值。 如果指定了 initialid , npass 的值会忽略，并且聚类算法只会运行一次。 **method (默认: a)**指定确定聚类中心的方法:当指定 method 使用其他值时，算法会采用算数平均值。 method=\u0026lsquo;a\u0026rsquo;: 算数平均值 (k-means clustering); method=\u0026rsquo;m\u0026rsquo;: 中间值 (k-medians clustering). **dist (默认: \u0026rsquo;e\u0026rsquo; , Euclidean distance)**选择使用的距离函数 (见 15.1 ). 这个函数返回的是一个元组 (clusterid, error, nfound) , 其中 clusterid 是一个每行或则列对应的类的编号。 error 是最优解的类内的距离和， nfound 是最优解被发现的次数。 计算Self-Organizing Map 可以利用以下命令，计算对存储在record中元素计算 Self-Organizing Map ：\n\u0026gt;\u0026gt;\u0026gt; clusterid, celldata = record.somcluster() **transpose (默认: 0 )**选择 使用 data 的行 ( transpose0 ), 或者列 ( transpose1 )计算距离. **nxgrid, nygrid (默认: 2, 1)**当Self-Organizing Map计算时，在矩形网格里的横向和纵向格子数目 **inittau (默认: 0.02)**用于SOM算法的参数 τ 的初始值。默认的 inittau 是0.02，同Michael Eisen’s Cluster/TreeView 程序中 使用的参数一致。 **niter (默认: 1 )**迭代运行的次数。 **dist (默认: \u0026rsquo;e\u0026rsquo; , Euclidean distance)**选择使用的距离函数(见 15.1 ). 函数返回一个元组 (clusterid, celldata) : **clusterid:**一个二维数组，行数同待聚类的元素数目相同。每行的内容对应着该元素在矩形SOM方格内 x 和 y 的坐标。 **celldata:**格式为一个矩阵，如果是对行聚类，内容为 ( nxgrid , nygrid , 列数)，如果是对列聚类， 那么内容为 ( nxgrid , nygrid , 行数) 。矩阵中，坐标 [ix][iy] 对应的是该坐标的网格里的 基因表达数据的聚类中心的一维向量 保存聚类结果 为了保存聚类结果，可以利用：\n\u0026gt;\u0026gt;\u0026gt; record.save(jobname, geneclusters, expclusters) 包含以下参数:\njobname字符串 jobname 作为保存的文件名。 geneclusters这个参数指的是基因（以行聚类）的结果。在 k-means 聚类中，这个参数是一个一维的数组，包含着 每个基因对应的类别，可以通过 kcluster 得到。在系统聚类中， geneclusters 是一个 Tree 对象。 expclusters这个参数指的是实验条件（以列聚类）的结果。在 k-means 聚类中，这个参数是一个一维的数组，包含着 每个实验条件对应的类别，可以通过 kcluster 得到。在系统聚类中， geneclusters 是一个Tree 对象。 这个方法会生成文本文件 jobname.cdt, jobname.gtr, jobname.atr, jobname_.kgg, 和/或 jobname_.kag 。 这些文件可以用于后续分析。如果 geneclusters 和 expclusters 都是 None , 那这个方法只会生成 jobname.cdt ; 这个文件可以被读取，生成一个新的 Record 对象. 示例 以下是一个系统聚类的例子，其中使用最短距离法对基因进行聚类，用最大距离法对实验条件进行聚类。 由于使用 Euclidean 距离对基因进行聚类，因此需要将节点距离 genetree 进行调整，使其处于0和1之间。 这种调整对于Java TreeView正确显示树结构也是很必须的。同时使用 uncentered correlation 对实验条件进行聚类。 在这种情况下，不需要任何的调整，因为 exptree 中的结果已经位于0和2之间。 示例中使用的 文件 cyano.txt 可以从 data 文件夹中找到。\n\u0026gt;\u0026gt;\u0026gt; from Bio import Cluster \u0026gt;\u0026gt;\u0026gt; handle = open(\u0026#34;cyano.txt\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = Cluster.read(handle) \u0026gt;\u0026gt;\u0026gt; handle.close() \u0026gt;\u0026gt;\u0026gt; genetree = record.treecluster(method=\u0026#39;s\u0026#39;) \u0026gt;\u0026gt;\u0026gt; genetree.scale() \u0026gt;\u0026gt;\u0026gt; exptree = record.treecluster(dist=\u0026#39;u\u0026#39;, transpose=1) \u0026gt;\u0026gt;\u0026gt; record.save(\u0026#34;cyano_result\u0026#34;, genetree, exptree) 这个命令会生成 cyano_result.cdt , cyano_result.gtr , 和 cyano_result.atr 等文件。\n同样的，也可以保存一个 k-means 聚类的结果:\n\u0026gt;\u0026gt;\u0026gt; from Bio import Cluster \u0026gt;\u0026gt;\u0026gt; handle = open(\u0026#34;cyano.txt\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = Cluster.read(handle) \u0026gt;\u0026gt;\u0026gt; handle.close() \u0026gt;\u0026gt;\u0026gt; (geneclusters, error, ifound) = record.kcluster(nclusters=5, npass=1000) \u0026gt;\u0026gt;\u0026gt; (expclusters, error, ifound) = record.kcluster(nclusters=2, npass=100, transpose=1) \u0026gt;\u0026gt;\u0026gt; record.save(\u0026#34;cyano_result\u0026#34;, geneclusters, expclusters) 上述代码将生成文件 cyano_result_K_G2_A2.cdt , cyano_result_K_G2.kgg , 和 cyano_result_K_A2.kag 。s\n附加函数 median(data) 返回一维数组 data 的中值\nmean(data) 返回一维数组 data 的均值。\nversion() 返回使用的C聚类库的版本号。\n","date":"2021-08-15T00:00:00Z","permalink":"https://example.com/p/ch15_%E8%81%9A%E7%B1%BB%E5%88%86%E6%9E%90/","title":"ch15_聚类分析"},{"content":"biopython官方地址：https://biopython.org/\ngithub地址：https://github.com/biopython/biopython/blob/master/\n中文版教程：https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html\nbiopython包的说明（具体到每个模块了）：https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html\n这章主要的介绍Biopython中的 Bio.motifs 包。这个包是为了方便那些需要进行模体序列分析的人们而特意提供的，所以我想你们在使用时肯定对模体序列分析的一些相关要点都很熟悉。假如在使用中遇到不清楚的地方，请您查阅 相关章节以获得有关的信息。\n这章的大部分内容是介绍Biopython 1.61 之前版本中新加入的 Bio.motifs 包，该包替代了Biopython 1.50版本中的 Bio.Motif 包，而 Bio.Motif 包是基于较早版本的Biopython 中的两个模块 Bio.AlignAce 和 Bio.MEME 。Bio.motifs 包较好地综合了上述的几个模块的功能，做为一个统一模块工具。\n说到其他库，看到这里，你或许会对 TAMO 感兴趣，这是另一个分析模体序列的Python库。它能提供更多关于 de-novo 模体的查找方式，不过它并没有纳入到Biopython中，而且在商业用途上还有一些限制\n模体对象 由于我们感兴趣的是模体分析，所以我们需要先看看 Motif 对象。对此我们需要先导入Bio.motifs包：\n\u0026gt;\u0026gt;\u0026gt; from Bio import motifs 然后我们可以开始创建我们第一个模体对象。我们可以从模体的实例列表中创建一个 Motif 对象，也可以通过读取模体数据库中或模体查找软件产生的文件来获得一个 Motif 对象\n\u0026gt;\u0026gt;\u0026gt; from Bio import motifs 从实例中创建一个模体 假设我们有一些DNA模体的实例\n\u0026gt;\u0026gt;\u0026gt; from Bio.Seq import Seq \u0026gt;\u0026gt;\u0026gt; instances = [Seq(\u0026#34;TACAA\u0026#34;), ... Seq(\u0026#34;TACGC\u0026#34;), ... Seq(\u0026#34;TACAC\u0026#34;), ... Seq(\u0026#34;TACCC\u0026#34;), ... Seq(\u0026#34;AACCC\u0026#34;), ... Seq(\u0026#34;AATGC\u0026#34;), ... Seq(\u0026#34;AATGC\u0026#34;), ... ] 然后我们可以如下创建一个模体对象：\n\u0026gt;\u0026gt;\u0026gt; m = motifs.create(instances) 这些实例被存储在一个名为 m.instances 的属性中，这个其实也就是一个Python的列表，只不过附加了一些功能，这些功能将在之后介绍。将这些模体对象打印出来后就可以看出这些实例是从哪构建出来的。\n\u0026gt;\u0026gt;\u0026gt; print m TACAA TACGC TACAC TACCC AACCC AATGC AATGC 模体的长度像其他一些实例一些被定义为序列的长度：\n\u0026gt;\u0026gt;\u0026gt; len(m) 5 模体对象有一个 .counts 属性，可以用来查看碱基在每个位置的数目。可以把这个统计表用易读的格式打印出来：\n\u0026gt;\u0026gt;\u0026gt; print m.counts 0 1 2 3 4 A: 3.00 7.00 0.00 2.00 1.00 C: 0.00 0.00 5.00 2.00 6.00 G: 0.00 0.00 0.00 3.00 0.00 T: 4.00 0.00 2.00 0.00 0.00 你也可以像使用字典一样获取这些数目：\n\u0026gt;\u0026gt;\u0026gt; m.counts[\u0026#39;A\u0026#39;] [3, 7, 0, 2, 1] 但是你也可以把它看成一个二维数列，核苷酸作为列，位置作为行：\n\u0026gt;\u0026gt;\u0026gt; m.counts[\u0026#39;T\u0026#39;,0] 4 \u0026gt;\u0026gt;\u0026gt; m.counts[\u0026#39;T\u0026#39;,2] 2 \u0026gt;\u0026gt;\u0026gt; m.counts[\u0026#39;T\u0026#39;,3] 0 你还可以直接获得核苷酸数目矩阵中的列\n\u0026gt;\u0026gt;\u0026gt; m.counts[:,3] {\u0026#39;A\u0026#39;: 2, \u0026#39;C\u0026#39;: 2, \u0026#39;T\u0026#39;: 0, \u0026#39;G\u0026#39;: 3} 除了使用核苷酸本身，你还可以使用模体碱基序列按字符排序后的核苷酸索引：\n\u0026gt;\u0026gt;\u0026gt; m.alphabet IUPACUnambiguousDNA() \u0026gt;\u0026gt;\u0026gt; m.alphabet.letters \u0026#39;GATC\u0026#39; \u0026gt;\u0026gt;\u0026gt; sorted(m.alphabet.letters) [\u0026#39;A\u0026#39;, \u0026#39;C\u0026#39;, \u0026#39;G\u0026#39;, \u0026#39;T\u0026#39;] \u0026gt;\u0026gt;\u0026gt; m.counts[\u0026#39;A\u0026#39;,:] (3, 7, 0, 2, 1) \u0026gt;\u0026gt;\u0026gt; m.counts[0,:] (3, 7, 0, 2, 1) 模体有一个相关联的一致序列，这个序列被定义为由 .counts 矩阵相应列中具有最大值的碱基，这些碱基是按模体序列排列的\n\u0026gt;\u0026gt;\u0026gt; m.consensus Seq(\u0026#39;TACGC\u0026#39;, IUPACUnambiguousDNA()) 反一致序列也一样，只不过是由 .counts 矩阵中相应列的最小值来选：\n\u0026gt;\u0026gt;\u0026gt; m.anticonsensus Seq(\u0026#39;GGGTG\u0026#39;, IUPACUnambiguousDNA()) 你也可以利用简并一致序列，用不确定核苷酸来表示序列某一位置的所有核苷酸：\n\u0026gt;\u0026gt;\u0026gt; m.degenerate_consensus Seq(\u0026#39;WACVC\u0026#39;, IUPACAmbiguousDNA()) 此处，W和R都是按照IUPAC不确定核苷酸表规定的：W代表A或T，V代表A，C或G [10] 。这些简并一致序列是按照Cavener指定的规则 [11] 来建立的\n\u0026gt;\u0026gt;\u0026gt; r = m.reverse_complement() \u0026gt;\u0026gt;\u0026gt; r.consensus Seq(\u0026#39;GCGTA\u0026#39;, IUPACUnambiguousDNA()) \u0026gt;\u0026gt;\u0026gt; r.degenerate_consensus Seq(\u0026#39;GBGTW\u0026#39;, IUPACAmbiguousDNA()) \u0026gt;\u0026gt;\u0026gt; print r TTGTA GCGTA GTGTA GGGTA GGGTT GCATT GCATT #反向互补序列和简并一致序列都只在DNA模体中有 读取模体 从实例手动创建一个模体确实有点无趣，所以用一些I/O函数来读写模体是很有用的。目前对于如何存储模体还没有一些真正的标准，不过有一些格式用得比其他更经常。这其中最重要的区别在于模体表示是基于实例还是某种PWM矩阵\nJASPAR\n作为一个最流行的模体数据库 JASPAR 它不是以一系列的实例就是频率矩阵。比如，下面就是JASPAR Arnt.sites 文件的开头和结尾行显示了老鼠螺旋-环-螺旋转录因子Arnt的结合位点：\n\u0026gt;MA0004 ARNT 1 CACGTGatgtcctc \u0026gt;MA0004 ARNT 2 CACGTGggaggtac \u0026gt;MA0004 ARNT 3 CACGTGccgcgcgc ... \u0026gt;MA0004 ARNT 18 AACGTGacagccctcc \u0026gt;MA0004 ARNT 19 AACGTGcacatcgtcc \u0026gt;MA0004 ARNT 20 aggaatCGCGTGc``` 那些用大字字母表示的序列的一部分就是被用来相互比对的模体实例。 我们可以从下面的实例创建一个 Motif 对象： ```python \u0026gt;\u0026gt;\u0026gt; from Bio import motifs \u0026gt;\u0026gt;\u0026gt; arnt = motifs.read(open(\u0026#34;Arnt.sites\u0026#34;), \u0026#34;sites\u0026#34;) 从这个模体创建的实例存储在该模体的 .instances 属性：\n\u0026gt;\u0026gt;\u0026gt; print arnt.instances[:3] [Seq(\u0026#39;CACGTG\u0026#39;, IUPACUnambiguousDNA()), Seq(\u0026#39;CACGTG\u0026#39;, IUPACUnambiguousDNA()), Seq(\u0026#39;CACGTG\u0026#39;, IUPACUnambiguousDNA())] \u0026gt;\u0026gt;\u0026gt; for instance in arnt.instances: ... print instance ... CACGTG CACGTG CACGTG CACGTG CACGTG CACGTG CACGTG CACGTG CACGTG CACGTG CACGTG CACGTG CACGTG CACGTG CACGTG AACGTG AACGTG AACGTG AACGTG CGCGTG 这个模体的计数矩阵可以从这些实例中自动计算出来：\n\u0026gt;\u0026gt;\u0026gt; print arnt.counts 0 1 2 3 4 5 A: 4.00 19.00 0.00 0.00 0.00 0.00 C: 16.00 0.00 20.00 0.00 0.00 0.00 G: 0.00 1.00 0.00 20.00 0.00 20.00 T: 0.00 0.00 0.00 0.00 20.00 0.00 JASPAR数据库也可以让模体像计数矩阵一样获得，不需要那些创建它们的实例。比如，下面这个JASPAR文件 SRF.pfm 包含了人类SRF转录因子的计数矩阵：\n2 9 0 1 32 3 46 1 43 15 2 2 1 33 45 45 1 1 0 0 0 1 0 1 39 2 1 0 0 0 0 0 0 0 44 43 4 2 0 0 13 42 0 45 3 30 0 0 我们可以如下为计数矩阵创建一个模体：\n\u0026gt;\u0026gt;\u0026gt; srf = motifs.read(open(\u0026#34;SRF.pfm\u0026#34;),\u0026#34;pfm\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print srf.counts 0 1 2 3 4 5 6 7 8 9 10 11 A: 2.00 9.00 0.00 1.00 32.00 3.00 46.00 1.00 43.00 15.00 2.00 2.00 C: 1.00 33.00 45.00 45.00 1.00 1.00 0.00 0.00 0.00 1.00 0.00 1.00 G: 39.00 2.00 1.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 44.00 43.00 T: 4.00 2.00 0.00 0.00 13.00 42.00 0.00 45.00 3.00 30.00 0.00 0.00 由于这个模体是由计数矩阵直接创建的，所以它没有相关的实例：\n\u0026gt;\u0026gt;\u0026gt; print srf.instances None 我们可以获得这两个模体的一致序列：\n\u0026gt;\u0026gt;\u0026gt; print arnt.counts.consensus CACGTG \u0026gt;\u0026gt;\u0026gt; print srf.counts.consensus GCCCATATATGG MEME\nMEME 是一个用来在一堆相关DNA或蛋白质序列中发现模体的工具。它输入一组相关DNA或蛋白质序列，输出所要求的模体。因此和JASPAR文件相比，MEME输出文件里面一般是含有多个模体。例子如下\n在输出文件的开头，有一些MEME生成的关于MEME和所用MEME版本的背景信息：\n******************************************************************************** MEME - Motif discovery tool ******************************************************************************** MEME version 3.0 (Release date: 2004/08/18 09:07:01) ... 再往下，简要概括了输入的训练序列集：\n******************************************************************************** TRAINING SET ******************************************************************************** DATAFILE= INO_up800.s ALPHABET= ACGT Sequence name Weight Length Sequence name Weight Length ------------- ------ ------ ------------- ------ ------ CHO1 1.0000 800 CHO2 1.0000 800 FAS1 1.0000 800 FAS2 1.0000 800 ACC1 1.0000 800 INO1 1.0000 800 OPI3 1.0000 800 ******************************************************************************** 以及所使用到的命令：\n******************************************************************************** COMMAND LINE SUMMARY ******************************************************************************** This information can also be useful in the event you wish to report a problem with the MEME software. command: meme -mod oops -dna -revcomp -nmotifs 2 -bfile yeast.nc.6.freq INO_up800.s ... 接下来就是每个被发现模体的详细信息：\n******************************************************************************** MOTIF 1 width = 12 sites = 7 llr = 95 E-value = 2.0e-001 ******************************************************************************** -------------------------------------------------------------------------------- Motif 1 Description -------------------------------------------------------------------------------- Simplified A :::9:a::::3: pos.-specific C ::a:9:11691a probability G ::::1::94:4: matrix T aa:1::9::11: 使用下面的方法来读取这个文件（以 meme.dna.oops.txt 存储）：\n\u0026gt;\u0026gt;\u0026gt; handle = open(\u0026#34;meme.dna.oops.txt\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = motifs.parse(handle, \u0026#34;meme\u0026#34;) \u0026gt;\u0026gt;\u0026gt; handle.close() motifs.parse 命令直接读取整个文件，所以在使用后可以关闭这个文件。其中头文件信息被存储于属性中\n\u0026gt;\u0026gt;\u0026gt; record.version \u0026#39;3.0\u0026#39; \u0026gt;\u0026gt;\u0026gt; record.datafile \u0026#39;INO_up800.s\u0026#39; \u0026gt;\u0026gt;\u0026gt; record.command \u0026#39;meme -mod oops -dna -revcomp -nmotifs 2 -bfile yeast.nc.6.freq INO_up800.s\u0026#39; \u0026gt;\u0026gt;\u0026gt; record.alphabet IUPACUnambiguousDNA() \u0026gt;\u0026gt;\u0026gt; record.sequences [\u0026#39;CHO1\u0026#39;, \u0026#39;CHO2\u0026#39;, \u0026#39;FAS1\u0026#39;, \u0026#39;FAS2\u0026#39;, \u0026#39;ACC1\u0026#39;, \u0026#39;INO1\u0026#39;, \u0026#39;OPI3\u0026#39;] 这个数据记录是 Bio.motifs.meme.Record 类的一个对象。这个类继承于列表（list），所以你可以把这个 record 看成模体对象的一个列表：\n\u0026gt;\u0026gt;\u0026gt; len(record) 2 \u0026gt;\u0026gt;\u0026gt; motif = record[0] \u0026gt;\u0026gt;\u0026gt; print motif.consensus TTCACATGCCGC \u0026gt;\u0026gt;\u0026gt; print motif.degenerate_consensus TTCACATGSCNC 除了一般的模体属性外，每个模体还同时保存着它们由MEME计算的各自特异信息。例如\n\u0026gt;\u0026gt;\u0026gt; motif.num_occurrences 7 \u0026gt;\u0026gt;\u0026gt; motif.length 12 \u0026gt;\u0026gt;\u0026gt; evalue = motif.evalue \u0026gt;\u0026gt;\u0026gt; print \u0026#34;%3.1g\u0026#34; % evalue 0.2 \u0026gt;\u0026gt;\u0026gt; motif.name \u0026#39;Motif 1\u0026#39; 除了像上面所做的用索引来获得相关记录，你也可以用它的名称来找到这个记录\n\u0026gt;\u0026gt;\u0026gt; motif = record[\u0026#39;Motif 1\u0026#39;] 每个模体都有一个 .instances 属性与在这个被发现模体中的序列实例，能够为每个实例提供一些信息：\n\u0026gt;\u0026gt;\u0026gt; len(motif.instances) 7 \u0026gt;\u0026gt;\u0026gt; motif.instances[0] Instance(\u0026#39;TTCACATGCCGC\u0026#39;, IUPACUnambiguousDNA()) \u0026gt;\u0026gt;\u0026gt; motif.instances[0].motif_name \u0026#39;Motif 1\u0026#39; \u0026gt;\u0026gt;\u0026gt; motif.instances[0].sequence_name \u0026#39;INO1\u0026#39; \u0026gt;\u0026gt;\u0026gt; motif.instances[0].start 620 \u0026gt;\u0026gt;\u0026gt; motif.instances[0].strand \u0026#39;-\u0026#39; \u0026gt;\u0026gt;\u0026gt; motif.instances[0].length 12 \u0026gt;\u0026gt;\u0026gt; pvalue = motif.instances[0].pvalue \u0026gt;\u0026gt;\u0026gt; print \u0026#34;%5.3g\u0026#34; % pvalue 1.85e-08 MAST\nTRANSFAC\nTRANSFAC是一个为转录因子手动创建的一个专业数据库，同时还包括染色体结合位点和DNA结合的描述 [27] 。TRANSFAC数据库中所用的文件格式至今还被其他工具所使用，我们下面将介绍TRANSFAC文件格式\nTRANSFAC文件格式简单概括如下：\nID motif1 P0 A C G T 01 1 2 2 0 S 02 2 1 2 0 R 03 3 0 1 1 A 04 0 5 0 0 C 05 5 0 0 0 A 06 0 0 4 1 G 07 0 1 4 0 G 08 0 0 0 5 T 09 0 0 5 0 G 10 0 1 2 2 K 11 0 2 0 3 Y 12 1 0 3 1 G // 这个文件显示了模体 motif1 中12个核苷酸的频率矩阵。总的来说，一个TRANSFAC文件里面可以包含多个模体。以下是示例文件 transfac.dat 的内容：\nVV EXAMPLE January 15, 2013 XX // ID motif1 P0 A C G T 01 1 2 2 0 S 02 2 1 2 0 R 03 3 0 1 1 A ... 11 0 2 0 3 Y 12 1 0 3 1 G // ID motif2 P0 A C G T 01 2 1 2 0 R 02 1 2 2 0 S ... 09 0 0 0 5 T 10 0 2 0 3 Y // 可用如下方法读取TRANSFAC文件：\n\u0026gt;\u0026gt;\u0026gt; handle = open(\u0026#34;transfac.dat\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = motifs.parse(handle, \u0026#34;TRANSFAC\u0026#34;) \u0026gt;\u0026gt;\u0026gt; handle.close() 如果有总版本号的话，它是存储在 record.version 中：\n\u0026gt;\u0026gt;\u0026gt; record.version \u0026#39;EXAMPLE January 15, 2013\u0026#39; 每个在 record 中的模体都是 Bio.motifs.transfac.Motif 类的实例，这些实例同时继承 Bio.motifs.Motif 类和Python字典的属性。这些字典用双字母的键来存储关于这个模体的其他附加信息：\n\u0026gt;\u0026gt;\u0026gt; motif = record[0] \u0026gt;\u0026gt;\u0026gt; motif.degenerate_consensus # Using the Bio.motifs.Motif method Seq(\u0026#39;SRACAGGTGKYG\u0026#39;, IUPACAmbiguousDNA()) \u0026gt;\u0026gt;\u0026gt; motif[\u0026#39;ID\u0026#39;] # Using motif as a dictionary \u0026#39;motif1\u0026#39; TRANSFAC文件一般比这些例子更详细，包含了许多关于模体的附加信息。表格 列出了在TRANSFAC文件常见的双字母含义：\ntable : TRANSFAC文件中常见的字段 AC\tAccession numbers 序列号 AS\tAccession numbers, secondary 第二序列号 BA\tStatistical basis 统计依据 BF\tBinding factors 结合因子 BS\tFactor binding sites underlying the matrix 基于矩阵的转录结合位点 CC\tComments 注解 CO\tCopyright notice 版权事项 DE\tShort factor description 短因子说明 DR\tExternal databases 外部数据库 DT\tDate created/updated 创建或更新日期 HC\tSubfamilies 亚家庭名称 HP\tSuperfamilies 超家庭名称 ID\tIdentifier 身份证 NA\tName of the binding factor 结合因子的名称 OC\tTaxonomic classification 分类 OS\tSpecies/Taxon 种类或分类 OV\tOlder version 旧版本 PV\tPreferred version 首选版本 TY\tType 类型 XX\tEmpty line; these are not stored in the Record. 空白行;没在记录中存储的数据 每个模体同时也有一个包含与这个模体相关参考资料的 references 属性，用下面的双字母键来获得：\nTable : TRANSFAC文件中用来存储参考资料的字段 RN\tReference number 参考数目 RA\tReference authors 参考资料作者 RL\tReference data 参考数据 RT\tReference title 参考标题 RX\tPubMed ID 将TRANSFAC文件按原来格式打印出来：\n\u0026gt;\u0026gt;\u0026gt; print record VV EXAMPLE January 15, 2013 XX // ID motif1 XX P0 A C G T 01 1 2 2 0 S 02 2 1 2 0 R 03 3 0 1 1 A 04 0 5 0 0 C 05 5 0 0 0 A 06 0 0 4 1 G 07 0 1 4 0 G 08 0 0 0 5 T 09 0 0 5 0 G 10 0 1 2 2 K 11 0 2 0 3 Y 12 1 0 3 1 G XX // ID motif2 XX P0 A C G T 01 2 1 2 0 R 02 1 2 2 0 S 03 0 5 0 0 C 04 3 0 1 1 A 05 0 0 4 1 G 06 5 0 0 0 A 07 0 1 4 0 G 08 0 0 5 0 G 09 0 0 0 5 T 10 0 2 0 3 Y XX // 通过用字符串形式来截取输出并且保存在文件中，你可以按TRANSFAC的格式导出这些模体：\n\u0026gt;\u0026gt;\u0026gt; text = str(record) \u0026gt;\u0026gt;\u0026gt; handle = open(\u0026#34;mytransfacfile.dat\u0026#34;, \u0026#39;w\u0026#39;) \u0026gt;\u0026gt;\u0026gt; handle.write(text) \u0026gt;\u0026gt;\u0026gt; handle.close() 模体写出 说到导出，我们可以先看看导出函数。以JASPAR .pfm 格式导出模体文件，可以用：\n\u0026gt;\u0026gt;\u0026gt; print m.format(\u0026#34;pfm\u0026#34;) 3 7 0 2 1 0 0 5 2 6 0 0 0 3 0 4 0 2 0 0 用类似TRANSFAC的格式导出一个模体：\n\u0026gt;\u0026gt;\u0026gt; print m.format(\u0026#34;transfac\u0026#34;) P0 A C G T 01 3 0 0 4 W 02 7 0 0 0 A 03 0 5 0 2 C 04 2 2 3 0 V 05 1 6 0 0 C XX // 你可以用 motifs.write 来写出多个模体。这个函数在使用的时候不必担心这些模体来自于TRANSFAC文件。比如：\n\u0026gt;\u0026gt; two_motifs = [arnt, srf] \u0026gt;\u0026gt;\u0026gt; print motifs.write(two_motifs, \u0026#39;transfac\u0026#39;) P0 A C G T 01 4 16 0 0 C 02 19 0 1 0 A 03 0 20 0 0 C 04 0 0 20 0 G 05 0 0 0 20 T 06 0 0 20 0 G XX // P0 A C G T 01 2 1 39 4 G 02 9 33 2 2 C 03 0 45 1 0 C 04 1 45 0 0 C 05 32 1 0 13 A 06 3 1 0 42 T 07 46 0 0 0 A 08 1 0 0 45 T 09 43 0 0 3 A 10 15 1 0 30 T 11 2 0 44 0 G 12 2 1 43 0 G XX // 绘制序列标识图 如果能够联网，我们可以创建一个 weblogo ：\n将得到的标识图存储成PNG格式\n\u0026gt;\u0026gt;\u0026gt; arnt.weblogo(\u0026#34;Arnt.png\u0026#34;) 位置权重矩阵 模体对象的 .counts 属性能够显示在序列上每个位置核苷酸出现的次数。我们可以把这矩阵除以序列中的实例数目来标准化这矩阵，得到每个核苷酸在序列位置上出现概率。我们把这概率看作位置权重矩阵。不过，要知道在字面上，这个术语也可以用来说明位置特异性得分矩阵，这个我们将会在下面讨论。\n通常来说，伪计数（pseudocounts）在归一化之前都已经加到每个位置中。这样可以避免在这序列上过度拟合位置权重矩阵以至趋向于模体的实例的有限数量，还可以避免概率为0。向每个位置的核苷酸添加一个固定的伪计数，可以为 pseudocounts 参数指定一个数值：\n\u0026gt;\u0026gt;\u0026gt; pwm = m.counts.normalize(pseudocounts=0.5) \u0026gt;\u0026gt;\u0026gt; print pwm 0 1 2 3 4 A: 0.39 0.83 0.06 0.28 0.17 C: 0.06 0.06 0.61 0.28 0.72 G: 0.06 0.06 0.06 0.39 0.06 T: 0.50 0.06 0.28 0.06 0.06 另外， pseudocounts 可以利用字典为每个核苷酸指定一个伪计数值。例如，由于在人类基因组中GC含量大概为40%,因此可以选择下面这些伪计数值：\n\u0026gt;\u0026gt;\u0026gt; pwm = m.counts.normalize(pseudocounts={\u0026#39;A\u0026#39;:0.6, \u0026#39;C\u0026#39;: 0.4, \u0026#39;G\u0026#39;: 0.4, \u0026#39;T\u0026#39;: 0.6}) \u0026gt;\u0026gt;\u0026gt; print pwm 0 1 2 3 4 A: 0.40 0.84 0.07 0.29 0.18 C: 0.04 0.04 0.60 0.27 0.71 G: 0.04 0.04 0.04 0.38 0.04 T: 0.51 0.07 0.29 0.07 0.07 位置权重矩阵有它自己的方法计算一致序列、反向一致序列和简并一致序列：\n\u0026gt;\u0026gt;\u0026gt; pwm.consensus Seq(\u0026#39;TACGC\u0026#39;, IUPACUnambiguousDNA()) \u0026gt;\u0026gt;\u0026gt; pwm.anticonsensus Seq(\u0026#39;GGGTG\u0026#39;, IUPACUnambiguousDNA()) \u0026gt;\u0026gt;\u0026gt; pwm.degenerate_consensus Seq(\u0026#39;WACNC\u0026#39;, IUPACAmbiguousDNA()) 应当注意到由于伪计数的原因，由位置仅重矩阵计算得到的简并一致序列和由模体中实例计算得到的简并一致序列有一点不同：\n\u0026gt;\u0026gt;\u0026gt; m.degenerate_consensus Seq(\u0026#39;WACVC\u0026#39;, IUPACAmbiguousDNA()) 位置权重矩阵的反向互补矩阵可以直接用 pwm 计算出来：\n\u0026gt;\u0026gt;\u0026gt; rpwm = pwm.reverse_complement() \u0026gt;\u0026gt;\u0026gt; print rpwm 0 1 2 3 4 A: 0.07 0.07 0.29 0.07 0.51 C: 0.04 0.38 0.04 0.04 0.04 G: 0.71 0.27 0.60 0.04 0.04 T: 0.18 0.29 0.07 0.84 0.40 位置特异性得分矩阵 使用背景分布和加入伪计数的PWM，很容易就能计算出log-odds比率，提供特定标记的log odds值，这值来自于在这个背景的模体。我们可以用在位置仅重矩阵中 .log-odds() 方法：\n\u0026gt;\u0026gt;\u0026gt; pssm = pwm.log_odds() \u0026gt;\u0026gt;\u0026gt; print pssm 0 1 2 3 4 A: 0.68 1.76 -1.91 0.21 -0.49 C: -2.49 -2.49 1.26 0.09 1.51 G: -2.49 -2.49 -2.49 0.60 -2.49 T: 1.03 -1.91 0.21 -1.91 -1.91 这时我们可以更经常看到特定标记和背景下的正值和负值。0.0意味着在模体和背景中观察到一个标记有相等的可能性。\n上面是假设A,C,G和T在背景中出现的概率是相同的。那在A,C,G和T出现概率不同的情况下，为了计算特定背景下的位置特异性得分矩阵，可以使用 background 参数。例如，在40%GC含量的背景下，可以用：\n\u0026gt;\u0026gt;\u0026gt; background = {\u0026#39;A\u0026#39;:0.3,\u0026#39;C\u0026#39;:0.2,\u0026#39;G\u0026#39;:0.2,\u0026#39;T\u0026#39;:0.3} \u0026gt;\u0026gt;\u0026gt; pssm = pwm.log_odds(background) \u0026gt;\u0026gt;\u0026gt; print pssm 0 1 2 3 4 A: 0.42 1.49 -2.17 -0.05 -0.75 C: -2.17 -2.17 1.58 0.42 1.83 G: -2.17 -2.17 -2.17 0.92 -2.17 T: 0.77 -2.17 -0.05 -2.17 -2.17 从PSSM中得到的最大和最小值被存储在 .max 和 .min 属性中：\n\u0026gt;\u0026gt;\u0026gt; print \u0026#34;%4.2f\u0026#34; % pssm.max 6.59 \u0026gt;\u0026gt;\u0026gt; print \u0026#34;%4.2f\u0026#34; % pssm.min -10.85 在特定背景下计算平均值和标准方差使用 .mean 和 .std 方法。\n\u0026gt;\u0026gt;\u0026gt; mean = pssm.mean(background) \u0026gt;\u0026gt;\u0026gt; std = pssm.std(background) \u0026gt;\u0026gt;\u0026gt; print \u0026#34;mean = %0.2f, standard deviation = %0.2f\u0026#34; % (mean, std) mean = 3.21, standard deviation = 2.59 如果没有指定特定的背景，就会使用一个统一的背景。因为同KL散度或相对熵的值相同，所以平均值就显得特别重要，并且它也是同背景相比的模体信息含量的测量方法。由于在Biopython中用以2为底的对数来计算log-odds值，信息含量的的单位是bit。\n.reverse_complement, .consensus, .anticonsensus 和 .degenerate_consensus 方法可以直接对PSSM使用。\n搜索实例 模体最常用的功能就是在序列中的查找它的实例。在这节，我们会用如下的序列作为例子：\n\u0026gt;\u0026gt;\u0026gt; test_seq=Seq(\u0026#34;TACACTGCATTACAACCCAAGCATTA\u0026#34;,m.alphabet) \u0026gt;\u0026gt;\u0026gt; len(test_seq) 26 搜索准确匹配实例 查找实例最简单的方法就是查找模体实例的准确匹配：\n\u0026gt;\u0026gt;\u0026gt; for pos,seq in m.instances.search(test_seq): ... print pos, seq ... 0 TACAC 10 TACAA 13 AACCC 我们可获得反向互补序列（找到互补链的实例）：\n\u0026gt;\u0026gt;\u0026gt; for pos,seq in r.instances.search(test_seq): ... print pos, seq ... 6 GCATT 20 GCATT 用PSSM得分搜索匹配实例 在模体中很容易找出相应的位置,引起对模体的高log-odds值：\n\u0026gt;\u0026gt;\u0026gt; for position, score in pssm.search(test_seq, threshold=3.0): ... print \u0026#34;Position %d: score = %5.3f\u0026#34; % (position, score) ... Position 0: score = 5.622 Position -20: score = 4.601 Position 10: score = 3.037 Position 13: score = 5.738 Position -6: score = 4.601 负值的位置是指在测试序列的反向链中找到的模体的实例，而且得力于Python的索引。在 pos 的模体实例可以用 test_seq[pos:pos+len(m)] 来定位，不管 pos 值是正还是负。\n你可能注意到阀值参数，在这里随意地设为3.0。这里是 _log_2 ，所以我们现在开始寻找那些在模体中出现概率为背景中出现概率8倍序列。默认的阀值是0.0,在此阀值下，会把所有比背景中出现概率大的模体实例都找出来。\n\u0026gt;\u0026gt;\u0026gt; pssm.calculate(test_seq) array([ 5.62230396, -5.6796999 , -3.43177247, 0.93827754, -6.84962511, -2.04066086, -10.84962463, -3.65614533, -0.03370807, -3.91102552, 3.03734159, -2.14918518, -0.6016975 , 5.7381525 , -0.50977498, -3.56422281, -8.73414803, -0.09919716, -0.6016975 , -2.39429784, -10.84962463, -3.65614533], dtype=float32) 通常来说，上述是计算PSSM得分的最快方法。这些得分只能由前导链用 pssm.calculate 计算得到。为了得到互补链的PSSM值，你可以利用PSSM的互补矩阵\n\u0026gt;\u0026gt;\u0026gt; rpssm = pssm.reverse_complement() \u0026gt;\u0026gt;\u0026gt; rpssm.calculate(test_seq) array([ -9.43458748, -3.06172252, -7.18665981, -7.76216221, -2.04066086, -4.26466274, 4.60124254, -4.2480607 , -8.73414803, -2.26503372, -6.49598789, -5.64668512, -8.73414803, -10.84962463, -4.82356262, -4.82356262, -5.64668512, -8.73414803, -4.15613794, -5.6796999 , 4.60124254, -4.2480607 ], dtype=float32) 选择得分阈值 如果不想刚才那么随意设定一个阀值，你可以探究一下PSSM得分的分布。由于得分的空间分布随着模体长度而成倍增长，我们用一个近似于给定精度值来计算，如此可使计算成本更容易控制：\n\u0026gt;\u0026gt;\u0026gt; distribution = pssm.distribution(background=background, precision=10**4) distribution 对象可以用来决定许多不同的阀值。我们可以指定一个需要的的假阳性率（找到一个由序列在此背景下产生的模体实例的概率）：\n\u0026gt;\u0026gt;\u0026gt; threshold = distribution.threshold_fpr(0.01) \u0026gt;\u0026gt;\u0026gt; print \u0026#34;%5.3f\u0026#34; % threshold 4.009 或者假阴性率（找不到模体产生的实例概率）：\n\u0026gt;\u0026gt;\u0026gt; threshold = distribution.threshold_fnr(0.1) \u0026gt;\u0026gt;\u0026gt; print \u0026#34;%5.3f\u0026#34; % threshold -0.510 或者一个阀值（近似），满足假阳性率和假阴性率之间的关系（fnr/fpr≃ t)：\n\u0026gt;\u0026gt;\u0026gt; threshold = distribution.threshold_balanced(1000) \u0026gt;\u0026gt;\u0026gt; print \u0026#34;%5.3f\u0026#34; % threshold 6.241 或者一个阀值能够大体满足假阳性率和信息含量的 −_log_ 值之间的相等关系（与Hertz和Stormo的Patser软件所用的一样）\n\u0026gt;\u0026gt;\u0026gt; threshold = distribution.threshold_patser() \u0026gt;\u0026gt;\u0026gt; print \u0026#34;%5.3f\u0026#34; % threshold 0.346 比如在我们这个模体中，当以1000比率的平衡阀值查找实例，你可以得到一个让你获得相同结果的阀值（对这个序列来说）。\n\u0026gt;\u0026gt;\u0026gt; threshold = distribution.threshold_fpr(0.01) \u0026gt;\u0026gt;\u0026gt; print \u0026#34;%5.3f\u0026#34; % threshold 4.009 \u0026gt;\u0026gt;\u0026gt; for position, score in pssm.search(test_seq,threshold=threshold): ... print \u0026#34;Position %d: score = %5.3f\u0026#34; % (position, score) ... Position 0: score = 5.622 Position -20: score = 4.601 Position 13: score = 5.738 Position -6: score = 4.601 模体对象自身相关的位置特异性得分矩阵 为了更好的利用PSSMs来查找潜在的TFBSs，每个模体都同位置权重矩阵和位置特异性得分矩阵相关联。用Arnt模体来举个例子：\n\u0026gt;\u0026gt;\u0026gt; from Bio import motifs \u0026gt;\u0026gt;\u0026gt; handle = open(\u0026#34;Arnt.sites\u0026#34;) \u0026gt;\u0026gt;\u0026gt; motif = motifs.read(handle, \u0026#39;sites\u0026#39;) \u0026gt;\u0026gt;\u0026gt; print motif.counts 0 1 2 3 4 5 A: 4.00 19.00 0.00 0.00 0.00 0.00 C: 16.00 0.00 20.00 0.00 0.00 0.00 G: 0.00 1.00 0.00 20.00 0.00 20.00 T: 0.00 0.00 0.00 0.00 20.00 0.00 \u0026gt;\u0026gt;\u0026gt; print motif.pwm 0 1 2 3 4 5 A: 0.20 0.95 0.00 0.00 0.00 0.00 C: 0.80 0.00 1.00 0.00 0.00 0.00 G: 0.00 0.05 0.00 1.00 0.00 1.00 T: 0.00 0.00 0.00 0.00 1.00 0.00 \u0026gt;\u0026gt;\u0026gt; print motif.pssm 0 1 2 3 4 5 A: -0.32 1.93 -inf -inf -inf -inf C: 1.68 -inf 2.00 -inf -inf -inf G: -inf -2.32 -inf 2.00 -inf 2.00 T: -inf -inf -inf -inf 2.00 -inf 在这出现的负无穷大是由于在频率矩阵中相关项的值为0,并且我们默认使用0作为伪计数：\n\u0026gt;\u0026gt;\u0026gt; for letter in \u0026#34;ACGT\u0026#34;: ... print \u0026#34;%s: %4.2f\u0026#34; % (letter, motif.pseudocounts[letter]) ... A: 0.00 C: 0.00 G: 0.00 T: 0.00 如果你更改了 .pseudocouts 属性，那么位置频率矩阵和位置特异性得分矩阵就都会自动重新计算：\n\u0026gt;\u0026gt;\u0026gt; motif.pseudocounts = 3.0 \u0026gt;\u0026gt;\u0026gt; for letter in \u0026#34;ACGT\u0026#34;: ... print \u0026#34;%s: %4.2f\u0026#34; % (letter, motif.pseudocounts[letter]) ... A: 3.00 C: 3.00 G: 3.00 T: 3.00 \u0026gt;\u0026gt;\u0026gt; print motif.pwm 0 1 2 3 4 5 A: 0.22 0.69 0.09 0.09 0.09 0.09 C: 0.59 0.09 0.72 0.09 0.09 0.09 G: 0.09 0.12 0.09 0.72 0.09 0.72 T: 0.09 0.09 0.09 0.09 0.72 0.09 \u0026gt;\u0026gt;\u0026gt; print motif.pssm 0 1 2 3 4 5 A: -0.19 1.46 -1.42 -1.42 -1.42 -1.42 C: 1.25 -1.42 1.52 -1.42 -1.42 -1.42 G: -1.42 -1.00 -1.42 1.52 -1.42 1.52 T: -1.42 -1.42 -1.42 -1.42 1.52 -1.42 如果你想对4个核苷酸使用不同的伪计数，可以使用字典来设定4个核苷酸的 pseudocounts 。把 motif.pseudocounts 设为 None 会让伪计数重置为0的默认值。\n位置特异性得分矩阵依赖于一个默认均一的背景分布：\n\u0026gt;\u0026gt;\u0026gt; for letter in \u0026#34;ACGT\u0026#34;: ... print \u0026#34;%s: %4.2f\u0026#34; % (letter, motif.background[letter]) ... A: 0.25 C: 0.25 G: 0.25 T: 0.25 同样，如果你更改了背景分布，位置特异性得分矩阵也会重新计算：\n\u0026gt;\u0026gt;\u0026gt; motif.background = {\u0026#39;A\u0026#39;: 0.2, \u0026#39;C\u0026#39;: 0.3, \u0026#39;G\u0026#39;: 0.3, \u0026#39;T\u0026#39;: 0.2} \u0026gt;\u0026gt;\u0026gt; print motif.pssm 0 1 2 3 4 5 A: 0.13 1.78 -1.09 -1.09 -1.09 -1.09 C: 0.98 -1.68 1.26 -1.68 -1.68 -1.68 G: -1.68 -1.26 -1.68 1.26 -1.68 1.26 T: -1.09 -1.09 -1.09 -1.09 1.85 -1.09 把 motif.backgroud 设为 None 后会将其重置为均一的分布。\n\u0026gt;\u0026gt;\u0026gt; motif.background = None \u0026gt;\u0026gt;\u0026gt; for letter in \u0026#34;ACGT\u0026#34;: ... print \u0026#34;%s: %4.2f\u0026#34; % (letter, motif.background[letter]) ... A: 0.25 C: 0.25 G: 0.25 T: 0.25 如果你把 motif.background 设为一个单一值，这个值将会被看成是GC含量：\n\u0026gt;\u0026gt;\u0026gt; motif.background = 0.8 \u0026gt;\u0026gt;\u0026gt; for letter in \u0026#34;ACGT\u0026#34;: ... print \u0026#34;%s: %4.2f\u0026#34; % (letter, motif.background[letter]) ... A: 0.10 C: 0.40 G: 0.40 T: 0.10 #应当注意到你能够在当前计算背景下计算PSSM的平均值 \u0026gt;\u0026gt;\u0026gt; print \u0026#34;%f\u0026#34; % motif.pssm.mean(motif.background) 4.703928 #它的标准方差也是一样： \u0026gt;\u0026gt;\u0026gt; print \u0026#34;%f\u0026#34; % motif.pssm.std(motif.background) 3.290900 和它的分布\n\u0026gt;\u0026gt;\u0026gt; distribution = motif.pssm.distribution(background=motif.background) \u0026gt;\u0026gt;\u0026gt; threshold = distribution.threshold_fpr(0.01) \u0026gt;\u0026gt;\u0026gt; print \u0026#34;%f\u0026#34; % threshold 3.854375 请注意，每当你调用 motif.pwm 或 motif.pssm ，位置仅重矩阵和位置特异性得分矩阵都会重新计算。如果看重速度并且需要重复用到PWM或PSSM时，你可以把他们保存成变量，如下所示：\n\u0026gt;\u0026gt;\u0026gt; pssm = motif.pssm 模体比较 在我们开始比较之前，应当要指出模体的边界通常比较模糊。这也就是说我们需要比较不同长度的模体，因此这些比较也要涉及到相关的比对。所以我们需要考虑两个东西：\n模体比对 比较比对后模体的相关函数 为了比对模体，我们使用PSSMs的不含间隔的比对，并且用0来代替矩阵开始和结束位置缺失的列。这说明我们能够有效地利用背景分布来代替PSSM中缺失的列。距离函数然后可以返回模体间最小的距离，以及比对中相应的偏移量。\n举个例子，先导入和测试模体 m 相似的模体： \u0026gt;\u0026gt;\u0026gt; m_reb1 = motifs.read(open(\u0026#34;REB1.pfm\u0026#34;), \u0026#34;pfm\u0026#34;) \u0026gt;\u0026gt;\u0026gt; m_reb1.consensus Seq(\u0026#39;GTTACCCGG\u0026#39;, IUPACUnambiguousDNA()) \u0026gt;\u0026gt;\u0026gt; print m_reb1.counts 0 1 2 3 4 5 6 7 8 A: 30.00 0.00 0.00 100.00 0.00 0.00 0.00 0.00 15.00 C: 10.00 0.00 0.00 0.00 100.00 100.00 100.00 0.00 15.00 G: 50.00 0.00 0.00 0.00 0.00 0.00 0.00 60.00 55.00 T: 10.00 100.00 100.00 0.00 0.00 0.00 0.00 40.00 15.00 为了让模体能够进行相互比较，选择和模体 m 相同伪计数和背景值：\n\u0026gt;\u0026gt;\u0026gt; m_reb1.pseudocounts = {\u0026#39;A\u0026#39;:0.6, \u0026#39;C\u0026#39;: 0.4, \u0026#39;G\u0026#39;: 0.4, \u0026#39;T\u0026#39;: 0.6} \u0026gt;\u0026gt;\u0026gt; m_reb1.background = {\u0026#39;A\u0026#39;:0.3,\u0026#39;C\u0026#39;:0.2,\u0026#39;G\u0026#39;:0.2,\u0026#39;T\u0026#39;:0.3} \u0026gt;\u0026gt;\u0026gt; pssm_reb1 = m_reb1.pssm \u0026gt;\u0026gt;\u0026gt; print pssm_reb1 0 1 2 3 4 5 6 7 8 A: 0.00 -5.67 -5.67 1.72 -5.67 -5.67 -5.67 -5.67 -0.97 C: -0.97 -5.67 -5.67 -5.67 2.30 2.30 2.30 -5.67 -0.41 G: 1.30 -5.67 -5.67 -5.67 -5.67 -5.67 -5.67 1.57 1.44 T: -1.53 1.72 1.72 -5.67 -5.67 -5.67 -5.67 0.41 -0.97 我们将用皮尔逊相关（Pearson correlation）来比较这些模体。由于我们想要让它偏向于一个距离长度，我们实际上取1−_r_ ，其中 r 是皮尔逊相关系数（Pearson correlation coefficient，PCC）：\n\u0026gt;\u0026gt;\u0026gt; distance, offset = pssm.dist_pearson(pssm_reb1) \u0026gt;\u0026gt;\u0026gt; print \u0026#34;distance = %5.3g\u0026#34; % distance distance = 0.239 \u0026gt;\u0026gt;\u0026gt; print offset -2 这意味着模体 m 和 m_reb1 间最佳PCC可以从下面的比对中获得：\n其中 b 代表背景分布。PCC值大概为1−0.239=0.761\nm: bbTACGCbb m_reb1: GTTACCCGG 查找De novo模体 如今，Biopython对 De novo 模体查找的支持是有限的。也就是说，我们支持AlignAce和MEME的运行和读取。由于模体查找工具如雨后春笋般出现，所以很欢迎新的分析程序加入进来\nMEME 假设用MEME以你喜欢的参数设置来跑序列，并把结果保存在文件 meme.out 中。你可以用以下的命令来得到MEME输出的模体：\n\u0026gt;\u0026gt;\u0026gt; from Bio import motifs \u0026gt;\u0026gt;\u0026gt; motifsM = motifs.parse(open(\u0026#34;meme.out\u0026#34;), \u0026#34;meme\u0026#34;) \u0026gt;\u0026gt;\u0026gt; motifsM [\u0026lt;Bio.motifs.meme.Motif object at 0xc356b0\u0026gt;] 除了最想要的一系列模体外，结果中还包含了很多有用的信息，可以通过那些一目了然的属性名获得：\n.alphabet .datafile .sequence_names .version .command 由MEME解析得到的模体可以像平常的模体对象（有实例）一样处理，它们也提供了一些额外的功能，可以为实例增加额外的信息。 \u0026gt;\u0026gt;\u0026gt; motifsM[0].consensus Seq(\u0026#39;CTCAATCGTA\u0026#39;, IUPACUnambiguousDNA()) \u0026gt;\u0026gt;\u0026gt; motifsM[0].instances[0].sequence_name \u0026#39;SEQ10;\u0026#39; \u0026gt;\u0026gt;\u0026gt; motifsM[0].instances[0].start 3 \u0026gt;\u0026gt;\u0026gt; motifsM[0].instances[0].strand \u0026#39;+\u0026#39; \u0026gt;\u0026gt;\u0026gt; motifsM[0].instances[0].pvalue 8.71e-07 AlignAce 我们可以用AlignACE程序实现类似的效果。假如，你把结果保存在 alignace.out 文件中。你可以用下面的代码读取结果：\n\u0026gt;\u0026gt;\u0026gt; from Bio import motifs \u0026gt;\u0026gt;\u0026gt; motifsA = motifs.parse(open(\u0026#34;alignace.out\u0026#34;),\u0026#34;alignace\u0026#34;) 同样，你的模体也和正常的模体对象有相同的属性：\n\u0026gt;\u0026gt;\u0026gt; motifsA[0].consensus Seq(\u0026#39;TCTACGATTGAG\u0026#39;, IUPACUnambiguousDNA()) 事实上，你甚至可以观察到，AlignAce找到了一个和MEME非常相似的模体。下面只是MEME模体互补链的一个较长版本：\n\u0026gt;\u0026gt;\u0026gt; motifsM[0].reverse_complement().consensus Seq(\u0026#39;TACGATTGAG\u0026#39;, IUPACUnambiguousDNA()) 如果你的机器上安装了AlignAce，你可以直接从Biopython中运行AlignAce。下面就是一个如何运行AlignAce的简单例子（其他参数可以用关键字参数来调用）：\n\u0026gt;\u0026gt;\u0026gt; command=\u0026#34;/opt/bin/AlignACE\u0026#34; \u0026gt;\u0026gt;\u0026gt; input_file=\u0026#34;test.fa\u0026#34; \u0026gt;\u0026gt;\u0026gt; from Bio.motifs.applications import AlignAceCommandline \u0026gt;\u0026gt;\u0026gt; cmd = AlignAceCommandline(cmd=command,input=input_file,gcback=0.6,numcols=10) \u0026gt;\u0026gt;\u0026gt; stdout,stderr= cmd() 由于AlignAce把所有的结果输出到标准输出，所以你可以通过读取结果的第一部分来获得模体：\n\u0026gt;\u0026gt;\u0026gt; motifs = motifs.parse(stdout,\u0026#34;alignace\u0026#34;) 相关链接 Sequence motif in wikipedia http://en.wikipedia.org/wiki/Sequence_motif PWM in wikipedia http://en.wikipedia.org/wiki/Position_weight_matrix Consensus sequence in wikipedia http://en.wikipedia.org/wiki/Consensus_sequence Comparison of different motif finding programs http://bio.cs.washington.edu/assessment/ ","date":"2021-08-14T00:00:00Z","permalink":"https://example.com/p/ch14_%E4%BD%BF%E7%94%A8bio.motifs%E8%BF%9B%E8%A1%8C%E6%A8%A1%E4%BD%93%E5%BA%8F%E5%88%97%E5%88%86%E6%9E%90/","title":"ch14_使用Bio.motifs进行模体序列分析"},{"content":"biopython官方地址：https://biopython.org/\ngithub地址：https://github.com/biopython/biopython/blob/master/\n中文版教程：https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html\nbiopython包的说明（具体到每个模块了）：https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html\nBiopython1.54开始引入了Bio.Phylo模块，与SeqIO和AlignIO类似，它的目的是提供 一个通用的独立于源数据格式的方法来使用系统进化树，同时提供一致的API来进行 I/O操作。\nBio.Phylo在一篇开放获取的期刊文章中有介绍 [9, Talevich et al., 2012], 这可能对您也有所帮助。\n树中有什么？ 为了熟悉这个模块，让我们首先从一个已经创建好的树开始，从几个不同的角度来审视 它。接着我们将给树的分支上颜色，并使用一个特殊的phyloXML特性，最终保存树\n译者注：本翻译中， 分支 对应原文中的 branch ，原文中一般代表 某一个节点的前导连线；而 进化枝 对应原文中的 clade ，代表某个节点所代表的整个 进化分支，包括本身和它所有的后代；若clade代表biopython中的对象则保留原文\n在终端中使用你喜欢的编辑器创建一个简单的Newick文件：\n% cat \u0026gt; simple.dnd \u0026lt;\u0026lt;EOF \u0026gt; (((A,B),(C,D)),(E,F,G)); \u0026gt; EOF 这棵树没有分支长度，只有一个拓扑结构和标记的端点。（如果你有一个真正的树文件， 你也可以使用它来替代进行示例操作。）\n选择启动你的Python解释器：\n% ipython -pylab 对于交互式操作，使用参数 -pylab 启动IPython解释器能启用 matplotlib 整合 功能，这样图像就能自动弹出来。我们将在这个示例中使用该功能。\n现在，在Python终端中，读取树文件，给定文件名和格式名。\n\u0026gt;\u0026gt;\u0026gt; from Bio import Phylo \u0026gt;\u0026gt;\u0026gt; tree = Phylo.read(\u0026#34;simple.dnd\u0026#34;, \u0026#34;newick\u0026#34;) 以字符串打印该树对象我们将得到整个对象的层次结构概况。\n\u0026gt;\u0026gt;\u0026gt; print tree Tree(weight=1.0, rooted=False, name=\u0026#34;\u0026#34;) Clade(branch_length=1.0) Clade(branch_length=1.0) Clade(branch_length=1.0) Clade(branch_length=1.0, name=\u0026#34;A\u0026#34;) Clade(branch_length=1.0, name=\u0026#34;B\u0026#34;) Clade(branch_length=1.0) Clade(branch_length=1.0, name=\u0026#34;C\u0026#34;) Clade(branch_length=1.0, name=\u0026#34;D\u0026#34;) Clade(branch_length=1.0) Clade(branch_length=1.0, name=\u0026#34;E\u0026#34;) Clade(branch_length=1.0, name=\u0026#34;F\u0026#34;) Clade(branch_length=1.0, name=\u0026#34;G\u0026#34;) Tree 对象包含树的全局信息，如树是有根树还是无根树。它包含一个根进化枝， 和以此往下以列表嵌套的所有进化枝，直至叶子分支。\n函数 draw_ascii 创建一个简单的ASCII-art(纯文本)系统发生图。在没有更好 图形工具的情况下，这对于交互研究来说是一个方便的可视化展示方式。\n\u0026gt;\u0026gt;\u0026gt; Phylo.draw_ascii(tree) ________________________ A ________________________| | |________________________ B ________________________| | | ________________________ C | |________________________| _| |________________________ D | | ________________________ E | | |________________________|________________________ F | |________________________ G 如果你安装有 matplotlib 或者 pylab, 你可以使用 draw 函数一个图像\n\u0026gt;\u0026gt;\u0026gt; tree.rooted = True \u0026gt;\u0026gt;\u0026gt; Phylo.draw(tree) image.png 给树的分支上颜色 函数 draw 和 draw_graphviz 支持在树中显示不同的颜色和分支宽度。 从Biopython 1.59开始，Clade对象就开始支持 color 和 width 属性， 且使用他们不需要额外支持。这两个属性都表示导向给定的进化枝前面的分支的 属性，并依次往下作用，所以所有的后代分支在显示时也都继承相同的宽度和颜 色。\n在早期的Biopython版本中，PhyloXML树有些特殊的特性，使用这些属性需要首先 将这个树转换为一个基本树对象的子类Phylogeny，该类在Bio.Phylo.PhyloXML模 块中。\n在Biopython 1.55和之后的版本中，这是一个很方便的树方法：\n\u0026gt;\u0026gt;\u0026gt; tree = tree.as_phyloxml() 在Biopython 1.54中, 你能通过导入一个额外的模块实现相同的事情：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Phylo.PhyloXML import Phylogeny \u0026gt;\u0026gt;\u0026gt; tree = Phylogeny.from_tree(tree) 注意Newick和Nexus文件类型并不支持分支颜色和宽度，如果你在Bio.Phylo中使用 这些属性，你只能保存这些值到PhyloXML格式中。（你也可以保存成Newick或Nexus 格式，但是颜色和宽度信息在输出的文件时会被忽略掉。）\n现在我们开始指定颜色。首先，我们将设置根进化枝为灰色。我们能通过赋值24位 的颜色值来实现，用三位数的RGB值、HTML格式的十六进制字符串、或者预先设置好的 颜色名称\n\u0026gt;\u0026gt;\u0026gt; tree.root.color = (128,128,128) #等同于 \u0026gt;\u0026gt;\u0026gt; tree.root.color = \u0026#34;#808080\u0026#34; #等同于 \u0026gt;\u0026gt;\u0026gt; tree.root.color = \u0026#34;gray\u0026#34; 一个进化枝的颜色会被当作从上而下整个进化枝的颜色，所以我们这里设置根的 的颜色会将整个树的颜色变为灰色。我们能通过在树中下面分支赋值不同的颜色 来重新定义某个分支的颜色。\n让我们先定位“E”和“F”最近祖先（MRCA）节点。方法 common_ancestor 返回 原始树中这个进化枝的引用，所以当我们设置该进化枝为“salmon”颜色时，这个颜 色则会在原始的树中显示出来。\n\u0026gt;\u0026gt;\u0026gt; mrca = tree.common_ancestor({\u0026#34;name\u0026#34;: \u0026#34;E\u0026#34;}, {\u0026#34;name\u0026#34;: \u0026#34;F\u0026#34;}) \u0026gt;\u0026gt;\u0026gt; mrca.color = \u0026#34;salmon\u0026#34; 当我们碰巧明确地知道某个进化枝在树中的位置，以嵌套列表的形式，我们就能 通过索引的方式直接跳到那个位置。这里，索引 [0,1] 表示根节点的第一个 子代节点的第二个子代。\n\u0026gt;\u0026gt;\u0026gt; tree.clade[0,1].color = \u0026#34;blue\u0026#34; 最后，展示一下我们的工作结果\n\u0026gt;\u0026gt;\u0026gt; Phylo.draw(tree) 注意进化枝的颜色包括导向它的分支和它的子代的分支。E和F的共同祖先结果刚好 在根分支下面，而通过这样上色，我们能清楚的看出这个树的根在哪里。\n我们已经完成了很多！现在让我们休息一下，保存一下我们的工作。使用一个文件 名或句柄（这里我们使用标准输出来查看将会输出什么）和 phyloxml 格式来 调用 write 函数。PhyloXML格式保存了我们设置的颜色，所以你能通过其他树 查看工具，如Archaeopteryx，打开这个phyloXML文件，这些颜色也会显示出来。\n\u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; Phylo.write(tree, sys.stdout, \u0026#34;phyloxml\u0026#34;) \u0026lt;phy:phyloxml xmlns:phy=\u0026#34;http://www.phyloxml.org\u0026#34;\u0026gt; \u0026lt;phy:phylogeny rooted=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;phy:clade\u0026gt; \u0026lt;phy:branch_length\u0026gt;1.0\u0026lt;/phy:branch_length\u0026gt; \u0026lt;phy:color\u0026gt; \u0026lt;phy:red\u0026gt;128\u0026lt;/phy:red\u0026gt; \u0026lt;phy:green\u0026gt;128\u0026lt;/phy:green\u0026gt; \u0026lt;phy:blue\u0026gt;128\u0026lt;/phy:blue\u0026gt; \u0026lt;/phy:color\u0026gt; \u0026lt;phy:clade\u0026gt; \u0026lt;phy:branch_length\u0026gt;1.0\u0026lt;/phy:branch_length\u0026gt; \u0026lt;phy:clade\u0026gt; \u0026lt;phy:branch_length\u0026gt;1.0\u0026lt;/phy:branch_length\u0026gt; \u0026lt;phy:clade\u0026gt; \u0026lt;phy:name\u0026gt;A\u0026lt;/phy:name\u0026gt; ... I/O函数 和SeqIO、AlignIO类似, Phylo使用四个函数处理文件的输入输出： parse 、 read 、 write 和 convert ，所有的函数都支持Newick、NEXUS、 phyloXML和NeXML等树文件格式。\nread 函数解析并返回给定文件中的单个树。注意，如果文件中包含多个或不包含任何树，它将抛出一个错误。\n\u0026gt;\u0026gt;\u0026gt; from Bio import Phylo \u0026gt;\u0026gt;\u0026gt; tree = Phylo.read(\u0026#34;Tests/Nexus/int_node_labels.nwk\u0026#34;, \u0026#34;newick\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print tree 处理多个（或者未知个数）的树文件，需要使用 parse 函数迭代给定文件中的每一个树。\n\u0026gt;\u0026gt;\u0026gt; trees = Phylo.parse(\u0026#34;Tests/PhyloXML/phyloxml_examples.xml\u0026#34;, \u0026#34;phyloxml\u0026#34;) \u0026gt;\u0026gt;\u0026gt; for tree in trees: ... print tree 使用 write 函数输出一个或多个可迭代的树。\n\u0026gt;\u0026gt;\u0026gt; trees = list(Phylo.parse(\u0026#34;phyloxml_examples.xml\u0026#34;, \u0026#34;phyloxml\u0026#34;)) \u0026gt;\u0026gt;\u0026gt; tree1 = trees[0] \u0026gt;\u0026gt;\u0026gt; others = trees[1:] \u0026gt;\u0026gt;\u0026gt; Phylo.write(tree1, \u0026#34;tree1.xml\u0026#34;, \u0026#34;phyloxml\u0026#34;) 1 \u0026gt;\u0026gt;\u0026gt; Phylo.write(others, \u0026#34;other_trees.xml\u0026#34;, \u0026#34;phyloxml\u0026#34;) 12 使用 convert 函数转换任何支持的树格式。\n\u0026gt;\u0026gt;\u0026gt; Phylo.convert(\u0026#34;tree1.dnd\u0026#34;, \u0026#34;newick\u0026#34;, \u0026#34;tree1.xml\u0026#34;, \u0026#34;nexml\u0026#34;) 1 \u0026gt;\u0026gt;\u0026gt; Phylo.convert(\u0026#34;other_trees.xml\u0026#34;, \u0026#34;phyloxml\u0026#34;, \u0026#34;other_trees.nex\u0026#34;, \u0026#39;nexus\u0026#34;) 12 和SeqIO和AlignIO类似，当使用字符串而不是文件作为输入输出时，需要使用 ‵‵StringIO`` 函数\n\u0026gt;\u0026gt;\u0026gt; from Bio import Phylo \u0026gt;\u0026gt;\u0026gt; from StringIO import StringIO \u0026gt;\u0026gt;\u0026gt; handle = StringIO(\u0026#34;(((A,B),(C,D)),(E,F,G));\u0026#34;) \u0026gt;\u0026gt;\u0026gt; tree = Phylo.read(handle, \u0026#34;newick\u0026#34;) 查看和导出树 了解一个 Tree 对象概况的最简单的方法是用 print 函数将它打印出来：\n\u0026gt;\u0026gt;\u0026gt; tree = Phylo.read(\u0026#34;Tests/PhyloXML/example.xml\u0026#34;, \u0026#34;phyloxml\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print tree Phylogeny(rooted=\u0026#39;True\u0026#39;, description=\u0026#39;phyloXML allows to use either a \u0026#34;branch_length\u0026#34; attribute...\u0026#39;, name=\u0026#39;example from Prof. Joe Felsenstein\u0026#39;s book \u0026#34;Inferring Phyl...\u0026#39;) Clade() Clade(branch_length=\u0026#39;0.06\u0026#39;) Clade(branch_length=\u0026#39;0.102\u0026#39;, name=\u0026#39;A\u0026#39;) Clade(branch_length=\u0026#39;0.23\u0026#39;, name=\u0026#39;B\u0026#39;) Clade(branch_length=\u0026#39;0.4\u0026#39;, name=\u0026#39;C\u0026#39;) 上面实际上是Biopython的树对象层次结构的一个概况。然而更可能的情况是，你希望见到 画出树的形状，这里有三个函数来做这件事情。\n如我们在demo中看到的一样， draw_ascii 打印一个树的ascii-art图像（有根进化树） 到标准输出，或者一个打开的文件句柄，若有提供。不是所有关于树的信息被显示出来，但是它提供了一个 不依靠于任何外部依赖的快速查看树的方法。\n\u0026gt;\u0026gt;\u0026gt; tree = Phylo.read(\u0026#34;example.xml\u0026#34;, \u0026#34;phyloxml\u0026#34;) \u0026gt;\u0026gt;\u0026gt; Phylo.draw_ascii(tree) __________________ A __________| _| |___________________________________________ B | |___________________________________________________________________________ C draw 函数则使用matplotlib类库画出一个更加好看的图像。查看API文档以获得关于它所接受的 用来定制输出的参数。\n\u0026gt;\u0026gt;\u0026gt; tree = Phylo.read(\u0026#34;example.xml\u0026#34;, \u0026#34;phyloxml\u0026#34;) \u0026gt;\u0026gt;\u0026gt; Phylo.draw(tree, branch_labels=lambda c: c.branch_length) draw_graphviz 则画出一个无根的进化分枝图（cladogram），但是它要求你安装有Graphviz、 PyDot或PyGraphviz、Network和matplotlib（或pylab）。使用上面相同的例子，和Graphviz中的 dot 程序，让我们来画一个有根树\n（提示：如果你使用 -pylab 选项执行IPython，调用 draw_graphviz 将导致matplotlib 查看器自动运行，而不需要手动的调用 show() 方法。）\n\u0026gt;\u0026gt;\u0026gt; tree = Phylo.read(\u0026#34;example.xml\u0026#34;, \u0026#34;phyloxml\u0026#34;) \u0026gt;\u0026gt;\u0026gt; Phylo.draw_graphviz(tree, prog=\u0026#39;dot\u0026#39;) \u0026gt;\u0026gt;\u0026gt; import pylab \u0026gt;\u0026gt;\u0026gt; pylab.show() # Displays the tree in an interactive viewer \u0026gt;\u0026gt;\u0026gt; pylab.savefig(\u0026#39;phylo-dot.png\u0026#39;) # Creates a PNG file of the same graphic 这将输出树对象到一个NetworkX图中，使用Graphviz来布局节点的位置，并使用matplotlib来显示 它。这里有几个关键词参数来修改结果图像，包括大多数被NetworkX函数 networkx.draw 和 networkx.draw_graphviz 所接受的参数。\n最终的显示也受所提供的树对象的 rooted 属性的影响。有根树在每个分支（branch）上显示 一个“head”来表明它的方向\n\u0026gt;\u0026gt;\u0026gt; tree = Phylo.read(\u0026#34;simple.dnd\u0026#34;, \u0026#34;newick\u0026#34;) \u0026gt;\u0026gt;\u0026gt; tree.rooted = True \u0026gt;\u0026gt;\u0026gt; Phylo.draw_graphiz(tree) “prog”参数指定Graphviz的用来布局的引擎。默认的引擎 twopi 对任何大小的树都表现很好， 很可靠的避免交叉的分支出现。neato 程序可能画出更加好看的中等大小的树，但是有时候会 有交叉分支出现（见图. ）。 dot 程序或许对小型的树有用， 但是对于大一点的树的布局易产生奇怪的事情。\n\u0026gt;\u0026gt;\u0026gt; Phylo.draw_graphviz(tree, prog=\u0026#34;neato\u0026#34;) 这个查看方式非常方便研究大型的树，因为matplotlib查看器可以放大选择的区域，使得杂乱的图像 变得稀疏\n\u0026gt;\u0026gt;\u0026gt; tree = Phylo.read(\u0026#34;apaf.xml\u0026#34;, \u0026#34;phyloxml\u0026#34;) \u0026gt;\u0026gt;\u0026gt; Phylo.draw_graphviz(tree, prog=\u0026#34;neato\u0026#34;, node_size=0) 注意，分支长度并没有被正确地显示，因为Graphviz在布局时忽略了他们。然而，分支长度可以在输出 树为NetworkX图对象（ to_networkx ）时重新获得\n使用Tree和Clade对象 parse 和 read 方法产生的 Tree 对象是一些包含递归的子树的容器，连接到 Tree 对象的 root 属性（不管进化树实际上被认为是否有根）。一个 Tree 包含进化树的全局信息， 如有根性（rootedness）和指向一个单独的 Clade 的引用; 一个 Clade 包含节点和进化枝 特异性信息，如分支长度（branch length）和一个它自身后代 Clade 实例的列表，附着在 clades 属性上。\n所以，这里 tree 和 tree.root 间是有区别的. 然而，实际操作中，你几乎不需要担心它。为了 缓和这个不同，Tree 和 Clade 两者都继承自 TreeMixin，它包含常用的用来查找、审视和 修改树和任何它的进化枝的方法的实现。这意味着，所有 tree 所支持的方法在 tree.root 和 任何它下面的clade中都能用。（ Clade 也有一个 root 属性，它返回clade对象本身。）\n查找和遍历类方法 为了方便起见，我们提供了两个简化的方法来直接返回所有的外部或内部节点为列表：\nget_terminals\n创建一个包含树的所有末端（叶子）节点的列表。\nget_nonterminals\n创建一个包含树的所有非末端（内部）节点的列表。\n这两个都包装了一个能完全控制树的遍历的方法 find_clades。另外两个遍历方法 find_elements 和 find_any 依赖于同样的核心功能，也接受同样的参数，没有更好的描述我们就把这个参数叫做 “目标说明”（target specification）吧。它们指定哪些树中的对象将被匹配并在迭代过程中返回。 第一个参数可以是下面的任何类型：\n一个 TreeElement 实例 ，那个树的元素将根据一致性被匹配——这样，使用Clade实例作为目标将找到 树中的这个Clade； 一个 string ，匹配树元素的字符串表示——特别地，Clade的 name (在Biopython 1.56中引入)； 一个 class 或 type，这样每一个类型（或子类型）相同的树元素都被匹配； 一个 dictionary ，其中键（key）是树元素的属性名，值（value）将匹配到每个树元素相应的属性值。 它变得更加详细：由于浮点数值可能产生奇怪的行为，我们不支持直接匹配 floats 类型。作为替代，使用boolean值 True 来匹配每个元素中指定属性的非零值，然后再对这个属性用不等式（或精确地数值，如果你喜欢 危险地活着）进行手动过滤。如果该字典包含多个条目，匹配的元素必须匹配所有给定的属性值——以“and”方式思考，而不是“or”。 如果提供的是 int 类型，它将匹配数值上相等的属性，即，1将匹配1或者1.0 如果提供的是boolean类型（True或者False），对应的属性值将被当做boolean求值和检验 None 匹配 None 如果提供的是字符串，将被当做正则表达式对待（必须匹配对应元素属性的全部，不能只是前面的部分）。 提供没有特殊正则表达式字符的字符串将精准的匹配字符串属性，所以如果你不适用正则表达式，不用 担心它。例如，包含进化枝名称Foo1、Foo2和Foo3的一个树， tree.find_clades({\u0026ldquo;name\u0026rdquo;: \u0026ldquo;Foo1\u0026rdquo;}) 将匹配 Foo1， {\u0026ldquo;name\u0026rdquo;: \u0026ldquo;Foo.*\u0026rdquo;} 匹配所有的三个进化枝，而 {\u0026ldquo;name\u0026rdquo;: \u0026ldquo;Foo\u0026rdquo;} 并不匹配任何进化枝。 一个接受一个参数（它将应用于树中的每一个元素），返回True或False的函数 function 。为方便起见， LookupError、AttributeError和ValueError被沉默，这样就提供了另外一个在树中查找浮点值的安全方式， 或者一些更加复杂的特性。 在目标参数后面，有两个可选的关键词参数：\nterminal\n— 用来选择或排除末端进化枝（或者叫叶子节点）的一个boolean值：True仅搜索末端进化枝，False则搜索 非末端（内部）进化枝，而默认为None，同时搜索末端和非末端进化枝，包括没有 is_terminal 方法的 任何树元素。\norder\n— 树遍历的顺序：\u0026ldquo;preorder\u0026rdquo; （默认值）是深度优先搜索（depth-first search，DFS）， \u0026ldquo;postorder\u0026rdquo; 是子节点先于父节点的DFS搜索， \u0026ldquo;level\u0026rdquo; 是宽度优先搜索（breadth-first search，BFS）。\n最后，这些方法接受任意的关键词参数，这些参数将被以和词典“目标说明”相同的方式对待：键表示要搜索的元素 属性的名称，参数值（string、integer、None或者boolean）将和找到的每个属性的值进行比较。如果没有提供 关键词参数，则任何TreeElement类型将被匹配。这个的代码普遍比传入一个词典作为“目标说明”要短： tree.find_clades({\u0026ldquo;name\u0026rdquo;: \u0026ldquo;Foo1\u0026rdquo;}) 可以简化为 tree.find_clades(name=\u0026ldquo;Foo1\u0026rdquo;)。\n（在Biopython 1.56和以后的版本中，这可以更短：tree.find_clades(\u0026ldquo;Foo1\u0026rdquo;) ）\n现在我们已经掌握了“目标说明”，这里有一些遍历树的方法：\nfind_clades\n查找每个包含匹配元素的进化枝。就是说，用 find_elements 查找每个元素，然而返回对应的clade对象。 （这通常是你想要的。）\n最终的结果是一个包含所有匹配对象的迭代器，默认为深度优先搜索。这不一定是和Newick、Nexus或XML原文件 中显示的相同的顺序。\nfind_elements\n查找和给定属性匹配的所有树元素，返回匹配的元素本身。简单的Newick树没有复杂的子元素，所以它将和\nfind_clades 的行为一致。PhyloXML树通常在clade上附加有复杂的对象，所以这个方法对提取这些信息 非常有用。\nfind_any\n返回 find_elements() 所找到的第一个元素，或者None。这对于检测树中是否存在匹配的元素也非常有用， 可以在条件判断语句中使用。\n另外两个用于帮助在树的节点间导航的方法：\nget_path\n直接列出从树的根节点（或当前进化枝）到给定的目标间的所有clade。返回包含这个路径上所有clade对象的 列表，以给定目标为结尾，但不包含根进化枝。\ntrace\n列出树中两个目标间的所有clade对象，不包含起始和结尾。 信息类方法 这些方法提供关于整个树（或任何进化枝）的信息。\ncommon_ancestor\n查找所提供的所有目标的最近共同祖先（the most recent common ancestor） （这将是一个Clade对象）。如果没有提供任何目标，将返回当前Clade（调用该 方法的那个）的根；如果提供一个目标，将返回目标本身。然而，如果有任何提供 的目标无法在当前tree（或clade）中找到，将引起一个异常。\ncount_terminals\n计算树中末端（叶子）节点的个数。\ndepths\n创建一个树中进化枝到其深度的映射。结果是一个字典，其中键是树中所有的Clade 实例，值是从根到每个clade（包含末端）的距离。默认距离是到这个clade的分支 长度累加，然而使用 unit_branch_lengths=True 选项，将只计算分支的个数 （其在树中的级数）。\ndistance\n计算两个目标间的分支长度总和。如果只指定一个目标，另一个则为该树的根。\ntotal_branch_length\n计算这个树中的分支长度总和。这在系统发生学中通常就称为树的长度“length”， 但是我们使用更加明确的名称，以避免和Python的术语混淆。\n余下的方法是boolean检测方法：\nis_bifurcating\n如果树是严格的二叉树；即，所有的节点有2个或者0个子代（对应的，内部或外部）。 根节点可能有三个后代，然而仍然被认为是二叉树的一部分。\nis_monophyletic\n检验给定的所有目标是否组成一个完成的子进化枝——即，存在一个进化枝满足：它的 末端节点和给定的目标是相同的集合。目标需要时树中的末端节点。为方便起见，若 给定目标是一个单系（monophyletic），这个方法将返回它们的共同祖先（MCRA）（ 而不是 True ），否则将返回 False 。\nis_parent_of\n若目标是这个树的后代（descendant）则为True——不必为直接后代。检验一个进化枝的 直接后代，只需要用简单的列表成员检测方法： if subclade in clade: \u0026hellip;\nis_preterminal\n若所有的直接后代都为末端则为True；否则任何一个直接后代不为末端则为False。\n修改类的方法 这些方法都在原地对树进行修改，所以如果你想保持原来的树不变，你首先要使用Python的 copy 模块对树进行完整的拷贝\ntree = Phylo.read(\u0026#39;example.xml\u0026#39;, \u0026#39;phyloxml\u0026#39;) import copy newtree = copy.deepcopy(tree) collapse\n从树中删除目标，重新连接它的子代（children）到它的父亲节点（parent）。\ncollapse_all\n删除这个树的所有后代（descendants），只保留末端节点（terminals）。 分支长度被保留，即到每个末端节点的距离保持不变。如指定一个目标（见上）， 只坍塌（collapses）和指定匹配的内部节点。\nladderize\n根据末端节点的个数，在原地对进化枝（clades）进行排序。越深的进化枝默认被放到最后， 使用 reverse=True 将其放到最前。\nprune\n从树中修剪末端进化枝（terminal clade）。如果分类名（taxon）来自一个二叉枝（bifurcation）， 连接的节点将被坍塌，它的分支长度将被加到剩下的末端节点上。这可能不再是一个有意义的值。\nroot_with_outgroup\n使用包含给定目标的外群进化枝（outgroup clade）重新确定树的根节点，即外群的共同祖先。该方法 只在Tree对象中能用，不能用于Clade对象。\n如果外群和self.root一致，将不发生改变。如果外群进化枝是末端（即一个末端节点被作为外群），一个 新的二叉根进化枝将被创建，且到给定外群的分支长度为0。否则，外群根部的内部节点变为整个树的一个 三叉根。如果原先的根是一个二叉，它将被从树中遗弃。\n在所有的情况下，树的分支长度总和保持不变。\nroot_at_midpoint\n重新选择树中两个最远的节点的中点作为树的根。（这实际上是使用 root_with_outgroup 函数。）\nsplit\n产生 n （默认为2）个 新的后代。在一个物种树中，这是一个物种形成事件。新的进化枝拥有给定的 branch_length 以及和这个进化枝的根相同的名字，名字后面包含一个整数后缀（从0开始计数）—— 例如，分割名为“A”的进化枝将生成子进化枝“A0”和“A1”。\nphyloXML文件格式包含用来注释树的，采用额外数据格式和图像提示的字段。\n参加Biopython维基上的PhyloXML页面 (http://biopython.org/wiki/PhyloXML) 以查看关于使用PhyloXML提供的额外注释特性的描述和例子。\n运行外部程序 尽管Bio.Phylo本身不从序列比对推断进化树，但这里有一些第三方的程序可以使用。 他们通过 Bio.Phylo.Applications 模块获得支持，使用和 Bio.Emboss.Applications 、 Bio.Align.Applications 以及其他模块相同的通用框架\nBiopython 1.58引入了一个PhyML的打包程序（wrapper） (http://www.atgc-montpellier.fr/phyml/)。 该程序接受一个 phylip-relaxed 格式（它是Phylip格式，然而没有对分类名称的10个字符的限制） 的比对输入和多种参数。一个快速的例子是：\n\u0026gt;\u0026gt;\u0026gt; from Bio import Phylo \u0026gt;\u0026gt;\u0026gt; from Bio.Phylo.Applications import PhymlCommandline \u0026gt;\u0026gt;\u0026gt; cmd = PhymlCommandline(input=\u0026#39;Tests/Phylip/random.phy\u0026#39;) \u0026gt;\u0026gt;\u0026gt; out_log, err_log = cmd() 这生成一个树文件盒一个统计文件，名称为： [input filename]_phyml_tree.txt 和 [input filename]_phyml_stats.txt. 树文件的格式是Newick格式：\n\u0026gt;\u0026gt;\u0026gt; tree = Phylo.read(\u0026#39;Tests/Phylip/random.phy_phyml_tree.txt\u0026#39;, \u0026#39;newick\u0026#39;) \u0026gt;\u0026gt;\u0026gt; Phylo.draw_ascii(tree) PAML整合 Biopython 1.58引入了对PAML的支持 (http://abacus.gene.ucl.ac.uk/software/paml.html), 它是一个采用最大似然法（maximum likelihood）进行系统进化分析的程序包。目前，对程序codeml、baseml和yn00的支持 已经实现。由于PAML使用控制文件而不是命令行参数来控制运行时选项，这个打包程序（wrapper）的使用格式和Biopython 的其他应用打包程序有些差异。\n一个典型的流程是：初始化一个PAML对象，指定一个比对文件，一个树文件，一个输出文件和工作路径。下一步，运行时 选项通过 set_options() 方法或者读入一个已有的控制文件来设定。最后，程序通过 run() 方法来运行，输出文件 将自动被解析到一个结果目录。\n\u0026gt;\u0026gt;\u0026gt; from Bio.Phylo.PAML import codeml \u0026gt;\u0026gt;\u0026gt; cml = codeml.Codeml() \u0026gt;\u0026gt;\u0026gt; cml.alignment = \u0026#34;Tests/PAML/alignment.phylip\u0026#34; \u0026gt;\u0026gt;\u0026gt; cml.tree = \u0026#34;Tests/PAML/species.tree\u0026#34; \u0026gt;\u0026gt;\u0026gt; cml.out_file = \u0026#34;results.out\u0026#34; \u0026gt;\u0026gt;\u0026gt; cml.working_dir = \u0026#34;./scratch\u0026#34; \u0026gt;\u0026gt;\u0026gt; cml.set_options(seqtype=1, ... verbose=0, ... noisy=0, ... RateAncestor=0, ... model=0, ... NSsites=[0, 1, 2], ... CodonFreq=2, ... cleandata=1, ... fix_alpha=1, ... kappa=4.54006) \u0026gt;\u0026gt;\u0026gt; results = cml.run() \u0026gt;\u0026gt;\u0026gt; ns_sites = results.get(\u0026#34;NSsites\u0026#34;) \u0026gt;\u0026gt;\u0026gt; m0 = ns_sites.get(0) \u0026gt;\u0026gt;\u0026gt; m0_params = m0.get(\u0026#34;parameters\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print m0_params.get(\u0026#34;omega\u0026#34;) 已有的输出文件也可以通过模块的 read() 方法来解析：\n这个新模块的详细介绍目前在Biopython维基上可以看到： http://biopython.org/wiki/PAML\n\u0026gt;\u0026gt;\u0026gt; results = codeml.read(\u0026#34;Tests/PAML/Results/codeml/codeml_NSsites_all.out\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print results.get(\u0026#34;lnL max\u0026#34;) ","date":"2021-08-13T00:00:00Z","permalink":"https://example.com/p/ch13_bio.phylo%E7%B3%BB%E7%BB%9F%E5%8F%91%E8%82%B2%E5%88%86%E6%9E%90/","title":"ch13_Bio.Phylo系统发育分析"},{"content":"biopython官方地址：https://biopython.org/\ngithub地址：https://github.com/biopython/biopython/blob/master/\n中文版教程：https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html\nbiopython包的说明（具体到每个模块了）：https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html\nBio.PopGen是一个群体遗传学相关的模块，在Biopython 1.44及以后的版本中可用。\n该模块的中期目标是支持各种类型的数据格式、应用程序和数据库。目前，该模块正在紧张的开发中，并会快速实现对新特征的支持。这可能会带来一些不稳定的API，尤其是当你使用的是开发版。不过，我们正式公开发行的API应该更加稳定。\nGenePop GenePop（ http://genepop.curtin.edu.au/）是一款主流的群体遗传学软件包，支持Hardy-Weinberg检验、连锁不平衡、群体分化、基础统计计算、 FstFst 和迁移率估计等等。GenePop并不支持基于序列的统计计算，因为它并不能处理序列数据。GenePop文件格式广泛用于多种其它的群体遗传学应用软件，因此成为群体遗传学领域重要格式。\nBio.PopGen提供GenePop文件格式解析器和生成器，同时也提供操作记录内容的小工具。此处有个关于怎样读取GenePop文件的示例（你可以在Biopython的Test/PopGen文件夹下找到GenePop示例文件）：\nfrom Bio.PopGen import GenePop handle = open(\u0026#34;example.gen\u0026#34;) rec = GenePop.read(handle) handle.close() 它将读取名为example.gen的文件并解析。如果你输出rec，那么该记录将会以GenePop格式再次输出。\n在rec中最重要的信息是基因座名称和群体信息（当然不止这些，请使用help(GenePop.Record)获得API帮助文档）。基因座名称可以在rec.loci_list中找到，群体信息可以在rec.populations中找到。群体信息是一个列表，每个群体（population）作为其中一个元素。每个元素本身又是包含个体（individual）的列表，每个个体包含个体名和等位基因列表（每个marker两个元素），下面是一个rec.populations的示例：\n[ [ (\u0026#39;Ind1\u0026#39;, [(1, 2), (3, 3), (200, 201)], (\u0026#39;Ind2\u0026#39;, [(2, None), (3, 3), (None, None)], ], [ (\u0026#39;Other1\u0026#39;, [(1, 1), (4, 3), (200, 200)], ] ] 在上面的例子中，我们有两个群体，第一个群体包含两个个体，第二个群体只包含一个个体。第一个群体的第一个个体叫做Ind1，紧接着是3个基因座各自的等位基因信息。请注意，对于任何的基因座，信息可以缺失（如上述个体Ind2）。\n有几个可用的工具函数可以处理GenePop记录，如下例：\nfrom Bio.PopGen import GenePop #Imagine that you have loaded rec, as per the code snippet above... rec.remove_population(pos) #Removes a population from a record, pos is the population position in # rec.populations, remember that it starts on position 0. # rec is altered. rec.remove_locus_by_position(pos) #Removes a locus by its position, pos is the locus position in # rec.loci_list, remember that it starts on position 0. # rec is altered. rec.remove_locus_by_name(name) #Removes a locus by its name, name is the locus name as in # rec.loci_list. If the name doesn\u0026#39;t exist the function fails # silently. # rec is altered. rec_loci = rec.split_in_loci() #Splits a record in loci, that is, for each loci, it creates a new # record, with a single loci and all populations. # The result is returned in a dictionary, being each key the locus name. # The value is the GenePop record. # rec is not altered. rec_pops = rec.split_in_pops(pop_names) #Splits a record in populations, that is, for each population, it creates # a new record, with a single population and all loci. # The result is returned in a dictionary, being each key # the population name. As population names are not available in GenePop, # they are passed in array (pop_names). # The value of each dictionary entry is the GenePop record. # rec is not altered. GenePop不支持群体名，这种限制有时会很麻烦。Biopython对群体名的支持正在规划中，这些功能扩展仍会保持对标准格式的兼容性。同时，中期目标是对GenePop网络服务的支持.\n溯祖模拟 Coalescent simulation 溯祖模拟是一种对群体遗传学信息根据时间向后推算的模型（backward model）。对祖先的模拟是通过寻找到最近共同祖先（Most Recent Common Ancestor，MRCA）完成。从MRCA到目前这一代样本间的血统关系有时称为家系（genealogy）。简单的情况是假定群体大小固定，单倍型，无群体结构，然后模拟无选择压的单个基因座的等位基因。\n溯祖理论被广泛用于多种领域，如选择压力检测、真实群体的群体参数估计以及疾病基因图谱。\nBiopython对溯祖的实现不是去创建一个新的内置模拟器，而是利用现有的SIMCOAL2（ http://cmpg.unibe.ch/software/simcoal2/ ）。与其他相比，SIMCOAL2允许存在群体结构，多群体事件，多种类型的可发生重组的基因座（SNPs，序列，STRs/微卫星和RFLPs），具有多染色体的二倍体和测量偏倚（ascertainment bias）。注意，SIMCOAL2并不支持所有的选择模型。建议阅读上述链接中的SIMCOAL2帮助文档。\nSIMCOAL2的输入是一个指定所需的群体和基因组的文件，输出是一系列文件（通常在1000个左右），它们包括每个亚群（subpopulation）中模拟的个体基因组。这些文件有多种途径，如计算某些统计数据（e.g. FstFst 或Tajima D）的置信区间以得到可信的范围。然后将真实的群体遗传数据统计结果与这些置信区间相比较。\nBiopython溯祖模拟可以创建群体场景（demographic scenarios）和基因组，然后运行SIMCOAL2。\n创建场景 scenario 创建场景包括创建群体及其染色体结构。多数情况下（如计算近似贝斯估计量（Approximate Bayesian Computations – ABC）），测试不同参数的变化很重要（如不同的有效群体大小 NeNe ， 从10，50，500到1000个体）。提供的代码可以很容易地模拟具有不同群体参数的场景。\n下面我们将学习怎样创建场景，然后是怎样进行模拟。\n群体 有一些内置的预定义群体，均包含两个共同的参数：同群种（deme）的样本大小（在模板中称为 sample_size ，其使用请见下文）和同群种大小，如亚群大小（pop_size）。所有的群体都可以作为模板，所有的参数也可以变化，每个模板都有自己的系统名称。这些预定义的群体/模板（template）包括：\nSingle population, constant size\n单一种群固定群体大小。标准的参数即可满足它，模板名称：simple.\nSingle population, bottleneck\n单一群体瓶颈效应，如图 所示。参数有当前种群大小（图中ne3模板的pop_size）、种群扩张时间 - 扩张发生后代数（expand_gen），瓶颈发生时的有效群体大小（ne2），种群收缩时间（contract_gen）以及原始种群大小（ne3）。模板名：bottle。\nIsland model\n典型的岛屿模型。同群种（deme）总数表示为total_demes，迁移率表示为mig。模板名：island。\nStepping stone model - 1 dimension\n一维脚踏石模型（Stepping stone model），极端状态下种群分布不连续。同群种（deme）总数表示为total_demes，迁移率表示为mig。模板名：ssm_1d。\nStepping stone model - 2 dimensions\n二维脚踏石模型，极端状态下种群分布不连续。参数有表示水平维度的x和表示垂直维度的y（同群种总数即为x × y），以及表示迁移率的mig。模板名：ssm_2d。\n在我们的第一个示例中，将生成一个单一种群固定群体大小（Single population, constant size）模板，样本大小（sample size）为30，同群种大小（deme size）为500。代码如下：\nfrom Bio.PopGen.SimCoal.Template import generate_simcoal_from_template generate_simcoal_from_template(\u0026#39;simple\u0026#39;, [(1, [(\u0026#39;SNP\u0026#39;, [24, 0.0005, 0.0])])], [(\u0026#39;sample_size\u0026#39;, [30]), (\u0026#39;pop_size\u0026#39;, [100])]) 执行该段代码将会在当前目录生成一个名为simple_100_300.par的文件，该文件可作为SIMCOAL2的输入文件，用于模拟群体（下面将会展示Biopython是如何调用SIMCOAL2）。\n这段代码仅包含一个函数的调用，让我们一个参数一个参数地讨论。\n第一个参数是模板id（从上面的模板列表中选择）。我们使用 ’simple’，表示的是单一群体固定种群大小模板。\n第二个参数是染色体结构，将在下一节详细阐述。\n第三个参数是所有需要的参数列表及其所有可能的值（此列中所有的参数都只含有一个可能值）。\n现在让我们看看岛屿模型示例。我们希望生成几个岛屿模型，并对不同大小的同群种感兴趣：10、50和100，迁移率为1%。样本大小和同群种大小与上一个示例一致，代码如下：\nfrom Bio.PopGen.SimCoal.Template import generate_simcoal_from_template generate_simcoal_from_template(\u0026#39;island\u0026#39;, [(1, [(\u0026#39;SNP\u0026#39;, [24, 0.0005, 0.0])])], [(\u0026#39;sample_size\u0026#39;, [30]), (\u0026#39;pop_size\u0026#39;, [100]), (\u0026#39;mig\u0026#39;, [0.01]), (\u0026#39;total_demes\u0026#39;, [10, 50, 100])]) 此例将会生成3个文件：island_100_0.01_100_30.par，island_10_0.01_100_30.par 和 island_50_0.01_100_30.par。注意，生成文件名的规律是：模板名，然后是参数值逆序排列。\n染色体结构 我们强烈建议你阅读SIMCOAL2文档，以完整理解染色体结构建模的各种使用。在本小节，我们只讨论如何使用Biopython接口实现指定的染色体结构，不会涉及SIMCOAL2可实现哪些染色体结构。\n我们首先实现一条染色体，包含24个SNPs，每个相邻基因座的重组率为0.0005，次等位基因的最小频率为0。这些由以下列表指定（作为第二个参数传递给generate_simcoal_from_template函数）\n[(1, [(\u0026#39;SNP\u0026#39;, [24, 0.0005, 0.0])])] 这实际上是上一个示例使用的染色体结构。\n染色体结构表示为一个包含所有染色体的列表，每条染色体（即列表中的每个元素）由一个元组（tuple）组成，元组包括一对元素组成。元组的第一个元素是染色体被重复的次数（因为有可能需要多次重复同一条染色体）。元组的第二个元素是一个表示该染色体的实际组成的列表，每个列表元素又包括一对元素，第一个是基因座类型，第二个是该基因座的参数列表。是否有点混淆了呢？在我们展示示例之前，先让我们回顾下上一个示例：我们有一个列表（表示一条染色体），该染色体只有一个实例（因此不会被重复），它由24个SNPs组成，每个相邻SNP间的重组率为0.0005，次等位基因的最小频率为0.0（即它可以在某些染色体中缺失）。\n现在让我们看看更复杂的示例：\n[ (5, [ (\u0026#39;SNP\u0026#39;, [24, 0.0005, 0.0]) ] ), (2, [ (\u0026#39;DNA\u0026#39;, [10, 0.0, 0.00005, 0.33]), (\u0026#39;RFLP\u0026#39;, [1, 0.0, 0.0001]), (\u0026#39;MICROSAT\u0026#39;, [1, 0.0, 0.001, 0.0, 0.0]) ] ) ] 首先，我们有5条与上一示例具有相同结构组成的染色体（即24SNPs）。然后是2条这样的染色体：包含一段具有重组率为0.0、突变率为0.0005及置换率为0.33的10个核苷酸长度的DNA序列，一段具有重组率为0.0、突变率为0.0001的RFLP，一段具有重组率为0.0、突变率为0.001、几何参数为0.0、范围限制参数为0.0的微卫星（microsatellite，STR）序列（注意，因为这是单个微卫星，接下来没有基因座，因此这里的重组率没有任何影响，更多关于这些参数的信息请查阅SIMCOAL2文档，你可以使用它们模拟各种突变模型，包括典型的微卫星渐变突变模型）。\n运行SIMCOAL2 现在我们讨论如何从Biopython内部运行SIMCOAL2。这需要SIMCOAL2的可执行二进制文件名为simcoal2（在Windows平台下为simcoal2.exe），请注意，从官网下载的程序命名格式通常为simcoal2_x_y。因此，当安装SIMCOAL2时，需要重命名可执行文件，这样Biopython才能正确调用。\nSIMCOAL2可以处理不是使用上诉方法生成的文件（如手动配置的参数文件），但是我们将使用上述方法得到的文件创建模型\nfrom Bio.PopGen.SimCoal.Template import generate_simcoal_from_template from Bio.PopGen.SimCoal.Controller import SimCoalController generate_simcoal_from_template(\u0026#39;simple\u0026#39;, [ (5, [ (\u0026#39;SNP\u0026#39;, [24, 0.0005, 0.0]) ] ), (2, [ (\u0026#39;DNA\u0026#39;, [10, 0.0, 0.00005, 0.33]), (\u0026#39;RFLP\u0026#39;, [1, 0.0, 0.0001]), (\u0026#39;MICROSAT\u0026#39;, [1, 0.0, 0.001, 0.0, 0.0]) ] ) ], [(\u0026#39;sample_size\u0026#39;, [30]), (\u0026#39;pop_size\u0026#39;, [100])]) ctrl = SimCoalController(\u0026#39;.\u0026#39;) ctrl.run_simcoal(\u0026#39;simple_100_30.par\u0026#39;, 50) 需要注意的是最后两行（以及新增的import行）。首先是创建一个应用程序控制器对象，需要指定二进制可执行文件所在路径。\n模拟器在最后一行运行：从上述阐述的规律可知，文件名为simple_100_30.par的输入文件是我们创建的模拟参数文件，然后我们指定了希望运行50次独立模拟。默认情况下，Biopython模拟二倍体数据，但是可以添加第三个参数用于模拟单倍体数据（字符串’0’）。然后，SIMCOAL2将会执行（这需要运行很长时间），并创建一个包含模拟结果的文件夹，结果文件可便可用于分析（尤其是研究Arlequin3数据）。在未来的Biopython版本中，可能会支持Arlequin3格式文件的读取，从而在Biopython中便能分析SIMCOAL2结果。\n其他应用程序 这里我们讨论一些处理其它的群体遗传学中应用程序的接口和小工具，这些应用程序具有争议，使用得较少。\nFDist：检测选择压力和分子适应 FDist是一个选择压力检测的应用程序包，基于通过 FstFst 和杂合度计算（即模拟）得到的“中性”（“neutral”）置信区间。“中性”置信区间外的Markers（可以是SNPs，微卫星，AFLPs等等）可以被认为是候选的受选择marker。\nFDist主要运用在当marker数量足够用于估计平均 FstFst ，而不足以从数据集中计算出离群点 - 直接地或者在知道大多数marker在基因组中的相对位置的情况下使用基于如Extended Haplotype Heterozygosity （EHH）的方法\n典型的FDist的使用如下：\n从其它格式读取数据为FDist格式； 计算平均 FstFst ，由FDist的datacal完成； 根据平均 FstFst 和期望的总群体数模拟“中性”markers，这是核心部分，由FDist的fdist完成； 根据指定的置信范围（通常是95%或者是99%）计算置信区间，由cplot完成，主要用于对区间作图； 用模拟的“中性”置信区间评估每个Marker的状态，由pv完成，用于检测每个marker与模拟的相比的离群状态； 我们将以示例代码讨论每一步（FDist可执行二进制文件需要在PATH环境变量中）。\nFDist数据格式是该应用程序特有的，不被其它应用程序使用。因此你需要转化你的数据格式到FDist可使用的格式。Biopython可以帮助你完成这个过程。这里有一个将GenePop格式转换为FDist格式的示例（同时包括后面示例将用到的import语句） from Bio.PopGen import GenePop from Bio.PopGen import FDist from Bio.PopGen.FDist import Controller from Bio.PopGen.FDist.Utils import convert_genepop_to_fdist gp_rec = GenePop.read(open(\u0026#34;example.gen\u0026#34;)) fd_rec = convert_genepop_to_fdist(gp_rec) in_file = open(\u0026#34;infile\u0026#34;, \u0026#34;w\u0026#34;) in_file.write(str(fd_rec)) in_file.close() 在该段代码中，我们解析GenePop文件并转化为FDist记录（record）。\n输出FDist记录将得到可以直接保存到可用于FDist的文件的字符串。FDist需要输入文件名为infile，因此我们将记录保存到文件名为infile的文件。\nFDist记录最重要的字段（field）是：num_pops，群体数量；num_loci，基因座数量和loci_data，marker数据。记录的许多信息对用户来说可能没有用处，仅用于传递给FDist。\n下一步是计算平均数据集的 FstFst （以及样本大小）：\nctrl = Controller.FDistController() fst, samp_size = ctrl.run_datacal() 第一行我们创建了一个控制调用FDist软件包的对象，该对象被用于调用该包的其它应用程序。\n第二行我们调用datacal应用程序，它用于计算 FstFst 和样本大小。值得注意的是，用datacal计算得到的 FstFst 是 Weir-Cockerham θ的“变种”（ variation ）。\n现在我们可以调用主程序fdist模拟中性Markers。\nsim_fst = ctrl.run_fdist(npops = 15, nsamples = fd_rec.num_pops, fst = fst, sample_size = samp_size, mut = 0, num_sims = 40000) npops\n现存自然群体数量，完全是一个“瞎猜值”（“guestimate”），必须小于100。\nnsamples\n抽样群体数量，需要小于npops。\nfst\n平均 FstFst 。\nsample_size\n每个群体抽样个体平均数\nmut\n突变模型：0 - 无限等位基因突变模型；1 - 渐变突变模型\nnum_sims\n执行模拟的次数。通常，40000左右的数值即可，但是如果得到的执行区间范围比较大（可以通过下面的置信区间作图检测到），可以上调此值（建议每次调整10000次模拟）。\n样本数量和样本大小措辞上的混乱源于原始的应用程序。\n将会得到一个名为out.dat的文件，它包含模拟的杂合度和 FstFst 值，行数与模拟的次数相同。\n注意，fdist返回它可以模拟的平均 FstFst ，关于此问题更多的细节，请阅读下面的“估计期望的平均 FstFst ”\n下一步（可选步骤）是计算置信区间：\ncpl_interval = ctrl.run_cplot(ci=0.99) 只能在运行fdist之后才能调用cplot。\n这将计算先前fdist结果的置信区间（此例中为99%）。第一个元素是杂合度，第二个是该杂合度的 FstFst 置信下限，第三个是 FstFst 平均值，第四个是置信上限。可以用于记录置信区间等高线。该列表也可以输出到out.cpl文件。\n这步的主要目的是返回一系列的点用于对置信区间作图。如果只是需要根据模拟结果对每个marker的状态进行评估，可以跳过此步。\npy_data = ctrl.run_pv() 只能在运行fdist之后才能调用pv。\n这将使用模拟marker对每个个体真实的marker进行评估，并返回一个列表，顺序与FDist记录中loci_list一致（loci_list又与GenePop顺序一致）。列表中每个元素包含四个元素，其中最重要的是最后一个元素（关于其他的元素，为了简单起见，我们不在这里讨论，请见pv帮助文档），它返回模拟的 FstFst 低于 marker FstFst 的概率。较大值说明极有可能是正选择（positive selection）marker，较小值表明可能是平衡选择（balancing selection）marker，中间值则可能是中性marker。怎样的值是“较大值”、“较小值”或者“中间值”是一个很主观的问题，但当使用置信区间方法及95%的置信区间时，“较大值”在0.95 - 1.00之间，“较小值”在0.00 - 0.05之间，“中间值”在0.05 - 0.05之间。\n估计期望的平均Fst FDist通过对由下例公式得到的迁移率进行溯祖模拟估计期望的平均 Fst：\n在实践中，当群体数量比较小，突变模型为渐进突变模型，样本大小增加，fdist将不能模拟得到可接受的近似平均 FstFst 。\n为了解决这个问题，Biopython提供了一个使用迭代方法的函数，通过依次运行几个fdist得到期望的值。该方法比运行单个fdist相比耗费更多计算资源，但是可以得到更好的结果。以下代码运行fdist得到期望的 FstFst\nsim_fst = ctrl.run_fdist_force_fst(npops = 15, nsamples = fd_rec.num_pops, fst = fst, sample_size = samp_size, mut = 0, num_sims = 40000, limit = 0.05) 与run_fdist相比，唯一一个新的可选参数是limit，它表示期望的最大错误率。run_fdist可以（或许应该）由run_fdist_force_fst替代。\n计算平均 FstFst 的过程可能比这里呈现的要复杂得多。更多的信息请查阅FDist README文件。同时，Biopython的代码也可用于实现更复杂的过程。\n尽管如此，已经完成的功能模块正在逐步加入到Bio.PopGen，这些代码覆盖了程序FDist和SimCoal2，HapMap和UCSC Table Browser数据库，以及一些简单的统计计算，如 FstFst ， 或等位基因数。\n","date":"2021-08-12T00:00:00Z","permalink":"https://example.com/p/ch12_bio.popgen_%E7%BE%A4%E4%BD%93%E9%81%97%E4%BC%A0%E5%AD%A6/","title":"ch12_Bio.PopGen_群体遗传学"},{"content":"biopython官方地址：https://biopython.org/\ngithub地址：https://github.com/biopython/biopython/blob/master/\n中文版教程：https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html\nbiopython包的说明（具体到每个模块了）：https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html\nSwiss-Prot ( http://www.expasy.org/sprot )是一个 蛋白质序列数据库。 Biopython能够解析纯文本的Swiss-Prot文件, 这种格式也被Swiss-Prot、TrEMBL和PIRPSD的UniProt数据库使用。然而我们并 不支持UniProKB的XML格式文件。\n解析Swiss-Prot和ExPASy 你可以将 Swiss-Prot记录存到 Bio.SwissProt.Record 对象, 这实际上存储了Swiss-Prot记录 中所包含的的全部信息。在这部分我们将介绍怎样从一个Swiss-Prot文件中提 取 Bio.SwissProt.Record 对象。\n获取Swiss-Prot文件记录的方式 为了解析Swiss-Prot记录，我们首先需要得到一个Swiss-Prot记录文件。根据该Swiss-Prot 记录的储存位置和储存方式，获取该记录文件的方式也有所不同：\n#本地打开Swiss-Prot文件： \u0026gt;\u0026gt;\u0026gt; handle = open(\u0026#34;myswissprotfile.dat\u0026#34;) #打开使用gzip压缩的Swiss-Prot文件： \u0026gt;\u0026gt;\u0026gt; import gzip \u0026gt;\u0026gt;\u0026gt; handle = gzip.open(\u0026#34;myswissprotfile.dat.gz\u0026#34;) #在线打开Swiss-Prot文件： \u0026gt;\u0026gt;\u0026gt; import urllib \u0026gt;\u0026gt;\u0026gt; handle = urllib.urlopen(\u0026#34;http://www.somelocation.org/data/someswissprotfile.dat\u0026#34;) #从ExPASy数据库在线打开Swiss-Prot文件 \u0026gt;\u0026gt;\u0026gt; from Bio import ExPASy \u0026gt;\u0026gt;\u0026gt; handle = ExPASy.get_sprot_raw(myaccessionnumber) 读取Swiss-Prot文件 通过 Bio.SeqIO 来获取格式未知的 SeqRecord 对象。此外，我们也可以 用 Bio.SwissProt 来获取更加匹配基本文件格式的 Bio.SwissProt.Record 对象。\n我们使用 read() 函数来从文件中读取一个Swiss-Prot记录：\n\u0026gt;\u0026gt;\u0026gt; from Bio import SwissProt \u0026gt;\u0026gt;\u0026gt; record = SwissProt.read(handle) 该函数只适用于仅存储了一个Swiss-Prot记录的文件，而当文件中没有或存在 多个记录时使用该函数，会出现 ValueError 提示。\n现在我们可以输出一些与这些记录相关的信息：\n\u0026gt;\u0026gt;\u0026gt; print record.description \u0026#39;RecName: Full=Chalcone synthase 3; EC=2.3.1.74; AltName: Full=Naringenin-chalcone synthase 3;\u0026#39; \u0026gt;\u0026gt;\u0026gt; for ref in record.references: ... print \u0026#34;authors:\u0026#34;, ref.authors ... print \u0026#34;title:\u0026#34;, ref.title ... authors: Liew C.F., Lim S.H., Loh C.S., Goh C.J.; title: \u0026#34;Molecular cloning and sequence analysis of chalcone synthase cDNAs of Bromheadia finlaysoniana.\u0026#34;; \u0026gt;\u0026gt;\u0026gt; print record.organism_classification [\u0026#39;Eukaryota\u0026#39;, \u0026#39;Viridiplantae\u0026#39;, \u0026#39;Streptophyta\u0026#39;, \u0026#39;Embryophyta\u0026#39;, ..., \u0026#39;Bromheadia\u0026#39;] 为了解析包含多个Swiss-Prot记录的文件，我们使用 parse 函数。这个函数能够让我们对 文件中的记录进行循环迭代操作。\n比如，我们要解析整个Swiss-Prot数据库并且收集所有的描述。你可以从 ExPAYs FTP site 下载这些gzip压缩文件 uniprot_sprot.dat.gz (大约 300MB)。文件中含有 uniprot_sprot.dat 一个文件(至少1.5GB)。\n如同这一部分刚开始所描述的，你可以按照如下所示的方法使用python 的 gzip 模块打开并解压 .gz 文件:\n\u0026gt;\u0026gt;\u0026gt; import gzip \u0026gt;\u0026gt;\u0026gt; handle = gzip.open(\u0026#34;uniprot_sprot.dat.gz\u0026#34;) 然而，解压一个大文件比较耗时，而且每次用这种方式打开一个 文件都是比较慢的。所以，如果你有空闲的硬盘空间并且在 最开始就在硬盘里通过解压到来得到 uniprot_sprot.dat ，这样能够在以后就可以像平常那样来打开文件：\n\u0026gt;\u0026gt;\u0026gt; handle = open(\u0026#34;uniprot_sprot.dat\u0026#34;) 从Swiss-Prot记录中提取任何你想要的信息也同样简单。比如你想看看一个 Swiss-Prot记录中的成员，就输入：\n\u0026gt;\u0026gt;\u0026gt; dir(record) [\u0026#39;__ doc__ \u0026#39;, \u0026#39;__ init__ \u0026#39;, \u0026#39;__ module__ \u0026#39;, \u0026#39;accessions\u0026#39;, \u0026#39;annotation_update\u0026#39;, \u0026#39;comments\u0026#39;, \u0026#39;created\u0026#39;, \u0026#39;cross_references\u0026#39;, \u0026#39;data_class\u0026#39;, \u0026#39;description\u0026#39;, \u0026#39;entry_name\u0026#39;, \u0026#39;features\u0026#39;, \u0026#39;gene_name\u0026#39;, \u0026#39;host_organism\u0026#39;, \u0026#39;keywords\u0026#39;, \u0026#39;molecule_type\u0026#39;, \u0026#39;organelle\u0026#39;, \u0026#39;organism\u0026#39;, \u0026#39;organism_classification\u0026#39;, \u0026#39;references\u0026#39;, \u0026#39;seqinfo\u0026#39;, \u0026#39;sequence\u0026#39;, \u0026#39;sequence_length\u0026#39;, \u0026#39;sequence_update\u0026#39;, \u0026#39;taxonomy_id\u0026#39;] 解析Swiss-Prot关键词和分类列表 这个章我跳过了,主要是Swiss-Prot可能不怎么会用 ","date":"2021-08-11T00:00:00Z","permalink":"https://example.com/p/ch10_swiss-prot_expasy/","title":"Ch10_Swiss-Prot_ExPASy"},{"content":"biopython官方地址：https://biopython.org/\ngithub地址：https://github.com/biopython/biopython/blob/master/\n中文版教程：https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html\nbiopython包的说明（具体到每个模块了）：https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html\nBio.PDB是Biopython中处理生物大分子晶体结构的模块。除了别的类之外，Bio.PDB包含PDBParser类，此类能够产生一个Structure对象，以一种较方便的方式获取文件中的原子数据。只是在处理PDB文件头所包含的信息时，该类有一定的局限性。\n晶体结果文件的读与写 读取PDB文件 首先，我们创建一个 PDBParser 对象：\n\u0026gt;\u0026gt;\u0026gt; from Bio.PDB.PDBParser import PDBParser \u0026gt;\u0026gt;\u0026gt; p = PDBParser(PERMISSIVE=1) PERMISSIV 标签表示一些与PDB文件相关的问题会被忽略（注意某些原子和/或残基会丢失）。如果没有这个标签，则会在解析器运行期间有问题被检测到的时候生成一个 PDBConstructionException 标签。\n接着通过 PDBParser 解析PDB文件，就产生了Structure对象（在此例子中，PDB文件为’pdb1fat.ent’，’1fat’是用户定义的结构名称）:\n\u0026gt;\u0026gt;\u0026gt; structure_id = \u0026#34;1fat\u0026#34; \u0026gt;\u0026gt;\u0026gt; filename = \u0026#34;pdb1fat.ent\u0026#34; \u0026gt;\u0026gt;\u0026gt; s = p.get_structure(structure_id, filename) 你可以从PDBParser对象中用 get_header 和 get_trailer 方法来提取PDB文件中的文件头和文件尾（简单的字符串列表）。然而许多PDB文件头包含不完整或错误的信息。许多错误在等价的mmCIF格式文件中得到修正。 因此，如果你对文件头信息感兴趣，可以用下面即将讲到的 MMCIF2Dict 来提取信息，而不用处理PDB文件文件头。\n现在澄清了，让我们回到解析PDB文件头这件事上。结构对象有个属性叫 header ，这是一个将头记录映射到其相应值的Python字典。\n例子：\n\u0026gt;\u0026gt;\u0026gt; resolution = structure.header[\u0026#39;resolution\u0026#39;] \u0026gt;\u0026gt;\u0026gt; keywords = structure.header[\u0026#39;keywords\u0026#39;] 在这个字典中可用的关键字有 name 、 head 、 deposition_date 、 release_date 、 structure_method 、 resolution 、 structure_reference （映射到一个参考文献列表）、 journal_reference 、 author 、和 compound （映射到一个字典，其中包含结晶化合物的各种信息）。\n没有创建 Structure 对象的时候，也可以创建这个字典，比如直接从PDB文件创建:\n\u0026gt;\u0026gt;\u0026gt; file = open(filename,\u0026#39;r\u0026#39;) \u0026gt;\u0026gt;\u0026gt; header_dict = parse_pdb_header(file) \u0026gt;\u0026gt;\u0026gt; file.close() 读取mmCIF文件 与PDB文件的情形类似，先创建一个 MMCIFParser 对象：\n\u0026gt;\u0026gt;\u0026gt; from Bio.PDB.MMCIFParser import MMCIFParser \u0026gt;\u0026gt;\u0026gt; parser = MMCIFParser() 然后用这个解析器从mmCIF文件创建一个结构对象：\n\u0026gt;\u0026gt;\u0026gt; structure = parser.get_structure(\u0026#39;1fat\u0026#39;, \u0026#39;1fat.cif\u0026#39;) 为了尽量少访问mmCIF文件，可以用 MMCIF2Dict 类创建一个Python字典来将所有mmCIF文件中各种标签映射到其对应的值上。若有多个值（像 _atom_site.Cartn_y 标签，储存的是所有原子的_y_坐标值），则这个标签映射到一个值列表。从mmCIF文件创建字典如下：\n\u0026gt;\u0026gt;\u0026gt; from Bio.PDB.MMCIF2Dict import MMCIF2Dict \u0026gt;\u0026gt;\u0026gt; mmcif_dict = MMCIF2Dict(\u0026#39;1FAT.cif\u0026#39;) 例：从mmCIF文件获取溶剂含量:\n\u0026gt;\u0026gt;\u0026gt; sc = mmcif_dict[\u0026#39;_exptl_crystal.density_percent_sol\u0026#39;] 例：获取包含所有原子_y_坐标的列表:\n\u0026gt;\u0026gt;\u0026gt; y_list = mmcif_dict[\u0026#39;_atom_site.Cartn_y\u0026#39;] 写PDB文件 可以用PDBIO类实现。当然也可很方便地输出一个结构的特定部分\n例子：保存一个结构\n\u0026gt;\u0026gt;\u0026gt; io = PDBIO() \u0026gt;\u0026gt;\u0026gt; io.set_structure(s) \u0026gt;\u0026gt;\u0026gt; io.save(\u0026#39;out.pdb\u0026#39;) 如果你想写出结构的一部分，可以用 Select 类（也在 PDBIO 中）来实现。 Select 有如下四种方法：\naccept_model(model) accept_chain(chain) accept_residue(residue) accept_atom(atom)\u0026hellip;.01 在默认情况下，每种方法的返回值都为1（表示model/chain/residue/atom被包含在输出结果中）。通过子类化 Select 和返回值0，你可以从输出中排除model、chain等。也许麻烦，但很强大。接下来的代码将只输出甘氨酸残基：0 \u0026gt;\u0026gt;\u0026gt; class GlySelect(Select): ... def accept_residue(self, residue): ... if residue.get_name()==\u0026#39;GLY\u0026#39;: ... return True ... else: ... return False ... \u0026gt;\u0026gt;\u0026gt; io = PDBIO() \u0026gt;\u0026gt;\u0026gt; io.set_structure(s) \u0026gt;\u0026gt;\u0026gt; io.save(\u0026#39;gly_only.pdb\u0026#39;, GlySelect()) 如果这部分对你来说太复杂，那么 Dice 模块有一个很方便的 extract 函数，它可以输出一条链中起始和终止氨基酸残基之间的所有氨基酸残基。\n结构的表示 一个 Structure 对象的整体布局遵循称为SMCRA（Structure/Model/Chain/Residue/Atom，结构/模型/链/残基/原子）的体系架构：\n结构由模型组成 模型由多条链组成 链由残基组成 多个原子构成残基 这是很多结构生物学家/生物信息学家看待结构的方法，也是处理结构的一种简单而有效的方法。在需要的时候加上额外的材料。一个 Structure 对象的UML图（暂时忘掉 Disordered 吧）如下图所示 。这样的数据结构不一定最适用于表示一个结构的生物大分子内容，但要很好地解释一个描述结构的文件中所呈现的数据（最典型的如PDB或MMCIF文件），这样的数据结构就是必要的了。如果这种层次结构不能表示一个结构文件的内容，那么可以相当确定是这个文件有错误或至少描述结构不够明确。一旦不能生成SMCRA数据结构，就有理由怀疑出了故障。因此，解析PDB文件可用于检测可能的故障。我们将在 小节给出关于这一点的一些例子。\n结构，模型，链，残基都是实体基类的子类。原子类仅仅（部分）实现了实体接口（因为原子类没有子类）。\n对于每个实体子类，你可以用该子类的一个唯一标识符作为键来提取子类（比如，可以用原子名称作为键从残基对象中提取一个原子对象；用链的标识符作为键从域对象中提取链）。\n紊乱原子和残基用DisorderedAtom和DisorderedResidue类来表示，二者都是DisorderedEntityWrapper基类的子类。它们隐藏了紊乱的复杂性，表现得与原子和残基对象无二。\n一般地，一个实体子类（即原子，残基，链，模型）能通过标识符作为键来从父类（分别为残基，链，模型，结构）中提取。 \u0026gt;\u0026gt;\u0026gt; child_entity = parent_entity[child_id] 你可以从一个父实体对象获得所有子实体的列表。需要注意的是，这个列表以一种特定的方式排列（例如根据在模型对象中链对象的链标识符来排序）。\n\u0026gt;\u0026gt;\u0026gt; child_list = parent_entity.get_list() 你也可以从子类得到父类：\n\u0026gt;\u0026gt;\u0026gt; parent_entity = child_entity.get_parent() 在SMCRA的所有层次水平，你还可以提取一个 完整id 。完整id是包含所有从顶层对象（结构）到当前对象的id的一个元组。一个残基对象的完整id可以这么得到：\n\u0026gt;\u0026gt;\u0026gt; full_id = residue.get_full_id() \u0026gt;\u0026gt;\u0026gt; print full_id (\u0026#34;1abc\u0026#34;, 0, \u0026#34;A\u0026#34;, (\u0026#34;\u0026#34;, 10, \u0026#34;A\u0026#34;)) 这对应于：\nid为”1abc”的结构 id为0的模型 id为”A”的链 id为(” “, 10, “A”)的残基 这个残基id表示该残基不是异质残基（也不是水分子），因为其异质值为空；而序列标识符为10，插入码为”A”。\n要得到实体的id，用 get_id 方法即可： \u0026gt;\u0026gt;\u0026gt; entity.get_id() 可以用 has_id 方法来检查这个实体是否有子类具有给定id：\n\u0026gt;\u0026gt;\u0026gt; entity.has_id(entity_id) 实体的长度等于其子类的个数：\n\u0026gt;\u0026gt;\u0026gt; nr_children = len(entity) 对于从父实体得到的子实体，可以删除，重命名，添加等等，但这并不包含任何完整性检查（比如，有可能添加两个相同id的残基到同一条链上）。这就真的需要包含完整性检查的装饰类（Decorator）来完成了，但是如果你想使用原始接口的话可以查看源代码（Entity.py)。\n结构 结构对象是层次中的最高层。其id是用户指定的一个字符串。结构包含一系列子模型。大部分晶体结构（但不是全部）含有一个单一模型，但是NMR结构通常由若干模型构成。晶体结构中大部分子的乱序也能导致多个模型。\n模型 结构域对象的id是一个整数，源自该模型在所解析文件中的位置（自动从0开始）。晶体结构通常只有一个模型（id为0），而NMR文件通常含有多个模型。然而许多PDB解析器都假定只有一个结构域， Bio.PDB 中的 Structure 类就设计成能轻松处理含有不止一个模型的PDB文件。\n举个例子，从一个结构对象中获取其第一个模型：\n\u0026gt;\u0026gt;\u0026gt; first_model = structure[0] 模型对象存储着子链的列表。\n链 链对象的id来自PDB/mmCIF文件中的链标识符，是个单字符（通常是一个字母）。模型中的每个链都具有唯一的id。例如，从一个模型对象中取出标识符为“A”的链对象：\n\u0026gt;\u0026gt;\u0026gt; chain_A = model[\u0026#34;A\u0026#34;] 链对象储存着残基对象的列表\n残基 一个残基id是一个三元组：\n异质域 (hetfield)，即：采用这种体制的理由在 11.4.1 部分有叙述。 \u0026lsquo;W\u0026rsquo; 代表水分子 \u0026lsquo;H_\u0026rsquo; 后面紧跟残基名称，代表其它异质残基（例如 \u0026lsquo;H_GLC\u0026rsquo; 表示一个葡萄糖分子） 空值表示标准的氨基酸和核酸 序列标识符 （resseq），一个描述该残基在链上的位置的整数（如100）； 插入码 （icode），一个字符串，如“A”。插入码有时用来保存某种特定的、想要的残基编号体制。一个Ser 80的插入突变（比如在Thr 80和Asn 81残基间插入）可能具有如下序列标识符和插入码：Thr 80 A, Ser 80 B, Asn 81。这样一来，残基编号体制保持与野生型结构一致。 因此，上述的葡萄酸残基id就是 (’H_GLC’, 100, ’A’) 。如果异质标签和插入码为空，那么可以只使用序列标识符： # Full id \u0026gt;\u0026gt;\u0026gt; residue=chain[(\u0026#39; \u0026#39;, 100, \u0026#39; \u0026#39;)] # Shortcut id \u0026gt;\u0026gt;\u0026gt; residue=chain[100] 异质标签的起因是许许多多的PDB文件使用相同的序列标识符表示一个氨基酸和一个异质残基或一个水分子，这会产生一个很明显的问题，如果不使用异质标签的话。\n毫不奇怪，一个残基对象存储着一个子原子集，它还包含一个表示残基名称的字符串（如 “ASN”）和残基的片段标识符（这对X-PLOR的用户来说很熟悉，但是在SMCRA数据结构的构建中没用到）。\n让我们来看一些例子。插入码为空的Asn 10具有残基id (’ ’, 10, ’ ’) ；Water 10，残基id (’W’, 10, ’ ’)；一个序列标识符为10的葡萄糖分子（名称为GLC的异质残基），残基id为 (’H_GLC’, 10, ’ ’) 。在这种情况下，三个残基（具有相同插入码和序列标识符）可以位于同一条链上，因为它们的残基id是不同的。\n大多数情况下，hetflag和插入码均为空，如 (’ ’, 10, ’ ’) 。在这些情况下，序列标识符可以用作完整id的快捷方式：\n# use full id \u0026gt;\u0026gt;\u0026gt; res10 = chain[(\u0026#39; \u0026#39;, 10, \u0026#39; \u0026#39;)] # use shortcut \u0026gt;\u0026gt;\u0026gt; res10 = chain[10] 一个链对象中每个残基对象都应该具有唯一的id。但是对含紊乱原子的残基，要以一种特殊的方式来处理\n一个残基对象还有大量其它方法：\n\u0026gt;\u0026gt;\u0026gt; residue.get_resname() # returns the residue name, e.g. \u0026#34;ASN\u0026#34; \u0026gt;\u0026gt;\u0026gt; residue.is_disordered() # returns 1 if the residue has disordered atoms \u0026gt;\u0026gt;\u0026gt; residue.get_segid() # returns the SEGID, e.g. \u0026#34;CHN1\u0026#34; \u0026gt;\u0026gt;\u0026gt; residue.has_id(name) # test if a residue has a certain atom 你可以用 is_aa(residue) 来检验一个残基对象是否为氨基酸\n原子 原子对象储存着所有与原子有关的数据，它没有子类。原子的id就是它的名称（如，“OG”代表Ser残基的侧链氧原子）。在残基中原子id必需是唯一的。此外，对于紊乱原子会产生异常，\n原子id就是原子名称（如 ’CA’ ）。在实践中，原子名称是从PDB文件中原子名称去除所有空格而创建的。\n但是在PDB文件中，空格可以是原子名称的一部分。通常，钙原子称为 ’CA..’ 是为了和Cα原子（叫做 ’.CA.’ ）区分开。在这种情况下，如果去掉空格就会产生问题（如统一个残基中的两个原子都叫做 ’CA’ ），所以保留空格。\n在PDB文件中，一个原子名字由4个字符组成，通常头尾皆为空格。为了方便使用，空格通常可以去掉（在PDB文件中氨基酸的Cα原子标记为“.CA.”，点表示空格）。为了生成原子名称（然后是原子id），空格删掉了，除非会在一个残基中造成名字冲突（如两个原子对象有相同的名称和id）。对于后面这种情况，会尝试让原子名称包含空格。这种情况可能会发生在，比如残基包含名称为“.CA.”和“CA..”的原子，尽管这不怎么可能。\n所存储的原子数据包括原子名称，原子坐标（如果有的话还包括标准差），B因子（包括各向异性B因子和可能存在的标准差），altloc标识符和完整的、包括空格的原子名称。较少用到的项如原子序号和原子电荷（有时在PDB文件中规定）也就没有存储。\n为了处理原子坐标，可以用 ’Atom’ 对象的 transform 方法。用 set_coord 方法可以直接设定原子坐标。\n一个Atom对象还有如下其它方法：\n\u0026gt;\u0026gt;\u0026gt; a.get_name() # atom name (spaces stripped, e.g. \u0026#34;CA\u0026#34;) \u0026gt;\u0026gt;\u0026gt; a.get_id() # id (equals atom name) \u0026gt;\u0026gt;\u0026gt; a.get_coord() # atomic coordinates \u0026gt;\u0026gt;\u0026gt; a.get_vector() # atomic coordinates as Vector object \u0026gt;\u0026gt;\u0026gt; a.get_bfactor() # isotropic B factor \u0026gt;\u0026gt;\u0026gt; a.get_occupancy() # occupancy \u0026gt;\u0026gt;\u0026gt; a.get_altloc() # alternative location specifier \u0026gt;\u0026gt;\u0026gt; a.get_sigatm() # standard deviation of atomic parameters \u0026gt;\u0026gt;\u0026gt; a.get_siguij() # standard deviation of anisotropic B factor \u0026gt;\u0026gt;\u0026gt; a.get_anisou() # anisotropic B factor \u0026gt;\u0026gt;\u0026gt; a.get_fullname() # atom name (with spaces, e.g. \u0026#34;.CA.\u0026#34;) siguij，各向异性B因子和sigatm Numpy阵列可以用来表示原子坐标.。\nget_vector 方法会返回一个代表 Atom 对象坐标的 Vector 对象，可以对原子坐标进行向量运算。 Vector 实现了完整的三维向量运算、矩阵乘法（包括左乘和右乘）和一些高级的、与旋转相关的操作。\n举个Bio.PDB的 Vector 模块功能的例子，假设你要查找Gly残基的Cβ原子的位置，如果存在的话。将Gly残基的N原子沿Cα-C化学键旋转-120度，能大致将其放在一个真正的Cβ原子的位置上。怎么做呢？就是下面这样使用 Vector 模块中的rotaxis 方法（能用来构造一个绕特定坐标轴的旋转）：\n# get atom coordinates as vectors \u0026gt;\u0026gt;\u0026gt; n = residue[\u0026#39;N\u0026#39;].get_vector() \u0026gt;\u0026gt;\u0026gt; c = residue[\u0026#39;C\u0026#39;].get_vector() \u0026gt;\u0026gt;\u0026gt; ca = residue[\u0026#39;CA\u0026#39;].get_vector() # center at origin \u0026gt;\u0026gt;\u0026gt; n = n - ca \u0026gt;\u0026gt;\u0026gt; c = c - ca # find rotation matrix that rotates n # -120 degrees along the ca-c vector \u0026gt;\u0026gt;\u0026gt; rot = rotaxis(-pi * 120.0/180.0, c) # apply rotation to ca-n vector \u0026gt;\u0026gt;\u0026gt; cb_at_origin = n.left_multiply(rot) # put on top of ca atom \u0026gt;\u0026gt;\u0026gt; cb = cb_at_origin+ca 这个例子展示了在原子数据上能进行一些相当不平凡的向量运算，这些运算会很有用。除了所有常用向量运算（叉积（用 * ），点积（用 ），角度， 取范数等）和上述提到的 rotaxis 函数，Vector 模块还有方法能旋转（ rotmat ）或反射（ refmat ）一个向量到另外一个向量上。\n从结构中提取指定的Atom/Residue/Chain/Model 举些例子如下：\n\u0026gt;\u0026gt;\u0026gt; model = structure[0] \u0026gt;\u0026gt;\u0026gt; chain = model[\u0026#39;A\u0026#39;] \u0026gt;\u0026gt;\u0026gt; residue = chain[100] \u0026gt;\u0026gt;\u0026gt; atom = residue[\u0026#39;CA\u0026#39;] 还可以用一个快捷方式：\n\u0026gt;\u0026gt;\u0026gt; atom = structure[0][\u0026#39;A\u0026#39;][100][\u0026#39;CA\u0026#39;] 紊乱 Bio.PDB能够处理紊乱原子和点突变（比如Gly和Ala残基在相同位置上）\n紊乱可以从两个角度来解决：原子和残基的角度。一般来说，我们尝试压缩所有由紊乱引起的复杂性。如果你仅仅想遍历所有Cα原子，那么你不必在意一些具有紊乱侧链的残基。另一方面，应该考虑在数据结构中完整地表示紊乱性。因此，紊乱原子或残基存储在特定的对象中，这些对象表现得就像毫无紊乱。这可以通过表示紊乱原子或残基的子集来完成。至于挑选哪个子集（例如使用Ser残基的哪两个紊乱OG侧链原子位置），由用户来决定\n紊乱原子 紊乱原子可以用普通的 Atom 对象来表示，但是所有表示相同物理原子的 Atom 对象都存储在一个 DisorderedAtom 对象中。 DisorderedAtom 对象中每个 Atom 对象都能用它的altloc标识符来唯一地索引。 DisorderedAtom 对象将所有未捕获方法的调用发送给选定的Atom对象，缺省对象是代表最高使用率的原子的那个。当然用户可以使用其altloc标识符来更改选定的 Atom 对象。以这种方式，原子紊乱就正确地表示出来而没有很多额外的复杂性。换言之，如果你对原子紊乱不感兴趣，你也不会被它困扰。\n每个紊乱原子都有一个特征性的altloc标识符。你可以设定：一个 DisorderedAtom 对象表现得像与一个指定的altloc标识符相关的 Atom 对象\n\u0026gt;\u0026gt;\u0026gt; atom.disordered_select(\u0026#39;A\u0026#39;) # select altloc A atom \u0026gt;\u0026gt;\u0026gt; print atom.get_altloc() \u0026#34;A\u0026#34; \u0026gt;\u0026gt;\u0026gt; atom.disordered_select(\u0026#39;B\u0026#39;) # select altloc B atom \u0026gt;\u0026gt;\u0026gt; print atom.get_altloc() \u0026#34;B\u0026#34; 紊乱残基 普通例子 最常见的例子是一个残基包含一个或多个紊乱原子。这显然可以通过用DisorderedAtom对象表示这些紊乱原子来解决，并将DisorderedAtom对象存储在一个Residue对象中，就像正常的Atom对象那样。通过将所有未捕获方法调用发送给其中一个Atom对象（被选定的Atom对象），DisorderedAtom对象表现完全像一个正常的原子对象（事实上这个原子有最高的使用率）。\n点突变 个特殊的例子就是当紊乱是由点突变导致的时候，也就是说，在晶体结构中出现一条多肽的两或多个点突变。关于这一点，可以在PDB结构1EN2中找到一个例子。\n既然这些残基属于不同的残基类型（举例说Ser 60 和Cys 60），那么它们不应该像通常情况一样存储在一个单一 Residue 对象中。这种情况下每个残基用一个 Residue 对象来表示，两种 Residue 对象都保存在一个单一 DisorderedResidue 对象中。\nDisorderedResidue 对象将所有未捕获方法发送给选定的 Residue 对象（默认是所添加的最后一个 Residue 对象），因此表现得像一个正常的残基。在 DisorderedResidue 中每个 Residue 对象可通过残基名称来唯一标识。在上述例子中，残基Ser 60在 DisorderedResidue 对象中的id为“SER”，而残基Cys 60则是“CYS”。用户可以通过这个id选择在 DisorderedResidue 中的有效 Residue 对象。\n例子：假设一个链在位置10有一个由Ser和Cys残基构成的点突变。确信这个链的残基10表现为Cys残基。\n\u0026gt;\u0026gt;\u0026gt; residue = chain[10] \u0026gt;\u0026gt;\u0026gt; residue.disordered_select(\u0026#39;CYS\u0026#39;) 另外，通过使用 (Disordered)Residue 对象的 get_unpacked_list 方法，你能获得所有 Atom 对象的列表（也就是说，所有 DisorderedAtom 对象解包到它们各自的 Atom 对象\n异质残基 相关问题 关于异质残基的一个很普遍的问题是同一条链中的若干异质和非异质残基有同样的序列标识符（和插入码）。因此，要为每个异质残基生成唯一的id，水分子和其他异质残基应该以不同的方式来对待。\n记住Residue残基有一个元组（hetfield, resseq, icode）作为id。hetfield值为空(“ ”)表示为氨基酸和核酸；为一个字符串，则表示水分子和其他异质残基。hetfield的内容将在下面解释。\n水残基hetfiled \u0026ldquo;W\u0026rdquo; 水残基的hetfield字符串由字母“W”构成。所以水分子的一个典型的残基id为(“W”, 1, “ ”)。\n其他异质残基 hetfileld \u0026ldquo;H_\u0026rdquo; 其他异质残基的hetfield字符以“H_”起始，后接残基名称。一个葡萄糖分子，比如残基名称为“GLC”，则hetfield字符为“H_GLC”；它的残基id可以是(“H_GLC”, 1, “ ”)\n浏览Structure对象 解析PDB文件，提取一些Model,China,Residue和Atom对象 模型上的一条链，一条链上的残基，残基上的原子\n\u0026gt;\u0026gt;\u0026gt; from Bio.PDB.PDBParser import PDBParser \u0026gt;\u0026gt;\u0026gt; parser = PDBParser() \u0026gt;\u0026gt;\u0026gt; structure = parser.get_structure(\u0026#34;test\u0026#34;, \u0026#34;1fat.pdb\u0026#34;) \u0026gt;\u0026gt;\u0026gt; model = structure[0] \u0026gt;\u0026gt;\u0026gt; chain = model[\u0026#34;A\u0026#34;] \u0026gt;\u0026gt;\u0026gt; residue = chain[1] \u0026gt;\u0026gt;\u0026gt; atom = residue[\u0026#34;CA\u0026#34;] 迭代遍历一个结构中的所有原子 \u0026gt;\u0026gt;\u0026gt; p = PDBParser() \u0026gt;\u0026gt;\u0026gt; structure = p.get_structure(\u0026#39;X\u0026#39;, \u0026#39;pdb1fat.ent\u0026#39;) \u0026gt;\u0026gt;\u0026gt; for model in structure: ... for chain in model: ... for residue in chain: ... for atom in residue: ... print atom ... 有个快捷方式可以遍历一个结构中所有原子：\n\u0026gt;\u0026gt;\u0026gt; atoms = structure.get_atoms() \u0026gt;\u0026gt;\u0026gt; for atom in atoms: ... print atom ... 类似地，遍历一条链中的所有原子，可以这么做：\n\u0026gt;\u0026gt;\u0026gt; atoms = chain.get_atoms() \u0026gt;\u0026gt;\u0026gt; for atom in atoms: ... print atom ... 便利模型中的所有残基 \u0026gt;\u0026gt;\u0026gt; residues = model.get_residues() \u0026gt;\u0026gt;\u0026gt; for residue in residues: ... print residue ... 你也可以用 Selection.unfold_entities 函数来获取一个结构的所有残基：\n\u0026gt;\u0026gt;\u0026gt; res_list = Selection.unfold_entities(structure, \u0026#39;R\u0026#39;) 或者获得链上的所有原子：\n\u0026gt;\u0026gt;\u0026gt; atom_list = Selection.unfolf_entities(chain,\u0026#34;A\u0026#34;) 明显的是， A=atom, R=residue, C=chain, M=model, S=structure 。你可以用这种标记返回层次中的上层，如从一个 Atoms 列表得到（唯一的） Residue 或 Chain 父类的列表\n从链中提取异质残基（如resseq10的葡萄糖（GLC）部分） \u0026gt;\u0026gt;\u0026gt; residue_id = (\u0026#34;H_GLC\u0026#34;, 10, \u0026#34; \u0026#34;) \u0026gt;\u0026gt;\u0026gt; residue = chain[residue_id] 打印链中所有异质残基 \u0026gt;\u0026gt;\u0026gt; for residue in chain.get_list(): ... residue_id = residue.get_id() ... hetfield = residue_id[0] ... if hetfield[0]==\u0026#34;H\u0026#34;: ... print residue_id ... 输出一个结构分子中所有B因子大于50的CA原子的坐标 \u0026gt;\u0026gt;\u0026gt; for model in structure.get_list(): ... for chain in model.get_list(): ... for residue in chain.get_list(): ... if residue.has_id(\u0026#34;CA\u0026#34;): ... ca = residue[\u0026#34;CA\u0026#34;] ... if ca.get_bfactor() \u0026gt; 50.0: ... print ca.get_coord() ... 输出所有紊乱原子的残基 \u0026gt;\u0026gt;\u0026gt; for model in structure.get_list(): ... for chain in model.get_list(): ... for residue in chain.get_list(): ... if residue.is_disordered(): ... resseq = residue.get_id()[1] ... resname = residue.get_resname() ... model_id = model.get_id() ... chain_id = chain.get_id() ... print model_id, chain_id, resname, resseq ... 遍历所有紊乱原子，并选取所有具有atloc A的原子（如果有的话） \u0026gt;\u0026gt;\u0026gt; for model in structure.get_list(): ... for chain in model.get_list(): ... for residue in chain.get_list(): ... if residue.is_disordered(): ... for atom in residue.get_list(): ... if atom.is_disordered(): ... if atom.disordered_has_id(\u0026#34;A\u0026#34;): ... atom.disordered_select(\u0026#34;A\u0026#34;) ... 从structure对象中提取多肽 为了从一个结构中提取多肽，需要用 PolypeptideBuilder 从 Structure 构建一个 Polypeptide 对象的列表，如下所示\n\u0026gt;\u0026gt;\u0026gt; model_nr = 1 \u0026gt;\u0026gt;\u0026gt; polypeptide_list = build_peptides(structure, model_nr) \u0026gt;\u0026gt;\u0026gt; for polypeptide in polypeptide_list: ... print polypeptide ... Polypeptide对象正是Residue对象的一个UserList，总是从单结构域（在此例中为模型1）中创建而来。你可以用所得 Polypeptide 对象来获取序列作为 Seq 对象，或获得Cα原子的列表。多肽可以通过一个C-N 化学键或一个Cα-Cα化学键距离标准来建立。\n例子：\n# Using C-N \u0026gt;\u0026gt;\u0026gt; ppb=PPBuilder() \u0026gt;\u0026gt;\u0026gt; for pp in ppb.build_peptides(structure): ... print pp.get_sequence() ... # Using CA-CA \u0026gt;\u0026gt;\u0026gt; ppb=CaPPBuilder() \u0026gt;\u0026gt;\u0026gt; for pp in ppb.build_peptides(structure): ... print pp.get_sequence() ... 需要注意的是，上例中通过 PolypeptideBuilder 只考虑了结构的模型 0。尽管如此，还是可以用 PolypeptideBuilder 从 Model 和 Chain 对象创建 Polypeptide 对象。\n获取结构序列 要做的第一件事就是从结构中提取所有多肽（如上所述）。然后每条多肽的序列就容易从 Polypeptide 对象获得。该序列表示为一个Biopython Seq 对象，它的字母表由 ProteinAlphabet 对象来定义。\n例子：\n\u0026gt;\u0026gt;\u0026gt; seq = polypeptide.get_sequence() \u0026gt;\u0026gt;\u0026gt; print seq Seq(\u0026#39;SNVVE...\u0026#39;, \u0026lt;class Bio.Alphabet.ProteinAlphabet\u0026gt;) 分析结构 度量距离 重载原子的减法运算来返回两个原子之间的距离。\n# Get some atoms \u0026gt;\u0026gt;\u0026gt; ca1 = residue1[\u0026#39;CA\u0026#39;] \u0026gt;\u0026gt;\u0026gt; ca2 = residue2[\u0026#39;CA\u0026#39;] # Simply subtract the atoms to get their distance \u0026gt;\u0026gt;\u0026gt; distance = ca1-ca2 度量角度 用原子坐标的向量表示，和vector模块中的calc_angle函数可以计算角度\n\u0026gt;\u0026gt;\u0026gt; vector1 = atom1.get_vector() \u0026gt;\u0026gt;\u0026gt; vector2 = atom2.get_vector() \u0026gt;\u0026gt;\u0026gt; vector3 = atom3.get_vector() \u0026gt;\u0026gt;\u0026gt; angle = calc_angle(vector1, vector2, vector3) 度量扭转角 用原子坐标的向量表示，然后用 Vector 模块中的 calc_dihedral 函数可以计算角度。\n\u0026gt;\u0026gt;\u0026gt; vector1 = atom1.get_vector() \u0026gt;\u0026gt;\u0026gt; vector2 = atom2.get_vector() \u0026gt;\u0026gt;\u0026gt; vector3 = atom3.get_vector() \u0026gt;\u0026gt;\u0026gt; vector4 = atom4.get_vector() \u0026gt;\u0026gt;\u0026gt; angle = calc_dihedral(vector1, vector2, vector3, vector4) 确定原子-原子触点 用 NeighborSearch 来进行邻接查询。用C语言写的（使得运行很快）KD树模块（见 Bio.KDTree ）可以用来完成邻接查询。它也包含了一个快速方法来找出相距一定距离的所有点对。\n叠加两个结构 可以用 Superimposer 对象将两个坐标集叠加。这个对象计算出旋转和平移矩阵，该矩阵旋转两个列表上相重叠的原子使其满足RMSD最小。当然这两个列表含有相同数目的原子。 Superimposer 对象也可以将旋转/平移应用在一列原子上。旋转和平移作为一个元组储存在 Superimposer 对象的 rotran 属性中（注意，旋转是右乘），RMSD储存在属性 rmsd 中。\nSuperimposer 使用的算法来自[ 17 , Golub \u0026amp; Van Loan]并使用了奇异值分解（这是通用 Bio.SVDSuperimposer 模块中实现了的）。\n例子\n\u0026gt;\u0026gt;\u0026gt; sup = Superimposer() # Specify the atom lists # \u0026#39;fixed\u0026#39; and \u0026#39;moving\u0026#39; are lists of Atom objects # The moving atoms will be put on the fixed atoms \u0026gt;\u0026gt;\u0026gt; sup.set_atoms(fixed, moving) # Print rotation/translation/rmsd \u0026gt;\u0026gt;\u0026gt; print sup.rotran \u0026gt;\u0026gt;\u0026gt; print sup.rms # Apply rotation/translation to the moving atoms \u0026gt;\u0026gt;\u0026gt; sup.apply(moving) 为了基于有效位点来叠加两个结构，用有效位点的原子来计算旋转/平移矩阵（如上所述），并应用到整个分子\n双向映射两个相关结构的残基 首先，创建一个FASTA格式的比对文件，然后使用StructureAlignment 类。这个类也可以用来比对两个以上的结构\n计算半球暴露 半球暴露（Half Sphere Exposure，HSE）是对溶剂暴露的一种新的二维度量。根本上，它计数了围绕一个残基，在其侧链方向上及反方向（在13 Å范围内）的Cα原子。尽管简单，它表现得比溶剂暴露的其它度量都要好\nHSE有两种风味：HSEα和HSEβ。前者仅用到Cα原子的位置，而后者用到Cα和Cβ原子的位置。HSE度量是由 HSExposure 类来计算的，这个类也能计算触点数目。后一个类有方法能返回一个字典，该字典将一个Residue 对象映射到相应的HSEα,HSEβ和触点数目值。\n例子\n\u0026gt;\u0026gt;\u0026gt; model = structure[0] \u0026gt;\u0026gt;\u0026gt; hse = HSExposure() # Calculate HSEalpha \u0026gt;\u0026gt;\u0026gt; exp_ca = hse.calc_hs_exposure(model, option=\u0026#39;CA3\u0026#39;) # Calculate HSEbeta \u0026gt;\u0026gt;\u0026gt; exp_cb=hse.calc_hs_exposure(model, option=\u0026#39;CB\u0026#39;) # Calculate classical coordination number \u0026gt;\u0026gt;\u0026gt; exp_fs = hse.calc_fs_exposure(model) # Print HSEalpha for a residue \u0026gt;\u0026gt;\u0026gt; print exp_ca[some_residue] 确定二级结构 为了这个功能，你需要安装DSSP（并获得一个对学术性使用免费的证书，参见 http://www.cmbi.kun.nl/gv/dssp/ ）。然后用 DSSP 类，可以映射 Residue 对象到其二级结构上（和溶剂可及表面区域）。DSSP代码如下表所列表 。注意DSSP（程序及其相应的类）不能处理多个模型！\nDSSP 类也可以用来计算残基的溶剂可及表面\n计算残基深度 残基深度是残基原子到溶剂可及表面的平均距离。它是溶剂可及性的一种相当新颖和非常强大的参数化。为了这个功能，你需要安装Michel Sanner的 MSMS程序（ http://www.scripps.edu/pub/olson-web/people/sanner/html/msms_home.html ）。然后使用 ResidueDepth 类。这个类像字典一样将 Residue 对象映射到相应的（残基深度，Cα深度）元组。Cα深度是残基的Cα原子到溶剂可及表面的距离\n例子\n\u0026gt;\u0026gt;\u0026gt; model = structure[0] \u0026gt;\u0026gt;\u0026gt; rd = ResidueDepth(model, pdb_file) \u0026gt;\u0026gt;\u0026gt; residue_depth, ca_depth=rd[some_residue] 你也可以以带有表面点的数值Python数组的形式获得分子表面本身（通过 get_surface 函数）。\nPDB文件中的常见问题 众所周知，很多PDB文件包含语义错误（不是结构本身的错误，而是在PDB文件中的表示）。Bio.PDB可以有两种方式来处理这个问题。PDBParser对象能表现出两种方式：严格方式和宽容方式（默认方式）：\n例子\n# Permissive parser \u0026gt;\u0026gt;\u0026gt; parser = PDBParser(PERMISSIVE=1) \u0026gt;\u0026gt;\u0026gt; parser = PDBParser() # The same (default) # Strict parser \u0026gt;\u0026gt;\u0026gt; strict_parser = PDBParser(PERMISSIVE=0) 在宽容状态（默认），明显包含错误的PDB文件会被“纠正”（比如说一些残基或原子丢失）。这些错误包括：\n多个残基使用同一个标识符 多个原子使用统一个标识符（考虑altloc识别符） 这些错误暗示了PDB文件中确实存在错误（详情见 [18, Hamelryck and Manderick, 2003] ）。在严格模式，带错的PDB文件会引发异常，这有助于发现PDB文件中的错误。\n但是有些错误能自动修正。正常情况下，每个紊乱原子应该会有一个非空altloc标识符。可是很多结构没有遵循这个惯例，而在同一原子的两个紊乱位置存在一个空的和一个非空的标识符。这个错误会被以正确的方式自动解析。\n有时候一个结构会有这样的情况：一部分残基属于A链，接下来一部分残基属于B链，然后又有一部分残基属于A链，也就是说，这种链是“断的”。这也能被自动正确解析 例子 PDBParser/Structure类经过了将近800个结构（每个都属于不同的SCOP超家族）上的测试。测试总共耗时20分钟左右，或者说平均每个结构只需1.5秒。在一台1000 MHz的PC上只需10秒就可解析包含近64000个原子的大核糖体亚基（1FKK）的结构。\n当不能建立明确的数据结构时会发生三类异常。在这三类异常中，可能的起因是PDB文件中一个本应修正的错误。这些情况下产生异常要比冒险地错误描述一个数据结构中的结构好得多。\n重复残基 一个结构包含在一条链中具有相同的序列标识符（resseq 3）和icode的两个氨基酸残基。仔细观察可以发现这条链包含残基：Thr A3, …, Gly A202, Leu A3, Glu A204。很明显第二个Leu A3应该是Leu A203。类似的情况也存在于结构1FFK（比如它包含残基Gly B64, Met B65, Glu B65, Thr B67，也就是说Glu B65应该是Glu B66）上。\n重复原子 结构1EJG含有在A链22位的一个Ser/Pro点突变。依次，Ser 22含一些紊乱原子。和期望的一样，所有属于 Ser 22的原子都有一个非空的altloc标识符（B或C）。所有Pro 22的原子都有altloc A，除了含空altloc的N原子。这会生成一个异常，因为一个点突变处属于两个残基的所有原子都应该有非空的altloc。结果这个原子很可能被Ser 和 Pro 22共用，而Ser22丢失了这个N原子。此外，这也点出了文件中的一个问题：这个N原子应该出现在Ser和Pro残基中，两种情形下都与合适的altloc标识符关联\n自动纠正 一些错误相当普遍且能够在没有太大误解风险的情况下容易地纠正过来。这些错误列在下面\n紊乱原子的空altloc 正常情况下，每个紊乱原子应该会有一个非空altloc标识符，可是很多结构没有遵循这个惯例，而是在同一原子的两个紊乱位置存在一个空的和一个非空的标识符。这个错误会被以正确的方式自动解析\n断链 有时候一个结构会有这样的情况：一部分残基属于A链，接下来一部分残基属于B链，然后又有一部分残基属于A链，也就是说，链是“断的”，这也能被正确的解析\n致命错误 有时候一个PDB文件不能被明确解释。这会产生异常并等待用户去修正这个PDB文件，而不是猜测和冒出错的风险。这些异常列在下面\n重复残基 在一条链上的所有残基都应该有一个唯一的id。该id基于下述生成：\n序列标识符（resseq） 插入码（icode） hetfield字符（“W”代表水，“H_”后面的残基名称代表其他异质残基） 发生点突变的残基的名称（在DisorderedResidue对象中存储Residue对象） 如果这样还不能生成一个唯一的id，那么肯定是一些地方出了错，这时会生成一个异常。 重复原子 一个残基上所有原子应该有一个唯一的id，这个id基于下述产生：\n原子名称（不带空格，否则会报错） altloc标识符 如果这样还不能生成一个唯一的id，那么肯定是一些地方出了错，这时会生成一个异常。 访问Protein Data Bank 使用retrieve_pdb_file从Protein Data Bank下载结构 结构可以从PDB（Protein Data Bank）通过 PDBList 对象的 retrieve_pdb_file 方法下载。这种方法的要点是结构的PDB标识符。\n\u0026gt;\u0026gt;\u0026gt; pdbl = PDBList() \u0026gt;\u0026gt;\u0026gt; pdbl.retrieve_pdb_file(\u0026#39;1FAT\u0026#39;) PDBList 类也能用作命令行工具：\npython PDBList.py 1fat 下载的文件将以 pdb1fat.ent 为名保存在当前工作目录。注意 retrieve_pdb_file 方法还有个可选参数 pdir 用来指定一个特定的路径来保存所下载的PDB文件。\nretrieve_pdb_file 方法还有其他选项可以指定下载所用的压缩格式（默认的 .Z 格式和 gunzip 格式）。另外，在创建 PDBList 对象时还可以指定PDB ftp站点。默认使用Worldwide Protein Data Bank（ ftp://ftp.wwpdb.org/pub/pdb/data/structures/divided/pdb/ ）\n下载全部pdb 下面的命令将会保存所有PDB文件至 /data/pdb 目录：\npython PDBList.py all /data/pdb python PDBList.py all /data/pdb -d 在API中这个方法叫做 download_entire_pdb 。添加 -d 会在同一目录下保存所有文件。否则将分别保存至PDB风格的、与其PDB ID对应的子目录中。根据网速，完整的下载全部PDB文件大概需要2-4天。\n保持本地的PDB拷贝的更新 这也能通过 PDBList 对象来完成。可以简单的创建一个 PDBList 对象（指定本地PDB拷贝的目录），然后调用 update_pdb 方法：\n\u0026gt;\u0026gt;\u0026gt; pl = PDBList(pdb=\u0026#39;/data/pdb\u0026#39;) \u0026gt;\u0026gt;\u0026gt; pl.update_pdb() 当然还可以每周用 cronjob 实现本地拷贝自动更新。还可以指定PDB ftp站点（详见API文档）。\nPDBList 有其他许多其它方法可供调用。 get_all_obsolete 方法可以获取所有已经废弃的PDB项的一个列表； changed_this_week 方法可以用于获得当前一周内新增加、修改或废弃的PDB项。更多 PDBList 的用法参见API文档\n常见的问题 是否支持分子图形的展示 不直接支持，很大程度上是因为已有相当多基于Python或Python-aware的解决方案，也可能会用到Bio.PDB。顺便说一下，我的选择是Pymol（我在Pymol中使用Bio.PDB非常成功，将来Bio.PDB中会有特定的PyMol模块）。基于Python或Python-aware的分子图形解决方案包括：\nPyMol: http://pymol.sourceforge.net/ Chimera: http://www.cgl.ucsf.edu/chimera/ PMV: http://www.scripps.edu/~sanner/python/ Coot: http://www.ysbl.york.ac.uk/~emsley/coot/ CCP4mg: http://www.ysbl.york.ac.uk/~lizp/molgraphics.html mmLib: http://pymmlib.sourceforge.net/ VMD: http://www.ks.uiuc.edu/Research/vmd/ MMTK: http://starship.python.net/crew/hinsen/MMTK/ ","date":"2021-08-11T00:00:00Z","permalink":"https://example.com/p/ch11_pdb%E6%A8%A1%E5%9D%97/","title":"ch11_PDB模块"},{"content":"biopython官方地址：https://biopython.org/\ngithub地址：https://github.com/biopython/biopython/blob/master/\n中文版教程：https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html\nbiopython包的说明（具体到每个模块了）：https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html\n这一章只挑取自己感觉有需要的去记录\n简介 Entrez (http://www.ncbi.nlm.nih.gov/Entrez) 是一个给客户提供NCBI各个数据库（如PubMed, GeneBank, GEO等等）访问的检索系统。 用户可以通过浏览器手动输入查询条目访问Entrez，也可以使用Biopython的 Bio.Entrez 模块以编程方式访问来访问Entrez。 如果使用第二种方法，用户用一个Python脚本就可以实现在PubMed里面搜索或者从GenBank下载数据。\nBio.Entrez 模块利用了Entrez Programming Utilities（也称作EUtils），包含八个工具，详情请见NCBI的网站： http://www.ncbi.nlm.nih.gov/entrez/utils/. 每个工具都能在Python的 Bio.Entrez 模块中找到对应函数，后面会详细讲到。这个模块可以保证用来查询的URL 的正确性，并且向NCBI要求的一样，每三秒钟查询的次数不超过一。\nEUtils返回的输出结果通常是XML格式，我们有以下不同的方法来解析这种类型的输入文件：\n使用 Bio.Entrez解析器将XML输出的解析成Python对象; 使用Python标准库中的DOM (Document Object Model)解析器; 使用Python标准库中的SAX (Simple API for XML)解析器; 把XML输出当做原始的文本文件，通过字符串查找和处理来进行解析； 对于DOM和SAX解析器，可以查看Python的文档. Bio.Entrez 中使用到的解析器将会在下面讨论.\nNCBI使用DTD (Document Type Definition)文件来描述XML文件中所包含信息的结构. 大多数NCBI使用的DTD文件 格式都包含在了Biopython发行包里。当NCBI Entrez读入一个XML格式的文件的时候，Bio.Entrez 将会使用DTD文件。\n有时候，你可能会发现与某种特殊的XML相关的DTD文件在Biopython发行包里面不存在。当NCBI升级它的 DTD文件的时候，这种情况可能发生。如果发生这种情况，Entrez.read 将会显示丢失的DTD文件名字和URL的 警示信息。解析器会通过互联网获取缺失的DTD文件，让XML的分析继续正常进行。如果本地存在对应的DTD文件的 话，处理起来会更快。因此，为了更快的处理，我们可以通过警示信息里面的URL来下载对应的DTD文件，将文件放在DTD 文件默认存放的文件夹 \u0026hellip;site-packages/Bio/Entrez/DTDs 。如果你没有权限进入这个文件夹，你也可以把 DTD文件放到 ~/.biopython/Bio/Entrez/DTDs 这个目录，~ 表示的是你的Home目录。因为这个目录会先于 \u0026hellip;site-packages/Bio/Entrez/DTDs 被解析器读取，所以当 \u0026hellip;site-packages/Bio/Entrez/DTDs 下面的DTD文件过时的时候，你也可以将最新版本的DTD文件放到Home目录的那个文件夹下面。当然也有其他方案，如果你 是通过源码来安装的Biopython，你可以将DTD文件放到源码的 Bio/Entrez/DTDs 文件夹下，然后重新安装Biopython。 这样会将新的DTD文件和之前的一样地安装到正确的位置。\nEntrez Programming Utilities也可以生成其他格式的输出文件，比如Fasta、序列数据库里面的GenBank文件格式 或者文献数据库里面的MedLine格式 详细的访问规范 对任何连续超过100次的访问请求，请在周末时间或者避开美国的使用高峰时间。这个取决于你是否遵从。 使用这个网址 http://eutils.ncbi.nlm.nih.gov ， 而不是通常的NCBI网址。Biopython使用的是这个网址。 每秒钟不要超过三次请求（比2009年年初的每三秒钟最多一次请求要宽松）。这个由Biopython自动强制实行。 使用email参数，这样如果遇到什么问题，NCBI可以通过邮件联系到你。你可以在每次请求Entrez的时候明确的设置 这个参数（例如，在参数列表中包含 email=\u0026quot;A.N.Other@example.com\u0026quot; ），或者你也可以设置一个全局的email 地址： \u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; Entrez.email = \u0026#34;A.N.Other@example.com\u0026#34; Bio.Entrez 将会在每次向Entrez请求的时候使用这个邮件地址。请千万不要胡乱的填写邮件地址，不填写都比 这要好。邮件的参数从2010年6月1日将是强制的参数。在过度使用的情况下，NCBI会在封锁用户访问E-utilities之前尝试通过 用户提供的邮件地址联系。 如果你是在一个大的软件包里面使用Biopython的，请通过tool这个参数明确说明。你既可以在每次请求访问Entrez 的时候通过参数明确地指明使用的工具（例如，在参数列表中包含 tool=\u0026ldquo;MyLocalScript\u0026rdquo; ），或者你也可以 设置一个全局的tool名称： \u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; Entrez.tool = \u0026#34;MyLocalScript\u0026#34; 最后，根据你的使用情况选择不同的策略。如果你打算下载大量的数据，最好使用其他的方法。比如，你想得到所有人的 基因的数据，那么考虑通过FTP得到每个染色体的GenBank文件，然后将这些文件导入到你自己的BioSQL数据库里面去\nEInfo 获取Entrez数据库的信息 EInfo为每个NCBI的数据库提供了条目索引，最近更新的时间以及可用的链接。此外，你可以很容易的使用EInfo通过 Entrez获取所有数据库名字的列表\n\u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; Entrez.email = \u0026#34;A.N.Other@example.com\u0026#34; \u0026gt;\u0026gt;\u0026gt; handle = Entrez.einfo() \u0026gt;\u0026gt;\u0026gt; result = handle.read() 变量result为XML格式的数据库列表 \u0026gt;\u0026gt;\u0026gt; print result \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE eInfoResult PUBLIC \u0026#34;-//NLM//DTD eInfoResult, 11 May 2002//EN\u0026#34; \u0026#34;http://www.ncbi.nlm.nih.gov/entrez/query/DTD/eInfo_020511.dtd\u0026#34;\u0026gt; \u0026lt;eInfoResult\u0026gt; \u0026lt;DbList\u0026gt; \u0026lt;DbName\u0026gt;pubmed\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;protein\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;nucleotide\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;nuccore\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;nucgss\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;nucest\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;structure\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;genome\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;books\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;cancerchromosomes\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;cdd\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;gap\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;domains\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;gene\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;genomeprj\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;gensat\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;geo\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;gds\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;homologene\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;journals\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;mesh\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;ncbisearch\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;nlmcatalog\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;omia\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;omim\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;pmc\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;popset\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;probe\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;proteinclusters\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;pcassay\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;pccompound\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;pcsubstance\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;snp\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;taxonomy\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;toolkit\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;unigene\u0026lt;/DbName\u0026gt; \u0026lt;DbName\u0026gt;unists\u0026lt;/DbName\u0026gt; \u0026lt;/DbList\u0026gt; \u0026lt;/eInfoResult\u0026gt; 使用Bio.Entrez 将XML文件读入到python中 因为这是一个相当简单的XML文件，我们可以简单的通过字符串查找提取里面所包含的信息。使用 Bio.Entrez 的解析器， 我们可以直接将这个XML读入到一个Python对象里面去：\n\u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; handle = Entrez.einfo() \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(handle) 现在 record 是拥有一个确定键值的字典：\n\u0026gt;\u0026gt;\u0026gt; record.keys() [u\u0026#39;DbList\u0026#39;] 这个键对应的值存储了上面XML文件里面包含的数据库名字的列表：\n\u0026gt;\u0026gt;\u0026gt; record[\u0026#34;DbList\u0026#34;] [\u0026#39;pubmed\u0026#39;, \u0026#39;protein\u0026#39;, \u0026#39;nucleotide\u0026#39;, \u0026#39;nuccore\u0026#39;, \u0026#39;nucgss\u0026#39;, \u0026#39;nucest\u0026#39;, \u0026#39;structure\u0026#39;, \u0026#39;genome\u0026#39;, \u0026#39;books\u0026#39;, \u0026#39;cancerchromosomes\u0026#39;, \u0026#39;cdd\u0026#39;, \u0026#39;gap\u0026#39;, \u0026#39;domains\u0026#39;, \u0026#39;gene\u0026#39;, \u0026#39;genomeprj\u0026#39;, \u0026#39;gensat\u0026#39;, \u0026#39;geo\u0026#39;, \u0026#39;gds\u0026#39;, \u0026#39;homologene\u0026#39;, \u0026#39;journals\u0026#39;, \u0026#39;mesh\u0026#39;, \u0026#39;ncbisearch\u0026#39;, \u0026#39;nlmcatalog\u0026#39;, \u0026#39;omia\u0026#39;, \u0026#39;omim\u0026#39;, \u0026#39;pmc\u0026#39;, \u0026#39;popset\u0026#39;, \u0026#39;probe\u0026#39;, \u0026#39;proteinclusters\u0026#39;, \u0026#39;pcassay\u0026#39;, \u0026#39;pccompound\u0026#39;, \u0026#39;pcsubstance\u0026#39;, \u0026#39;snp\u0026#39;, \u0026#39;taxonomy\u0026#39;, \u0026#39;toolkit\u0026#39;, \u0026#39;unigene\u0026#39;, \u0026#39;unists\u0026#39;] 使用EInfo获得更多信息 对于这些数据库，我们可以使用EInfo获得更多的信息：\n\u0026gt;\u0026gt;\u0026gt; handle = Entrez.einfo(db=\u0026#34;pubmed\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(handle) \u0026gt;\u0026gt;\u0026gt; record[\u0026#34;DbInfo\u0026#34;][\u0026#34;Description\u0026#34;] \u0026#39;PubMed bibliographic record\u0026#39; \u0026gt;\u0026gt;\u0026gt; record[\u0026#34;DbInfo\u0026#34;][\u0026#34;Count\u0026#34;] \u0026#39;17989604\u0026#39; \u0026gt;\u0026gt;\u0026gt; record[\u0026#34;DbInfo\u0026#34;][\u0026#34;LastUpdate\u0026#34;] \u0026#39;2008/05/24 06:45\u0026#39; 通过 record[\u0026ldquo;DbInfo\u0026rdquo;].keys() 可以获取存储在这个记录里面的其他信息。这里面最有用的信息之一是一个ESearch可用的 搜索值列表：\n\u0026gt;\u0026gt;\u0026gt; for field in record[\u0026#34;DbInfo\u0026#34;][\u0026#34;FieldList\u0026#34;]: ... print \u0026#34;%(Name)s, %(FullName)s, %(Description)s\u0026#34; % field ALL, All Fields, All terms from all searchable fields UID, UID, Unique number assigned to publication FILT, Filter, Limits the records TITL, Title, Words in title of publication WORD, Text Word, Free text associated with publication MESH, MeSH Terms, Medical Subject Headings assigned to publication MAJR, MeSH Major Topic, MeSH terms of major importance to publication AUTH, Author, Author(s) of publication JOUR, Journal, Journal abbreviation of publication AFFL, Affiliation, Author\u0026#39;s institutional affiliation and address ... 这是一个很长的列表，但是间接的告诉你在使用PubMed的时候，你可以通过 Jones[AUTH] 搜索作者，或者通过 Sanger[AFFL] 将作者范围限制在Sanger Centre。这个会非常方便，特别是在你对某个数据库不太熟悉的时候。\nESearch:搜素Entrez数据库 使用Bio.Entrez,esearch()来搜素任意数据库 在PubMed中搜索跟Biopython相关的文献 我们可以使用 Bio.Entrez.esearch() 来搜索任意的数据库。例如，我们在PubMed中搜索跟Biopython相关的文献：\n\u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; Entrez.email = \u0026#34;A.N.Other@example.com\u0026#34; # Always tell NCBI who you are \u0026gt;\u0026gt;\u0026gt; handle = Entrez.esearch(db=\u0026#34;pubmed\u0026#34;, term=\u0026#34;biopython\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(handle) \u0026gt;\u0026gt;\u0026gt; record[\u0026#34;IdList\u0026#34;] [\u0026#39;19304878\u0026#39;, \u0026#39;18606172\u0026#39;, \u0026#39;16403221\u0026#39;, \u0026#39;16377612\u0026#39;, \u0026#39;14871861\u0026#39;, \u0026#39;14630660\u0026#39;, \u0026#39;12230038\u0026#39;] 在输出的结果中，我们可以看到七个PubMed IDs（包括19304878，这个是Biopython应用笔记的PMID），你可以通过 EFetch来获取这些文献\n通过ESearch来搜索GenBank 你也可以通过ESearch来搜索GenBank。我们将以在_Cypripedioideae_ orchids中搜索_matK_基因为例，快速展示一下\n\u0026gt;\u0026gt;\u0026gt; handle = Entrez.esearch(db=\u0026#34;nucleotide\u0026#34;,term=\u0026#34;Cypripedioideae[Orgn] AND matK[Gene]\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(handle) \u0026gt;\u0026gt;\u0026gt; record[\u0026#34;Count\u0026#34;] \u0026#39;25\u0026#39; \u0026gt;\u0026gt;\u0026gt; record[\u0026#34;IdList\u0026#34;] [\u0026#39;126789333\u0026#39;, \u0026#39;37222967\u0026#39;, \u0026#39;37222966\u0026#39;, \u0026#39;37222965\u0026#39;, ..., \u0026#39;61585492\u0026#39;] 每个IDs(126789333, 37222967, 37222966, …)是GenBank的一个标识,后面再讲怎么下载这些GenaBank的记录和信息\n注意，不是像 Cypripedioideae[Orgn] 这样在搜索的时候加上特定的物种名字，而是需要在搜索的时候使用NCBI的 taxon ID，像 txid158330[Orgn] 这样。这个并没有记录在ESearch的帮助页面上，NCBI通过邮件回复解释了这个 问题。你可以通过经常和Entrez的网站接口互动，来推断搜索条目的格式。例如，在基因组搜索的时候加上 complete[prop] 可以把结果限制在完成的基因组上。\n获取一个computational journal名字的列表 作为最后一个例子，让我们获取一个computational journal名字的列表：\n\u0026gt;\u0026gt;\u0026gt; handle = Entrez.esearch(db=\u0026#34;journals\u0026#34;, term=\u0026#34;computational\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(handle) \u0026gt;\u0026gt;\u0026gt; record[\u0026#34;Count\u0026#34;] \u0026#39;16\u0026#39; \u0026gt;\u0026gt;\u0026gt; record[\u0026#34;IdList\u0026#34;] [\u0026#39;30367\u0026#39;, \u0026#39;33843\u0026#39;, \u0026#39;33823\u0026#39;, \u0026#39;32989\u0026#39;, \u0026#39;33190\u0026#39;, \u0026#39;33009\u0026#39;, \u0026#39;31986\u0026#39;, \u0026#39;34502\u0026#39;, \u0026#39;8799\u0026#39;, \u0026#39;22857\u0026#39;, \u0026#39;32675\u0026#39;, \u0026#39;20258\u0026#39;, \u0026#39;33859\u0026#39;, \u0026#39;32534\u0026#39;, \u0026#39;32357\u0026#39;, \u0026#39;32249\u0026#39;] 同样，我们可以通过EFetch来获得关于每个journal IDs更多的消息。\nEFetch:从Entrez下载更多的记录 NCBI大部分的数据库都支持多种不同的文件格式。当使用 Bio.Entrez.efetch() 从Entrez下载特定的某种格式的时候， 需要 rettype 和或者 retmode 这些可选的参数。对于不同数据库类型不同的搭配在下面的网页中有描述（http://www.ncbi.nlm.nih.gov/entrez/query/static/efetch_help.html），例如： literature, sequences and taxonomy).\n通过 Bio.Entrez.efetch 从GenBank下载记录186972394 一种常用的用法是下载FASTA或者GenBank/GenPept的文本格式 (接着可以使用 Bio.SeqIO 来解析）。从上面 Cypripedioideae 的例子,我们可以通过 Bio.Entrez.efetch 从GenBank下载记录186972394。\n参数 rettype=\u0026ldquo;gb\u0026rdquo; 和 retmode=\u0026ldquo;text\u0026rdquo; 让我们下载的数据为GenBank格式。\n需要注意的是直到2009年，Entrez EFetch API要求使用 “genbank” 作为返回类型，然而现在NCBI坚持使用官方的 “gb” 或 “gbwithparts” （或者针对蛋白的“gp”) 返回类型。同样需要注意的是，直到2012年2月， Entrez EFetch API默认的返回格式为纯文本格式文件，现在默认的为XML格式\n作为另外的选择，你也可以使用 rettype=\u0026ldquo;fasta\u0026rdquo; 来获取Fasta格式的文件；(https://www.ncbi.nlm.nih.gov/entrez/query/static/efetchseq_help.html)\n记住，可选的数据格式决定于你要下载的数据库——请参见\n(http://eutils.ncbi.nlm.nih.gov/entrez/query/static/efetch_help.html)\n\u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; Entrez.email = \u0026#34;A.N.Other@example.com\u0026#34; # Always tell NCBI who you are \u0026gt;\u0026gt;\u0026gt; handle = Entrez.efetch(db=\u0026#34;nucleotide\u0026#34;, id=\u0026#34;186972394\u0026#34;, rettype=\u0026#34;gb\u0026#34;, retmode=\u0026#34;text\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print handle.read() LOCUS EU490707 1302 bp DNA linear PLN 05-MAY-2008 DEFINITION Selenipedium aequinoctiale maturase K (matK) gene, partial cds; chloroplast. ACCESSION EU490707 VERSION EU490707.1 GI:186972394 KEYWORDS . SOURCE chloroplast Selenipedium aequinoctiale ORGANISM Selenipedium aequinoctiale Eukaryota; Viridiplantae; Streptophyta; Embryophyta; Tracheophyta; Spermatophyta; Magnoliophyta; Liliopsida; Asparagales; Orchidaceae; Cypripedioideae; Selenipedium. REFERENCE 1 (bases 1 to 1302) AUTHORS Neubig,K.M., Whitten,W.M., Carlsward,B.S., Blanco,M.A., Endara,C.L., Williams,N.H. and Moore,M.J. TITLE Phylogenetic utility of ycf1 in orchids JOURNAL Unpublished REFERENCE 2 (bases 1 to 1302) AUTHORS Neubig,K.M., Whitten,W.M., Carlsward,B.S., Blanco,M.A., Endara,C.L., Williams,N.H. and Moore,M.J. TITLE Direct Submission JOURNAL Submitted (14-FEB-2008) Department of Botany, University of Florida, 220 Bartram Hall, Gainesville, FL 32611-8526, USA FEATURES Location/Qualifiers source 1..1302 /organism=\u0026#34;Selenipedium aequinoctiale\u0026#34; /organelle=\u0026#34;plastid:chloroplast\u0026#34; /mol_type=\u0026#34;genomic DNA\u0026#34; /specimen_voucher=\u0026#34;FLAS:Blanco 2475\u0026#34; /db_xref=\u0026#34;taxon:256374\u0026#34; gene \u0026lt;1..\u0026gt;1302 /gene=\u0026#34;matK\u0026#34; CDS \u0026lt;1..\u0026gt;1302 /gene=\u0026#34;matK\u0026#34; /codon_start=1 /transl_table=11 /product=\u0026#34;maturase K\u0026#34; /protein_id=\u0026#34;ACC99456.1\u0026#34; /db_xref=\u0026#34;GI:186972395\u0026#34; /translation=\u0026#34;IFYEPVEIFGYDNKSSLVLVKRLITRMYQQNFLISSVNDSNQKG FWGHKHFFSSHFSSQMVSEGFGVILEIPFSSQLVSSLEEKKIPKYQNLRSIHSIFPFL EDKFLHLNYVSDLLIPHPIHLEILVQILQCRIKDVPSLHLLRLLFHEYHNLNSLITSK KFIYAFSKRKKRFLWLLYNSYVYECEYLFQFLRKQSSYLRSTSSGVFLERTHLYVKIE HLLVVCCNSFQRILCFLKDPFMHYVRYQGKAILASKGTLILMKKWKFHLVNFWQSYFH FWSQPYRIHIKQLSNYSFSFLGYFSSVLENHLVVRNQMLENSFIINLLTKKFDTIAPV ISLIGSLSKAQFCTVLGHPISKPIWTDFSDSDILDRFCRICRNLCRYHSGSSKKQVLY RIKYILRLSCARTLARKHKSTVRTFMRRLGSGLLEEFFMEEE\u0026#34; ORIGIN 1 attttttacg aacctgtgga aatttttggt tatgacaata aatctagttt agtacttgtg 61 aaacgtttaa ttactcgaat gtatcaacag aattttttga tttcttcggt taatgattct 121 aaccaaaaag gattttgggg gcacaagcat tttttttctt ctcatttttc ttctcaaatg 181 gtatcagaag gttttggagt cattctggaa attccattct cgtcgcaatt agtatcttct 241 cttgaagaaa aaaaaatacc aaaatatcag aatttacgat ctattcattc aatatttccc 301 tttttagaag acaaattttt acatttgaat tatgtgtcag atctactaat accccatccc 361 atccatctgg aaatcttggt tcaaatcctt caatgccgga tcaaggatgt tccttctttg 421 catttattgc gattgctttt ccacgaatat cataatttga atagtctcat tacttcaaag 481 aaattcattt acgccttttc aaaaagaaag aaaagattcc tttggttact atataattct 541 tatgtatatg aatgcgaata tctattccag tttcttcgta aacagtcttc ttatttacga 601 tcaacatctt ctggagtctt tcttgagcga acacatttat atgtaaaaat agaacatctt 661 ctagtagtgt gttgtaattc ttttcagagg atcctatgct ttctcaagga tcctttcatg 721 cattatgttc gatatcaagg aaaagcaatt ctggcttcaa agggaactct tattctgatg 781 aagaaatgga aatttcatct tgtgaatttt tggcaatctt attttcactt ttggtctcaa 841 ccgtatagga ttcatataaa gcaattatcc aactattcct tctcttttct ggggtatttt 901 tcaagtgtac tagaaaatca tttggtagta agaaatcaaa tgctagagaa ttcatttata 961 ataaatcttc tgactaagaa attcgatacc atagccccag ttatttctct tattggatca 1021 ttgtcgaaag ctcaattttg tactgtattg ggtcatccta ttagtaaacc gatctggacc 1081 gatttctcgg attctgatat tcttgatcga ttttgccgga tatgtagaaa tctttgtcgt 1141 tatcacagcg gatcctcaaa aaaacaggtt ttgtatcgta taaaatatat acttcgactt 1201 tcgtgtgcta gaactttggc acggaaacat aaaagtacag tacgcacttt tatgcgaaga 1261 ttaggttcgg gattattaga agaattcttt atggaagaag aa // 如果你要获取记录的格式是 Bio.SeqIO 所接受的一种格式 你可以直接将其解析为一个 SeqRecord ：\n\u0026gt;\u0026gt;\u0026gt; from Bio import Entrez, SeqIO \u0026gt;\u0026gt;\u0026gt; handle = Entrez.efetch(db=\u0026#34;nucleotide\u0026#34;, id=\u0026#34;186972394\u0026#34;,rettype=\u0026#34;gb\u0026#34;, retmode=\u0026#34;text\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = SeqIO.read(handle, \u0026#34;genbank\u0026#34;) \u0026gt;\u0026gt;\u0026gt; handle.close() \u0026gt;\u0026gt;\u0026gt; print record ID: EU490707.1 Name: EU490707 Description: Selenipedium aequinoctiale maturase K (matK) gene, partial cds; chloroplast. Number of features: 3 ... Seq(\u0026#39;ATTTTTTACGAACCTGTGGAAATTTTTGGTTATGACAATAAATCTAGTTTAGTA...GAA\u0026#39;, IUPACAmbiguousDNA()) 需要注意的是，一种更加典型的用法是先把序列数据保存到一个本地文件，然后 使用 Bio.SeqIO 来解析。这样就避免了 在运行脚本的时候需要重复的下载同样的文件，并减轻NCBI服务器的负载。例如：\nimport os from Bio import SeqIO from Bio import Entrez Entrez.email = \u0026#34;A.N.Other@example.com\u0026#34; # Always tell NCBI who you are filename = \u0026#34;gi_186972394.gbk\u0026#34; if not os.path.isfile(filename): # Downloading... net_handle = Entrez.efetch(db=\u0026#34;nucleotide\u0026#34;,id=\u0026#34;186972394\u0026#34;,rettype=\u0026#34;gb\u0026#34;, retmode=\u0026#34;text\u0026#34;) out_handle = open(filename, \u0026#34;w\u0026#34;) out_handle.write(net_handle.read()) out_handle.close() net_handle.close() print \u0026#34;Saved\u0026#34; print \u0026#34;Parsing...\u0026#34; record = SeqIO.read(filename, \u0026#34;genbank\u0026#34;) print record 为了得到XML格式的输出，你可以使用 Bio.Entrez.read() 函数和参数 retmode=\u0026ldquo;xml\u0026rdquo; 进行解析，：\n\u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; handle = Entrez.efetch(db=\u0026#34;nucleotide\u0026#34;, id=\u0026#34;186972394\u0026#34;, retmode=\u0026#34;xml\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(handle) \u0026gt;\u0026gt;\u0026gt; handle.close() \u0026gt;\u0026gt;\u0026gt; record[0][\u0026#34;GBSeq_definition\u0026#34;] \u0026#39;Selenipedium aequinoctiale maturase K (matK) gene, partial cds; chloroplast\u0026#39; \u0026gt;\u0026gt;\u0026gt; record[0][\u0026#34;GBSeq_source\u0026#34;] \u0026#39;chloroplast Selenipedium aequinoctiale\u0026#39; 就像这样处理数据。例如解析其他数据库特异的文件格式（例如，PubMed中用到的 MEDLINE 格式）\n使用ELink在NCBI Entrez中搜索相关的条目 ELink，在Biopython中是 Bio.Entrez.elink() ，可以用来在NCBI Entrez数据库中寻找相关的条目。例如，你 可以使用它在gene数据库中寻找核苷酸条目，或者其他很酷的事情\n简单的例子（ Bioinformatics 杂志中寻找与Biopython应用相关的文章） 让我们使用ELink来在2009年的 Bioinformatics 杂志中寻找与Biopython应用相关的文章。这篇文章的PubMed ID 是19304878：\n\u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; Entrez.email = \u0026#34;A.N.Other@example.com\u0026#34; \u0026gt;\u0026gt;\u0026gt; pmid = \u0026#34;19304878\u0026#34; \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(Entrez.elink(dbfrom=\u0026#34;pubmed\u0026#34;, id=pmid)) 变量 record 包含了一个Python列表，列出了已经搜索过的数据库。因为我们特指了一个PubMed ID来搜索，所以 record 只包含了一个条目。这个条目是一个字典变量，包含了我们需要寻找的条目的信息，以及能搜索到的所有相关 的内容：\n字典存储的结果 \u0026gt;\u0026gt;\u0026gt; record[0][\u0026#34;DbFrom\u0026#34;] \u0026#39;pubmed\u0026#39; \u0026gt;\u0026gt;\u0026gt; record[0][\u0026#34;IdList\u0026#34;] [\u0026#39;19304878\u0026#39;] 键值LinkSetDb 键 \u0026ldquo;LinkSetDb\u0026rdquo; 包含了搜索结果，将每个目标数据库保存为一个列表。在我们这个搜索中，我们只在PubMed数据库 中找到了结果（尽管已经被分到了不同的分类）：\n\u0026gt;\u0026gt;\u0026gt; len(record[0][\u0026#34;LinkSetDb\u0026#34;]) 5 \u0026gt;\u0026gt;\u0026gt; for linksetdb in record[0][\u0026#34;LinkSetDb\u0026#34;]: ... print linksetdb[\u0026#34;DbTo\u0026#34;], linksetdb[\u0026#34;LinkName\u0026#34;], len(linksetdb[\u0026#34;Link\u0026#34;]) ... pubmed pubmed_pubmed 110 pubmed pubmed_pubmed_combined 6 pubmed pubmed_pubmed_five 6 pubmed pubmed_pubmed_reviews 5 pubmed pubmed_pubmed_reviews_five 5 实际的搜索结果被保存在键值为 \u0026ldquo;Link\u0026rdquo; 的字典下 实际的搜索结果被保存在键值为 \u0026ldquo;Link\u0026rdquo; 的字典下。在标准搜索下，总共找到了110个条目。让我们现在看看我们第一个 搜索结果：\n\u0026gt;\u0026gt;\u0026gt; record[0][\u0026#34;LinkSetDb\u0026#34;][0][\u0026#34;Link\u0026#34;][0] {u\u0026#39;Id\u0026#39;: \u0026#39;19304878\u0026#39;} 这个就是我们搜索的文章，从中并不能看到更多的结果，所以让我们来看看我们的第二个搜索结果：\n\u0026gt;\u0026gt;\u0026gt; record[0][\u0026#34;LinkSetDb\u0026#34;][0][\u0026#34;Link\u0026#34;][1] {u\u0026#39;Id\u0026#39;: \u0026#39;14630660\u0026#39;} 这个PubMed ID为14530660的文章是关于Biopython PDB解析器的。\n\u0026gt;\u0026gt;\u0026gt; for link in record[0][\u0026#34;LinkSetDb\u0026#34;][0][\u0026#34;Link\u0026#34;] : print link[\u0026#34;Id\u0026#34;] 19304878 14630660 18689808 17121776 16377612 12368254 ...... EGQuery:全局搜索-统计搜索的条目 GQuery提供搜索字段在每个Entrez数据库中的数目。当我们只需要知道在每个数据库中能找到的条目的个数， 而不需要知道具体搜索结果的时候，这个非常的有用\nBio.Entrz.egquery()获取跟\u0026quot;Biopython\u0026quot; \u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; Entrez.email = \u0026#34;A.N.Other@example.com\u0026#34; # Always tell NCBI who you are \u0026gt;\u0026gt;\u0026gt; handle = Entrez.egquery(term=\u0026#34;biopython\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(handle) \u0026gt;\u0026gt;\u0026gt; for row in record[\u0026#34;eGQueryResult\u0026#34;]: print row[\u0026#34;DbName\u0026#34;], row[\u0026#34;Count\u0026#34;] ... pubmed 6 pmc 62 journals 0 ... ESpell获得拼写建议 使用Bio.Entrez.espell()获得Biopython正确的拼写 \u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; Entrez.email = \u0026#34;A.N.Other@example.com\u0026#34; # Always tell NCBI who you are \u0026gt;\u0026gt;\u0026gt; handle = Entrez.espell(term=\u0026#34;biopythooon\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(handle) \u0026gt;\u0026gt;\u0026gt; record[\u0026#34;Query\u0026#34;] \u0026#39;biopythooon\u0026#39; \u0026gt;\u0026gt;\u0026gt; record[\u0026#34;CorrectedQuery\u0026#34;] \u0026#39;biopython\u0026#39; 解析大的Entrez XML文件 Entrez.read 函数将Entrez返回的结果读取到一个Python对象里面去，这个对象被保存在内存中。对于解析太大的 XML文件而内存不够时，可以使用 Entrez.parse 这个函数。这是一个生成器函数，它将一个一个的读取XML文件里面的内容。只有XML 文件是一个列表对象的时候，这个函数才有用（换句话说，如果在一个内存无限的计算机上 Entrez.read 将返回一个 Python列表）。\n例如，你可以通过NCBI的FTP站点从Entrez Gene 数据库中下载某个物种全部的条目作为一个文件。这个文件可能很大。 作为一个例子，在2009年9月4日，文件 Homo_sapiens.ags.gz 包含了Entrez Gene数据库中人的序列，文件大小 有116576kB。这个文件是 ASN 格式，可以通过NCBI的 gene2xml 程序转成XML格式（请到NCBI的FTP站点获取 更多的信息）：\ngene2xml -b T -i Homo_sapiens.ags -o Homo_sapiens.xml XML结果文件有6.1GB. 在大多数电脑上尝试 Entrez.read 都会导致 MemoryError 。\nXML文件 Homo_sapiens.xml 包含了一个Entrez gene记录的列表，每个对应于人的一个Entrez基因信息。 Entrez.parse 将一个一个的读取这些记录。这样你可以通过遍历每个记录的方式打印或者存储每个记录相关的信息。例如，下面这个脚本 遍历了Entrez基因里面的记录，打印了每个基因的数目和名字：\n\u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; handle = open(\u0026#34;Homo_sapiens.xml\u0026#34;) \u0026gt;\u0026gt;\u0026gt; records = Entrez.parse(handle) \u0026gt;\u0026gt;\u0026gt; for record in records: ... status = record[\u0026#39;Entrezgene_track-info\u0026#39;][\u0026#39;Gene-track\u0026#39;][\u0026#39;Gene-track_status\u0026#39;] ... if status.attributes[\u0026#39;value\u0026#39;]==\u0026#39;discontinued\u0026#39;: ... continue ... geneid = record[\u0026#39;Entrezgene_track-info\u0026#39;][\u0026#39;Gene-track\u0026#39;][\u0026#39;Gene-track_geneid\u0026#39;] ... genename = record[\u0026#39;Entrezgene_gene\u0026#39;][\u0026#39;Gene-ref\u0026#39;][\u0026#39;Gene-ref_locus\u0026#39;] ... print geneid, genename 将会打印以下内容:\n1 A1BG 2 A2M 3 A2MP 8 AA 9 NAT1 10 NAT2 11 AACP 12 SERPINA3 13 AADAC 14 AAMP 15 AANAT 16 AARS 17 AAVS1 ... 解析GEO记录 GEO ( Gene Expression Omnibus ) 是高通量基因表达和杂交芯片 数据的数据库。 Bio.Geo 模块可以用来解析GEO格式的数据\n下面的代码展示了怎样将一个名称为 GSE16.txt 的GEO文件存进一个记录，并打印该记录：\n\u0026gt;\u0026gt;\u0026gt; from Bio import Geo \u0026gt;\u0026gt;\u0026gt; handle = open(\u0026#34;GSE16.txt\u0026#34;) \u0026gt;\u0026gt;\u0026gt; records = Geo.parse(handle) \u0026gt;\u0026gt;\u0026gt; for record in records: ... print record 你可以使用 ESearch 来搜索 “gds” 数据库 (GEO 数据集) :\n\u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; Entrez.email = \u0026#34;A.N.Other@example.com\u0026#34; # Always tell NCBI who you are \u0026gt;\u0026gt;\u0026gt; handle = Entrez.esearch(db=\u0026#34;gds\u0026#34;,term=\u0026#34;GSE16\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(handle) \u0026gt;\u0026gt;\u0026gt; record[\u0026#34;Count\u0026#34;] 2 \u0026gt;\u0026gt;\u0026gt; record[\u0026#34;IdList\u0026#34;] [\u0026#39;200000016\u0026#39;, \u0026#39;100000028\u0026#39;] 使用代理 通常状况下，你不需要使用代理，但是如果你的网络有问题的时候，我们有以下应对方法。在内部， Bio.Entrez 使用 一个标准的 Python 库 urllib 来访问 NCBI的服务器。这个将检查叫做 http_proxy 的环境变量来自动配置简单 的代理服务。不幸的是，这个模块不支持需要认证的代理。\n你可以选择设定环境变量 http_proxy 。同样，你可以在Python脚本开头的地方设置这个参数，例如：\nimport os os.environ[\u0026#34;http_proxy\u0026#34;] = \u0026#34;http://proxyhost.example.com:8080\u0026#34; 实例 PubMed和Medline 在这个例子当中，我们要查询PubMed当中所有跟Orchids相关的文章,我们首先看看有多少这样的文章：\n先看下有多少这样的文章 \u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; Entrez.email = \u0026#34;A.N.Other@example.com\u0026#34; # Always tell NCBI who you are \u0026gt;\u0026gt;\u0026gt; handle = Entrez.egquery(term=\u0026#34;orchid\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(handle) \u0026gt;\u0026gt;\u0026gt; for row in record[\u0026#34;eGQueryResult\u0026#34;]: ... if row[\u0026#34;DbName\u0026#34;]==\u0026#34;pubmed\u0026#34;: ... print row[\u0026#34;Count\u0026#34;] 463 使用Bio.Entrez.efetch（）下载上述PubMed IDs \u0026gt;\u0026gt;\u0026gt; handle = Entrez.esearch(db=\u0026#34;pubmed\u0026#34;, term=\u0026#34;orchid\u0026#34;, retmax=463) \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(handle) \u0026gt;\u0026gt;\u0026gt; idlist = record[\u0026#34;IdList\u0026#34;] \u0026gt;\u0026gt;\u0026gt; print idlist 返回值是一个Python列表，包含了所有和orchids相关文章的PubMed IDs：\n[\u0026#39;18680603\u0026#39;, \u0026#39;18665331\u0026#39;, \u0026#39;18661158\u0026#39;, \u0026#39;18627489\u0026#39;, \u0026#39;18627452\u0026#39;, \u0026#39;18612381\u0026#39;, \u0026#39;18594007\u0026#39;, \u0026#39;18591784\u0026#39;, \u0026#39;18589523\u0026#39;, \u0026#39;18579475\u0026#39;, \u0026#39;18575811\u0026#39;, \u0026#39;18575690\u0026#39;, ... 这样我们就得到了这些信息，显然我们想要得到对应的Medline records和更多额外的信息。这里，我们将以纯文本的 形式下载和Medline records相关的信息，然后使用 Bio.Medline 模块来解析他们：\n\u0026gt;\u0026gt;\u0026gt; from Bio import Medline \u0026gt;\u0026gt;\u0026gt; handle = Entrez.efetch(db=\u0026#34;pubmed\u0026#34;, id=idlist, rettype=\u0026#34;medline\u0026#34;, retmode=\u0026#34;text\u0026#34;) \u0026gt;\u0026gt;\u0026gt; records = Medline.parse(handle) 请记住 records 是一个迭代器，所以你只能访问这些records一次。如果你想保存这些records，你需要把他们转成列表：\n\u0026gt;\u0026gt;\u0026gt; records = list(records) 现在让我们迭代这些records，然后分别打印每一个record的信息：\n\u0026gt;\u0026gt;\u0026gt; for record in records: ... print \u0026#34;title:\u0026#34;, record.get(\u0026#34;TI\u0026#34;, \u0026#34;?\u0026#34;) ... print \u0026#34;authors:\u0026#34;, record.get(\u0026#34;AU\u0026#34;, \u0026#34;?\u0026#34;) ... print \u0026#34;source:\u0026#34;, record.get(\u0026#34;SO\u0026#34;, \u0026#34;?\u0026#34;) ... print 这个的输出结果是这样的:\n特别有意思的是作者的列表，作者的列表会作为一个标准的Python列表返回。这使得用标准的Python工具操作和搜索 变得简单。例如，我们可以像下面的代码这样循环读取所有条目来查找某个特定的作者：\n\u0026gt;\u0026gt;\u0026gt; search_author = \u0026#34;Waits T\u0026#34; \u0026gt;\u0026gt;\u0026gt; for record in records: ... if not \u0026#34;AU\u0026#34; in record: ... continue ... if search_author in record[\u0026#34;AU\u0026#34;]: ... print \u0026#34;Author %s found: %s\u0026#34; % (search_author, record[\u0026#34;SO\u0026#34;]) 搜索、下载、和解析Entrez核酸记录 这里我们将展示一个关于远程Entrez查询的简单例子。我们讲到了使用NCBI 的Entrez网站来搜索 NCBI 的核酸数据库来获得关于Cypripedioideae的信息。现在我们看看如何使用Python脚本 自动的处理。在这个例子当中，我们仅仅展示如何使用Entrez模块来连接，获取结果，解析他们。\n在下载这些结果之前，使用EGQuery来计算结果的数目 首先，我们在下载这些结果之前，使用EGQuery来计算结果的数目。EGQuery 将会告诉我们在每个数据库中分别有多少 搜索结果，但在我们这个例子当中，我们只对核苷酸感兴趣：\n\u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; Entrez.email = \u0026#34;A.N.Other@example.com\u0026#34; # Always tell NCBI who you are \u0026gt;\u0026gt;\u0026gt; handle = Entrez.egquery(term=\u0026#34;Cypripedioideae\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(handle) \u0026gt;\u0026gt;\u0026gt; for row in record[\u0026#34;eGQueryResult\u0026#34;]: ... if row[\u0026#34;DbName\u0026#34;]==\u0026#34;nuccore\u0026#34;: ... print row[\u0026#34;Count\u0026#34;] 814 所以，我们预期能找到814个 Entrez 核酸记录（这是我在2008年得到的结果；在未来这个结果应该会增加）。如果你得 到了高的不可思议的结果数目时，你可能得重新考虑是否需要下载所有的这些结果，下载是我们的下一步\n下载数据 \u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; handle = Entrez.esearch(db=\u0026#34;nucleotide\u0026#34;, term=\u0026#34;Cypripedioideae\u0026#34;, retmax=814) \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(handle) 在这里, record 是一个包含了搜索结果和一些辅助信息的Python字典。仅仅作为参考信息，让我们看看在这些字典当中 究竟存储了些什么内容：\n\u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; handle = Entrez.esearch(db=\u0026#34;nucleotide\u0026#34;, term=\u0026#34;Cypripedioideae\u0026#34;, retmax=814) \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(handle) 在这里, record 是一个包含了搜索结果和一些辅助信息的Python字典。仅仅作为参考信息，让我们看看在这些字典当中 究竟存储了些什么内容：\n\u0026gt;\u0026gt;\u0026gt; print record.keys() [u\u0026#39;Count\u0026#39;, u\u0026#39;RetMax\u0026#39;, u\u0026#39;IdList\u0026#39;, u\u0026#39;TranslationSet\u0026#39;, u\u0026#39;RetStart\u0026#39;, u\u0026#39;QueryTranslation\u0026#39;] 首先, 让我们检查看看我们得到了多少个结果:\n\u0026gt;\u0026gt;\u0026gt; print record[\u0026#34;Count\u0026#34;] \u0026#39;814\u0026#39; 这个结果是我们所期望的。这814个结果被存在了 record[\u0026lsquo;IdList\u0026rsquo;] 中:\n\u0026gt;\u0026gt;\u0026gt; print len(record[\u0026#34;IdList\u0026#34;]) 814 让我们看看前五个结果:\n\u0026gt;\u0026gt;\u0026gt; print record[\u0026#34;IdList\u0026#34;][:5] [\u0026#39;187237168\u0026#39;, \u0026#39;187372713\u0026#39;, \u0026#39;187372690\u0026#39;, \u0026#39;187372688\u0026#39;, \u0026#39;187372686\u0026#39;] 我们可以使用 efetch 来下载这些结果. 尽管你可以一个一个的下载这些记录，但为了减少 NCBI 服务器的负载，最好呢还是 一次性的下载所有的结果。\n\u0026gt;\u0026gt;\u0026gt; idlist = \u0026#34;,\u0026#34;.join(record[\u0026#34;IdList\u0026#34;][:5]) \u0026gt;\u0026gt;\u0026gt; print idlist 187237168,187372713,187372690,187372688,187372686 \u0026gt;\u0026gt;\u0026gt; handle = Entrez.efetch(db=\u0026#34;nucleotide\u0026#34;, id=idlist, retmode=\u0026#34;xml\u0026#34;) \u0026gt;\u0026gt;\u0026gt; records = Entrez.read(handle) \u0026gt;\u0026gt;\u0026gt; print len(records) 5 每个这样的records对应一个GenBank record.\n\u0026gt;\u0026gt;\u0026gt; print records[0].keys() [u\u0026#39;GBSeq_moltype\u0026#39;, u\u0026#39;GBSeq_source\u0026#39;, u\u0026#39;GBSeq_sequence\u0026#39;, u\u0026#39;GBSeq_primary-accession\u0026#39;, u\u0026#39;GBSeq_definition\u0026#39;, u\u0026#39;GBSeq_accession-version\u0026#39;, u\u0026#39;GBSeq_topology\u0026#39;, u\u0026#39;GBSeq_length\u0026#39;, u\u0026#39;GBSeq_feature-table\u0026#39;, u\u0026#39;GBSeq_create-date\u0026#39;, u\u0026#39;GBSeq_other-seqids\u0026#39;, u\u0026#39;GBSeq_division\u0026#39;, u\u0026#39;GBSeq_taxonomy\u0026#39;, u\u0026#39;GBSeq_references\u0026#39;, u\u0026#39;GBSeq_update-date\u0026#39;, u\u0026#39;GBSeq_organism\u0026#39;, u\u0026#39;GBSeq_locus\u0026#39;, u\u0026#39;GBSeq_strandedness\u0026#39;] \u0026gt;\u0026gt;\u0026gt; print records[0][\u0026#34;GBSeq_primary-accession\u0026#34;] DQ110336 \u0026gt;\u0026gt;\u0026gt; print records[0][\u0026#34;GBSeq_other-seqids\u0026#34;] [\u0026#39;gb|DQ110336.1|\u0026#39;, \u0026#39;gi|187237168\u0026#39;] \u0026gt;\u0026gt;\u0026gt; print records[0][\u0026#34;GBSeq_definition\u0026#34;] Cypripedium calceolus voucher Davis 03-03 A maturase (matR) gene, partial cds; mitochondrial \u0026gt;\u0026gt;\u0026gt; print records[0][\u0026#34;GBSeq_organism\u0026#34;] Cypripedium calceolus 搜索、下载和解析GenBank record GenBank record 格式是保存序列信息、序列特征和其他相关信息非常普遍的一种方法。这种格式是从 NCBI 数据库 http://www.ncbi.nlm.nih.gov/ 获取信息非常好的一种方式 .\n在这个例子当中，我们将展示怎样去查询 NCBI 数据库，根据query提取记录，然后使用 Bio.SeqIO 解析他们 —— 在中提到过这些。简单起见，\n首先，我们想要查询找出要获取的记录的ID。这里我们快速的检索我们最喜欢的一个物种 Opuntia (多刺的梨型仙人掌)。我们 可以做一个快速的检索来获得所有满足要求的GIs（GenBank标志符）。首先我们看看有多少个记录：\n\u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; Entrez.email = \u0026#34;A.N.Other@example.com\u0026#34; # Always tell NCBI who you are \u0026gt;\u0026gt;\u0026gt; handle = Entrez.egquery(term=\u0026#34;Opuntia AND rpl16\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(handle) \u0026gt;\u0026gt;\u0026gt; for row in record[\u0026#34;eGQueryResult\u0026#34;]: ... if row[\u0026#34;DbName\u0026#34;]==\u0026#34;nuccore\u0026#34;: ... print row[\u0026#34;Count\u0026#34;] ... 9 现在我们下载GenBank identifiers的列表：\n\u0026gt;\u0026gt;\u0026gt; handle = Entrez.esearch(db=\u0026#34;nuccore\u0026#34;, term=\u0026#34;Opuntia AND rpl16\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(handle) \u0026gt;\u0026gt;\u0026gt; gi_list = record[\u0026#34;IdList\u0026#34;] \u0026gt;\u0026gt;\u0026gt; gi_list [\u0026#39;57240072\u0026#39;, \u0026#39;57240071\u0026#39;, \u0026#39;6273287\u0026#39;, \u0026#39;6273291\u0026#39;, \u0026#39;6273290\u0026#39;, \u0026#39;6273289\u0026#39;, \u0026#39;6273286\u0026#39;, \u0026#39;6273285\u0026#39;, \u0026#39;6273284\u0026#39;] 现在我们使用这些GIs来下载GenBank records —— 注意在老的Biopython版本中，你必须将GI号用逗号隔开传递给Entrez，例如 在 Biopython 1.59中，你可以传递一个列表，下面的内容会为你做转换：\n\u0026gt;\u0026gt;\u0026gt; gi_str = \u0026#34;,\u0026#34;.join(gi_list) \u0026gt;\u0026gt;\u0026gt; handle = Entrez.efetch(db=\u0026#34;nuccore\u0026#34;, id=gi_str, rettype=\u0026#34;gb\u0026#34;, retmode=\u0026#34;text\u0026#34;) 如果你想看原始的 GenBank 文件，你可以从这个句柄中读取并打印结果：\n\u0026gt;\u0026gt;\u0026gt; text = handle.read() \u0026gt;\u0026gt;\u0026gt; print text LOCUS AY851612 892 bp DNA linear PLN 10-APR-2007 DEFINITION Opuntia subulata rpl16 gene, intron; chloroplast. ACCESSION AY851612 VERSION AY851612.1 GI:57240072 KEYWORDS . SOURCE chloroplast Austrocylindropuntia subulata ORGANISM Austrocylindropuntia subulata Eukaryota; Viridiplantae; Streptophyta; Embryophyta; Tracheophyta; Spermatophyta; Magnoliophyta; eudicotyledons; core eudicotyledons; Caryophyllales; Cactaceae; Opuntioideae; Austrocylindropuntia. REFERENCE 1 (bases 1 to 892) AUTHORS Butterworth,C.A. and Wallace,R.S. ... 在这个例子当中，我们只是得到了原始的记录。为了得到对Python友好的格式，我们可以使用 Bio.SeqIO 将GenBank 数据转化成 SeqRecord 对象，包括 SeqFeature 对象\n\u0026gt;\u0026gt;\u0026gt; from Bio import SeqIO \u0026gt;\u0026gt;\u0026gt; handle = Entrez.efetch(db=\u0026#34;nuccore\u0026#34;, id=gi_str, rettype=\u0026#34;gb\u0026#34;, retmode=\u0026#34;text\u0026#34;) \u0026gt;\u0026gt;\u0026gt; records = SeqIO.parse(handle, \u0026#34;gb\u0026#34;) 我们现在可以逐个查看这些record来寻找我们感兴趣的信息：\n\u0026gt;\u0026gt;\u0026gt; for record in records: \u0026gt;\u0026gt;\u0026gt; ... print \u0026#34;%s, length %i, with %i features\u0026#34; \\ \u0026gt;\u0026gt;\u0026gt; ... % (record.name, len(record), len(record.features)) AY851612, length 892, with 3 features AY851611, length 881, with 3 features AF191661, length 895, with 3 features AF191665, length 902, with 3 features AF191664, length 899, with 3 features AF191663, length 899, with 3 features AF191660, length 893, with 3 features AF191659, length 894, with 3 features AF191658, length 896, with 3 features 最后，如果你计划重复你的分析，你应该下载这些record 一次 ，然后将他们保存在你的硬盘里，在本地进行分析；而不是 从 NCBI 下载之后就马上进行分析（像这个例子一样）\n查看物种的谱系关系 仍然以植物为例子，让我们找出Cyripedioideae兰花家族的谱系。首先让我们在Taxonomy数据库中查找跟Cypripedioideae 相关的记录，确实找到了一个确切的 NCBI taxonomy 标识号：\n\u0026gt;\u0026gt;\u0026gt; from Bio import Entrez \u0026gt;\u0026gt;\u0026gt; Entrez.email = \u0026#34;A.N.Other@example.com\u0026#34; # Always tell NCBI who you are \u0026gt;\u0026gt;\u0026gt; handle = Entrez.esearch(db=\u0026#34;Taxonomy\u0026#34;, term=\u0026#34;Cypripedioideae\u0026#34;) \u0026gt;\u0026gt;\u0026gt; record = Entrez.read(handle) \u0026gt;\u0026gt;\u0026gt; record[\u0026#34;IdList\u0026#34;] [\u0026#39;158330\u0026#39;] \u0026gt;\u0026gt;\u0026gt; record[\u0026#34;IdList\u0026#34;][0] \u0026#39;158330\u0026#39; 现在，我们使用 efetch 从 Taxonomy 数据库中下载这些条目，然后解析它：\n\u0026gt;\u0026gt;\u0026gt; handle = Entrez.efetch(db=\u0026#34;Taxonomy\u0026#34;, id=\u0026#34;158330\u0026#34;, retmode=\u0026#34;xml\u0026#34;) \u0026gt;\u0026gt;\u0026gt; records = Entrez.read(handle) 再次，这个record保存了许多的信息：\n\u0026gt;\u0026gt;\u0026gt; records[0].keys() [u\u0026#39;Lineage\u0026#39;, u\u0026#39;Division\u0026#39;, u\u0026#39;ParentTaxId\u0026#39;, u\u0026#39;PubDate\u0026#39;, u\u0026#39;LineageEx\u0026#39;, u\u0026#39;CreateDate\u0026#39;, u\u0026#39;TaxId\u0026#39;, u\u0026#39;Rank\u0026#39;, u\u0026#39;GeneticCode\u0026#39;, u\u0026#39;ScientificName\u0026#39;, u\u0026#39;MitoGeneticCode\u0026#39;, u\u0026#39;UpdateDate\u0026#39;] 我们可以直接从这个record获得谱系信息：\n\u0026gt;\u0026gt;\u0026gt; records[0][\u0026#34;Lineage\u0026#34;] \u0026#39;cellular organisms; Eukaryota; Viridiplantae; Streptophyta; Streptophytina; Embryophyta; Tracheophyta; Euphyllophyta; Spermatophyta; Magnoliophyta; Liliopsida; Asparagales; Orchidaceae\u0026#39; ","date":"2021-08-10T00:00:00Z","permalink":"https://example.com/p/ch9_%E8%AE%BF%E9%97%AEncbi_entrez%E6%95%B0%E6%8D%AE%E5%BA%93/","title":"ch9_访问NCBI_Entrez数据库"},{"content":"biopython官方地址：https://biopython.org/\ngithub地址：https://github.com/biopython/biopython/blob/master/\n中文版教程：https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html\nbiopython包的说明（具体到每个模块了）：https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html\n生物序列的鉴定是生物信息工作的主要部分。有几个工具像BLAST（可能是最流行 的），FASTA ，HMMER还有许多其它的都有这个功能，每个工具都有独特的算法和 途径。一般来说，这些工具都是用你的序列在数据库中搜索可能的匹配。随着序列 数量的增加（匹配数也会随之增加），将会有成百上千的可能匹配，解析这些结果 无疑变得越来越困难。理所当然，人工解析搜索结果变得不可能。而且你可能会同 时用几种不同的搜索工具，每种工具都有独特的统计方法、规则和输出格式。可以 想象，同时用多种工具搜索多条序列是多么恐怖的事。 我们对此非常了解，所以我们在Biopython构建了 Bio.SearchIO 亚模块。Bio.SearchIO 模块使从搜索结果中提取信息变得简单，并且可以处理不同工具 的不同标准和规则。SearchIO 和BioPerl中模块名称一致。 在本章中，我们将学习 Bio.SearchIO 的主要功能，了解它可以做什么。我 们将使用两个主要的搜索工具：BLAST和FASTA。它们只是用来阐明思路，你可以轻 易地把工作流程应用到 Bio.SearchIO 支持的其他工具中\nSearchIO对象模型 尽管多数搜索工具的输出风格极为不同，但是它们蕴含的理念很相似：\n输出文件可能包含一条或更多的搜索查询的结果。 在每次查询中，你会在给定的数据库中得到一个或更多的hit（命中）。 在每个hit中，你会得到一个或更多包含query（查询)序列和数据库序列实际比对的区域。 一些工具如BLAT和Exonerate可能会把这些区域分成几个比对片段（或在BLAT中 称为区块，在Exonerate中称为可能外显子）。这并不是很常见，像BLAST和 HMMER就不这么做。 一些名词介绍 知道这些共性之后，我们决定把它们作为创造 Bio.SearchIO 对象模型的基础。对 象模型是Python对象组成的嵌套分级系统，每个对象都代表一个上面列出的概念。这些对 象是：\nQueryResult，代表单个搜索查询。 Hit，代表单个的数据库hit。Hit 对象包含在 QueryResult 中， 每个 QueryResult 中有0个或多个 Hit 对象。 HSP (high-scoring pair（高分片段）)，代表query和hit序列中有 意义比对的区域。HSP 对象包含在 Hit 对象中，而且每个 Hit 有一个 或更多的 HSP 对象。 HSPFragment，代表query和hit序列中单个的邻近比对。 HSPFragment 对象包含在 HSP 对象中。多数的搜索工具如BLAST和HMMER把 HSP 和 HSPFragment 合并，因为一个 HSP 只含有一个 HSPFragment。但是 像BLAT和Exonerate会产生含有多个 HSPFragment 的 HSP 。似乎有些困 惑？不要紧，稍后我们将详细介绍这两个对象。 这四个对象是当你用 Bio.SearchIO 会碰到的。 Bio.SearchIO 四 个主要方法： read ，parse，index 或 index_db 中任意一个都可 以产生这四个对象。这些方法的会在后面的部分详细介绍。这部分只会用到 read 和 parse ，这两个方法和 Bio.SeqIO 以及 Bio.AlignIO 中的 read 和 parse 方法功 能相似： read 用于单query对输出文件进行搜索并且返回一个 QueryResult 对象。 parse 用于多query对输出文件进行搜索并且返回一个可以yield QueryResult 对象的generator。 完成这些之后，让我们开始学习每个 Bio.SearchIO 对象，从 QueryResult 开始。 QueryResult QueryResult，代表单query搜索，每个 QueryResult 中有0个或多个 Hit 对象。我们来看看BLAST文件是什么样的：\n\u0026gt;\u0026gt;\u0026gt; from Bio import SearchIO \u0026gt;\u0026gt;\u0026gt; blast_qresult = SearchIO.read(\u0026#39;my_blast.xml\u0026#39;, \u0026#39;blast-xml\u0026#39;) \u0026gt;\u0026gt;\u0026gt; print blast_qresult Program: blastn (2.2.27+) Query: 42291 (61) mystery_seq Target: refseq_rna Hits: ---- ----- ---------------------------------------------------------- # # HSP ID + description ---- ----- ---------------------------------------------------------- 0 1 gi|262205317|ref|NR_030195.1| Homo sapiens microRNA 52... 1 1 gi|301171311|ref|NR_035856.1| Pan troglodytes microRNA... 2 1 gi|270133242|ref|NR_032573.1| Macaca mulatta microRNA ... 3 2 gi|301171322|ref|NR_035857.1| Pan troglodytes microRNA... 4 1 gi|301171267|ref|NR_035851.1| Pan troglodytes microRNA... 5 2 gi|262205330|ref|NR_030198.1| Homo sapiens microRNA 52... 6 1 gi|262205302|ref|NR_030191.1| Homo sapiens microRNA 51... 7 1 gi|301171259|ref|NR_035850.1| Pan troglodytes microRNA... 8 1 gi|262205451|ref|NR_030222.1| Homo sapiens microRNA 51... 9 2 gi|301171447|ref|NR_035871.1| Pan troglodytes microRNA... 10 1 gi|301171276|ref|NR_035852.1| Pan troglodytes microRNA... 11 1 gi|262205290|ref|NR_030188.1| Homo sapiens microRNA 51... 12 1 gi|301171354|ref|NR_035860.1| Pan troglodytes microRNA... 13 1 gi|262205281|ref|NR_030186.1| Homo sapiens microRNA 52... 14 2 gi|262205298|ref|NR_030190.1| Homo sapiens microRNA 52... 15 1 gi|301171394|ref|NR_035865.1| Pan troglodytes microRNA... 16 1 gi|262205429|ref|NR_030218.1| Homo sapiens microRNA 51... 17 1 gi|262205423|ref|NR_030217.1| Homo sapiens microRNA 52... 18 1 gi|301171401|ref|NR_035866.1| Pan troglodytes microRNA... 19 1 gi|270133247|ref|NR_032574.1| Macaca mulatta microRNA ... 20 1 gi|262205309|ref|NR_030193.1| Homo sapiens microRNA 52... 21 2 gi|270132717|ref|NR_032716.1| Macaca mulatta microRNA ... 22 2 gi|301171437|ref|NR_035870.1| Pan troglodytes microRNA... 23 2 gi|270133306|ref|NR_032587.1| Macaca mulatta microRNA ... 24 2 gi|301171428|ref|NR_035869.1| Pan troglodytes microRNA... 25 1 gi|301171211|ref|NR_035845.1| Pan troglodytes microRNA... 26 2 gi|301171153|ref|NR_035838.1| Pan troglodytes microRNA... 27 2 gi|301171146|ref|NR_035837.1| Pan troglodytes microRNA... 28 2 gi|270133254|ref|NR_032575.1| Macaca mulatta microRNA ... 29 2 gi|262205445|ref|NR_030221.1| Homo sapiens microRNA 51... ~~~ 97 1 gi|356517317|ref|XM_003527287.1| PREDICTED: Glycine ma... 98 1 gi|297814701|ref|XM_002875188.1| Arabidopsis lyrata su... 99 1 gi|397513516|ref|XM_003827011.1| PREDICTED: Pan panisc... 虽然我们才接触对象模型的皮毛，但是你已经可以看到一些有用的信息了。通过调用QueryResult 对象的 print 方法，你可以看到：\n程序的名称和版本 (blastn version 2.2.27+) query的ID，描述和序列长度(ID是42291，描述是 ‘mystery_seq’，长度是61) 搜索的目标数据库 (refseq_rna) hit结果的快速预览。对于我们的查询序列，有100个可能的hit（表格中表示的是 0-99）对于每个hit，我们可以看到它包含的高分比对片段（HSP)，ID和一个片 段描述。注意， Bio.SearchIO 截断了表格，只显示0-29，然后是97-99。 现在让我们用同样的步骤来检查BLAT的结果： \u0026gt;\u0026gt;\u0026gt; blat_qre \u0026gt;\u0026gt;\u0026gt; sult = SearchIO.read(\u0026#39;my_blat.psl\u0026#39;, \u0026#39;blat-psl\u0026#39;) \u0026gt;\u0026gt;\u0026gt; print blat_qresult Program: blat (\u0026lt;unknown version\u0026gt;) Query: mystery_seq (61) \u0026lt;unknown description\u0026gt; Target: \u0026lt;unknown target\u0026gt; Hits: ---- ----- ---------------------------------------------------------- # # HSP ID + description ---- ----- ---------------------------------------------------------- 0 17 chr19 \u0026lt;unknown description\u0026gt; 马上可以看到一些不同点。有些是由于BLAT使用PSL格式储存它的信息，稍后会看 到。其余是由于BLAST和BLAT搜索的程序和数据库之间明显的差异造成的：\n程序名称和版本。 Bio.SearchIO 知道程序是BLAST，但是在输出文件中没 有信息显示程序版本，所以默认是 inknown query的ID，描述和序列的长度。注意，这些细节和BLAST的细节只有细小的差别， ID是 ‘mystery_seq’ 而不是42991，缺少描述，但是序列长度仍是61。这 实际上是文件格式本身导致的差异。BLAST有时创建自己的query ID并且用你的原 始ID作为序列描述。 目标数据库是未知的，因为BLAT输出文件没提到相关信息。 最后，hit列表完全不同。这里，我们的查询序列只命中到 ‘chr19’ 数据库条 目，但是我们可以看到它含有17个HSP区域。这并不让人诧异，考虑到我们 使用的是不同的程序，并且这些程序都有自己的数据库。 所有通过调用 print 方法看到的信息都可以单独地用Python的对象属性获得（又叫点标记法）。同样还可以用相同的方法获得其他格式特有的属性。 \u0026gt;\u0026gt;\u0026gt; print \u0026#34;%s %s\u0026#34; % (blast_qresult.program, blast_qresult.version) blastn 2.2.27+ \u0026gt;\u0026gt;\u0026gt; print \u0026#34;%s %s\u0026#34; % (blat_qresult.program, blat_qresult.version) blat \u0026lt;unknown version\u0026gt; \u0026gt;\u0026gt;\u0026gt; blast_qresult.param_evalue_threshold # blast-xml specific 10.0 想获得一个可访问属性的完整列表，可以查询每个格式特有的文档。这些是 BLAST BLAT. 已经知道了在 QueryResult 对象上可以调用 print 方法，让我们研究的更深 一些。 QueryResult 到底是什么？就Python对象来说， QueryResult 混合 了列表和字典的特性。换句话说，也就是一个包含了列表和字典功能的容器对象。 和列表以及字典一样， QueryResult 对象是可迭代的。每次迭代返回一个hit 对象：\n\u0026gt;\u0026gt;\u0026gt; for hit in blast_qresult: ... hit Hit(id=\u0026#39;gi|262205317|ref|NR_030195.1|\u0026#39;, query_id=\u0026#39;42291\u0026#39;, 1 hsps) Hit(id=\u0026#39;gi|301171311|ref|NR_035856.1|\u0026#39;, query_id=\u0026#39;42291\u0026#39;, 1 hsps) Hit(id=\u0026#39;gi|270133242|ref|NR_032573.1|\u0026#39;, query_id=\u0026#39;42291\u0026#39;, 1 hsps) Hit(id=\u0026#39;gi|301171322|ref|NR_035857.1|\u0026#39;, query_id=\u0026#39;42291\u0026#39;, 2 hsps) Hit(id=\u0026#39;gi|301171267|ref|NR_035851.1|\u0026#39;, query_id=\u0026#39;42291\u0026#39;, 1 hsps) ... 要得到 QueryResult 对象有多少hit，可以简单调用Python的 len 方法：\n\u0026gt;\u0026gt;\u0026gt; len(blast_qresult) 100 \u0026gt;\u0026gt;\u0026gt; len(blat_qresult) 1 同列表类似，你可以用切片来获得 QueryResult 对象的hit：\n\u0026gt;\u0026gt;\u0026gt; blast_qresult[0] # retrieves the top hit Hit(id=\u0026#39;gi|262205317|ref|NR_030195.1|\u0026#39;, query_id=\u0026#39;42291\u0026#39;, 1 hsps) \u0026gt;\u0026gt;\u0026gt; blast_qresult[-1] # retrieves the last hit Hit(id=\u0026#39;gi|397513516|ref|XM_003827011.1|\u0026#39;, query_id=\u0026#39;42291\u0026#39;, 1 hsps) 要得到多个hit，你同样可以对 QueryResult 对象作切片。这种情况下，返回一个包含被切hit的新 QueryResult 对象：\n\u0026gt;\u0026gt;\u0026gt; blast_slice = blast_qresult[:3] # slices the first three hits \u0026gt;\u0026gt;\u0026gt; print blast_slice Program: blastn (2.2.27+) Query: 42291 (61) mystery_seq Target: refseq_rna Hits: ---- ----- ---------------------------------------------------------- # # HSP ID + description ---- ----- ---------------------------------------------------------- 0 1 gi|262205317|ref|NR_030195.1| Homo sapiens microRNA 52... 1 1 gi|301171311|ref|NR_035856.1| Pan troglodytes microRNA... 2 1 gi|270133242|ref|NR_032573.1| Macaca mulatta microRNA ... 同字典类似，可以通过ID获取hit。如果你知道一个特定的hit ID存在于一个搜索结果中时，特别有用：\n\u0026gt;\u0026gt;\u0026gt; blast_qresult[\u0026#39;gi|262205317|ref|NR_030195.1|\u0026#39;] Hit(id=\u0026#39;gi|262205317|ref|NR_030195.1|\u0026#39;, query_id=\u0026#39;42291\u0026#39;, 1 hsps) 你可以用 hits 方法获得完整的 Hit 对象，也可以用 hit_keys 方法获得完整的 Hit ID：\n\u0026gt;\u0026gt;\u0026gt; blast_qresult.hits [...] # list of all hits \u0026gt;\u0026gt;\u0026gt; blast_qresult.hit_keys [...] # list of all hit IDs 如果你想确定一个特定的hit是否存在于查询结果中该怎么做呢？可以用 in 关键字作一个简单的成员检验：\n\u0026gt;\u0026gt;\u0026gt; \u0026#39;gi|262205317|ref|NR_030195.1|\u0026#39; in blast_qresult True \u0026gt;\u0026gt;\u0026gt; \u0026#39;gi|262205317|ref|NR_030194.1|\u0026#39; in blast_qresult False 有时候，只知道一个hit是否存在是不够的，你可能也会想知道hit的排名。 index 方法可以帮助你：\n\u0026gt;\u0026gt;\u0026gt; blast_qresult.index(\u0026#39;gi|301171437|ref|NR_035870.1|\u0026#39;) 22 记住，我们用的是Python风格的索引，是从0开始。这代表hit的排名是23而不是22。 同样，注意你看的hit排名是基于原始搜索输出文件的本来顺序。不同的搜索工具可 能会基于不同的标准排列hit。 如果原本的hit排序不合你意，可以用 QueryResult 对象的 sort 方法。 它和Python的 list.sort 方法很相似，只是有个是否创建一个新的排序后的 QueryResult 对象的选项。 这里有个用 QueryResult.sort 方法对hit排序的例子，这个方法基于每个hit 的完整序列长度。对于这个特殊的排序，我们设置 in_place 参数等于 False ， 这样排序方法会返回一个新的 QueryResult 对象，而原来的对象是未排序的。 我们同样可以设置 reverse 参数等于 True 以递减排序。\n\u0026gt;\u0026gt;\u0026gt; for hit in blast_qresult[:5]: # id and sequence length of the first five hits ... print hit.id, hit.seq_len ... gi|262205317|ref|NR_030195.1| 61 gi|301171311|ref|NR_035856.1| 60 gi|270133242|ref|NR_032573.1| 85 gi|301171322|ref|NR_035857.1| 86 gi|301171267|ref|NR_035851.1| 80 \u0026gt;\u0026gt;\u0026gt; sort_key = lambda hit: hit.seq_len \u0026gt;\u0026gt;\u0026gt; sorted_qresult = blast_qresult.sort(key=sort_key, reverse=True, in_place=False) \u0026gt;\u0026gt;\u0026gt; for hit in sorted_qresult[:5]: ... print hit.id, hit.seq_len ... gi|397513516|ref|XM_003827011.1| 6002 gi|390332045|ref|XM_776818.2| 4082 gi|390332043|ref|XM_003723358.1| 4079 gi|356517317|ref|XM_003527287.1| 3251 gi|356543101|ref|XM_003539954.1| 2936 有 in_place 参数的好处是可以保留原本的顺序，后面会用到。注意这不是 QueryResult.sort 的默认行为，需要我们明确地设置 in_place 为 True 。 现在，你已经知道使用 QueryResult 对象。但是，在我们学习 Bio.SearchIO 模块下个对象前，先了解下可以让 QueryResult 对象更易使用的两个方法： filter 和 map 方法。 如果你对Python的列表推导式、generator表达式或内建的 filter 和 map 很熟悉，就知道（不知道就是看看吧!)它们在处理list-like的对象时有多有用。 你可以用这些内建的方法来操作 QueryResult 对象，但是这只对正常list有效，并且可操作性也会受到限制。 这就是为什么 QueryResult 对象提供自己特有的 filter 和 map 方法。对于 filter 有相似的 hit_filter 和 hsp_filter 方法， 从名称就可以看出，这些方法过滤 QueryResult 对象的 Hit 对象或者 HSP 对象。同样的，对于 map ， QueryResult 对象同样提供相似 的 hit_map 和 hsp_map 方法。这些方法分别应用于 QueryResult 对象 hit 或者 HSP 对象。 让我们来看看这些方法的功能，从 hit_filter 开始。这个方法接受一个回调 函数，这个函数检验给定的 Hit 是否符合你设定的条件。换句话说，这个方法 必须接受一个单独 Hit 对象作为参数并且返回 True 或 False 。 这里有个用 hit_filter 筛选出只有一个HSP的 Hit 对象的例子：\n\u0026gt;\u0026gt;\u0026gt; filter_func = lambda hit: len(hit.hsps) \u0026gt; 1 # the callback function \u0026gt;\u0026gt;\u0026gt; len(blast_qresult) # no. of hits before filtering 100 \u0026gt;\u0026gt;\u0026gt; filtered_qresult = blast_qresult.hit_filter(filter_func) \u0026gt;\u0026gt;\u0026gt; len(filtered_qresult) # no. of hits after filtering 37 \u0026gt;\u0026gt;\u0026gt; for hit in filtered_qresult[:5]: # quick check for the hit lengths ... print hit.id, len(hit.hsps) gi|301171322|ref|NR_035857.1| 2 gi|262205330|ref|NR_030198.1| 2 gi|301171447|ref|NR_035871.1| 2 gi|262205298|ref|NR_030190.1| 2 gi|270132717|ref|NR_032716.1| 2 hsp_filter 和 hit_filter 功能相同，只是它过滤每个hit中的 HSP 对象， 而不是 Hit 。 对于 map 方法，同样接受一个回调函数作为参数。但是回调函数返回修改过的 Hit 或 HSP 对象（取决于你是否使用 hit_map 或 hsp_map 方法）， 而不是返回 True 或 False。 来看一个用 hit_map 方法来重命名hit ID的例子：\n\u0026gt;\u0026gt;\u0026gt; def map_func(hit): ... hit.id = hit.id.split(\u0026#39;|\u0026#39;)[3] # renames \u0026#39;gi|301171322|ref|NR_035857.1|\u0026#39; to \u0026#39;NR_035857.1\u0026#39; ... return hit ... \u0026gt;\u0026gt;\u0026gt; mapped_qresult = blast_qresult.hit_map(map_func) \u0026gt;\u0026gt;\u0026gt; for hit in mapped_qresult[:5]: ... print hit.id NR_030195.1 NR_035856.1 NR_032573.1 NR_035857.1 NR_035851.1 同样的， hsp_map 和 hit_map 作用相似, 但是作用于 HSP 对象而不是 Hit 对象。\nHit Hit 对象代表从单个数据库获得所有查询结果。在 Bio.SearchIO 对象等级中是二级容器。它们被包含在 QueryResult 对象中，同时它们又包含 HSP 对象。 看看它们是什么样的，从我们的BLAST搜索开始：\n\u0026gt;\u0026gt;\u0026gt; from Bio import SearchIO \u0026gt;\u0026gt;\u0026gt; blast_qresult = SearchIO.read(\u0026#39;my_blast.xml\u0026#39;, \u0026#39;blast-xml\u0026#39;) \u0026gt;\u0026gt;\u0026gt; blast_hit = blast_qresult[3] # fourth hit from the query result \u0026gt;\u0026gt;\u0026gt; print blast_hit Query: 42291 mystery_seq Hit: gi|301171322|ref|NR_035857.1| (86) Pan troglodytes microRNA mir-520c (MIR520C), microRNA HSPs: ---- -------- --------- ------ --------------- --------------------- # E-value Bit score Span Query range Hit range ---- -------- --------- ------ --------------- --------------------- 0 8.9e-20 100.47 60 [1:61] [13:73] 1 3.3e-06 55.39 60 [0:60] [13:73] 可以看到我们获得了必要的信息：\nquery的ID和描述信息。一个hit总是和一个query绑定，所以我们同样希望记录原始 query。这些值可以通过 query_id 和 query_description 属性从hit 中获取。 我们同样得到了hit的ID、描述和序列全长。它们可以分别通过 id， description，和 seq_len 获取。 最后，有一个hit含有的HSP的简短信息表。在每行中，HSP重要信息被 列出来：HSP索引，e值，得分，长度（包括gap），query坐标和hit坐标。 现在，和BLAT结果作对比。记住，在BLAT搜索结果中，我们发现有一个含有17HSP的hit。 \u0026gt;\u0026gt;\u0026gt; blat_qresult = SearchIO.read(\u0026#39;my_blat.psl\u0026#39;, \u0026#39;blat-psl\u0026#39;) \u0026gt;\u0026gt;\u0026gt; blat_hit = blat_qresult[0] # the only hit \u0026gt;\u0026gt;\u0026gt; print blat_hit Query: mystery_seq \u0026lt;unknown description\u0026gt; Hit: chr19 (59128983) \u0026lt;unknown description\u0026gt; HSPs: ---- -------- --------- ------ --------------- --------------------- # E-value Bit score Span Query range Hit range ---- -------- --------- ------ --------------- --------------------- 0 ? ? ? [0:61] [54204480:54204541] 1 ? ? ? [0:61] [54233104:54264463] 2 ? ? ? [0:61] [54254477:54260071] 3 ? ? ? [1:61] [54210720:54210780] 4 ? ? ? [0:60] [54198476:54198536] 5 ? ? ? [0:61] [54265610:54265671] 6 ? ? ? [0:61] [54238143:54240175] 7 ? ? ? [0:60] [54189735:54189795] 8 ? ? ? [0:61] [54185425:54185486] 9 ? ? ? [0:60] [54197657:54197717] 10 ? ? ? [0:61] [54255662:54255723] 11 ? ? ? [0:61] [54201651:54201712] 12 ? ? ? [8:60] [54206009:54206061] 13 ? ? ? [10:61] [54178987:54179038] 14 ? ? ? [8:61] [54212018:54212071] 15 ? ? ? [8:51] [54234278:54234321] 16 ? ? ? [8:61] [54238143:54238196] 我们得到了和前面看到的BLAST hit详细程度相似的结果。但是有些不同点需要解释：\ne-value和bit score列的值。因为BLAT HSP没有e-values和bit scores，默 认显示‘?’. span列是怎么回事呢？span值本来是显示完整的比对长度，包含所有的残基和 gap。但是PSL格式目前还不支持这些信息并且 Bio.SearchIO 也不打算去猜它到底是多少，所有我们得到了和e-value以及bit score列相同的 ‘?’。 就Python对象来说， Hit 和列表行为最相似，但是额外含有 HSP 。如果 你对列表熟悉，在使用 Hit 对象是不会遇到困难。 和列表一样， Hit 对象是可迭代的，并且每次迭代返回一个 HSP 对象： \u0026gt;\u0026gt;\u0026gt; for hsp in blast_hit: ... hsp HSP(hit_id=\u0026#39;gi|301171322|ref|NR_035857.1|\u0026#39;, query_id=\u0026#39;42291\u0026#39;, 1 fragments) HSP(hit_id=\u0026#39;gi|301171322|ref|NR_035857.1|\u0026#39;, query_id=\u0026#39;42291\u0026#39;, 1 fragments) 你可以对 Hit 对象调用 len 方法查看它含有多少个 HSP 对象：\n\u0026gt;\u0026gt;\u0026gt; len(blast_hit) 2 \u0026gt;\u0026gt;\u0026gt; len(blat_hit) 17 你可以对 Hit 对象作切片取得单个或多个 HSP 对象，和 QueryResult 一样，如果切取多个 HSP ，会返回包含被切片 HSP 的一个新 Hit 对象。\n\u0026gt;\u0026gt;\u0026gt; blat_hit[0] # retrieve single items HSP(hit_id=\u0026#39;chr19\u0026#39;, query_id=\u0026#39;mystery_seq\u0026#39;, 1 fragments) \u0026gt;\u0026gt;\u0026gt; sliced_hit = blat_hit[4:9] # retrieve multiple items \u0026gt;\u0026gt;\u0026gt; len(sliced_hit) 5 \u0026gt;\u0026gt;\u0026gt; print sliced_hit Query: mystery_seq \u0026lt;unknown description\u0026gt; Hit: chr19 (59128983) \u0026lt;unknown description\u0026gt; HSPs: ---- -------- --------- ------ --------------- --------------------- # E-value Bit score Span Query range Hit range ---- -------- --------- ------ --------------- --------------------- 0 ? ? ? [0:60] [54198476:54198536] 1 ? ? ? [0:61] [54265610:54265671] 2 ? ? ? [0:61] [54238143:54240175] 3 ? ? ? [0:60] [54189735:54189795] 4 ? ? ? [0:61] [54185425:54185486] 你同样可以对一个 Hit 里的 HSP 排序，和你在 QueryResult 对象 中看到的方法一样。 最后，同样可以对 Hit 对象使用 filter 和 map 方法。和 QueryResult 不同， Hit 对象只有一种 filter (Hit.filter) 和一种 map (Hit.map)\nHSP HSP (高分片段)代表hit序列中的一个区域，该区域包含对于查询序列有意义的 比对。它包含了你的查询序列和一个数据库条目之间精确的匹配。由于匹配取决于 序列搜索工具的算法， HSP 含有大部分统计信息，这些统计是由搜索工具计 算得到的。这使得不同搜索工具的 HSP 对象之间的差异和你在 QueryResult 以及 Hit 对象看到的差异更加明显。\nBLAST HSP的例子 \u0026gt;\u0026gt;\u0026gt; from Bio import SearchIO \u0026gt;\u0026gt;\u0026gt; blast_qresult = SearchIO.read(\u0026#39;my_blast.xml\u0026#39;, \u0026#39;blast-xml\u0026#39;) \u0026gt;\u0026gt;\u0026gt; blast_hsp = blast_qresult[0][0] # first hit, first hsp \u0026gt;\u0026gt;\u0026gt; print blast_hsp Query: 42291 mystery_seq Hit: gi|262205317|ref|NR_030195.1| Homo sapiens microRNA 520b (MIR520... Query range: [0:61] (1) Hit range: [0:61] (1) Quick stats: evalue 4.9e-23; bitscore 111.29 Fragments: 1 (61 columns) Query - CCCTCTACAGGGAAGCGCTTTCTGTTGTCTGAAAGAAAAGAAAGTGCTTCCTTTTAGAGGG ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| Hit - CCCTCTACAGGGAAGCGCTTTCTGTTGTCTGAAAGAAAAGAAAGTGCTTCCTTTTAGAGGG 和 QueryResult 以及 Hit 类似，调用 HSP 的 print 方法, 显示细节：\n有query和hit的ID以及描述。我们需要这些来识别我们的 HSP 。 我们同样得到了query和hit序列的匹配范围。这里用的的切片标志着范围的表示 是使用Python的索引风格（从0开始，半开区间）。圆括号里的数字表示正负链。 这里，两条序列都是正链。 还有一些简短统计：e-value和bitscore。 还有一些HSP片段的信息。现在可以忽略，稍后会解释。 最后，还有query和hit的比对。 这些信息可以用点标记从它们本身获得，和 Hit 以及 QueryResult 相同： \u0026gt;\u0026gt;\u0026gt; blast_hsp.query_range (0, 61) \u0026gt;\u0026gt;\u0026gt; blast_hsp.evalue 4.91307e-23 它们并不是仅有的属性， HSP 对象有一系列的属性，使得获得它们的具体信 息更加容易。下面是一些例子：\n\u0026gt;\u0026gt;\u0026gt; blast_hsp.hit_start # start coordinate of the hit sequence 0 \u0026gt;\u0026gt;\u0026gt; blast_hsp.query_span # how many residues in the query sequence 61 \u0026gt;\u0026gt;\u0026gt; blast_hsp.aln_span # how long the alignment is 61 HSP片段 HSPFragment 代表query和hit之间单个连续匹配。应该把它当作对象模型 和搜索结果的核心，因为它决定你的搜索是否有结果。 在多数情况下，你不必直接处理 HSPFragment 对象，因为没有那么多搜索工具 断裂它们的HSP。当你确实需要处理它们时，需要记住的是 HSPFragment 对象 要被写地尽量压缩。在多数情况下，它们仅仅包含直接与序列有关的属性：正负链， 阅读框，字母表，位置坐标，序列本身以及它们的ID和描述。 当你对 HSPFragment 对象调用 print 方法时，这些属性可以非常简单地显示 出来。这里有个从我们BLAST搜索得到的例子\n\u0026gt;\u0026gt;\u0026gt; from Bio import SearchIO \u0026gt;\u0026gt;\u0026gt; blast_qresult = SearchIO.read(\u0026#39;my_blast.xml\u0026#39;, \u0026#39;blast-xml\u0026#39;) \u0026gt;\u0026gt;\u0026gt; blast_frag = blast_qresult[0][0][0] # first hit, first hsp, first fragment \u0026gt;\u0026gt;\u0026gt; print blast_frag Query: 42291 mystery_seq Hit: gi|262205317|ref|NR_030195.1| Homo sapiens microRNA 520b (MIR520... Query range: [0:61] (1) Hit range: [0:61] (1) Fragments: 1 (61 columns) Query - CCCTCTACAGGGAAGCGCTTTCTGTTGTCTGAAAGAAAAGAAAGTGCTTCCTTTTAGAGGG ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||| Hit - CCCTCTACAGGGAAGCGCTTTCTGTTGTCTGAAAGAAAAGAAAGTGCTTCCTTTTAGAGGG 在所有情况下，这些属性都可以通过我们最爱的点标记访问。一些例子：\n\u0026gt;\u0026gt;\u0026gt; blast_frag.query_start # query start coordinate 0 \u0026gt;\u0026gt;\u0026gt; blast_frag.hit_strand # hit sequence strand 1 \u0026gt;\u0026gt;\u0026gt; blast_frag.hit # hit sequence, as a SeqRecord object SeqRecord(seq=Seq(\u0026#39;CCCTCTACAGGGAAGCGCTTTCTGTTGTCTGAAAGAAAAGAAAGTGCTTCCTTT...GGG\u0026#39;, DNAAlphabet()), id=\u0026#39;gi|262205317|ref|NR_030195.1|\u0026#39;, name=\u0026#39;aligned hit sequence\u0026#39;, description=\u0026#39;Homo sapiens microRNA 520b (MIR520B), microRNA\u0026#39;, dbxrefs=[]) 一些关于SearchIO的标准和惯例的注意事项 在使用 Bio.SearchIO 时你可以认为有个三个隐含的标准：\n第一个适用于序列坐标。在 Bio.SearchIO 模块中，所有序列坐标遵循Python 的坐标风格： 从0开始，半开区间。例如，在一个BLAST XML输出文件中，HSP的起始和结束坐标 是10和28，它们在 Bio.SearchIO 中将变成9和28。起始坐标变成9因为Python 中索引是从0开始，而结束坐标仍然是28因为Python索引删除了区间中最后一个 项目。 第二个是关于序列坐标顺序。在 Bio.SearchIO 中，开始坐标总是小于或 等于结束坐标。但是这不是在所有的序列搜索工具中都始终适用。因为当序列 为负链时，起始坐标会更大一些。 最后一个标准是关于链和阅读框的值。对于链值，只有四个可选值： 1 (正链)， -1 (负链)， 0 (蛋白序列)， 和 None (无链)。对于阅读框， 可选值是从 -3 至 3 的整型以及 None 。 注意，这些标准只是存在于 Bio.SearchIO 对象中。如果你把 Bio.SearchIO 对象写入一种输出格式， Bio.SearchIO 会使用该格式的标准来输出。它并不 强加它的标准到你的输出文件。 读取搜素输出文件 有两个方法，你可以用来读取搜索输出文件到 Bio.SearchIO 对象： read 和 parse。 它们和其他亚模块如 Bio.SeqIO 或 Bio.AlignIO 中的 read 和 parse 方法在 本质上是相似的。你都需要提供搜索输出文件名和文件格式名，都是Python字符串类型。你可以 查阅文档来获得 Bio.SearchIO 可以识别的格式清单。 Bio.SearchIO.read 用于读取单query的搜索输出文件并且返回一个 QueryResult 对象。你在前面的例子中已经看到过 read 的使用了。 你没看到的是， read 同样接受额外的关键字参数，取决于文件的格式。\n\u0026gt;\u0026gt;\u0026gt; from Bio import SearchIO \u0026gt;\u0026gt;\u0026gt; qresult = SearchIO.read(\u0026#39;tab_2226_tblastn_003.txt\u0026#39;, \u0026#39;blast-tab\u0026#39;) \u0026gt;\u0026gt;\u0026gt; qresult QueryResult(id=\u0026#39;gi|16080617|ref|NP_391444.1|\u0026#39;, 3 hits) \u0026gt;\u0026gt;\u0026gt; qresult2 = SearchIO.read(\u0026#39;tab_2226_tblastn_007.txt\u0026#39;, \u0026#39;blast-tab\u0026#39;, comments=True) \u0026gt;\u0026gt;\u0026gt; qresult2 QueryResult(id=\u0026#39;gi|16080617|ref|NP_391444.1|\u0026#39;, 3 hits) 对于 Bio.SearchIO.parse，是用来读取含有任意数量query的搜索输出文件。 这个方法返回一个generator对象，在每次迭代中yield一个 QueryResult 对象。 和 Bio.SearchIO.read 一样，它同样接受格式特异的关键字参数：\n\u0026gt;\u0026gt;\u0026gt; from Bio import SearchIO \u0026gt;\u0026gt;\u0026gt; qresults = SearchIO.parse(\u0026#39;tab_2226_tblastn_001.txt\u0026#39;, \u0026#39;blast-tab\u0026#39;) \u0026gt;\u0026gt;\u0026gt; for qresult in qresults: ... print qresult.id gi|16080617|ref|NP_391444.1| gi|11464971:4-101 \u0026gt;\u0026gt;\u0026gt; qresults2 = SearchIO.parse(\u0026#39;tab_2226_tblastn_005.txt\u0026#39;, \u0026#39;blast-tab\u0026#39;, comments=True) \u0026gt;\u0026gt;\u0026gt; for qresult in qresults2: ... print qresult.id random_s00 gi|16080617|ref|NP_391444.1| gi|11464971:4-101 用索引处理含有大量搜素输出的文件 有时，你得到了一个包含成百上千个query的搜索输出文件要分析，你当然可以使用 Bio.SearchIO.parse 来处理，但是如果你仅仅需要访问少数query的话，效率 是及其低下的。这是因为 parse 会分析所有的query，直到找到你感兴趣。 在这种情况下，理想的选择是用 Bio.SearchIO.index 或 Bio.SearchIO.index_db 来索引文件。如果名字听起来很熟悉，是因为你之前已经见过了，在章节这些方法和 Bio.SeqIO 中相应的方法行为很相似，只是多了些格式特异的关键字参数。 这里有一些例子。你可以只用文件名和格式名来 index\n\u0026gt;\u0026gt;\u0026gt; from Bio import SearchIO \u0026gt;\u0026gt;\u0026gt; idx = SearchIO.index(\u0026#39;tab_2226_tblastn_001.txt\u0026#39;, \u0026#39;blast-tab\u0026#39;) \u0026gt;\u0026gt;\u0026gt; sorted(idx.keys()) [\u0026#39;gi|11464971:4-101\u0026#39;, \u0026#39;gi|16080617|ref|NP_391444.1|\u0026#39;] \u0026gt;\u0026gt;\u0026gt; idx[\u0026#39;gi|16080617|ref|NP_391444.1|\u0026#39;] QueryResult(id=\u0026#39;gi|16080617|ref|NP_391444.1|\u0026#39;, 3 hits) 或者依旧使用格式特异的关键字参数：\n\u0026gt;\u0026gt;\u0026gt; idx = SearchIO.index(\u0026#39;tab_2226_tblastn_005.txt\u0026#39;, \u0026#39;blast-tab\u0026#39;, comments=True) \u0026gt;\u0026gt;\u0026gt; sorted(idx.keys()) [\u0026#39;gi|11464971:4-101\u0026#39;, \u0026#39;gi|16080617|ref|NP_391444.1|\u0026#39;, \u0026#39;random_s00\u0026#39;] \u0026gt;\u0026gt;\u0026gt; idx[\u0026#39;gi|16080617|ref|NP_391444.1|\u0026#39;] QueryResult(id=\u0026#39;gi|16080617|ref|NP_391444.1|\u0026#39;, 3 hits) 或者使用 key_function 参数，和 Bio.SeqIO 中一样：\n\u0026gt;\u0026gt;\u0026gt; key_function = lambda id: id.upper() # capitalizes the keys \u0026gt;\u0026gt;\u0026gt; idx = SearchIO.index(\u0026#39;tab_2226_tblastn_001.txt\u0026#39;, \u0026#39;blast-tab\u0026#39;, key_function=key_function) \u0026gt;\u0026gt;\u0026gt; sorted(idx.keys()) [\u0026#39;GI|11464971:4-101\u0026#39;, \u0026#39;GI|16080617|REF|NP_391444.1|\u0026#39;] \u0026gt;\u0026gt;\u0026gt; idx[\u0026#39;GI|16080617|REF|NP_391444.1|\u0026#39;] QueryResult(id=\u0026#39;gi|16080617|ref|NP_391444.1|\u0026#39;, 3 hits) 写入和转换搜素输出文件 Bio.SearchIO.write 有时候，读取一个搜索输出文件，作些调整并写到一个新的文件是很有用的。 Bio.SearchIO 提供了一个 write 方法，让你可以准确地完成这种工作。 它需要的参数是：一个可迭代返回 QueryResult 的对象，输出文件名，输出文件 格式和一些可选的格式特异的关键字参数。它返回一个4项目的元组，分别代表 被写入的 QueryResult， Hit， HSP， 和 HSPFragment 对象的数量。\n\u0026gt;\u0026gt;\u0026gt; from Bio import SearchIO \u0026gt;\u0026gt;\u0026gt; qresults = SearchIO.parse(\u0026#39;mirna.xml\u0026#39;, \u0026#39;blast-xml\u0026#39;) # read XML file \u0026gt;\u0026gt;\u0026gt; SearchIO.write(qresults, \u0026#39;results.tab\u0026#39;, \u0026#39;blast-tab\u0026#39;) # write to tabular file (3, 239, 277, 277) 你应该注意，不同的文件格式需要 QueryResult， Hit， HSP 和 HSPFragment 对象的不同属性。如果这些属性不存在，那么将不能写入。 也就是，你想写入的格式可能有时也会失效。举个例子，如果你读取一个BLASTXML文件， 你就不能将结果写入PSL文件，因为PSL文件需要一些属性，而这些属性BLAST却不能 提供（如重复匹配的数量）。如果你确实想写到PSL，可以手工设置这些属性。 和 read， parse， index 和 index_db 相似， write 同 样接受格式特异的关键字参数。查阅文档获得 Bio.SearchIO 可写格式和这些 格式的参数的完整清单。\nBio.SearchIO.convert() 最后， Bio.SearchIO 同样提供一个 convert 方法，可以理解为 Bio.SearchIO.parse 和 Bio.SearchIO.write 的简单替代方法。使用 convert 方法的例子如下：\n\u0026gt;\u0026gt;\u0026gt; from Bio import SearchIO \u0026gt;\u0026gt;\u0026gt; SearchIO.convert(\u0026#39;mirna.xml\u0026#39;, \u0026#39;blast-xml\u0026#39;, \u0026#39;results.tab\u0026#39;, \u0026#39;blast-tab\u0026#39;) (3, 239, 277, 277) 因为 convert 使用 write 方法，所以只有所有需要的属性都存在时，格式 转换才能正常工作。这里由于BLAST XML文件提供BLAST 表格文件所需的所有默认值， 格式转换才能正常完成。但是，其他格式转换就可能不会正常工作，因为你需要先手工指定所需的属性。\n","date":"2021-08-09T00:00:00Z","permalink":"https://example.com/p/ch8_blast%E5%92%8C%E5%85%B6%E4%BB%96%E5%BA%8F%E5%88%97%E6%90%9C%E7%B4%A0%E5%B7%A5%E5%85%B7/","title":"ch8_BLAST和其他序列搜素工具"},{"content":"biopython官方地址：https://biopython.org/\ngithub地址：https://github.com/biopython/biopython/blob/master/\n中文版教程：https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html\nbiopython包的说明（具体到每个模块了）：https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html\n这章是来解决使用Blast的一些麻烦地方——处理大量的BLAST比对结果数据 通常是困难的，还有怎么自动运行BLAST序列比对。 使用BLAST通常可以分成2个步。这两步都可以用上Biopython。第一步，提交你的查询 序列,运行BLAST，并得到输出数据。第二步，用Python解析BLAST的输出，并作进一步分析。 我们将在一个Python脚本里调用NCBI在线BLAST服务来开始这章的内容。\n通过Internet运行BLAST 我们使用 Bio.Blast.NCBIWWW 模块的函数 qblast() 来调用在线版本的BLAST。 这个函数有3个必需的参数:\n第一个参数是用来搜索的blast程序，这是小写的字符串。对这个参数的选项和描述可以在 http://www.ncbi.nlm.nih.gov/BLAST/blast_program.shtml. 查到。目前 qblast 只支持 blastn, blastp, blastx, tblast 和 tblastx. 第二个参数指定了将要搜索的数据库。同样地，这个参数的选项也可以在 http://www.ncbi.nlm.nih.gov/BLAST/blast_databases.shtml. 查到 第三个参数是一个包含你要查询序列的字符串。这个字符串可以是序列的本身 （fasta格式的），或者是一个类似GI的id。 qblast 函数还可以接受许多其他选项和参数。这些参数基本上类似于你在BLAST网站页面 能够设置的参数。在这里我们只强调其中的一些： qblast 函数可以返回多种格式的BLAST结果。你可以通过可选参数 format_type 指定格式关键字为：\u0026ldquo;HTML\u0026rdquo;, \u0026ldquo;Text\u0026rdquo;, \u0026ldquo;ASN.1\u0026rdquo;, 或 \u0026ldquo;XML\u0026rdquo; 。默认 格式是 \u0026ldquo;XML\u0026rdquo; ，这是解析器期望的格式， 参数 expect 指定期望值，即阀值 e-value 请注意，NCBI BLAST 网站上的默认参数和QBLAST的默认参数不完全相同。如果你得到了 不同的结果，你就需要检查下参数设置 （比如，e-value阈值和gap值）. 简单的例子 举个例子，如果你有条核酸序列，想使用BLAST对核酸数据库（nt）进行搜索，已知这条查询序列的GI号， 你可以这样做：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Blast import NCBIWWW \u0026gt;\u0026gt;\u0026gt; result_handle = NCBIWWW.qblast(\u0026#34;blastn\u0026#34;, \u0026#34;nt\u0026#34;, \u0026#34;8332116\u0026#34;) 或者，我们想要查询的序列在FASTA文件中，那么我们只需打开这个文件并把这条记录读入到字符串，然后用这个字符串作为查询参数:\n\u0026gt;\u0026gt;\u0026gt; from Bio.Blast import NCBIWWW \u0026gt;\u0026gt;\u0026gt; fasta_string = open(\u0026#34;m_cold.fasta\u0026#34;).read() \u0026gt;\u0026gt;\u0026gt; result_handle = NCBIWWW.qblast(\u0026#34;blastn\u0026#34;, \u0026#34;nt\u0026#34;, fasta_string) 我们同样可以读取FASTA文件为一个 SeqRecord 序列对象，然后以这个序列自身作为参数：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Blast import NCBIWWW \u0026gt;\u0026gt;\u0026gt; from Bio import SeqIO \u0026gt;\u0026gt;\u0026gt; record = SeqIO.read(\u0026#34;m_cold.fasta\u0026#34;, format=\u0026#34;fasta\u0026#34;) \u0026gt;\u0026gt;\u0026gt; result_handle = NCBIWWW.qblast(\u0026#34;blastn\u0026#34;, \u0026#34;nt\u0026#34;, record.seq) 只提供序列意味着BLAST会自动分配给你一个ID。你可能更喜欢用 SeqRecord 对象的format方法来包装一个fasta字符串，因为这个对象会包含fasta文件中已有的ID\n\u0026gt;\u0026gt;\u0026gt; from Bio.Blast import NCBIWWW \u0026gt;\u0026gt;\u0026gt; from Bio import SeqIO \u0026gt;\u0026gt;\u0026gt; record = SeqIO.read(\u0026#34;m_cold.fasta\u0026#34;,format=\u0026#34;fasta\u0026#34;) \u0026gt;\u0026gt;\u0026gt; result_handle = NCBIWWW.qblast(\u0026#34;blastn\u0026#34;,\u0026#34;nt\u0026#34;,record.format(\u0026#34;fasta\u0026#34;)) BLAST的输出文件xml 不论你给 qblast() 函数提供了什么参数，都应该返回一个handle object的结果( 默认是XML格式)。下一步就是将这个XML输出解析为代表BLAST搜索结果的Python 对象。 不过，也许你想先把这个XML输出保存一个本地文件副本。当调试从BLAST结果提取信息的代码的时候，我发现这样做 尤其有用。(因为重新运行在线BLAST搜索很慢并且会浪费NCBI服务器的运行时间)。 这里我们需要注意下：因为用 result_handle.read() 来读取BLAST结果只能用一次 - 再次调用 result_handle.read() 会返回一个空的字符串.\n\u0026gt;\u0026gt;\u0026gt; save_file = open(\u0026#34;my_blast.xml\u0026#34;, \u0026#34;w\u0026#34;) \u0026gt;\u0026gt;\u0026gt; save_file.write(result_handle.read()) \u0026gt;\u0026gt;\u0026gt; save_file.close() \u0026gt;\u0026gt;\u0026gt; result_handle.close() 这些做好后，结果已经存储在 my_blast.xml 文件中了并且原先的handle中的数据 已经被全部提取出来了(所以我们把它关闭了)。但是，BLAST解析器的 parse 函数 采用一个文件句柄类的对象，所以我们只需打开已经保存的文件作为输入。\n\u0026gt;\u0026gt;\u0026gt; result_handle = open(\u0026#34;my_blast.xml\u0026#34;) 既然现在已经把BLAST的结果又一次读回handle，我们可以分析下这些结果。所以我们正好可以去读 关于结果解析的章节\n本地运行BLAST 在本地运行BLAST 至少有2个主要优点：\n本地运行BLAST可能比通过internet运行更快； 本地运行可以让你建立自己的数据库来对序列进行搜索。 处理有版权的或者没有发表的序列数据也许是本地运行BLAST的另一个原因。你也许 不能泄露这些序列数据，所以没法提交给NCBI来BLAST。 不幸的是，本地运行也有些缺点 - 安装所有的东东并成功运行需要花些力气： 本地运行BLAST需要你安装相关命令行工具。 本地运行BLAST需要安装一个很大的BLAST的数据库（并且需要保持数据更新）. 更令人困惑的是，至少有4种不同的BLAST安装程序包，并且还有其他的一些工具能 产生类似的BLAST 输出文件，比如BLAT https://blast.ncbi.nlm.nih.gov/Blast.cgi?CMD=Web\u0026amp;PAGE_TYPE=BlastDocs\u0026amp;DOC_TYPE=Download 简单的应用 这节将简要地介绍怎样在Python中使用这些工具。如果你已经阅读了并试过 6.4 节的序列联配（alignment）工具，下面介绍 的方法应该是很简单直接的。首先，我们构建一个命令行字符串（就像你使用单机版 BLAST的时候，在终端打入命令行一样）。然后，我们在Python中运行这个命令。 举个例子，你有个FASTA格式的核酸序列文件，你想用它通过BLASTX（翻译）来搜索 非冗余（NR）蛋白质数据库。如果你（或者你的系统管理员）下载并安装好了这个数据库， 那么你只要运行：\nblastx -query opuntia.fasta -db nr -out opuntia.xml -evalue 0.001 -outfmt 5 这样就完成了运行BLASTX查找非冗余蛋白质数据库，用0.001的e值并产生XML格式的 输出结果文件（这样我们可以继续下一步解析）。在我的电脑上运行这条命令花了大约6分钟 - 这就是为什么我们需要保存输出到文件。这样我们就可以在需要时重复任何基于这个输出的分析。\n在biopython中的应用 在Biopython中，我们可以用NCBI BLASTX包装模块 Bio.Blast.Applications 来构建 命令行字符串并运行它：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Blast.Applications import NcbiblastxCommandline \u0026gt;\u0026gt;\u0026gt; help(NcbiblastxCommandline) ... \u0026gt;\u0026gt;\u0026gt; blastx_cline = NcbiblastxCommandline(query=\u0026#34;opuntia.fasta\u0026#34;, db=\u0026#34;nr\u0026#34;, evalue=0.001, ... outfmt=5, out=\u0026#34;opuntia.xml\u0026#34;) \u0026gt;\u0026gt;\u0026gt; blastx_cline NcbiblastxCommandline(cmd=\u0026#39;blastx\u0026#39;, out=\u0026#39;opuntia.xml\u0026#39;, outfmt=5, query=\u0026#39;opuntia.fasta\u0026#39;, db=\u0026#39;nr\u0026#39;, evalue=0.001) \u0026gt;\u0026gt;\u0026gt; print blastx_cline blastx -out opuntia.xml -outfmt 5 -query opuntia.fasta -db nr -evalue 0.001 \u0026gt;\u0026gt;\u0026gt; stdout, stderr = blastx_cline() 在这个例子中，终端里应该没有任何从BLASTX的输出，所以stdout和stderr是空的。 你可能想要检查下输出文件 opuntia.xml 是否已经创建。 如果你回想下这个指南的中的早先的例子，opuntia.fasta 包含7条序列， 所以BLAST XML 格式的结果输出文件应该包括多个结果。因此，我们在 下面将用 Bio.Blast.NCBIXML.parse() 来 解析这个结果文件。\n解析BLAST输出 跟上BLAST输出文件格式的改变很难，特别是当用户使用不同版本的BLAST的时候。 我们推荐使用XML格式的解析器。因为最近版本的BLAST能生成这种格式的输出结果。 XML格式的输出不仅比HTML 和纯文本格式的更稳定，而且解析起来更加容易自动化， 从而提高整个Biopython整体的稳定性。 你可以通过好几个途径来获得XML格式的BLAST输出文件。对解析器来说，不管你是 怎么生成输出的，只要是输出的格式是XML就行。 关键点就是你不必用Biopython脚本来获取数据才能解析它。通过以上任何一种方式 获取了结果输出，你然后需要获得文件句柄来处理它。在Python中，一个文件句柄就是一种 用于描述到任何信息源的输入的良好通用的方式，以便于这些信息能够使用 read() 和 readline() 函数来获取。\n简单的例子 如果你一直跟着上几节用来和BLAST交互的代码的话，你已经有了个 result_handle ，一个用来得到BLAST的结果文件句柄。 比如通过GI号来进行一个在线BLAST搜索：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Blast import NCBIWWW \u0026gt;\u0026gt;\u0026gt; result_handle = NCBIWWW.qblast(\u0026#34;blastn\u0026#34;, \u0026#34;nt\u0026#34;, \u0026#34;8332116\u0026#34;) 如果你通过其他方式运行了BLAST，并且XML格式的BLAST结果输出文件是 my_blast.xml , 那么你只需要打开文件来读：\n\u0026gt;\u0026gt;\u0026gt; result_handle = open(\u0026#34;my_blast.xml\u0026#34;) 解析结果的代码 当你只有一个输出结果的时候用 read 。当你有许多 输出结果的时候，可以用 parse 这个迭代器。 但是，我们调用函数获得结果 不是 SeqRecord 或者 MultipleSeqAlignment 对象，我们得到BLAST记录对象。 好的，现在我们已经有了个文件句柄，可以解析输出结果了。解析结果的代码 很短。如果你想要一条BLAST输出结果（就是说，你只用了一条序列去搜索）：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Blast import NCBIXML \u0026gt;\u0026gt;\u0026gt; blast_record = NCBIXML.read(result_handle) 或者， 你有许多搜索结果（就是说，你用了多条序列去BLAST搜索）\n\u0026gt;\u0026gt;\u0026gt; from Bio.Blast import NCBIXML \u0026gt;\u0026gt;\u0026gt; blast_records = NCBIXML.parse(result_handle) 为了能处理BLAST结果文件很大有很多结果这种情况， NCBIXML.parse() 返回一个迭代器。简单来说，一个迭代器可以让你一个接着一个地获得BLAST 的搜索结果。\n\u0026gt;\u0026gt; from Bio.Blast import NCBIXML \u0026gt;\u0026gt;\u0026gt; blast_records = NCBIXML.parse(result_handle) \u0026gt;\u0026gt;\u0026gt; blast_record = blast_records.next() # ... do something with blast_record \u0026gt;\u0026gt;\u0026gt; blast_record = blast_records.next() # ... do something with blast_record \u0026gt;\u0026gt;\u0026gt; blast_record = blast_records.next() # ... do something with blast_record \u0026gt;\u0026gt;\u0026gt; blast_record = blast_records.next() Traceback (most recent call last): File \u0026#34;\u0026lt;stdin\u0026gt;\u0026#34;, line 1, in \u0026lt;module\u0026gt; StopIteration # No further records 或者，你也可以使用 for - 循环\n\u0026gt;\u0026gt;\u0026gt; for blast_record in blast_records: ... # Do something with blast_record 注意对每个BLAST搜索结果只能迭代一次。通常，对于每个BLAST记录，你可能会保存你 感兴趣的信息。如果你想保存所有返回的BLAST记录，你可以把迭代 转换成列表。\n\u0026gt;\u0026gt;\u0026gt; blast_records = list(blast_records) 现在，你可以像通常的做法通过索引从这个列表中获得每一条BLAST结果。 如果你的BLAST输出 结果文件很大，那么当把它们全部放入一个列表时，你也许会遇到内存不够的情况。 一般来说，你会一次运行一个BLAST搜索。然后，你只需提取第一条BLAST 搜索记录到 blast_records :\n\u0026gt;\u0026gt;\u0026gt; from Bio.Blast import NCBIXML \u0026gt;\u0026gt;\u0026gt; blast_records = NCBIXML.parse(result_handle) \u0026gt;\u0026gt;\u0026gt; blast_record = blast_records.next() or more elegantly:\n\u0026gt;\u0026gt;\u0026gt; from Bio.Blast import NCBIXML \u0026gt;\u0026gt;\u0026gt; blast_record = NCBIXML.read(result_handle) BLAST纪录类 我猜你现在在想BLAST搜索记录中到底有什么 一个BLAST搜索结果记录包括了所有你想要从中提取出来的信息。现在，我们将用一个例子说明你怎么从BLAST搜索结果提取出一些信息。但是，如果你想从BLAST 搜索结果获得的信息没有在这里提到，你可以详细阅读BLAST搜索记录类， 并且可以参考下源代码 或者 是自动生成的文档 - 文档字符串里面包含了许多 关于各部分源代码是什么的很有用的信息。\n打印出想要的结果 继续我们的例子，让我们打印出所有大于某一特定阈值的BLAST命中结果的一些汇总信息。 代码如下\n\u0026gt;\u0026gt;\u0026gt; E_VALUE_THRESH = 0.04 \u0026gt;\u0026gt;\u0026gt; for alignment in blast_record.alignments: ... for hsp in alignment.hsps: ... if hsp.expect \u0026lt; E_VALUE_THRESH: ... print \u0026#39;****Alignment****\u0026#39; ... print \u0026#39;sequence:\u0026#39;, alignment.title ... print \u0026#39;length:\u0026#39;, alignment.length ... print \u0026#39;e value:\u0026#39;, hsp.expect ... print hsp.query[0:75] + \u0026#39;...\u0026#39; ... print hsp.match[0:75] + \u0026#39;...\u0026#39; ... print hsp.sbjct[0:75] + \u0026#39;...\u0026#39; 上面代码会打印出如下图的总结报告：\n****Alignment**** sequence: \u0026gt;gb|AF283004.1|AF283004 Arabidopsis thaliana cold acclimation protein WCOR413-like protein alpha form mRNA, complete cds length: 783 e value: 0.034 tacttgttgatattggatcgaacaaactggagaaccaacatgctcacgtcacttttagtcccttacatattcctc... ||||||||| | ||||||||||| || |||| || || |||||||| |||||| | | |||||||| ||| ||... tacttgttggtgttggatcgaaccaattggaagacgaatatgctcacatcacttctcattccttacatcttcttc... 基本上，一旦你解析了BLAST搜索结果文件，你可以提取任何你需要的信息。 当然，这取决于你想要获得什么信息。但是希望这里的例子能够帮助你开始工作。 在用Biopython提取BLAST搜索结果信息的时候，重要的是你需要考虑到信息存储在什么 （Biopython）对象中。在Biopython中，解析器返回 Record 对象，这个对象 可以是 Blast 类型的，也可以是 PSIBlast 类型的，具体哪个取决你 解析什么。这些对象的定义都可以在 Bio.Blast.Record 找到 并且很完整\nBLAST类图 image.png PSIBlast类图 image.png 废弃的BLAST解析器 取决于你使用Biopython的版本，纯文本格式的解析器也许有效也许失效。 用这个解析器的所带来的风险由你自己承担\n解析纯文本格式的BLAST输出 Bio.Blast.NCBIStandalone 纯文本格式的解析器在 Bio.Blast.NCBIStandalone 。 和xml解析器类似， 我们也需要一个能够传给解析器的文件句柄。这个文件句柄必须 实现了 readline() 方法 。通常要获得这样文件句柄，既可以用Biopython提供的 blastall 或 blastpgp 函数来调用本地的BLAST，或者从命令行运行本地的 BLAST， 并且如下处理：\n\u0026gt;\u0026gt;\u0026gt; result_handle = open(\u0026#34;my_file_of_blast_output.txt\u0026#34;) 好了，既然现在得到了个文件句柄（我们称它是 result_handle ）， 我们已经做好了解析它的准备。按下面的代码来解析：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Blast import NCBIStandalone \u0026gt;\u0026gt;\u0026gt; blast_parser = NCBIStandalone.BlastParser() \u0026gt;\u0026gt;\u0026gt; blast_record = blast_parser.parse(result_handle) 这样就能把BALST的搜索结果报告解析到Blast记录类中（取决你于你解析的对象， 解析结果可能返回一条 Blast 或者 PSIBlast记录）。\n提取信息 \u0026gt;\u0026gt;\u0026gt; E_VALUE_THRESH = 0.04 \u0026gt;\u0026gt;\u0026gt; for alignment in blast_record.alignments: ... for hsp in alignment.hsps: ... if hsp.expect \u0026lt; E_VALUE_THRESH: ... print \u0026#39;****Alignment****\u0026#39; ... print \u0026#39;sequence:\u0026#39;, alignment.title ... print \u0026#39;length:\u0026#39;, alignment.length ... print \u0026#39;e value:\u0026#39;, hsp.expect ... print hsp.query[0:75] + \u0026#39;...\u0026#39; ... print hsp.match[0:75] + \u0026#39;...\u0026#39; ... print hsp.sbjct[0:75] + \u0026#39;...\u0026#39; 好，解析一条记录是不错，那么如果我有一个包含许多记录的BLAST文件 - 我该怎么处理它们呢？好吧，不要害怕，答案就在下个章节中。\n解析包含多次BLAST结果的纯文本BLAST文件 我们可以用BLAST迭代器解析多次结果。为了得到一个迭代器，我们首先需要创建一个解析器，来 解析BLAST的搜索结果报告为Blast记录对象。\n\u0026gt;\u0026gt;\u0026gt; from Bio.Blast import NCBIStandalone \u0026gt;\u0026gt;\u0026gt; blast_parser = NCBIStandalone.BlastParser() 然后，我们假定我们有一个连接到一大堆blast记录的文件句柄，我们把这个文件句柄 叫做 result_handle 。 怎么得到一个文件句柄在上面blast解析章节有详细 描述。 好了，我们现在有了一个解析器和一个文件句柄，我们可以用以下命令来创建 一个迭代器。\n\u0026gt;\u0026gt;\u0026gt; blast_iterator = NCBIStandalone.Iterator(result_handle, blast_parser) 第二个参数，解析器，是可选的。如果我们没有提供一个解析器，那么迭代器将会 一次返回一个原始的BLAST搜索结果。 现在我们已经有了个迭代器，就可以开始通过 next() 方法来获取BLAST 记录（由我们的解析器产生）。\n\u0026gt;\u0026gt;\u0026gt; blast_record = blast_iterator.next() 每次调用next都会返回一条我们能处理的新记录。现在我们可以遍历所有记录，并打印一 个我们最爱、漂亮的、简洁的BLAST记录报告。\n\u0026gt;\u0026gt;\u0026gt; for blast_record in blast_iterator: ... E_VALUE_THRESH = 0.04 ... for alignment in blast_record.alignments: ... for hsp in alignment.hsps: ... if hsp.expect \u0026lt; E_VALUE_THRESH: ... print \u0026#39;****Alignment****\u0026#39; ... print \u0026#39;sequence:\u0026#39;, alignment.title ... print \u0026#39;length:\u0026#39;, alignment.length ... print \u0026#39;e value:\u0026#39;, hsp.expect ... if len(hsp.query) \u0026gt; 75: ... dots = \u0026#39;...\u0026#39; ... else: ... dots = \u0026#39;\u0026#39; ... print hsp.query[0:75] + dots ... print hsp.match[0:75] + dots ... print hsp.sbjct[0:75] + dots 迭代器允许你处理很多blast记录而不出现内存不足的问题。因为，它使一次处理 一个记录。我曾经用大处理过一个非常巨大的文件，没有出过任何问题。\n在巨大的BLAST纯文本文件发现不对的记录BlastErrorParser 当我开始解析一个巨大的blast 文件，有时候会碰到一个郁闷的问题就是解析器以一个 ValueError异常终止了。这是个严肃的问题。因为你无法分辨导致ValueError异常的是 解析器的问题还是BLAST的问题。更加糟糕是，你不知道在哪一行解析器失效了。所以， 你不能忽略这个错误。不然，可能会忽视一个重要的数据。 我们以前必须写一些小脚本来解决这个问题。不过，现在 Bio.Blast 模块包含了 BlastErrorParser ，可以更加简单地来解决这个问题。 BlastErrorParser 和常规的 BlastParser 类似，但是它加了特别一层来捕获由解析器产生的ValueErrors 异常，并尝试来诊断这些错误。 让我们来看看怎样用这个解析器 - 首先我们定义我们准备解析的文件和报告错误情况的 输出文件。\n\u0026gt;\u0026gt;\u0026gt; import os \u0026gt;\u0026gt;\u0026gt; blast_file = os.path.join(os.getcwd(), \u0026#34;blast_out\u0026#34;, \u0026#34;big_blast.out\u0026#34;) \u0026gt;\u0026gt;\u0026gt; error_file = os.path.join(os.getcwd(), \u0026#34;blast_out\u0026#34;, \u0026#34;big_blast.problems\u0026#34;) 现在我们想要一个 BlastErrorParser ：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Blast import NCBIStandalone \u0026gt;\u0026gt;\u0026gt; error_handle = open(error_file, \u0026#34;w\u0026#34;) \u0026gt;\u0026gt;\u0026gt; blast_error_parser = NCBIStandalone.BlastErrorParser(error_handle) 注意，解析器有个关于文件句柄的可选参数。如果传递了这个参数，那么解析器就会 把产生ValueError异常的记录写到这个文件句柄中。不然的话，这些错误记录就不会 被记录下来。 现在，我们可以像用常规的blast解析器一样地用 BlastErrorParser 。 特别的是，我们也许想要一个一次读入一个记录的迭代器并用 BlastErrorParser 来解析它。\n\u0026gt;\u0026gt;\u0026gt; result_handle = open(blast_file) \u0026gt;\u0026gt;\u0026gt; iterator = NCBIStandalone.Iterator(result_handle, blast_error_parser) 我们可以一次读一个记录，并且我们现在可以捕获并处理那些因为Blast引起的、 不是解析器本身导致的错误。\n\u0026gt;\u0026gt;\u0026gt; try: ... next_record = iterator.next() ... except NCBIStandalone.LowQualityBlastError, info: ... print \u0026#34;LowQualityBlastError detected in id %s\u0026#34; % info[1] .next() 方法通常被 for 循环间接地调用。现在， BlastErrorParser 能够捕获如下的错误：\nValueError - 这就是和常规BlastParser产生的一样的错误。这个错误产生 是因为解析器不能解析某个文件。通常是因为解析器有bug， 或者是 因为你使用解析器的版本和你BLAST命令的版本不一致。 LowQualityBlastError - 当Blast一条低质量的序列时（比如，一条 只有1个核苷酸的短序列），似乎Blast会终止并屏蔽掉整个序列，所有就没有什么可以 解析了。 这种情况下，Blast就会产生一个不完整的报告导致解析器出现ValueError 错误。 LowQualityBlastError 错误在这种情况下产生。这个错误返回如下 信息： item[0] – The error message item[0] - 错误消息 item[1] – The id of the input record that caused the error. This is really useful if you want to record all of the records that are causing problems. item[1] - 导致错误产生的输入记录id。如果你想记录所有导致问题 记录的时候很有用。 就像上面提到的那样，BlastErrorParser 将会把有问题的记录写到指定的error_handle。 然后，你可以排查这些有问题记录。你可以针对某条记录来调试解析器，或者找到 你运行blast中的问题。无论哪种方式，这些都是有用的经验。\n","date":"2021-08-07T00:00:00Z","permalink":"https://example.com/p/ch7_blast/","title":"ch7_blast"},{"content":"biopython官方地址：https://biopython.org/\ngithub地址：https://github.com/biopython/biopython/blob/master/\n中文版教程：https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html\nbiopython包的说明（具体到每个模块了）：https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html\nMSA的基本原理 多序列比对（Multiple Sequence Alignment, MSA）是指对多个序列进行对位排列。 这通常需要保证序列间的等同位点处在同一列上，并通过引进小横线（-）以保证最终的序列具有相同的长度。这种序列比对可以视作是由字符组成的矩阵。在Biopython中，多序列比对中每一个序列是以 SeqRecord 对象来表示的。 这里我们介绍一种新的对象 – MultipleSeqAlignment 来表示这样一类数据，我们还将介绍 Bio.AlignIO 模块来读写不同格式的多序列比对数据（ Bio.AlignIO 在设计上与之前介绍的 Bio.SeqIO 模块是类似的）。Biopython中， Bio.SeqIO 和 Bio.AlignIO 都能读写各种格式的多序列比对数据。在实际处理中，使用哪一个模块取决于用户需要对数据进行何种操作。\n读取多序列比对数据 在Biopython中，有两种方法读取多序列比对数据：\nio.AlignIO.read() 只能读取一个多序列比对，在大多数情况下，你所遇到的文件仅仅包括一个多序列比对。这时，你应该使用 Bio.AlignIO.read() ，这将返回一个 MultipleSeqAlignment 对象 Bio.AlignIO.parse() 可以依次读取多个序列比对数据，将会返回一个 MultipleSeqAlignment 的 迭代器（iterator） 。迭代器往往在循环中使用。在实际数据分析过程中会时常处理包含有多个多序列比对的文件 这两个函数的参数： 第一个参数为包含有多序列比对数据的 句柄（handle） 。在实际操作中，这往往是一个打开的文件/文件名 第二个参数为多序列比对文件格式（小写）（http://biopython.org/wiki/AlignIO 这里可以看支持哪几种格式） Bio.AlignIO 模块还接受一个可选参数 seq_count，它可以处理不确定的多序列比对格式，或者包含有多个序列的排列 单一的序列比对 F05371_seed.sth文件是一个FAM数据库中Phage_Coat_Gp8的种子排列（PF05371） 你会注意到，以上输出截短了中间一部分序列的内容。你也可以很容易地通过控制多序列比对中每一条序列（作为 SeqRecord 对象）来输出你所喜欢的格式。例如 你是否已经注意到以上原始数据文件中包含有蛋白数据库（PDB）交叉引用以及相关二级结构的信息？可以尝试以下代码：record.dbxrefs 多个序列比对 如果你想用 Bio.AlignIO 来读取这个文件，你可以使用： 与 Bio.SeqIO.parse 一样， Bio.SeqIO.parse() 将返回一个迭代器（iterator）。如果你希望把所有的序列比对都读取到内存中，以下代码将把它们储存在一个列表对象里 含糊的序列比对 将多个序列比对以FASTA格式储存并不方便。然而，在某些情况下，如果你一定要这么做， Bio.AlignIO 依然能够处理上述情形（但是所有的序列比对必须都含有相同的序列）。一个很常见的例子是，我们经常会使用EMBOSS工具箱中的 needle 和 water 来产生许多两两间的序列比对 —— 然而在这种情况下，你可以指定数据格式为“emboss”，Bio.AlignIO 仍然能够识别这些原始输出 为了处理这样的FASTA格式的数据，我们可以指定 Bio.AlignIO.parse() 的第三个可选参数 seq_count ，这一参数将告诉Biopython你所期望的每个序列比对中序列的个数。例如 就是这里将6条序列分成了三份，这三份单独去进行 如果你使用 Bio.AlignIO.read() 或者 Bio.AlignIO.parse() 而不指定 seq_count ，这将返回一个包含有六条序列的序列比对。对于上面的第三个例子，由于序列长度不同，导致它们不能被解析为一个序列比对，Biopython将会抛出一个异常。 如果数据格式本身包含有分割符， Bio.AlignIO 可以很聪明地自动确定文件中每一个序列比对，而无需指定 seq_count 选项。如果你仍然指定 seq_count 但是却与数据本身的分隔符相冲突，Biopython将产生一个错误。 注意指定这一可选的 seq_count 参数将假设文件中所有的序列比对都包含相同数目的序列。假如你真的遇到每一个序列比对都有不同数目的序列， Bio.AlignIO 将无法读取。这时，我们建议你使用 Bio.SeqIO 来读取数据，然后将序列转换为序列比对\n序列比对的写出 Bio.AlignIO.write() 接受三个参数：\n一个 MultipleSeqAlignment 对象（或者是一个 Alignment 对象），一个可写的文件句柄（handle）或者期望写出的文件名，以及写出文件的格式 我们手动创造一个MultipleSeqAlignment对象 from Bio.Seq import Seq from Bio.SeqRecord import SeqRecord from Bio.Align import MultipleSeqAlignment align1 = MultipleSeqAlignment([ SeqRecord(Seq(\u0026#34;ACTGCTAGCTAG\u0026#34;), id=\u0026#34;Alpha\u0026#34;), SeqRecord(Seq(\u0026#34;ACT-CTAGCTAG\u0026#34;), id=\u0026#34;Beta\u0026#34;), SeqRecord(Seq(\u0026#34;ACTGCTAGDTAG\u0026#34;), id=\u0026#34;Gamma\u0026#34;), ]) align2 = MultipleSeqAlignment([ SeqRecord(Seq(\u0026#34;GTCAGC-AG\u0026#34;), id=\u0026#34;Delta\u0026#34;), SeqRecord(Seq(\u0026#34;GACAGCTAG\u0026#34;), id=\u0026#34;Epsilon\u0026#34;), SeqRecord(Seq(\u0026#34;GTCAGCTAG\u0026#34;), id=\u0026#34;Zeta\u0026#34;), ]) align3 = MultipleSeqAlignment([ SeqRecord(Seq(\u0026#34;ACTAGTACAGCTG\u0026#34;), id=\u0026#34;Eta\u0026#34;), SeqRecord(Seq(\u0026#34;ACTAGTACAGCT-\u0026#34;), id=\u0026#34;Theta\u0026#34;), SeqRecord(Seq(\u0026#34;-CTACTACAGGTG\u0026#34;), id=\u0026#34;Iota\u0026#34;), ]) my_alignments = [align1, align2, align3] 将其写出为PHYLIP格式的对象\nfrom Bio import AlignIO AlignIO.write(my_alignments, \u0026#34;my_example.phy\u0026#34;, \u0026#34;phylip\u0026#34;) 序列比对的格式间转换 建议使用 Bio.AlignIO.parse() 来读取序列比对数据，然后使用 Bio.AlignIO.write() 函数来写出。或者你也可以直接使用 Bio.AlignIO.convert() 函数来实现格式的转换 在本例中，我们将读取PFAM/Stockholm格式的序列比对，然后将其保存为Clustal格式\nfrom Bio import AlignIO count = AlignIO.convert(\u0026#34;PF05371_seed.sth\u0026#34;,\u0026#34;stockholm\u0026#34;,\u0026#34;PF05371_seed.sth\u0026#34;,\u0026#34;clustal\u0026#34;) print(\u0026#34;Converted %i alignments\u0026#34; % count) 或者，使用 Bio.AlignIO.parse() 和 Bio.AlignIO.write()\n\u0026gt;\u0026gt;\u0026gt; from Bio import AlignIO \u0026gt;\u0026gt;\u0026gt; alignments = AlignIO.parse(\u0026#34;PF05371_seed.sth\u0026#34;, \u0026#34;stockholm\u0026#34;) \u0026gt;\u0026gt;\u0026gt; count = AlignIO.write(alignments, \u0026#34;PF05371_seed.aln\u0026#34;, \u0026#34;clustal\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Converted %i alignments\u0026#34; % count) Converted 1 alignments 将序列比对对象转换为格式化字符串（formatted strings) 因为 Bio.AlignIO 模块是基于文件句柄的，因此你如果想将序列比对读入为一个字符串对象，你需要做一些额外的工作。然而，我们提供一个 format() 方法来帮助你实现这项任务。 format() 方法需要用户提供一个小写的格式参数（这可以是任何 AlignIO 支持的序列比对格式）\n\u0026gt;\u0026gt;\u0026gt; from Bio import AlignIO \u0026gt;\u0026gt;\u0026gt; alignment = AlignIO.read(\u0026#34;PF05371_seed.sth\u0026#34;, \u0026#34;stockholm\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print(alignment.format(\u0026#34;clustal\u0026#34;)) CLUSTAL X (1.81) multiple sequence alignment COATB_BPIKE/30-81 AEPNAATNYATEAMDSLKTQAIDLISQTWPVVTTVVVAGLVIRLFKKFSS Q9T0Q8_BPIKE/1-52 AEPNAATNYATEAMDSLKTQAIDLISQTWPVVTTVVVAGLVIKLFKKFVS COATB_BPI22/32-83 DGTSTATSYATEAMNSLKTQATDLIDQTWPVVTSVAVAGLAIRLFKKFSS ... format() 方法是利用 StringIO 以及 Bio.AlignIO.write() 来实现以上输出的。如果你使用的是较老版本的Biopython，你可以使用以下代码来完成相同的工作\n\u0026gt;\u0026gt;\u0026gt; from Bio import AlignIO \u0026gt;\u0026gt;\u0026gt; from StringIO import StringIO \u0026gt;\u0026gt;\u0026gt; alignments = AlignIO.parse(\u0026#34;PF05371_seed.sth\u0026#34;, \u0026#34;stockholm\u0026#34;) \u0026gt;\u0026gt;\u0026gt; out_handle = StringIO() \u0026gt;\u0026gt;\u0026gt; AlignIO.write(alignments, out_handle, \u0026#34;clustal\u0026#34;) 1 \u0026gt;\u0026gt;\u0026gt; clustal_data = out_handle.getvalue() \u0026gt;\u0026gt;\u0026gt; print(clustal_data) CLUSTAL X (1.81) multiple sequence alignment COATB_BPIKE/30-81 AEPNAATNYATEAMDSLKTQAIDLISQTWPVVTTVVVAGLVIRLFKKFSS Q9T0Q8_BPIKE/1-52 AEPNAATNYATEAMDSLKTQAIDLISQTWPVVTTVVVAGLVIKLFKKFVS COATB_BPI22/32-83 DGTSTATSYATEAMNSLKTQATDLIDQTWPVVTSVAVAGLAIRLFKKFSS COATB_BPM13/24-72 AEGDDP---AKAAFNSLQASATEYIGYAWAMVVVIVGATIGIKLFKKFTS ... 序列比对的操纵 切片操纵 首先，用户可以认为读入的序列比对是一个由 SeqRecord 对象构成的Python列表（list）。有了这样一个印象以后，你可以使用 len() 方法来得到行数（序列比对的个数），你也可以对序列比对进行迭代。\n\u0026gt;\u0026gt;\u0026gt; from Bio import AlignIO \u0026gt;\u0026gt;\u0026gt; alignment = AlignIO.read(\u0026#34;PF05371_seed.sth\u0026#34;, \u0026#34;stockholm\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Number of rows: %i\u0026#34; % len(alignment)) Number of rows: 7 \u0026gt;\u0026gt;\u0026gt; for record in alignment: ... print(\u0026#34;%s - %s\u0026#34; % (record.seq, record.id)) ... AEPNAATNYATEAMDSLKTQAIDLISQTWPVVTTVVVAGLVIRLFKKFSSKA - COATB_BPIKE/30-81 AEPNAATNYATEAMDSLKTQAIDLISQTWPVVTTVVVAGLVIKLFKKFVSRA - Q9T0Q8_BPIKE/1-52 DGTSTATSYATEAMNSLKTQATDLIDQTWPVVTSVAVAGLAIRLFKKFSSKA - COATB_BPI22/32-83 AEGDDP---AKAAFNSLQASATEYIGYAWAMVVVIVGATIGIKLFKKFTSKA - COATB_BPM13/24-72 AEGDDP---AKAAFDSLQASATEYIGYAWAMVVVIVGATIGIKLFKKFASKA - COATB_BPZJ2/1-49 AEGDDP---AKAAFDSLQASATEYIGYAWAMVVVIVGATIGIKLFKKFTSKA - Q9T0Q9_BPFD/1-49 FAADDATSQAKAAFDSLTAQATEMSGYAWALVVLVVGATVGIKLFKKFVSRA - COATB_BPIF1/22-73 你可以使用列表所拥有的 append 和 extend 方法来给序列比对增加序列。请读者一定要正确理解序列比对与其包含的序列的关系，这样你就可以使用切片操作来获得其中某些序列比对。\n\u0026gt;\u0026gt;\u0026gt; print(alignment) Alignment with 7 rows and 52 columns AEPNAATNYATEAMDSLKTQAIDLISQTWPVVTTVVVAGLVIRL...SKA COATB_BPIKE/30-81 AEPNAATNYATEAMDSLKTQAIDLISQTWPVVTTVVVAGLVIKL...SRA Q9T0Q8_BPIKE/1-52 DGTSTATSYATEAMNSLKTQATDLIDQTWPVVTSVAVAGLAIRL...SKA COATB_BPI22/32-83 AEGDDP---AKAAFNSLQASATEYIGYAWAMVVVIVGATIGIKL...SKA COATB_BPM13/24-72 AEGDDP---AKAAFDSLQASATEYIGYAWAMVVVIVGATIGIKL...SKA COATB_BPZJ2/1-49 AEGDDP---AKAAFDSLQASATEYIGYAWAMVVVIVGATIGIKL...SKA Q9T0Q9_BPFD/1-49 FAADDATSQAKAAFDSLTAQATEMSGYAWALVVLVVGATVGIKL...SRA COATB_BPIF1/22-73 \u0026gt;\u0026gt;\u0026gt; print(alignment[3:7]) Alignment with 4 rows and 52 columns AEGDDP---AKAAFNSLQASATEYIGYAWAMVVVIVGATIGIKL...SKA COATB_BPM13/24-72 AEGDDP---AKAAFDSLQASATEYIGYAWAMVVVIVGATIGIKL...SKA COATB_BPZJ2/1-49 AEGDDP---AKAAFDSLQASATEYIGYAWAMVVVIVGATIGIKL...SKA Q9T0Q9_BPFD/1-49 FAADDATSQAKAAFDSLTAQATEMSGYAWALVVLVVGATVGIKL...SRA COATB_BPIF1/22-73 \u0026gt;\u0026gt;\u0026gt; print(alignment[2,6]) T \u0026gt;\u0026gt;\u0026gt; print(alignment[2].seq[6]) T \u0026gt;\u0026gt;\u0026gt; print(alignment[:,6]) TTT---T \u0026gt;\u0026gt;\u0026gt; print(alignment[3:6,:6]) Alignment with 3 rows and 6 columns AEGDDP COATB_BPM13/24-72 AEGDDP COATB_BPZJ2/1-49 AEGDDP Q9T0Q9_BPFD/1-49 \u0026gt;\u0026gt;\u0026gt; print(alignment[:,:6]) Alignment with 7 rows and 6 columns AEPNAA COATB_BPIKE/30-81 AEPNAA Q9T0Q8_BPIKE/1-52 DGTSTA COATB_BPI22/32-83 AEGDDP COATB_BPM13/24-72 AEGDDP COATB_BPZJ2/1-49 AEGDDP Q9T0Q9_BPFD/1-49 FAADDA COATB_BPIF1/22-73 现在，你可以通过列来操纵序列比对。这也是你能够去除序列比对中的许多列。例如：\n\u0026gt;\u0026gt;\u0026gt; edited = alignment[:,:6] + alignment[:,9:] \u0026gt;\u0026gt;\u0026gt; print(edited) Alignment with 7 rows and 49 columns AEPNAAATEAMDSLKTQAIDLISQTWPVVTTVVVAGLVIRLFKKFSSKA COATB_BPIKE/30-81 AEPNAAATEAMDSLKTQAIDLISQTWPVVTTVVVAGLVIKLFKKFVSRA Q9T0Q8_BPIKE/1-52 DGTSTAATEAMNSLKTQATDLIDQTWPVVTSVAVAGLAIRLFKKFSSKA COATB_BPI22/32-83 AEGDDPAKAAFNSLQASATEYIGYAWAMVVVIVGATIGIKLFKKFTSKA COATB_BPM13/24-72 AEGDDPAKAAFDSLQASATEYIGYAWAMVVVIVGATIGIKLFKKFASKA COATB_BPZJ2/1-49 AEGDDPAKAAFDSLQASATEYIGYAWAMVVVIVGATIGIKLFKKFTSKA Q9T0Q9_BPFD/1-49 FAADDAAKAAFDSLTAQATEMSGYAWALVVLVVGATVGIKLFKKFVSRA COATB_BPIF1/22-73 另一个经常使用的序列比对操作是将多个基因的序列比对拼接成一个大的序列比对（meta-alignment）。 在进行这种操作时一定要注意序列的ID需要匹配。为了达到这种目的，用 sort() 方法将序列ID按照字母顺序进行排列可能会有所帮助 注意：只有当两个序列比对拥有相同的行的时候才能进行序列比对的拼接。\n\u0026gt;\u0026gt;\u0026gt; edited.sort() \u0026gt;\u0026gt;\u0026gt; print(edited) Alignment with 7 rows and 49 columns DGTSTAATEAMNSLKTQATDLIDQTWPVVTSVAVAGLAIRLFKKFSSKA COATB_BPI22/32-83 FAADDAAKAAFDSLTAQATEMSGYAWALVVLVVGATVGIKLFKKFVSRA COATB_BPIF1/22-73 AEPNAAATEAMDSLKTQAIDLISQTWPVVTTVVVAGLVIRLFKKFSSKA COATB_BPIKE/30-81 AEGDDPAKAAFNSLQASATEYIGYAWAMVVVIVGATIGIKLFKKFTSKA COATB_BPM13/24-72 AEGDDPAKAAFDSLQASATEYIGYAWAMVVVIVGATIGIKLFKKFASKA COATB_BPZJ2/1-49 AEPNAAATEAMDSLKTQAIDLISQTWPVVTTVVVAGLVIKLFKKFVSRA Q9T0Q8_BPIKE/1-52 AEGDDPAKAAFDSLQASATEYIGYAWAMVVVIVGATIGIKLFKKFTSKA Q9T0Q9_BPFD/1-49 序列比对作为数组 有时将序列比对转换为字符数组是非常方便的。你可以用 Numpy 来实现这一目的\n\u0026gt;\u0026gt;\u0026gt; import numpy as np \u0026gt;\u0026gt;\u0026gt; from Bio import AlignIO \u0026gt;\u0026gt;\u0026gt; alignment = AlignIO.read(\u0026#34;PF05371_seed.sth\u0026#34;, \u0026#34;stockholm\u0026#34;) \u0026gt;\u0026gt;\u0026gt; align_array = np.array([list(rec) for rec in alignment], np.character) \u0026gt;\u0026gt;\u0026gt; print(\u0026#34;Array shape %i by %i\u0026#34; % align_array.shape) Array shape 7 by 52 如果你需要频繁地使用列操作，你可以让 Numpy 将序列比对以列的形式进行储存（与Fortran一样），而不是 Numpy 默认形式（与C一样以行储存）：\n\u0026gt;\u0026gt;\u0026gt; align_array = np.array([list(rec) for rec in alignment], np.character, order=\u0026#34;F\u0026#34;) 获取线路信息 substitutions属性报告路线中的字母相互替换的频率 路线的substitutions属性报告路线中的字母相互替换的频率。通过对路线中的所有行对进行计数，计算两个字母彼此对齐的次数并在所有对中求和计算得出\n\u0026gt;\u0026gt;\u0026gt; from Bio.Seq import Seq \u0026gt;\u0026gt;\u0026gt; from Bio.SeqRecord import SeqRecord \u0026gt;\u0026gt;\u0026gt; from Bio.Align import MultipleSeqAlignment \u0026gt;\u0026gt;\u0026gt; alignment = MultipleSeqAlignment( ... [ ... SeqRecord(Seq(\u0026#34;ACTCCTA\u0026#34;), id=\u0026#39;seq1\u0026#39;), ... SeqRecord(Seq(\u0026#34;AAT-CTA\u0026#34;), id=\u0026#39;seq2\u0026#39;), ... SeqRecord(Seq(\u0026#34;CCTACT-\u0026#34;), id=\u0026#39;seq3\u0026#39;), ... SeqRecord(Seq(\u0026#34;TCTCCTC\u0026#34;), id=\u0026#39;seq4\u0026#39;), ... ] ... ) ... \u0026gt;\u0026gt;\u0026gt; print(alignment) Alignment with 4 rows and 7 columns ACTCCTA seq1 AAT-CTA seq2 CCTACT- seq3 TCTCCTC seq4 \u0026gt;\u0026gt;\u0026gt; substitutions = alignment.substitutions \u0026gt;\u0026gt;\u0026gt; print(substitutions) A C T A 2.0 4.5 1.0 C 4.5 10.0 0.5 T 1.0 0.5 12.0 由于对的顺序是任意的，因此计数在对角线的上方和下方均分。例如，A到C的9个对齐在位置[‘A’，’C’]处为4.5，在位置[‘C’，’A’]处为4.5，当根据这些计数计算替换矩阵时，这种安排有助于简化数学运算 请注意，包含仅在路线中出现的字母的条目。 您可以使用select方法添加缺少字母的条目，例如\n\u0026gt;\u0026gt;\u0026gt; m = substitutions.select(\u0026#34;ATCG\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print(m) A T C G A 2.0 1.0 4.5 0.0 T 1.0 12.0 0.5 0.0 C 4.5 0.5 10.0 0.0 G 0.0 0.0 0.0 0.0 构建序列比对工具 目前有非常多的算法来帮助你构建一个序列比对，包括两两间的比对和多序列比对。这些算法在计算上往往是非常慢的，你一定不会希望用Python来实现他们。然而，你可以使用Biopython来运行命令行程序。通常你需要\n准备一个包含未比对序列的输入文件，一般为FASTA格式的序列。你可以使用 Bio.SeqIO 来创建一个 在Biopython中运行一个命令行程序来构建序列比对（我们将在这里详细介绍）。这需要通过Biopython的打包程序（wrapper）来实现 读取以上程序的输出，也就是排列好的序列比对。这往往可以通过 Bio.AlignIO 来实现 本章所介绍的所有的命令行打包程序都将以同样的方式使用。你创造一个命令行对象来指定各种参数（例如：输入文件名，输出文件名等），然后通过Python的系统命令模块来运行这一程序（例如：使用 subprocess 进程）\n大多数的打包程序都在 Bio.Align.Applications 中定义 \u0026gt;\u0026gt;\u0026gt; import Bio.Align.Applications \u0026gt;\u0026gt;\u0026gt; dir(Bio.Align.Applications) [\u0026#39;ClustalwCommandline\u0026#39;, \u0026#39;DialignCommandline\u0026#39;, \u0026#39;MafftCommandline\u0026#39;, \u0026#39;MuscleCommandline\u0026#39;, \u0026#39;PrankCommandline\u0026#39;, \u0026#39;ProbconsCommandline\u0026#39;, \u0026#39;TCoffeeCommandline\u0026#39; ...] ClustalW ClustalW是一个非常流行的进行多序列比对的命令行程序（其还有一个图形化的版本称之为ClustalX）。Biopython的 Bio.Align.Applications 模块包含这一多序列比对程序的打包程序 我们建议你在Python中使用ClustalW之前在命令行界面下手动使用ClustalW，这样能使你更清楚这一程序的参数。你会发现Biopython打包程序非常严格地遵循实际的命令行API：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Align.Applications import ClustalwCommandline \u0026gt;\u0026gt;\u0026gt; help(ClustalwCommandline) ClustalW在默认情况下会产生一个包括所有输入序列的序列比对以及一个由输入序列名字构成的指导树（guide tree）。例如，用上述文件作为输入，ClustalW将会输出 opuntia.aln 和 opuntia.dnd 两个文件：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Align.Applications import ClustalwCommandline \u0026gt;\u0026gt;\u0026gt; cline = ClustalwCommandline(\u0026#34;clustalw2\u0026#34;, infile=\u0026#34;opuntia.fasta\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print(cline) clustalw2 -infile=opuntia.fasta 注意这里我们给出的执行文件名是 clustalw2 ，这是ClustalW的第二个版本（第一个版本的文件名为 clustalw ）。ClustalW的这两个版本具有相同的参数，并且在功能上也是一致的 你可能会发现，尽管你安装了ClustalW，以上的命令行却无法正确运行。你可能会得到“command not found”的错误信息（尤其是在Windows上）。这往往是由于ClustalW的运行程序并不在系统的工作目录PATH下（一个包含着运行程序路径的环境变量）。你既可以修改PATH，使其包括ClustalW的运行程序（不同系统需要以不同的方式修改），或者你也可以直接指定程序的绝对路径 注意，Python中 \\n 和 \\t 会被解析为一个新行和制表空白（tab）。然而，如果你将一个小写的“r”放在字符串的前面，这一字符串就将保留原始状态，而不被解析。这种方式对于指定Windows风格的文件名来说是一种良好的习惯。\n\u0026gt;\u0026gt;\u0026gt; import os \u0026gt;\u0026gt;\u0026gt; from Bio.Align.Applications import ClustalwCommandline \u0026gt;\u0026gt;\u0026gt; clustalw_exe = r\u0026#34;C:\\Program Files\\new clustal\\clustalw2.exe\u0026#34; \u0026gt;\u0026gt;\u0026gt; clustalw_cline = ClustalwCommandline(clustalw_exe, infile=\u0026#34;opuntia.fasta\u0026#34;) \u0026gt;\u0026gt;\u0026gt; assert os.path.isfile(clustalw_exe), \u0026#34;Clustal W executable missing\u0026#34; \u0026gt;\u0026gt;\u0026gt; stdout, stderr = clustalw_cline() Biopython在内部使用较新的 subprocess 模块来实现打包程序，而不是 os.system() 和 os.popen*\n命令行工具到底是如何运作的 现在，我们有必要去了解命令行工具是如何工作的。当你使用一个命令行时，它往往会在屏幕上输出一些内容。这一输出可以被保存或重定向。在系统输出中，有两种管道（pipe）来区分不同的输出信息–标准输出（standard output）包含正常的输出内容，标准错误（standard error）显示错误和调试信息。同时，系统也接受标准输入（standard input）。这也是命令行工具如何读取数据文件的。当程序运行结束以后，它往往会返回一个整数。一般返回值为0意味着程序正常结束。 当你使用Biopython打包程序来调用命令行工具的时候，它将会等待程序结束，并检查程序的返回值。如果返回值不为0，Biopython将会提示一个错误信息。Biopython打包程序将会输出两个字符串，标准输出和标准错误。 在ClustalW的例子中，当你使用程序时，所有重要的输出都被保存到输出文件中。所有打印在屏幕上的内容（通过 stdout or stderr）可以被忽略掉（假设它已经成功运行）。\n运行clustalW时返回的.aln文件和dnd文件 当运行ClustalW的时候，我们所关心的往往是输出的序列比对文件和指导树文件。ClustalW会自动根据输入数据的文件名来命名输出文件。在本例中，输出文件将是 opuntia.aln 。当你成功运行完ClustalW以后，你可以使用 Bio.AlignIO 来读取输出结果：\n\u0026gt;\u0026gt;\u0026gt; from Bio import AlignIO \u0026gt;\u0026gt;\u0026gt; align = AlignIO.read(\u0026#34;opuntia.aln\u0026#34;, \u0026#34;clustal\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print(align) SingleLetterAlphabet() alignment with 7 rows and 906 columns TATACATTAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273285|gb|AF191659.1|AF191 TATACATTAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273284|gb|AF191658.1|AF191 TATACATTAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273287|gb|AF191661.1|AF191 TATACATAAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273286|gb|AF191660.1|AF191 TATACATTAAAGGAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273290|gb|AF191664.1|AF191 TATACATTAAAGGAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273289|gb|AF191663.1|AF191 TATACATTAAAGGAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273291|gb|AF191665.1|AF191 另一个输出文件 opuntia.dnd 中包含有一个newick格式的指导树，你可以使用Biopython中的 Bio.Phylo 来读取它\n\u0026gt;\u0026gt;\u0026gt; from Bio import Phylo \u0026gt;\u0026gt;\u0026gt; tree = Phylo.read(\u0026#34;opuntia.dnd\u0026#34;, \u0026#34;newick\u0026#34;) \u0026gt;\u0026gt;\u0026gt; Phylo.draw_ascii(tree) _______________ gi|6273291|gb|AF191665.1|AF191665 __________________________| | | ______ gi|6273290|gb|AF191664.1|AF191664 | |__| | |_____ gi|6273289|gb|AF191663.1|AF191663 | _|_________________ gi|6273287|gb|AF191661.1|AF191661 | |__________ gi|6273286|gb|AF191660.1|AF191660 | | __ gi|6273285|gb|AF191659.1|AF191659 |___| | gi|6273284|gb|AF191658.1|AF191658 MUSCLE MUSCLE是另一个较新的序列比对工具，Biopython的 Bio.Align.Applications 中也有针对Muscle的打包程序。与ClustalW一样，我们也建议你先在命令行界面下使用MUSCLE以后再使用Biopython打包程序。你会发现，Biopython的打包程序非常严格地包括了所有命令行输入参数：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Align.Applications import MuscleCommandline \u0026gt;\u0026gt;\u0026gt; help(MuscleCommandline) 简单的应用 作为最简单的例子，你只需要一个Fasta格式的数据文件作为输入。例如： opuntia.fasta 然后你可以告诉MUSCLE来读取该FASTA文件，并将序列比对写出： 注意，MUSCLE使用“-in”和“-out”来指定输入和输出文件，而在Biopython中，我们使用“input”和“out”作为关键字来指定输入输出。这是由于“in”是Python的一个关键词而被保留。\n\u0026gt;\u0026gt;\u0026gt; from Bio.Align.Applications import MuscleCommandline \u0026gt;\u0026gt;\u0026gt; cline = MuscleCommandline(input=\u0026#34;opuntia.fasta\u0026#34;, out=\u0026#34;opuntia.txt\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print(cline) muscle -in opuntia.fasta -out opuntia.txt 默认情况下，MUSCLE的输出文件将是包含间隔（gap）的FASTA格式文件。 当你指定 format=fasta 时， Bio.AlignIO 能够读取该FASTA文件。你也可以告诉MUSCLE来输出ClustalW-like的文件结果\n\u0026gt;\u0026gt;\u0026gt; from Bio.Align.Applications import MuscleCommandline \u0026gt;\u0026gt;\u0026gt; cline = MuscleCommandline(input=\u0026#34;opuntia.fasta\u0026#34;, out=\u0026#34;opuntia.aln\u0026#34;, clw=True) \u0026gt;\u0026gt;\u0026gt; print(cline) muscle -in opuntia.fasta -out opuntia.aln -clw 或者，严格的ClustalW的输出文件（这将输出原始的ClustalW的文件标签）。例如：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Align.Applications import MuscleCommandline \u0026gt;\u0026gt;\u0026gt; cline = MuscleCommandline(input=\u0026#34;opuntia.fasta\u0026#34;, out=\u0026#34;opuntia.aln\u0026#34;, clwstrict=True) \u0026gt;\u0026gt;\u0026gt; print(cline) muscle -in opuntia.fasta -out opuntia.aln -clwstrict 你可以使用 Bio.AlignIO 的 format=\u0026ldquo;clustal\u0026rdquo; 参数来读取这些序列比对输出。\nMUSCLE标准输出 使用以上的MUSCLE命令行将会把序列比对结果写出到一个文件中。然而MUSCLE也允许你将序列比对结果作为系统的标准输出。Biopython打包程序可以利用这一特性来避免创建一个临时文件。例如：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Align.Applications import MuscleCommandline \u0026gt;\u0026gt;\u0026gt; muscle_cline = MuscleCommandline(input=\u0026#34;opuntia.fasta\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print(muscle_cline) muscle -in opuntia.fasta 对于这个muscle_cline的输出要采用stringIO模块去读取 如果你使用打包程序运行上述命令，程序将返回一个字符串对象。为了读取它，我们可以使用 StringIO 模块。记住MUSCLE将默认以FASTA格式输出序列比对：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Align.Applications import MuscleCommandline \u0026gt;\u0026gt;\u0026gt; muscle_cline = MuscleCommandline(input=\u0026#34;opuntia.fasta\u0026#34;) \u0026gt;\u0026gt;\u0026gt; stdout, stderr = muscle_cline() \u0026gt;\u0026gt;\u0026gt; from StringIO import StringIO \u0026gt;\u0026gt;\u0026gt; from Bio import AlignIO \u0026gt;\u0026gt;\u0026gt; align = AlignIO.read(StringIO(stdout), \u0026#34;fasta\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print(align) Alignment with 7 rows and 906 columns TATACATTAAAGGAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273289|gb|AF191663.1|AF191663 TATACATTAAAGGAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273291|gb|AF191665.1|AF191665 TATACATTAAAGGAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273290|gb|AF191664.1|AF191664 TATACATTAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273287|gb|AF191661.1|AF191661 TATACATAAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273286|gb|AF191660.1|AF191660 TATACATTAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273285|gb|AF191659.1|AF191659 TATACATTAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273284|gb|AF191658.1|AF191658 对于大的数据，使用subprocess模块去解决 以上是一个非常简单的例子，如果你希望处理较大的输出数据，我们并不建议你将它们全部读入内存中。对于这种情况， subprocess 模块可以非常方便地处理。例如：\n\u0026gt;\u0026gt;\u0026gt; import subprocess \u0026gt;\u0026gt;\u0026gt; from Bio.Align.Applications import MuscleCommandline \u0026gt;\u0026gt;\u0026gt; muscle_cline = MuscleCommandline(input=\u0026#34;opuntia.fasta\u0026#34;) \u0026gt;\u0026gt;\u0026gt; child = subprocess.Popen(str(muscle_cline), ... stdout=subprocess.PIPE, ... stderr=subprocess.PIPE, ... shell=(sys.platform!=\u0026#34;win32\u0026#34;)) \u0026gt;\u0026gt;\u0026gt; from Bio import AlignIO \u0026gt;\u0026gt;\u0026gt; align = AlignIO.read(child.stdout, \u0026#34;fasta\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print(align) Alignment with 7 rows and 906 columns TATACATTAAAGGAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273289|gb|AF191663.1|AF191663 TATACATTAAAGGAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273291|gb|AF191665.1|AF191665 TATACATTAAAGGAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273290|gb|AF191664.1|AF191664 TATACATTAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273287|gb|AF191661.1|AF191661 TATACATAAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273286|gb|AF191660.1|AF191660 TATACATTAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273285|gb|AF191659.1|AF191659 TATACATTAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273284|gb|AF191658.1|AF191658 以标准输入和标准输出使用MUSCLE 事实上，我们并不需要将序列放在一个文件里来使用MUSCLE。MUSCLE可以读取系统标准输入的内容;为了让MUSCLE读取标准输入的内容，我们首先需要将未排列的序列以 SeqRecord 对象的形式读入到内存。在这里，我们将以一个规则来选择特定的序列（序列长度小于900bp的），使用生成器表达式.\n\u0026gt;\u0026gt;\u0026gt; from Bio import SeqIO \u0026gt;\u0026gt;\u0026gt; records = (r for r in SeqIO.parse(\u0026#34;opuntia.fasta\u0026#34;, \u0026#34;fasta\u0026#34;) if len(r) \u0026lt; 900) 随后，我们需要建立一个MUSCLE命令行，但是不指定输入和输出（MUSCLE默认为标准输入和标准输出）。这里，我们将指定输出格式为严格的Clustal格式：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Align.Applications import MuscleCommandline \u0026gt;\u0026gt;\u0026gt; muscle_cline = MuscleCommandline(clwstrict=True) \u0026gt;\u0026gt;\u0026gt; print(muscle_cline) muscle -clwstrict 我们使用Python的内置模块 subprocess 来实现这一目的：\n\u0026gt;\u0026gt;\u0026gt; import subprocess \u0026gt;\u0026gt;\u0026gt; import sys \u0026gt;\u0026gt;\u0026gt; child = subprocess.Popen(str(cline), ... stdin=subprocess.PIPE, ... stdout=subprocess.PIPE, ... stderr=subprocess.PIPE, ... universal_newlines=True, ... shell=(sys.platform!=\u0026#34;win32\u0026#34;)) 这一命令将启动MUSCLE，但是它将会等待FASTA格式的输入数据。我们可以通过标准输入句柄来提供给它：\n\u0026gt;\u0026gt;\u0026gt; SeqIO.write(records, child.stdin, \u0026#34;fasta\u0026#34;) 6 \u0026gt;\u0026gt;\u0026gt; child.stdin.close() 在将6条序列写入句柄后，MUSCLE仍将会等待，判断是否所有的FASTA序列全部输入完毕了。我们可以关闭句柄来提示给MUSCLE。这时，MUSCLE将开始运行。最后，我们可以在标准输出中获得结果：\n\u0026gt;\u0026gt;\u0026gt; from Bio import AlignIO \u0026gt;\u0026gt;\u0026gt; align = AlignIO.read(child.stdout, \u0026#34;clustal\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print(align) Alignment with 6 rows and 900 columns TATACATTAAAGGAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273290|gb|AF191664.1|AF19166 TATACATTAAAGGAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273289|gb|AF191663.1|AF19166 TATACATTAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273287|gb|AF191661.1|AF19166 TATACATAAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273286|gb|AF191660.1|AF19166 TATACATTAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273285|gb|AF191659.1|AF19165 TATACATTAAAGAAGGGGGATGCGGATAAATGGAAAGGCGAAAG...AGA gi|6273284|gb|AF191658.1|AF19165 EMBOSS包的序列比对工具——needle和water EMBOSS 包有两个序列比对程序—— water 和 needle 来实现Smith-Waterman做局部序列比对（local alignment）和Needleman-Wunsch算法来做全局排列（global alignment）。这两个程序具有相同的使用方式，因此我们仅以 needle 为例 这里以alpha.fasta / beta.fasta为例子 开始使用一个完整的 needle 命令行对象：\n\u0026gt;\u0026gt;\u0026gt; from Bio.Emboss.Applications import NeedleCommandline \u0026gt;\u0026gt;\u0026gt; needle_cline = NeedleCommandline(asequence=\u0026#34;alpha.faa\u0026#34;, bsequence=\u0026#34;beta.faa\u0026#34;, ... gapopen=10, gapextend=0.5, outfile=\u0026#34;needle.txt\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print(needle_cline) needle -outfile=needle.txt -asequence=alpha.faa -bsequence=beta.faa -gapopen=10 -gapextend=0.5 你可能会有疑问，为什么不直接在终端里运行这一程序呢？你会发现，它将进行一个序列两两间的排列，并把结果记录在 needle.txt 中（以EMBOSS默认的序列比对格式）。 或者这样写(PATH)\n\u0026gt;\u0026gt;\u0026gt; from Bio.Emboss.Applications import NeedleCommandline \u0026gt;\u0026gt;\u0026gt; needle_cline = NeedleCommandline(r\u0026#34;C:\\EMBOSS\\needle.exe\u0026#34;, ... asequence=\u0026#34;alpha.faa\u0026#34;, bsequence=\u0026#34;beta.faa\u0026#34;, ... gapopen=10, gapextend=0.5, outfile=\u0026#34;needle.txt\u0026#34;) 也可以直接对Needle指定参数 \u0026gt;\u0026gt;\u0026gt; from Bio.Emboss.Applications import NeedleCommandline \u0026gt;\u0026gt;\u0026gt; needle_cline = NeedleCommandline() \u0026gt;\u0026gt;\u0026gt; needle_cline.asequence=\u0026#34;alpha.faa\u0026#34; \u0026gt;\u0026gt;\u0026gt; needle_cline.bsequence=\u0026#34;beta.faa\u0026#34; \u0026gt;\u0026gt;\u0026gt; needle_cline.gapopen=10 \u0026gt;\u0026gt;\u0026gt; needle_cline.gapextend=0.5 \u0026gt;\u0026gt;\u0026gt; needle_cline.outfile=\u0026#34;needle.txt\u0026#34; \u0026gt;\u0026gt;\u0026gt; print(needle_cline) needle -outfile=needle.txt -asequence=alpha.faa -bsequence=beta.faa -gapopen=10 -gapextend=0.5 \u0026gt;\u0026gt;\u0026gt; print(needle_cline.outfile) needle.txt 现在我们获得了一个 needle 命令行，并希望在Python中运行它。我们在之前解释过，如果你希望完全地控制这一过程， subprocess 是最好的选择，但是如果你只是想尝试使用打包程序，以下命令足以达到目的：\n\u0026gt;\u0026gt;\u0026gt; stdout, stderr = needle_cline() \u0026gt;\u0026gt;\u0026gt; print(stdout + stderr) Needleman-Wunsch global alignment of two sequences 随后，我们需要载入 Bio.AlignIO 模块来读取needle输出（ emboss 格式）：\n\u0026gt;\u0026gt;\u0026gt; from Bio import AlignIO \u0026gt;\u0026gt;\u0026gt; align = AlignIO.read(\u0026#34;needle.txt\u0026#34;, \u0026#34;emboss\u0026#34;) \u0026gt;\u0026gt;\u0026gt; print(align) Alignment with 2 rows and 149 columns MV-LSPADKTNVKAAWGKVGAHAGEYGAEALERMFLSFPTTKTY...KYR HBA_HUMAN MVHLTPEEKSAVTALWGKV--NVDEVGGEALGRLLVVYPWTQRF...KYH HBB_HUMAN 在这个例子中，我们让EMBOSS将结果保存到一个输出文件中，但是你也可以让其写入标准输出中（这往往是在不需要临时文件的情况下的选择，你可以使用 stdout=True 参数而不是 outfile 参数）。与MUSCLE的例子一样，你也可以从标准输入里读取序列（ asequence=\u0026ldquo;stdin\u0026rdquo; 参数）。 以上例子仅仅介绍了 needle 和 water 最简单的使用。一个有用的小技巧是，第二个序列文件可以包含有多个序列，EMBOSS工具将将每一个序列与第一个文件进行两两序列比对。\n","date":"2021-08-06T00:00:00Z","permalink":"https://example.com/p/ch6_multiple_sequence_alignment_objects/","title":"Ch6_Multiple_Sequence_Alignment_objects"},{"content":"biopython官方地址：https://biopython.org/\ngithub地址：https://github.com/biopython/biopython/blob/master/\n中文版教程：https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html\nbiopython包的说明（具体到每个模块了）：https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html\nThe workhorse function Bio.SeqIO.parse() is used to read in sequence data as SeqRecord objects. This function expects two arguments:\nThe first argument is a handle to read the data from, or a filename. A The second argument is a lower case string specifying sequence format The Bio.SeqIO.parse() function returns an iterator which gives SeqRecord objects. Iterators are typically used in a for loop as shown below. Sometimes you\u0026rsquo;ll find yourself dealing with files which contain only a single record. For this situation use the function Bio.SeqIO.read() which takes the same arguments. Provided there is one and only one record in the file, this is returned as a SeqRecord object. Otherwise an exception is raised. SeqIO.parse文件读取 SeqIO.parse读取的对象可以用来直接迭代 The object returned by Bio.SeqIO is actually an iterator which returns SeqRecord objects. You get to see each record in turn, but once and only once. The plus point is that an iterator can save you memory when dealing with large files. 当然了也可以直接用列表生成式 next代替loop Instead of using a for loop, can also use the next() function on an iterator to step through the entries, like this: One special case to consider is when your sequence files have multiple records, but you only want the first one. In this situation the following code is very concise: 就是说使用next的话只会返回第一个loop的结果 A word of warning here – using the next() function like this will silently ignore any additional records in the file. If your files have one and only one record, like some of the online examples later in this chapter, or a GenBank file for a single chromosome, then use the new Bio.SeqIO.read() function instead. This will check there are no extra unexpected records present.\n使用list将SeqRecord装起来 装起来的也是SeqRecord对象 You can of course still use a for loop with a list of SeqRecord objects. Using a list is much more flexible than an iterator (for example, you can determine the number of records from the length of the list), but does need more memory because it will hold all the records in memory at once.\nExtracting data提取数据 像这些注释信息（是以字典的形式存起来的）可以这样取出来 如果想要物种的全部信息也可以这样提取出来 （gbf） Great. That was pretty easy because GenBank files are annotated in a standardised way. 如果想要物种的全部信息也可以这样提取出来 （fasta） let\u0026rsquo;s suppose you wanted to extract a list of the species from a FASTA file, rather than the GenBank file. The bad news is you will have to write some code to extract the data you want from the record\u0026rsquo;s description line - if the information is in the file in the first place! 这个为例子的fasta： 也可以用列表生成式这样写： Modifying data 修改数据 直接重新赋值即可 format为fasta数据时要记得赋上id和description 传入压缩文件 输出总共有多少条序列 打开gz/bz文件 从网上读取文件 parsing sequences from the net SeqIO.read() 是只能读取文件的第一个数据的 First of all, let\u0026rsquo;s fetch just one record. If you don\u0026rsquo;t care about the annotations and features downloading a FASTA file is a good choice as these are compact. Now remember, when you expect the handle to contain one and only one record, use the Bio.SeqIO.read() function:\nEntrez从网上NCBI获取文件 genbankfile 的格式输入 in Biopython 1.50 onwards, we support \u0026ldquo;gb\u0026rdquo; as an alias for \u0026ldquo;genbank\u0026rdquo; in Bio.SeqIO 多records 使用SeqIO.parse() Notice this time we have three features. Now let\u0026rsquo;s fetch several records. This time the handle contains multiple records, so we must use theBio.SeqIO.parse() function: 其实就是因为SeqIO.parse返回的是一个可迭代的对象 使用ExPASy从SwissProt上读取文件 SeqRecord变为Dictionary对象 这是一个简单的前例 SeqIO.to_dict 使用这个时会以id作为默认的key值，使用的对象为 a list or generator giving SeqRecordobjects. 现在试试用fasta文件 像这样的话最好指定下以什么为字典的key了， 先写个函数返回序列的id to_dict的参数key_function的作用 key_function可以传入一个函数，返回key划分的值 使用SEGUID checksum产生to_dict的索引 Sequence files as Dictionaries – Indexed files 相比前面的to_dict这样可以节省相当多的电脑内存\nSeqIO.index()的方法 实现的效果和to_dict完全一样 Note that Bio.SeqIO.index() won\u0026rsquo;t take a handle, but only a filename. However, alignment formats like PHYLIP or Clustal are not supported. Finally as an optional argument you can supply a key function. 同样对于这种也要需要去写个函数去重新捕获key\nSeqIO.index()的key_function参数 get_raw方法可以返回原始形式的数据 The dictionary-like object from Bio.SeqIO.index() gives you each entry as a SeqRecord object. However, it is sometimes useful to be able to get the original raw data straight from the file. For this use the get_raw()method which takes a single argument (the record identifier) and returns a bytes string (extracted from the file without modification). ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.dat.gz SeqIO.index可以直接打开bgz压缩文件 Writing Sequence file将SeqRecord写出 SeqIO.write() 这是一个小例子 Converting between sequence file formats 可以直接将SeqIO.parse()读取的SeqRecord写出文件 In previous example we used a list of SeqRecord objects as input to the Bio.SeqIO.write() function, but it will also accept a SeqRecord iterator like we get from Bio.SeqIO.parse() – this lets us do file conversion by combining these two functions. 使用convert命令也可以 The Bio.SeqIO.convert() function will take handles or filenames. Watch out though – if the output file already exists, it will overwrite it! To find out more, see the built in help: 使用convert获得反向互补链 这是之前的思路 Now, if we want to save these reverse complements to a file, we\u0026rsquo;ll need to make SeqRecord objects. We can use the SeqRecord object\u0026rsquo;s built in .reverse_complement() method (see Section 4.9) but we must decide how to name our new records. This is an excellent place to demonstrate the power of list comprehensions which make a list in memory: 就是说将Seq赋予为SeqIORecord对象是要赋予对应的ID和description 直接使用string写入文件，而不是将SeqRecord对象写入 Bio.SeqIO.FastaIO 这里可以参考下这个：https://www.osgeo.cn/biopython/Bio.SeqIO.FastaIO.html ","date":"2021-08-05T00:00:00Z","permalink":"https://example.com/p/ch5_sequence_input_output/","title":"Ch5_Sequence_Input_Output"},{"content":"biopython官方地址：https://biopython.org/\ngithub地址：https://github.com/biopython/biopython/blob/master/\n中文版教程：https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html\nbiopython包的说明（具体到每个模块了）：https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html\nSequences are essentially strings of letters like AGTACACTGGT, which seems very natural since this is the most common way that sequences are seen in biological file formats. The most important difference between Seq objects and standard Python strings is they have different methods. Although the Seq object supports many of the same methods as a plain string, its translate()method differs by doing biological translation, and there are also additional biologically relevant methods like reverse_complement().\nSeq像string Seq对象是只可读的，不可写\nenumerate方法 切片取值 count方法 Bio.SeqUtils 中的GC可以直接计算CG含量百分比 Seq转为string 即使是Seq对象，print的时候也只会跟str一样返回序列的结果，而不会带上属性 %用法 +号序列拼接 join方法拼接也可以 upper/lower方法 Seq对象获取compement互补链/reverse_complement反向互补链 这肯定不能用在protein seq上咯，因为protein seq不是ATCG\nTranscription转录 Consider the following (made up) stretch of double stranded DNA which encodes a short peptide: The actual biological transcription process works from the template strand, doing a reverse complement (TCAG → CUGA) to give the mRNA. However, in Biopython and bioinformatics in general, we typically work directly with the coding strand because this means we can get the mRNA sequence just by switching T → U.\n一步 两步 back_transcribe方法 translation翻译 将DNA序列翻译为protein序列 翻译table原则：https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi 翻译的table选择 考虑终止密码子的翻译 Notice that when you use the to_stop argument, the stop codon itself is not translated - and the stop symbol is not included at the end of your protein sequence. You can even specify the stop symbol if you don't like the default asterisk: 考虑细菌的起始密码子 Translation Tables选择翻译的table标准 let's just focus on two choices: the Standard translation table, and the translation table for Vertebrate Mitochondrial DNA 甚至可以直接返回终止密码子、起始密码子、直接翻译 序列的比较 Biopython can track the molecule type, so comparing two Seq objects could mean considering this too.Should a DNA fragment \u0026ldquo;ACG\u0026rdquo; and an RNA fragment \u0026ldquo;ACG\u0026rdquo; be equal? What about the peptide \u0026ldquo;ACG\u0026rdquo;? Or the Python string \u0026quot;ACG\u0026quot;? In everyday use, your sequences will generally all be the same type of (all DNA, all RNA, or all protein). Well, as of Biopython 1.65, sequence comparison only looks at the sequence and compares like the Python string. 空的seq对象 MutableSeq 对象 Seq对象只可读，而MutableSeq对象可写\nSeq对象 MutableSeq对象 UnkonwnSeq object对象 这样创建一个未知具体序列的对象效率会更快，而且占用的内存更小 传入长度和未知号码的替代符号： X/N的意义 For DNA or RNA sequences, unknown nucleotides are commonly denoted by the letter \u0026quot;N\u0026quot;, while for proteins \u0026quot;X\u0026quot; is commonly used for unknown amino acids. When creating an 'UnknownSeq', you can specify the character to be used instead of \u0026quot;?\u0026quot; to represent unknown letters.\n即使是UnkownSeq也可以使用Seq的方法 直接在string使用方法也是可以的 虽然这样是可以的，但是还是更加推荐使用Seq对象而不是String对象 ","date":"2021-08-04T00:00:00Z","permalink":"https://example.com/p/ch3_seq_object/","title":"ch3_seq_object"},{"content":"biopython官方地址：https://biopython.org/\ngithub地址：https://github.com/biopython/biopython/blob/master/\n中文版教程：https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html\nbiopython包的说明（具体到每个模块了）：https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html\nhelp查看帮助 SeqRecord对象的属性 创建SeqRecord对象 直接用Seq 可以直接赋予其相关信息 annotations的字典 letter_annotations的字典 读取fasta(NCBI)创建SeqRecord objects 读取GenBank files （GenBamk）创建SeqRecord对象 这里以NC_005816.gb为例 同样用SeqIO读取 SeqFeatrue objects 属性： Postions / locations The key idea about each SeqFeature object is to describe a region on a parent sequence, for which we use a location object, typically describing a range between two positions. Two try to clarify the terminology we\u0026rsquo;re using\nlocations的分类 具体的可以百度看下\nFeatureLocation object CompoundLocation object positions的分类 具体的可以百度看下\nFuzzy postion ExactPosition BeforePosition AfterPosition WithinPosition OneOfPosition UnknownPosition SeqFeature的使用 loop 返回相关注释信息 从Feature返回相关序列 Comparision 像biopython1.67后的版本则会出现raiseerror format格式化的方法 就是说可以将SeqRecord对象生成fasta格式的文件 SeqRecord对象也可以使用切片 增加SeqRecord objects 先搞出来两个SeqRecord的对象 如何将这两个SeqRecord连接在一起呢？ 直接相+ 但是这样对于部分文件来说合并的时候会使得基因注释信息有些搞不见，模糊化\n对于SeqRecord也可以用反向互补，但是要赋予一个新的id ","date":"2021-08-04T00:00:00Z","permalink":"https://example.com/p/ch4_sequence_annotation_object_%E5%BA%8F%E5%88%97%E6%B3%A8%E9%87%8A%E4%BF%A1%E6%81%AF/","title":"Ch4_Sequence_annotation_object_序列注释信息"},{"content":"biopython官方地址：https://biopython.org/\ngithub地址：https://github.com/biopython/biopython/blob/master/\n中文版教程：https://biopython-cn.readthedocs.io/zh_CN/latest/cn/chr06.html\nbiopython包的说明（具体到每个模块了）：https://www.osgeo.cn/biopython/Bio.HMM.DynamicProgramming.html\n安装biopython的库 安装命令：\npip install biopython 从biopython 1.77开始支持py3\n这样可以查看biopython版本 FAQ PAGES https://docs.python.org/3/faq/index.html question email http://biopython.org/wiki/Mailing_lists 引用说明 Peter J. A. Cock, Tiago Antao, Jexb;rey T. Chang, Brad A. Chapman, Cymon J. Cox, Andrew Dalke, Iddo\nFriedberg, Thomas Hamelryck, Frank Kauxb;, Bartek Wilczynski, Michiel J. L. de Hoon: \\Biopython:\nfreely available Python tools for computational molecular biology and bioinformatics\u0026quot;. Bioinformatics\n25 (11), 1422{1423 (2009). https://doi.org/10.1093/bioinformatics/btp163 快速上手 Seq对象 创建Seq对象 属性：SeqRecord Seq对象都会被赋予 SeqRecord的属性，这个属性会含有seq的信息（如 identifier, name and description）\nBio.SeqIO读取fasta文件是会自动赋予Seq对象 使用例子如下： ","date":"2021-08-03T00:00:00Z","permalink":"https://example.com/p/ch1-ch2-%E5%AE%89%E8%A3%85-%E5%BC%95%E7%94%A8-%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/","title":"ch1+ch2+安装+引用+快速上手"},{"content":"parse_dates参数： 将csv的时间字符串转换成日期格式 第一种 第二种 第三种 第四种 ","date":"2021-07-18T00:00:00Z","permalink":"https://example.com/p/pandas%E8%AF%BB%E5%8F%96%E6%96%87%E4%BB%B6%E7%9A%84read_csv%E6%96%B9%E6%B3%95%E7%9A%84parse_dates%E5%8F%82%E6%95%B0/","title":"pandas读取文件的read_csv()方法的parse_dates参数"},{"content":" 再点击完成\n点击开启虚拟机 Try and install\n等待即可\n重启计算\n安装tools\n点击虚拟机\n安装tool\n双击将tar.gz取出来\n将其复制到home下\ntar -xzvf 解压\n进入文件夹\nsudo ./vmware-install.pl\n接下来N多的enter，N多的YES，自己慢慢按吧\n重启虚拟机\n若还没有全屏显示，则将虚拟机的【查看】-\u0026gt;【自动调整大小】-\u0026gt;【自适应客户机】,都选上。即可实现全屏。\n安装vmware tools实现全屏后，即也实现了在主机（WIN7）和虚拟机VMware （ubuntu）间\n直接拖拽文件 联网 ","date":"2021-07-12T00:00:00Z","permalink":"https://example.com/p/%E5%AE%89%E8%A3%85%E8%99%9A%E6%8B%9F%E6%9C%BA/","title":"安装虚拟机"},{"content":" ","date":"2021-07-05T00:00:00Z","permalink":"https://example.com/p/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AA%E4%BA%BA%E7%AC%94%E8%AE%B0%E5%AE%8C%E6%95%B4%E7%89%88/","title":"《机器学习个人笔记完整版》"},{"content":" 这是为了加强Python基础，在Github上找来的练习，地址：https://github.com/darkprinx/break-the-ice-with-python\nQuestion 91 Question Please write a program which accepts a string from console and print it in reverse order.\n**Example:\nIf the following string is given as input to the program:***\nrise to vote sir\nThen, the output of the program should be:\nris etov ot esir\nHints Use list[::-1] to iterate a list in a reverse order.\nSolutions:\na = input() #逆序输出直接设定步长就好了 #运用切片的步长即可 -1 print(a[::-1]) rise to vote sir ris etov ot esir Question 92 Question Please write a program which accepts a string from console and print the characters that have even indexes.\n**_Example:\nIf the following string is given as input to the program:_**\nH1e2l3l4o5w6o7r8l9d Then, the output of the program should be:\nHelloworld Hints Use list[::2] to iterate a list by step 2.\nSolutions:\na = input() #温习下切片的步长：如这里指定为2表示从index0开始，取index+2的元素，即0，2，4，6，8.。。。 #注意是包含0索引的元素的 a[::2] H1e2l3l4o5w6o7r8l9d \u0026#39;Helloworld\u0026#39; Question 93 Question Please write a program which prints all permutations of [1,2,3]\nHints Use itertools.permutations() to get permutations of list.\nSolution:\n#排列组合(概率论的东西) #像这里的话是A33的组合 #可以采用itertools包的permutations函数 import itertools [ i for i in itertools.permutations([1,2,3])] [(1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), (3, 2, 1)] Question 94 Question **_Write a program to solve a classic ancient Chinese puzzle:\nWe count 35 heads and 94 legs among the chickens and rabbits in a farm. How many rabbits and how many chickens do we have?_**\nHints Use for loop to iterate all possible solutions.\nSolution:\n#暴力枚举解决鸡兔同笼问题 def Q94(heads,legs): #i代表鸡数，j代表兔数 #不管是鸡还是兔，都只有一个头 for i in range(heads+1): j = heads-i if i*2+j*4 == legs: #这里用yield而不用return 因为可能会存在多种结果 yield i,j [ i for i in Q94(35,94) ] [(23, 12)] The extended part of the repository starts from this page. Previous 94 problems were collected from the repository mentioned in intro. The following problems are collected from Hackerrank and other resources from internet.\nQuestion 95 Question Given the participants\u0026rsquo; score sheet for your University Sports Day, you are required to find the runner-up score. You are given scores. Store them in a list and find the score of the runner-up.\nIf the following string is given as input to the program:\n5\n2 3 6 6 5\nThen, the output of the program should be:\n5\nHints Make the scores unique and then find 2nd best number\nSolutions:\nrunner-up:第二名\n#思路：去重、排序 a = map(int,input().split()) #默认排序是升序即从小到大 sorted(set(a))[-2] 5 2 3 6 6 5 5 Question 96 Question **_You are given a string S and width W.\nYour task is to wrap the string into a paragraph of width._**\nIf the following string is given as input to the program:\nABCDEFGHIJKLIMNOQRSTUVWXYZ\n4\nThen, the output of the program should be:\nABCD\nEFGH\nIJKL\nIMNO\nQRST\nUVWX\nYZ\nHints Use wrap function of textwrap module\nSolutions:\n#切割文本 #用textwrap包的wrap方法 import textwrap a,b = input().split() #wrap(a,b) 传入a文本，b是int即按多次长度切分 textwrap.wrap(a,int(b)) ABCDEFGHIJKLIMNOQRSTUVWXYZ 4 [\u0026#39;ABCD\u0026#39;, \u0026#39;EFGH\u0026#39;, \u0026#39;IJKL\u0026#39;, \u0026#39;IMNO\u0026#39;, \u0026#39;QRST\u0026#39;, \u0026#39;UVWX\u0026#39;, \u0026#39;YZ\u0026#39;] Question 98 Question You are given a date. Your task is to find what the day is on that date.\nInput\nA single line of input containing the space separated month, day and year, respectively, in MM DD YYYY format.\n08 05 2015\nOutput\nOutput the correct day in capital letters.\nWEDNESDAY\nHints Use weekday function of calender module\nSolution:\n#日期输出星期几 返回的是个数字 #可以用calendar包的weekday(y,m,d)方法，依次传入年、月、日 import calendar m,d,y = map(int,input().split()) a = calendar.weekday(y,m,d) #这里返回是 2 顺便说下国际上星期日是一周的开始即0 #可以通过这个方法转化 []是用这个传值 calendar.day_name[a] 08 05 2015 \u0026#39;Wednesday\u0026#39; Question 99 Question Given 2 sets of integers, M and N, print their symmetric difference in ascending order. The term symmetric difference indicates those values that exist in either M or N but do not exist in both.\nInput\nThe first line of input contains an integer, M.The second line contains M space-separated integers.The third line contains an integer, N.The fourth line contains N space-separated integers.\n4\n2 4 5 9\n4\n2 4 11 12\nOutput\nOutput the symmetric difference integers in ascending order, one per line.\n5\n9\n11\n12\nHints Use \u0026lsquo;^\u0026rsquo; to make symmetric difference operation.\nSolution:\n#\u0026#34;^\u0026#34;作用就是两个集合的非交部分 a = input().split() b = input().split() #当然使用的前提是^两边是集合对象 \u0026amp;是交集符号 set(a) ^ set(b) 4 2 4 5 9 4 2 4 11 12 5 9 11 12 {\u0026#39;2\u0026#39;, \u0026#39;4\u0026#39;} Question 100 Question You are given a string.Your task is to count the frequency of letters of the string and print the letters in descending order of frequency.\nIf the following string is given as input to the program:\naabbbccde\nThen, the output of the program should be:\nb 3\na 2\nc 2\nd 1\ne 1\nHints Count frequency with dictionary and sort by Value from dictionary Items\nSolutions:\nsentence = input() [print(\u0026#34;{} {}\u0026#34;.format(i,sentence.count(i))) for i in set(sentence)] aabbbccde c 2 d 1 a 2 e 1 b 3 [None, None, None, None, None] 0 练习coding : 叶宇浩Miles\n","date":"2021-06-14T00:00:00Z","permalink":"https://example.com/p/py_basic_q91-100/","title":"py_basic_Q91-100"},{"content":" 这是为了加强Python基础，在Github上找来的练习，地址：https://github.com/darkprinx/break-the-ice-with-python\nQuestion 81 Question By using list comprehension, please write a program to print the list after removing numbers which are divisible by 5 and 7 in [12,24,35,70,88,120,155].\nHints Use list comprehension to delete a bunch of element from a list.\nSolutions:\n[ i for i in [12,24,35,70,88,120,155] if i%(5*7)!=0] [12, 24, 88, 120, 155] Question 82 Question By using list comprehension, please write a program to print the list after removing the 0th, 2nd, 4th,6th numbers in [12,24,35,70,88,120,155].\nHints **_Use list comprehension to delete a bunch of element from a list.\nUse enumerate() to get (index, value) tuple._**\nSolutions:\n#enumerate枚举的方法，enumerate(data) 返回的是data的index和value 多数与遍历有关 [ y for x,y in enumerate([12,24,35,70,88,120,155]) if x not in [0,2,4,6]] [24, 70, 120] Question 83 Question By using list comprehension, please write a program to print the list after removing the 2nd - 4th numbers in [12,24,35,70,88,120,155].\nHints **_Use list comprehension to delete a bunch of element from a list.\nUse enumerate() to get (index, value) tuple._**\nSolutions:\n#可以看出not in 后面可以跟renge表示范围 [ y for x,y in enumerate([12,24,35,70,88,120,155]) if x not in range(2,5) ] [12, 24, 120, 155] Question 84 Question By using list comprehension, please write a program generate a 358 3D array whose each element is 0.\nHints Use list comprehension to make an array.\nSolution:\n#三层循环填充 #列表理解式可以直接包含多层循环 #注意理解三维数组的概念 i*j*z i表示有多少个独立数组 j*z表示每个独立数组的二维 [[[0 for z in range(8)] for j in range(5)] for i in range(3)] [[[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]]] Question 85 Question By using list comprehension, please write a program to print the list after removing the 0th,4th,5th numbers in [12,24,35,70,88,120,155].\nHints Use list comprehension to delete a bunch of element from a list.Use enumerate() to get (index, value) tuple.\nSolutions:\n[ y for x,y in enumerate([12,24,35,70,88,120,155]) if x not in [0,4,5] ] [24, 35, 70, 155] Question 86 Question By using list comprehension, please write a program to print the list after removing the value 24 in [12,24,35,24,88,120,155].\nHints Use list\u0026rsquo;s remove method to delete a value.\nSolutions:\n[ y for x,y in enumerate([12,24,35,70,88,120,155]) if y not in [24] ] [12, 35, 70, 88, 120, 155] 0R\n#直接用py的内置函数remove移除指定元素,它会将相关元素取出去除 liss = [12,24,35,24,88,120,155] liss.remove(24) liss [12, 35, 24, 88, 120, 155] Question 87 Question With two given lists [1,3,6,78,35,55] and [12,24,35,24,88,120,155], write a program to make a list whose elements are intersection of the above given lists.\nHints Use set() and \u0026ldquo;\u0026amp;=\u0026rdquo; to do set intersection operation.\nSolutions:\n#集合的交集 x = set([1,3,6,78,35,55]) y = set([12,24,35,24,88,120,155]) print(x\u0026amp;y) {35} OR\nQuestion 88 Question With a given list [12,24,35,24,88,120,155,88,120,155], write a program to print this list after removing all duplicate values with original order reserved.\nHints Use set() to store a number of values without duplicate.\nSolutions:\n#set去重会维持升序 set([12,24,35,24,88,120,155,88,120,155]) {12, 24, 35, 88, 120, 155} Question 89 Question Define a class Person and its two child classes: Male and Female. All classes have a method \u0026ldquo;getGender\u0026rdquo; which can print \u0026ldquo;Male\u0026rdquo; for Male class and \u0026ldquo;Female\u0026rdquo; for Female class.\nHints Use Subclass(Parentclass) to define a child class.\nSolution:\nclass Person: def getGender(self): pass class Female(Person): def getGender(self): print(\u0026#34;Female\u0026#34;) class Male(Person): def getGender(self): print(\u0026#34;Male\u0026#34;) x,y = Female(),Male() # print(x.getGender) //这样返回的是一个声明之类的东西 \u0026lt;bound method Female.getGender of \u0026lt;__main__.Female object at 0x000002B809A68C40\u0026gt;\u0026gt; # print(y.getGender) //所以调用时记得加() x.getGender() y.getGender() Female Male Question 90 Question Please write a program which count and print the numbers of each character in a string input by console.\n**_Example:\nIf the following string is given as input to the program:_**\nabcdefgabc Then, the output of the program should be:\na,2 c,2 b,2 e,1 d,1 g,1 f,1 Hints **_Use dict to store key/value pairs.\nUse dict.get() method to lookup a key with default value._**\nSolutions:\nliss = [i for i in input()] #注意要set去重 #顺便说下format（）中可以直接用表达式如liss.count(i) 在列表生成式中 .format是直接跟在\u0026#34;\u0026#34;后的 [ print(\u0026#34;{},{}\u0026#34;.format(i,liss.count(i))) for i in sorted(set(liss)) ] abcdefgabc a,2 b,2 c,2 d,1 e,1 f,1 g,1 [None, None, None, None, None, None, None] 这里的None是因为函数没用到return即没用返回值,所以每次都None\n0 练习coding : 叶宇浩Miles\n","date":"2021-06-13T00:00:00Z","permalink":"https://example.com/p/py_basic_q81-90/","title":"py_basic_Q81-90"},{"content":"Part 1:Manipulation, Selection, and Chains Fetch:获取结构文件 顶部菜单栏menu选择 这里获取1zik\nFile... Fetch by ID... 会弹出下图右边的那个窗口，选择对应的数据库database,输入对应的结构ID即可 Action: 改变结构展现的形式 Cihmrea默认读取结构后，展现为ribbon带状，tan棕黄色 可以通过 顶部action 选择展现其他形式 (\u0026hellip; 代表空格，即需要点击鼠标才会出现后面的选项)\nActions... Atoms/Bonds... show 选择原子模式看看，从右图的结果可以看出，同PyMOL一样，结构的展现形式都是叠加上去的，即上一个展示形式仍然存在 注意下，不同的原子（主要是杂原子（非C原子））展现的颜色也不一样，即byelement\nO原子：红色 N原子：蓝色 更多的，如下图片所示 图片原来源[^2] ,经过处理 恢复ribbon带状的展现形式\nActions... Ribbon... hide Actions... Ribbon... show 鼠标的基本操作和属性 默认的鼠标设置下 左键旋转 滑轮 XY方向上放大缩小 右键 放大缩小 （需要长按） 我觉得滑轮和右键没啥区别\n调整 如果需要其他鼠标设置可以去顶部menu菜单栏自行调整\nFavorites... Preferences [category] Mouse 还是比较喜欢默认的，所以其他模式的没研究过\n鼠标接触蛋白的作用 显示残基的信息 格式为：\nres-name res-num.chain atom-name 方便展示，我重新加上了atom/bond的形式，下面的图上半部分是这个蛋白的pdb文件，下面是对应AA在Chimera鼠标轻触显示的信息（以11号的GLU谷氨酸，E，为例子）\n轻触的是键时,结果的格式是： 氨基酸名字 位置.链 原子-原子(互相连接的两个原子) 距离 如图中结果说明，这个蛋白A链11号上的位置是谷氨酸，这个氨基酸Ca与Cb成键的距离为1.520A 轻触的是的单个原子时，结果的格式是： 氨基酸名字 位置.链 所选原子名称 鼠标多选AA Ctrl + 单击 这能选中单个AA Shift + Ctrl + 单击 可以连续追加选中多个AA 补充 （keyboard - up） 但是实际上，这样选的可能还不是完整的整个AA，可能是其AA上的某个原子元素，可以通过键盘的方向键（上），选中整个AA（即这样可以通过单个原子或者元素，选到整个氨基酸） 等同于menu的 Select... Broaden Side View : 从上帝视觉看整个蛋白 启动Side View 顶部菜单栏menu\nFavorites... Side View 然后可以通过移动这个小框（图中红色部分）快速调整视角，在主窗口中移动蛋白的结构时，Side View也会跟着显示，可以将其理解为一个主窗口的小窗 Select: 快速选中同一类型的原子 \u0026amp; Color 修改颜色 选中同一类AA 如果想要选中同一个蛋白中全部某种类型的氨基酸，可以通过select直接选择(以赖氨酸为例)\nSelect... Residue... LYS 有显示绿色荧光样的AA，表示被选中了\n部分选中与全部选中的区别 部分选中 只有部分的AA被选中时，Chimera右下角的图标会显示绿色，那么所有工具的作用效果就是针对这些选中的AA 全部选中 （整个蛋白整体都被选择） 上面的那个图标会显示未绿色，且后续的作用对象（比如color）针对的是整个蛋白（chimrea默认是全部选中）\nselect 的补充 可以看到select 的菜单可以选择很多模式 例子 选中指定的链，修改颜色, 顶部menu (先讲之前选中的东西取消了，主窗口，Ctrl + 鼠标选择空白处)\nSelect... Chain... B Actions... Color... cyan Actions... Ribbon... hide 选中了了B链，修改了其颜色为cyan，并且将其带状的展现形式hide了 （可以看出actions操作是针对选中部分的，如果没有，默认为整体对象） 选中水分子，将其隐藏起来 Select... Structure... solvent Actions... Atoms/Bonds... hide 可以看到水分子和上面选择的B链都被hide起来了 （这里可以看出chimera的select mode是默认的append的，就是叠加形式，如果上一步的selection的部分没有被clean，则会追加到下一部分的selection） 选中指定的元素，修改展示形式 顶部菜单栏\nSelect... Chemistry... element... N Actions... Atoms/Bonds... sphere 可以看出，这里选择了所有的N元素，将其展示为sphere模式，其实上面被我们隐藏了的那一条链的N元素也展示为了sphere，只是上面被我们隐藏了，所有没展示在主窗口中，不要忘记了，追加被selected 清除select，恢复选中整体，修改整体的展现形式 顶部菜单栏\nSelect... Clear Selection ## 也可以在主窗口 Ctrl + 鼠标左键点击空白处 Actions... Atoms/Bonds... stick Action Atom/Bonds show 没有作用 重新select 对象即可\nSelect... Structure... protein Color:修改颜色 顶部菜单栏\nSelect... Residue... LYS Actions... Color... hot pink 可以看到上面被选择的全部赖氨酸都变成了粉红色（其中旁边红色的小点是水分子）\ncolor 更多颜色的选择，以及为特定的展现形式给予不同的色彩 （以选中全部的谷氨酸为例） 顶部的menu\nSelect... Residue... GLU Actions... Color... all options 接着可以在出现的color窗口的coloring applies to 指定展现为什么形式时，为对应的颜色 这里选择atoms/bonds 为蓝色 实际上，这里就是选择了GLU谷氨酸，并且当且展现出Atoms/Bonds的形式时，这些Atoms/Bonds为蓝色，可以看到它们的ribbon还是tan的颜色\n如果想不受展现形式的限制（即不管什么展现形式都是同一个颜色）的话，在coloring applies to 时选择all of the above 即可\nSequence: 快速选择序列区域 打开sequences 窗口 顶部\u0008menu\nFavorites... Sequence 拖拉使用 长按鼠标左键拖拉选择单个或者一个连续的区域 如果想选择多个单位点或者不连续的区域 长按鼠标左键+shift 即可选择 lable: 展示标签 2D/3D 标签的区别 在Chimera中有2d和3d label之分（PyMOL没有），那么3D的和2D的标签有什么区别呢\n根据官方说明[^3]可以看出，2D标签更加时候作图和movie，因为它可以加注释信息等，并且位置是固定的，不会随着蛋白的旋转而发生改变 3D的label会随着结构的旋转而跟着变动 调整label的大小 顶部menu\nFavorites... Preference 新弹出的Preference窗口\nCategory... Label 可能可以选择的字体大小有限制，但是可以自己输入大小 调整即可\n添加3D label 随便选中几个AA，然后展示它们的label name 顶部menu\nActions... Label... residue... name + specifier 可以看到这里展示出了 AA-name position.chain\n为了更加focus在这些label的residue上，只展示它们的stick结构 顶部menu broaden是为了确保选中到整个完整的AA，而不是某些原子\nSelect... Broaden Actions... Atoms/Bonds... show only 确实更加能突出这几个AA了\nModel Panel:Chimera的对象 顶部menu\n会打开名为Model Pandel 新的窗口\nmodel ID number 对象ID，其实这里我觉得相当于是每个object，就是每个导入的蛋白结构或者选择的部分都会赋予一个ID model color 在主窗口展示的颜色 A(ctive) - whether activated for motion 如果A✅了，则这个model可以被调整；如果不想的话，可以将A取消✅，这个很有用，因为取消后，我发现这个model就固定住了 S(hown) - whether display-enabled 是否在主窗口展示 model name 就是说Model Panel 是一个总的控制板\n学习来源： 1.https://www.cgl.ucsf.edu/chimera/current/docs/UsersGuide/frametut.html 2.https://www.cgl.ucsf.edu/chimera/current/docs/UsersGuide/colortables.html#byelement 3.https://www.cgl.ucsf.edu/chimera/current/docs/UsersGuide/frametut.html\n","date":"2021-06-12T00:00:00Z","permalink":"https://example.com/p/1_getting_started_menu_version-1/","title":"1_Getting_Started_Menu_Version-1"},{"content":" 这是为了加强Python基础，在Github上找来的练习，地址：https://github.com/darkprinx/break-the-ice-with-python\nQuestion 71 Question Please write a program to output a random number, which is divisible by 5 and 7, between 10 and 150 inclusive using random module and list comprehension.\nHints Use random.choice() to a random element from a list.\nSolutions:\nimport random result = [ i for i in range(10,151) if i%(5*7)==0] random.choice(result) 105 Question 72 Question Please write a program to generate a list with 5 random numbers between 100 and 200 inclusive.\nHints Use random.sample() to generate a list of random values.\nSolutions:\n#sample的功能和choice一样，但是choice只能输入一个，而sample可以输出指定个数个，且整体为list #sample(data,num) data传入的数据，num指定随机输出的数据个数 import random random.sample(range(100,201),5) [127, 134, 110, 119, 181] Question 73 Question Please write a program to randomly generate a list with 5 even numbers between 100 and 200 inclusive.\nHints Use random.sample() to generate a list of random values.\nSolutions:\nimport random #记得理解式加上对应的{} () [] random.sample([i for i in range(100,201,2)] ,5) [200, 136, 180, 152, 110] Question 74 Question Please write a program to randomly generate a list with 5 numbers, which are divisible by 5 and 7 , between 1 and 1000 inclusive.\nHints Use random.sample() to generate a list of random values.\nSolutions:\nimport random result = [ i for i in range(1,1001) if i%(5*7)==0] random.sample(result,5) [280, 105, 455, 805, 735] Question 75 Question Please write a program to randomly print a integer number between 7 and 15 inclusive.\nHints Use random.randrange() to a random integer in a given range.\nSolution:\nimport random random.randint(7,15) random.randrange(7,16) #这两个都是输出随机的整数，区别在于randint(a,b)对应[a,b];而randrange(a,b)对应[a,b) 跟range一样的用法，也可指定步长 10 Question 76 Question Please write a program to compress and decompress the string \u0026ldquo;hello world!hello world!hello world!hello world!\u0026rdquo;.\nHints Use zlib.compress() and zlib.decompress() to compress and decompress a string.\nSolution:\n#解压和压缩 import zlib #用到的是zlib的包 #压缩是compress 但是它不能压缩字符串，可以是压缩字节流，前面加个b就好了 comp = zlib.compress(b\u0026#34;hello world!hello world!hello world!hello world!\u0026#34;) decomp = zlib.decompress(comp) print(comp) print(decomp) b\u0026#39;x\\x9c\\xcbH\\xcd\\xc9\\xc9W(\\xcf/\\xcaIQ\\xcc \\x82\\r\\x00\\xbd[\\x11\\xf5\u0026#39; b\u0026#39;hello world!hello world!hello world!hello world!\u0026#39; Question 77 Question Please write a program to print the running time of execution of \u0026ldquo;1+1\u0026rdquo; for 100 times.\nHints Use timeit() function to measure the running time.\nSolutions:\n#计算时间 可以用time包，但是time包是时间戳的结果 import time before = time.time()#获取当前的时间戳 for i in range(2000000): a = 1+1 after = time.time() print(after-before)#可以看出输出的是一个 作为正常人不怎么看的懂得时间戳格式 0.17505717277526855 OR\n#用datetime得结果更直白 import datetime before = datetime.datetime.now()#获取当前的时间 for i in range(2000000): a = 1+1 after = datetime.datetime.now() print(after-before) 0:00:00.178051 Question 78 Question Please write a program to shuffle and print the list [3,6,7,8].\nHints Use shuffle() function to shuffle a list.\nSolutions:\nliss = [3,6,7,8] import random #random的方法shuffle可以将传入的数据中的元素顺序随机打乱，当然它是直接作用在原数据上的 random.shuffle(liss) liss [3, 6, 7, 8] OR\nliss = [3,6,7,8] import random #random的方法shuffle可以将传入的数据中的元素顺序随机打乱，当然它是直接作用在原数据上的 #可以指定随机种子(num)来保证固定的随机打乱，即每次打乱的结果是一致的 seed = 4 random.Random(seed).shuffle(liss) liss [7, 3, 8, 6] Question 79 Question Please write a program to generate all sentences where subject is in [\u0026ldquo;I\u0026rdquo;, \u0026ldquo;You\u0026rdquo;] and verb is in [\u0026ldquo;Play\u0026rdquo;, \u0026ldquo;Love\u0026rdquo;] and the object is in [\u0026ldquo;Hockey\u0026rdquo;,\u0026ldquo;Football\u0026rdquo;].\nHints Use list[index] notation to get a element from a list.\nSolutions:\na = [\u0026#34;I\u0026#34;, \u0026#34;You\u0026#34;] b = [\u0026#34;Play\u0026#34;, \u0026#34;Love\u0026#34;] c = [\u0026#34;Hockey\u0026#34;,\u0026#34;Football\u0026#34;] #不要想当然就rnage(a),a本身就是一个可以遍历的组合了 for i in a: for j in b: for z in c: print(\u0026#34;{} {} {}\u0026#34;.format(i,j,z)) I Play Hockey I Play Football I Love Hockey I Love Football You Play Hockey You Play Football You Love Hockey You Love Football Question 80 Question Please write a program to print the list after removing even numbers in [5,6,77,45,22,12,24].\nHints Use list comprehension to delete a bunch of element from a list.\nSolutions:\nliss = [5,6,77,45,22,12,24] #额 这个和前面重复太对了 #还是那句话 filter(a,b) a方法，b是要遍历的数据 #lambda i:function function可以是没有if的条件判断其实就是True/False list(filter(lambda i: i%2!=0, liss )) [5, 77, 45] 0 练习coding : 叶宇浩Miles**\n","date":"2021-06-12T00:00:00Z","permalink":"https://example.com/p/py_basic_q71-80/","title":"py_basic_Q71-80"},{"content":" 这是为了加强Python基础，在Github上找来的练习，地址：https://github.com/darkprinx/break-the-ice-with-python\nQuestion 61 Question The Fibonacci Sequence is computed based on the following formula:\n$f(n)=0$ if n=0\n$f(n)=1$ if n=1\n$f(n)=f(n-1)+f(n-2)$ if n\u0026gt;1\nPlease write a program to compute the value of f(n) with a given n input by console.\n**_Example:\nIf the following n is given as input to the program:_**\n7\nThen, the output of the program should be:\n13\nIn case of input data being supplied to the question, it should be assumed to be a console input.\nHints We can define recursive function in Python.\nSolutions:\ndef Q61(x): return x if x\u0026lt;2 else Q61(x-1)+Q61(x-2) Q61(7) 13 Question 62 Question The Fibonacci Sequence is computed based on the following formula:\n$f(n)=0$ if n=0\n$f(n)=1$ if n=1\n$f(n)=f(n-1)+f(n-2)$ if n\u0026gt;1\nPlease write a program to compute the value of f(n) with a given n input by console.\n**_Example:\nIf the following n is given as input to the program:_**\n7\nThen, the output of the program should be:\n0,1,1,2,3,5,8,13\nIn case of input data being supplied to the question, it should be assumed to be a console input.\nHints **_We can define recursive function in Python.\nUse list comprehension to generate a list from an existing list.\nUse string.join() to join a list of strings._**\nSolutions:\ndef Q61(x): return x if x\u0026lt;2 else Q61(x-1)+Q61(x-2) #从0-7的斐波那契数列 [ Q61(i) for i in range(8) ] [0, 1, 1, 2, 3, 5, 8, 13] Question 63 Question Please write a program using generator to print the even numbers between 0 and n in comma separated form while n is input by console.\n**_Example:\nIf the following n is given as input to the program:_**\n10\nThen, the output of the program should be:\n0,2,4,6,8,10\nIn case of input data being supplied to the question, it should be assumed to be a console input.\nHints Use yield to produce the next value in generator.\nSolution:\na = int(input()) [i for i in range(a+1) if i%2==0]#其实写列表生成式和匿名函数之类的，都是写函数的思维 10 [0, 2, 4, 6, 8, 10] OR\n#用range的步长 a = int(input()) [i for i in range(0,a+1,2) ] 10 [0, 2, 4, 6, 8, 10] Question 64 Question Please write a program using generator to print the numbers which can be divisible by 5 and 7 between 0 and n in comma separated form while n is input by console.\n**_Example:\nIf the following n is given as input to the program:_**\n100\nThen, the output of the program should be:\n0,35,70\nIn case of input data being supplied to the question, it should be assumed to be a console input.\nHints Use yield to produce the next value in generator.\nSolutions:\n#能被5和7整除，就是能处于5*7==0 [ i for i in range(101) if i%(5*7)==0] [0, 35, 70] Question 65 Question Please write assert statements to verify that every number in the list [2,4,6,8] is even.\nHints Use \u0026ldquo;assert expression\u0026rdquo; to make assertion.\nSolutions:\n#这里用到的是断言 #assert a,b a是判断条件，b是错误的信息，a错误才会引发 而且一旦触发会中断程序 liss = [2,4,9,6,8] for i in liss: assert i%2==0,\u0026#34;{}唔系even\u0026#34;.format(i) print(i) # [ assert i%2==0,\u0026#34;{}唔系even\u0026#34;.format(i) for i in liss] 这样不行 2 4 --------------------------------------------------------------------------- AssertionError Traceback (most recent call last) \u0026lt;ipython-input-2-493256e6bc11\u0026gt; in \u0026lt;module\u0026gt; 3 liss = [2,4,9,6,8] 4 for i in liss: ----\u0026gt; 5 assert i%2==0,\u0026#34;{}唔系even\u0026#34;.format(i) 6 print(i) 7 # [ assert i%2==0,\u0026#34;{}唔系even\u0026#34;.format(i) for i in liss] 这样不行 AssertionError: 9唔系even Question 66 Question Please write a program which accepts basic mathematic expression from console and print the evaluation result.\n**_Example:\nIf the following n is given as input to the program:_**\n35 + 3\nThen, the output of the program should be:\n38\nHints Use eval() to evaluate an expression.\nSolutions:\n#eval是py的内置函数，它的作用是可以直接执行输入的语句,并输入结果 eval(input()) 35 +3 38 Question 67 Question Please write a binary search function which searches an item in a sorted list. The function should return the index of element to be searched in the list.\nHints Use if/elif to deal with conditions.\n#这个本质上一个折返的查找结构了。。 def Q67(data,target): #初始化边界 left,right=0,len(data) while left\u0026lt;=right: # // 除法的结果取整 mid = (left+right) // 2 maybe_target = data[mid] if target == maybe_target: #有个return就会中止循环了 return mid elif target\u0026lt;maybe_target:#说明maybe_target偏大了，右边界减小 right = mid -1 elif target\u0026gt;maybe_target:#说明maybe_target偏小了，左边界增大 left = mid + 1 print(\u0026#34;莫得找到\u0026#34;) Q67([1,3,5,6,7],6) 3 Question 68 Question Please generate a random float where the value is between 10 and 100 using Python module.\nHints Use random.random() to generate a random float in [0,1].\nSolutions:\nimport random random.random()#可以产生0-1的一个随机小数 random.uniform(10,20)#可以产生[10，20]内的一个随机带小数的数 也就是说可以用uniform指定随机数的出现范围 random.randint(10,20)#可以产生[10，20]内的一个随机整数 18 Question 69 Question Please generate a random float where the value is between 5 and 95 using Python module.\nHints Use random.random() to generate a random float in [0,1].\nSolutions:\nimport random random.uniform(5,95) 30.994516300494574 Question 70 Question Please write a program to output a random even number between 0 and 10 inclusive using random module and list comprehension.\nHints Use random.choice() to a random element from a list.\nSolutions:\nimport random result = [ i for i in range(0,10,2)] #random的内置方法choic(a) 可以随机返回一个a数据中的一个值 random.choice(result) 2 0 **练习coding : 叶宇浩Miles\n","date":"2021-06-11T00:00:00Z","permalink":"https://example.com/p/py_basic_q61-70/","title":"py_basic_Q61-70"},{"content":" 这是为了加强Python基础，在Github上找来的练习，地址：https://github.com/darkprinx/break-the-ice-with-python\nQuestion 51 Question Write a function to compute 5/0 and use try/except to catch the exceptions.\nHints Use try/except to catch exceptions.\nSolutions:\n#首先要知道错误的类型 def Q51(): return 5/0 Q51() # ZeroDivisionError 可以看到是这个wrong类型 --------------------------------------------------------------------------- ZeroDivisionError Traceback (most recent call last) \u0026lt;ipython-input-2-1c67e02e3140\u0026gt; in \u0026lt;module\u0026gt; 2 def Q51(): 3 return 5/0 ----\u0026gt; 4 Q51() \u0026lt;ipython-input-2-1c67e02e3140\u0026gt; in Q51() 1 #首先要知道错误的类型 2 def Q51(): ----\u0026gt; 3 return 5/0 4 Q51() ZeroDivisionError: division by zero #try/except捕获异常 def Q51(): return 5/0 try: Q51() #except 后面可以指定错误类型 / 也可以不指定，则捕获全部类型 except ZeroDivisionError as ZD: print(\u0026#34;不能除于零！\u0026#34;) except: print(\u0026#34;唔知道！\u0026#34;) 不能除于零！ Question 52 Question Define a custom exception class which takes a string message as attribute.\nHints To define a custom exception, we need to define a class inherited from Exception.\nSolutions:\n#自定义一个错误异常，指定是要继承py的Exception的 class Q52(Exception): def __init__(self,msg): self.message = msg #提出异常 raise Q52(\u0026#34;错左\u0026#34;) --------------------------------------------------------------------------- Q52 Traceback (most recent call last) \u0026lt;ipython-input-4-05312282dd3c\u0026gt; in \u0026lt;module\u0026gt; 5 6 #提出异常 ----\u0026gt; 7 raise Q52(\u0026#34;错左\u0026#34;) Q52: 错左 Question 53 Question Assuming that we have some email addresses in the \u0026ldquo;username@companyname.com\u0026rdquo; format, please write program to print the user name of a given email address. Both user names and company names are composed of letters only.\n**_Example:\nIf the following email address is given as input to the\nprogram:_**\njohn@google.com\nThen, the output of the program should be:\njohn\nIn case of input data being supplied to the question, it should be assumed to be a console input.\nHints Use \\w to match letters.\nSolutions:\nemail = \u0026#34;john@google.com\u0026#34; email = email.split(\u0026#34;@\u0026#34;) print(email[0]) #当然可以直接用正则，但是感觉没必要 john Question 54 Question Assuming that we have some email addresses in the \u0026ldquo;username@companyname.com\u0026rdquo; format, please write program to print the company name of a given email address. Both user names and company names are composed of letters only.\n**_Example:\nIf the following email address is given as input to the program:_**\njohn@google.com\nThen, the output of the program should be:\ngoogle\nIn case of input data being supplied to the question, it should be assumed to be a console input.\nHints Use \\w to match letters.\nSolutions:\nemail = \u0026#34;john@google.com\u0026#34; email = email.split(\u0026#34;@\u0026#34;)[1].split(\u0026#34;.\u0026#34;) print(email[0]) google OR\nimport re email = \u0026#34;john@google.com\u0026#34; #提供个正则的模板先 # /w+代表多个字母，@，因为域名那部分的字母是我们要的，所以加个（） pattern = \u0026#34;\\w+@(\\w+).com\u0026#34;#对应\u0026#34;john@google.com\u0026#34; print(re.findall(pattern,email)) [\u0026#39;google\u0026#39;] Question 55 Question Write a program which accepts a sequence of words separated by whitespace as input to print the words composed of digits only.\n**_Example:\nIf the following words is given as input to the program:_**\n2 cats and 3 dogs.\nThen, the output of the program should be:\n[\u0026lsquo;2\u0026rsquo;, \u0026lsquo;3\u0026rsquo;]\nIn case of input data being supplied to the question, it should be assumed to be a console input.\nHints Use re.findall() to find all substring using regex.\nSolutions:\nsentence = input().split() [ i for i in sentence if i.isdigit()] 2 cats and 3 dogs. [\u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;] Question 56 Question Print a unicode string \u0026ldquo;hello world\u0026rdquo;.\nHints Use u\u0026rsquo;strings\u0026rsquo; format to define unicode string.\nQuestion 57 Question Write a program to read an ASCII string and to convert it to a unicode string encoded by utf-8.\nHints Use unicode()/encode() function to convert.\nSolutions:\nword = input() #py本身内置的encode方法可以转指定的编码格式 word = word.encode(\u0026#34;utf-8\u0026#34;) print(word) 我唔中意食叉烧包 b\u0026#39;\\xe6\\x88\\x91\\xe5\\x94\\x94\\xe4\\xb8\\xad\\xe6\\x84\\x8f\\xe9\\xa3\\x9f\\xe5\\x8f\\x89\\xe7\\x83\\xa7\\xe5\\x8c\\x85\u0026#39; Question 58 Question Write a special comment to indicate a Python source code file is in unicode.\nHints Use unicode() function to convert.\nSolution:\n#用来标注py程序的编码格式 # -*- coding:utf-8 -*- Question 59 Question Write a program to compute 1/2+2/3+3/4+\u0026hellip;+n/n+1 with a given n input by console (n\u0026gt;0).\n**_Example:\nIf the following n is given as input to the program:_**\n5\nThen, the output of the program should be:\n3.55\nIn case of input data being supplied to the question, it should be assumed to be a console input.\nHints Use float() to convert an integer to a float.Even if not converted it wont cause a problem because python by default understands the data type of a value\nSolutions:\n#用递归实现 def Q59(x): #注意这里的写法 return 0 if x==0 else Q59(x-1) + x/(x+1) round(Q59(5),2) 3.55 Question 60 Question Write a program to compute:\n$f(n)=f(n-1)+100$ when n\u0026gt;0\nand $f(0)=0$\nwith a given n input by console (n\u0026gt;0).\n**_Example:\nIf the following n is given as input to the program:_**\n5\nThen, the output of the program should be:\n500\nIn case of input data being supplied to the question, it should be assumed to be a console input.\nHints We can define recursive function in Python.\nSolutions:\ndef Q60(x): #同样递归 return 0 if x==0 else Q60(x-1) + 100 Q60(5) 500 OR\n#加强理解匿名函数 就是用写函数的思维去写的 #补充下，如果lambda后面没有可迭代对象，则由（）这里传入 Q60_la = lambda x:0 if x==0 else Q60(x-1)+100 Q60_la(5) 500 0 练习coding : 叶宇浩Miles**\n","date":"2021-06-10T00:00:00Z","permalink":"https://example.com/p/py_basic_q51-60/","title":"py_basic_Q51-60"},{"content":" 这是为了加强Python基础，在Github上找来的练习，地址：https://github.com/darkprinx/break-the-ice-with-python\nQuestion 41 Question: Write a program which can map() to make a list whose elements are square of elements in [1,2,3,4,5,6,7,8,9,10].\nHints: Use map() to generate a list.Use lambda to define anonymous functions.\nSolutions:\n#温习下map的使用，传入方法和可迭代对象 liss = [1,2,3,4,5,6,7,8,9,10] print(list(map(lambda i:i**2,liss))) [1, 4, 9, 16, 25, 36, 49, 64, 81, 100] Question 42 Question: Write a program which can map() and filter() to make a list whose elements are square of even number in [1,2,3,4,5,6,7,8,9,10].\nHints: Use map() to generate a list.Use filter() to filter elements of a list.Use lambda to define anonymous functions.\nSolutions:\n#这里将map和filter联合使用了 liss = [1,2,3,4,5,6,7,8,9,10] print(list(map(lambda i:i**2,filter(lambda i:i%2==0,liss)))) [4, 16, 36, 64, 100] Question 43 Question: Write a program which can filter() to make a list whose elements are even number between 1 and 20 (both included).\nHints: Use filter() to filter elements of a list.Use lambda to define anonymous functions.\nSolutions:\n#filter和map对象记得用list转为列表对象 #注意传入的可迭代对象这里的写法，还可以将列表生成式放进来 print(list(filter(lambda i:i%2==0,[i for i in range(1,21)]))) [2, 4, 6, 8, 10, 12, 14, 16, 18, 20] Question 44 Question: Write a program which can map() to make a list whose elements are square of numbers between 1 and 20 (both included).\nHints: Use map() to generate a list. Use lambda to define anonymous functions.\nSolutions:\nprint(list(map(lambda i:i**2,[i for i in range(1,21)]))) [1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400] Question 45 Question: Define a class named American which has a static method called printNationality.\nHints: Use @staticmethod decorator to define class static method.There are also two more methods.To know more, go to this link.\nSolutions:\nclass Q45: #@staticmethod 哪个方法上面有这个个就属于静态方法 #静态方法有什么用呢，可以不用实列化类就能直接调用 #当然也可以实例化再调用 @staticmethod def printNationality(): #print(\u0026#34;US\u0026#34;) 只有print，没有return的话 结果为 US None return \u0026#34;US\u0026#34; print(Q45.printNationality()) US Question 46 Question: Define a class named American and its subclass NewYorker.\nHints: Use class Subclass(ParentClass) to define a subclass.*\nSolutions:\nclass Q46: @staticmethod def printNationality(): return \u0026#34;US\u0026#34; #继承类的写法 在新的类（）中写入另一个类的名字 #因为继承性 所以可以在子类中调用父类中已经定义好的方法或者属性 只是换了类名而已 class Q46_son(Q46): pass print(Q46_son.printNationality()) US Question 47 Question Define a class named Circle which can be constructed by a radius. The Circle class has a method which can compute the area.\nHints Use def methodName(self) to define a method.\nSolutions:\nclass Circle: #__init__又称构造函数 def __init__(self,radius_value): #不要忘记了将值传给self self.radius = radius_value def area(self): return 3.14*(self.radius**2) circle = Circle(6) print(circle.area()) 113.04 Question 48 Question Define a class named Rectangle which can be constructed by a length and width. The Rectangle class has a method which can compute the area.\nHints Use def methodName(self) to define a method.\nSolutions:\nclass Rectangle: #__init__又称构造函数 def __init__(self,a,b): self.a = a self.b = b def area(self): return self.a*self.b rectangle = Rectangle(6,6) print(rectangle.area()) 36 Question 49 Question Define a class named Shape and its subclass Square. The Square class has an init function which takes a length as argument. Both classes have a area function which can print the area of the shape where Shape\u0026rsquo;s area is 0 by default.\nHints To override a method in super class, we can define a method with the same name in the super class.\nSolutions:\nclass Shape: def __init__(self): pass def area(self): return 0 #继承下Shape class Rectangle(Shape): #__init__又称构造函数 def __init__(self,a,b): self.a = a self.b = b def area(self): return self.a*self.b rectangle = Rectangle(6,6) print(rectangle.area()) 36 Question 50 Question Please raise a RuntimeError exception.\nHints Use raise to raise an exception.\nSolution:\n#raise语句可以自己触发异常，一旦触发代码就不进行了 #raise [Exception [, args [, traceback]]] #语句中 Exception 是异常的类型（例如，NameError）参数标准异常中任一种，args 是自已提供的异常参数。 raise RuntimeError(\u0026#34;time out!\u0026#34;)#这里的time out 就相当于抛出错误的提示 --------------------------------------------------------------------------- RuntimeError Traceback (most recent call last) \u0026lt;ipython-input-22-9f14c420cd64\u0026gt; in \u0026lt;module\u0026gt; 2 #raise [Exception [, args [, traceback]]] 3 #语句中 Exception 是异常的类型（例如，NameError）参数标准异常中任一种，args 是自已提供的异常参数。 ----\u0026gt; 4 raise RuntimeError(\u0026#34;time out!\u0026#34;) RuntimeError: time out! Conclusion Well It seems that the above problems are very much focused on basic concpets and implimantation of object oriented programming.As the concepts are not about to solve any functional problem rather design the structure , so both codes are very much similar in there implimantation part.\n0 练习coding : 叶宇浩Miles\n","date":"2021-06-09T00:00:00Z","permalink":"https://example.com/p/py_basic_q41-50/","title":"py_basic_Q41-50"},{"content":" 这是为了加强Python基础，在Github上找来的练习，地址：https://github.com/darkprinx/break-the-ice-with-python\nQuestion 31 Question: Define a function which can print a dictionary where the keys are numbers between 1 and 20 (both included) and the values are square of keys.\nHints: Use dict[key]=value pattern to put entry into a dictionary.Use ** operator to get power of a number.Use range() for loops.\nSolutions:\ndef Q31(): #用字典生成式 dict = {i:i**2 for i in range(1,21)} return dict Q31() {1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81, 10: 100, 11: 121, 12: 144, 13: 169, 14: 196, 15: 225, 16: 256, 17: 289, 18: 324, 19: 361, 20: 400} Question 32 Question: Define a function which can generate a dictionary where the keys are numbers between 1 and 20 (both included) and the values are square of keys. The function should just print the keys only.\nHints: Use dict[key]=value pattern to put entry into a dictionary.Use ** operator to get power of a number.Use range() for loops.Use keys() to iterate keys in the dictionary. Also we can use item() to get key/value pairs.\nSolutions:\ndef Q32(): #用字典生成式 dict = {i:i**2 for i in range(1,21)} #字典对象可以用keys获取全部键值 return dict.keys() Q32() dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]) Question 33 Question: Define a function which can generate and print a list where the values are square of numbers between 1 and 20 (both included).\nHints: Use ** operator to get power of a number.Use range() for loops.Use list.append() to add values into a list.\nSolutions:\ndef Q33(): #用列表理解式 list = [i**2 for i in range(1,21)] return list Q33() [1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400] Question 34 Question: Define a function which can generate a list where the values are square of numbers between 1 and 20 (both included). Then the function needs to print the first 5 elements in the list.\nHints: Use ** operator to get power of a number.Use range() for loops.Use list.append() to add values into a list.Use [n1:n2] to slice a list\nSolutions:\ndef Q34(): #用列表理解式 list = [i**2 for i in range(1,21)] return list[:5] Q34() [1, 4, 9, 16, 25] Question 35 Question: Define a function which can generate a list where the values are square of numbers between 1 and 20 (both included). Then the function needs to print the last 5 elements in the list.\nHints: Use ** operator to get power of a number.Use range() for loops.Use list.append() to add values into a list.Use [n1:n2] to slice a list\nSolutions:\ndef Q35(): #用列表理解式 list = [i**2 for i in range(1,21)] return list[-5:] Q35() [256, 289, 324, 361, 400] Question 36 Question: Define a function which can generate a list where the values are square of numbers between 1 and 20 (both included). Then the function needs to print all values except the first 5 elements in the list.\nHints: Use ** operator to get power of a number.Use range() for loops.Use list.append() to add values into a list.Use [n1:n2] to slice a list\nSolutions:\ndef Q36(): #用列表理解式 list = [i**2 for i in range(1,21)] return list[5:] Q36() [36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400] Question 37 Question: Define a function which can generate and print a tuple where the value are square of numbers between 1 and 20 (both included).\nHints: Use ** operator to get power of a number.Use range() for loops.Use list.append() to add values into a list.Use tuple() to get a tuple from a list.\nSolutions:\ndef Q37(): #同样使用理解式,元组理解式要这样写图tuple(i**2 for i in range(1,21)) 写成这样的话(i**2 for i in range(1,21))没有print tuplee = tuple(i**2 for i in range(1,21)) return tuplee Q37() (1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361, 400) Comment Problems of this section is very much easy and all of those are of a modification of same type problem which mainly focused on using some commonly used function works with list,dictionary, tupple.In my entire solutions, I havn\u0026rsquo;t tried to solve problems in efficient way.Rather I tried to solve in a different way that I can.This will help a beginner to know how simplest problems can be solved in different ways.\nQuestion 38 Question: With a given tuple (1,2,3,4,5,6,7,8,9,10), write a program to print the first half values in one line and the last half values in one line.\nHints: Use [n1:n2] notation to get a slice from a tuple.\nSolutions:\ntuplee = (1,2,3,4,5,6,7,8,9,10) tuplee_half = int(len(tuplee)/2) print(tuplee[:tuplee_half]) print(tuplee[tuplee_half:]) (1, 2, 3, 4, 5) (6, 7, 8, 9, 10) Question 39 Question: Write a program to generate and print another tuple whose values are even numbers in the given tuple (1,2,3,4,5,6,7,8,9,10).\nHints: Use \u0026ldquo;for\u0026rdquo; to iterate the tuple. Use tuple() to generate a tuple from a list.\nSolutions:\ntuplee = (1,2,3,4,5,6,7,8,9,10) tuplee = tuple(i for i in tuplee if i%2 == 0) print(tuplee) (2, 4, 6, 8, 10) OR\n#也可以用filter过滤 tuplee = (1,2,3,4,5,6,7,8,9,10) #再温习下fliter，传入方法和可迭代的对象，相当于从可迭代对象中将元素一个个取出，并且用传入的方法处理 #再温习下lambda ： 传入的方法可以是条件判断语句 tuplee = tuple(filter(lambda i:i%2 == 0,tuplee )) print(tuplee) (2, 4, 6, 8, 10) Question 40 Question: Write a program which accepts a string as input to print \u0026ldquo;Yes\u0026rdquo; if the string is \u0026ldquo;yes\u0026rdquo; or \u0026ldquo;YES\u0026rdquo; or \u0026ldquo;Yes\u0026rdquo;, otherwise print \u0026ldquo;No\u0026rdquo;.\nHints: Use if statement to judge condition.\nSolution: Python 3\na = input() if a == \u0026#34;YES\u0026#34; or a ==\u0026#34;yes\u0026#34; or a == \u0026#34;Yes\u0026#34;: print(\u0026#34;Yes\u0026#34;) else: print(\u0026#34;No\u0026#34;) yes Yes 0 练习coding : 叶宇浩Miles\n","date":"2021-06-08T00:00:00Z","permalink":"https://example.com/p/py_basic_q31-40/","title":"py_basic_Q31-40"},{"content":" 这是为了加强Python基础，在Github上找来的练习，地址：https://github.com/darkprinx/break-the-ice-with-python\nQuestion 21 Question: A robot moves in a plane starting from the original point (0,0). The robot can move toward UP, DOWN, LEFT and RIGHT with a given steps. The trace of robot movement is shown as the following:\nUP 5 DOWN 3 LEFT 3 RIGHT 2 The numbers after the direction are steps. Please write a program to compute the distance from current position after a sequence of movement and original point. If the distance is a float, then just print the nearest integer.\nExample:\nIf the following tuples are given as input to the program:\nUP 5 DOWN 3 LEFT 3 RIGHT 2 Then, the output of the program should be:\n2 Hints: In case of input data being supplied to the question, it should be assumed to be a console input.Here distance indicates to euclidean distance.Import math module to use sqrt function.\nSolutions:\nimport math x,y = 0,0 while True: #split默认为空格划分:如 RIGHT 2 划分为一个数组，且index[0]为RIGHT,index[1]为2 a = input().split() #空 if not a: break #注意前面将a划分了 if a[0] == \u0026#34;UP\u0026#34;: y+=int(a[1]) if a[0] == \u0026#34;DOWN\u0026#34;: y-=int(a[1]) if a[0] == \u0026#34;LEFT\u0026#34;: x-=int(a[1]) if a[0] == \u0026#34;RIGHT\u0026#34;: x+=int(a[1]) #round的作用为四舍五入到整数，当然可以指定小数位 result = round(math.sqrt(x**2+y**2)) print(result) UP 5 DOWN 3 LEFT 2 RIGHT 3 2 Question 22 Question: Write a program to compute the frequency of the words from the input. The output should output after sorting the key alphanumerically.\nSuppose the following input is supplied to the program:\nNew to Python or choosing between Python 2 and Python 3? Read Python 2 or Python 3.\nThen, the output should be:\n2:2 3.:1 3?:1 New:1 Python:5 Read:1 and:1 between:1 choosing:1 or:2 to:1 Hints In case of input data being supplied to the question, it should be assumed to be a console input.\nSolutions:\nsentence = input().split() #in 后面可以直接跟sorted出来的可迭代对象 for i in sorted(set(sentence)): #注意这里用到的format格式化输出,{0}、{1}是一一和format()中的顺序对应的 print(\u0026#34;{0}:{1}\u0026#34;.format(i,sentence.count(i))) New to Python or choosing between Python 2 and Python 3? Read Python 2 or Python 3. 2:2 3.:1 3?:1 New:1 Python:5 Read:1 and:1 between:1 choosing:1 or:2 to:1 OR\nsentence = input().split() dic = {} for i in sentence: #字典的setdefault用法：可以用来设置字典中的默认值，如果键不存在于字典中，将会添加键并将值设为默认值。存在则跳过 #dict.setdefault(key, default=None) dic.setdefault(i,sentence.count(i)) # print(dic.sort) // \u0026#39;dict\u0026#39; object has no attribute \u0026#39;sort\u0026#39; # print(sorted(dic))#这个排序会吞掉value print(dic) New to Python or choosing between Python 2 and Python 3? Read Python 2 or Python 3. {\u0026#39;New\u0026#39;: 1, \u0026#39;to\u0026#39;: 1, \u0026#39;Python\u0026#39;: 5, \u0026#39;or\u0026#39;: 2, \u0026#39;choosing\u0026#39;: 1, \u0026#39;between\u0026#39;: 1, \u0026#39;2\u0026#39;: 2, \u0026#39;and\u0026#39;: 1, \u0026#39;3?\u0026#39;: 1, \u0026#39;Read\u0026#39;: 1, \u0026#39;3.\u0026#39;: 1} Question 23 Question: Write a method which can calculate square value of number\nHints: Using the ** operator which can be written as n**p where means n^p\nSolutions:\ndef Q23(x): return x**2 a = int(input()) Q23(a) 2 4 Question 24 Question: Python has many built-in functions, and if you do not know how to use it, you can read document online or find some books. But Python has a built-in document function for every built-in functions.\nPlease write a program to print some Python built-in functions documents, such as abs(), int(), raw_input()\nAnd add document for your own function\nHints: The built-in document method is doc\nSolutions:\n#method.__doc__可以查看method这个方法的文档 print(sorted.__doc__) #本质是这个 def fun(x): \u0026#34;\u0026#34;\u0026#34; 这是演示文档 \u0026#34;\u0026#34;\u0026#34; return x print(fun.__doc__) Return a new list containing all items from the iterable in ascending order. A custom key function can be supplied to customize the sort order, and the reverse flag can be set to request the result in descending order. 这是演示文档 Question 25 Question: Define a class, which have a class parameter and have a same instance parameter.\nHints: Define an instance parameter, need add it in init method.You can init an object with construct parameter or set the value later\nSolutions:\nclass Q25: def __init__(self,name=\u0026#34;A\u0026#34;): #这里实际上是类的属性了 #另外，因为是在init中的即初始化中的，所以传值时可以在实例化类的时候传入：看下面的y self.name = name x = Q25() #默认为A print(x.name) x.name = \u0026#34;C\u0026#34;#也可以这样写个默认值 print(x.name) #修改默认值 y = Q25(\u0026#34;B\u0026#34;) print(y.name) A C B Question 26 Question: Define a function which can compute the sum of two numbers.\nHints: Define a function with two numbers as arguments. You can compute the sum in the function and return the value.\nSolutions:\n#可以考虑巧用匿名函数，匿名函数本质上还是函数，所以和函数的使用一致 #这里相当于定义了一个def add():。。。 add = lambda x,y:x+y print(add(1,2)) 3 Question 27 Question: Define a function that can convert a integer into a string and print it in console.\nHints: Use str() to convert a number to string.\nSolutions:\nQ27 = lambda x:str(x) result = Q27(27) print(result) print(type(result)) 27 \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; Question 28 Question: Define a function that can receive two integer numbers in string form and compute their sum and then print it in console.\nHints: Use int() to convert a string to integer.\nSolutions:\nQ28 = lambda x,y:int(x)+int(y) print(Q28(28,28)) 56 Question 29 Question: Define a function that can accept two strings as input and concatenate them and then print it in console.\nHints: Use + sign to concatenate the strings.\nSolutions:\nQ29 = lambda x,y:x+y print(Q29(\u0026#34;29\u0026#34;,\u0026#34;29\u0026#34;)) 2929 Question 30 Question: Define a function that can accept two strings as input and print the string with maximum length in console. If two strings have the same length, then the function should print all strings line by line.\nHints: Use len() function to get the length of a string.\nSolutions:\n#max中的参数key可以指定max的判定标准 #print(max((a,b),key=len)) if len(a) != len(b) else print(a+\u0026#34;\\n\u0026#34;+b) //注意这个写法 Q30 = lambda a,b:print(max((a,b),key=len)) if len(a) != len(b) else print(a+\u0026#34;\\n\u0026#34;+b) a = input().split() print(Q30(a[0],a[1])) yeyuhao miles yeyuhao None 0 练习coding : 叶宇浩Miles\n","date":"2021-06-07T00:00:00Z","permalink":"https://example.com/p/py_basic_q21-30/","title":"py_basic_Q21-30"},{"content":" 这是为了加强Python基础，在Github上找来的练习，地址：https://github.com/darkprinx/break-the-ice-with-python\nQuestion 1 Question: **_Write a program which will find all such numbers which are divisible by 7 but are not a multiple of 5,\nbetween 2000 and 3200 (both included).The numbers obtained should be printed in a comma-separated sequence on a single line._**\nHints: Consider use range(#begin, #end) method.\nSolutions:\nUsing for loops #注意range是左闭右开，range(a,b)产生[a,b)区间的int for i in range(2000,3201): if i%7 == 0 and i%5 != 0: print(i,end=\u0026#34;,\u0026#34;)#end=\u0026#34;,\u0026#34;每个对象的结尾以，分开 print(\u0026#34;\\b\u0026#34;)#光标倒退指定长度的字符，最会一个的，去掉 2002,2009,2016,2023,2037,2044,2051,2058,2072,2079,2086,2093,2107,2114,2121,2128,2142,2149,2156,2163,2177,2184,2191,2198,2212,2219,2226,2233,2247,2254,2261,2268,2282,2289,2296,2303,2317,2324,2331,2338,2352,2359,2366,2373,2387,2394,2401,2408,2422,2429,2436,2443,2457,2464,2471,2478,2492,2499,2506,2513,2527,2534,2541,2548,2562,2569,2576,2583,2597,2604,2611,2618,2632,2639,2646,2653,2667,2674,2681,2688,2702,2709,2716,2723,2737,2744,2751,2758,2772,2779,2786,2793,2807,2814,2821,2828,2842,2849,2856,2863,2877,2884,2891,2898,2912,2919,2926,2933,2947,2954,2961,2968,2982,2989,2996,3003,3017,3024,3031,3038,3052,3059,3066,3073,3087,3094,3101,3108,3122,3129,3136,3143,3157,3164,3171,3178,3192,3199, Using generators and list comprehension #进阶一点的写法，用生成器和列表理解式,理解式中是以空格分开的 [ print(i,end=\u0026#34;,\u0026#34;) for i in range(2000,3201) if i%7 == 0 and i%5 != 0 ] print(\u0026#34;\\b\u0026#34;) 2002,2009,2016,2023,2037,2044,2051,2058,2072,2079,2086,2093,2107,2114,2121,2128,2142,2149,2156,2163,2177,2184,2191,2198,2212,2219,2226,2233,2247,2254,2261,2268,2282,2289,2296,2303,2317,2324,2331,2338,2352,2359,2366,2373,2387,2394,2401,2408,2422,2429,2436,2443,2457,2464,2471,2478,2492,2499,2506,2513,2527,2534,2541,2548,2562,2569,2576,2583,2597,2604,2611,2618,2632,2639,2646,2653,2667,2674,2681,2688,2702,2709,2716,2723,2737,2744,2751,2758,2772,2779,2786,2793,2807,2814,2821,2828,2842,2849,2856,2863,2877,2884,2891,2898,2912,2919,2926,2933,2947,2954,2961,2968,2982,2989,2996,3003,3017,3024,3031,3038,3052,3059,3066,3073,3087,3094,3101,3108,3122,3129,3136,3143,3157,3164,3171,3178,3192,3199, #或者拿个list存起来 list = [ str(i) for i in range(2000,3201) if i%7 == 0 and i%5 != 0 ] #注意join(i)传入的对象list中的元素是string类型的，且前面要指定以什么划分每个数据，这就要求list是个可迭代的对象了 #就是说join会将list的每个元素取出，并且以，作为分隔符 print(\u0026#34;,\u0026#34;.join(list)) 2002,2009,2016,2023,2037,2044,2051,2058,2072,2079,2086,2093,2107,2114,2121,2128,2142,2149,2156,2163,2177,2184,2191,2198,2212,2219,2226,2233,2247,2254,2261,2268,2282,2289,2296,2303,2317,2324,2331,2338,2352,2359,2366,2373,2387,2394,2401,2408,2422,2429,2436,2443,2457,2464,2471,2478,2492,2499,2506,2513,2527,2534,2541,2548,2562,2569,2576,2583,2597,2604,2611,2618,2632,2639,2646,2653,2667,2674,2681,2688,2702,2709,2716,2723,2737,2744,2751,2758,2772,2779,2786,2793,2807,2814,2821,2828,2842,2849,2856,2863,2877,2884,2891,2898,2912,2919,2926,2933,2947,2954,2961,2968,2982,2989,2996,3003,3017,3024,3031,3038,3052,3059,3066,3073,3087,3094,3101,3108,3122,3129,3136,3143,3157,3164,3171,3178,3192,3199 Question 2 Question: **_Write a program which can compute the factorial of a given numbers.The results should be printed in a comma-separated sequence on a single line.Suppose the following input is supplied to the program: 8\nThen, the output should be:40320_**\nHints: In case of input data being supplied to the question, it should be assumed to be a console input.\nSolutions:\nfactorial阶乘的意思\nUsing For Loop #input默认是string类型的数据 #这里将str转为int a = int(input()) result = 1#记得初始化未出现的变量 for i in range(1,a+1):#注意range的范围 result *=i print(result) 8 40320 Using Function a = int(input()) def Q2(x): #这里用了递归fun以及if/else的一种特殊写法 #什么意思呢，就是Q2(x-1)会调用所在的函数，并将x-1传入;简单来说这样可以实现函数调用本身吧 return 1 if x==1 else Q2(x-1)*x print(Q2(a)) 8 40320 Using Lambda Function #匿名函数的写法,lambda a:fun 即a是一个对象，将其用fun处理 #map(a,b)函数的使用:传入一个a方法，以及b可迭代对象；实质就是b使用a的函数,且返回一个全部新元素的list a = int(input()) itera = [ i for i in range(1,a+1) ] print(itera) print(map(lambda x:x*x, itera )) 8 [1, 2, 3, 4, 5, 6, 7, 8] \u0026lt;map object at 0x000001E7C32C5BE0\u0026gt; Question 3 Question: With a given integral number n, write a program to generate a dictionary that contains (i, i x i) such that is an integral number between 1 and n (both included). and then the program should print the dictionary.Suppose the following input is supplied to the program: 8\nThen, the output should be:\n{1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64}\nHints: In case of input data being supplied to the question, it should be assumed to be a console input.Consider use dict()\nSolutions:\nUsing For loop #这里只需要注意下dictionary对象的key和value就好了；形式 dict[key] = value ；不要忽略等于号！ dict = {} a = int(input()) for i in range(1,a+1): dict[i] = i*i print(dict) 8 {1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64} Using dictionary comprehension #进阶一点的写法，和list一样可以用理解式，就是写成一句话的样子 #不过要注意这样的话 不能这样写了dict[key] = value；而是这样key:value；因为它本身就已经在dictionary中了 a = int(input()) dict = {i:i*i for i in range(1,a+1)} print(dict) 8 {1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64} Question 4 Question: Write a program which accepts a sequence of comma-separated numbers from console and generate a list and a tuple which contains every number.Suppose the following input is supplied to the program:\n34,67,55,33,12,98\nThen, the output should be:\n[\u0026lsquo;34\u0026rsquo;, \u0026lsquo;67\u0026rsquo;, \u0026lsquo;55\u0026rsquo;, \u0026lsquo;33\u0026rsquo;, \u0026lsquo;12\u0026rsquo;, \u0026lsquo;98\u0026rsquo;]\n(\u0026lsquo;34\u0026rsquo;, \u0026lsquo;67\u0026rsquo;, \u0026lsquo;55\u0026rsquo;, \u0026lsquo;33\u0026rsquo;, \u0026lsquo;12\u0026rsquo;, \u0026lsquo;98\u0026rsquo;)\nHints: In case of input data being supplied to the question, it should be assumed to be a console input.tuple() method can convert list to tuple\nSolutions:\n#input如果输入的是许多以分隔符分隔的对象，且split会将input输入的每个元素为string类型，且其会构成一个list对象 a = input().split(\u0026#34;,\u0026#34;) #可以直接通过tuple将对象格式转为元组 tpl = tuple(a) print(a) print(tpl) 34,67,55,33,12,98 [\u0026#39;34\u0026#39;, \u0026#39;67\u0026#39;, \u0026#39;55\u0026#39;, \u0026#39;33\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;98\u0026#39;] (\u0026#39;34\u0026#39;, \u0026#39;67\u0026#39;, \u0026#39;55\u0026#39;, \u0026#39;33\u0026#39;, \u0026#39;12\u0026#39;, \u0026#39;98\u0026#39;) Question 5 Question: Define a class which has at least two methods:\ngetString: to get a string from console input printString: to print the string in upper case. Also please include simple test function to test the class methods.\nHints: Use init method to construct some parameters\nSolutions:\n#创造类时是不用在类名后面 #然而类中的方法即函数是需要（）传值，而且都需要传self class Q5: def __init__(self): pass#什么都不用做的话就pass掉 def getString(self,strings): #这里self.相当于为class Q5赋予可一个string的属性,使其等于输入的strings #这样有什么好处呢:可以方便在其他函数中处理这个值 self.string = strings def printString(self): print(self.string.upper()) #调用类时,都需要实例化对应的类 Q = Q5() #Q就相当于class中的self了 #然后就可以通过方法的形式调用类中定义的方法了 Q.getString(\u0026#34;会不会 有一天 时间真的能倒退\u0026#34;) Q.printString() 会不会 有一天 时间真的能倒退 Question 6 Question: Write a program that calculates and prints the value according to the given formula:\nQ = Square root of [(2 _ C _ D)/H]\nFollowing are the fixed values of C and H:\nC is 50. H is 30.\n**_D is the variable whose values should be input to your program in a comma-separated sequence.For example\nLet us assume the following comma separated input sequence is given to the program:_**\n100,150,180\nThe output of the program should be:\n18,22,24\nHints: If the output received is in decimal form, it should be rounded off to its nearest value (for example, if the output received is 26.0, it should be printed as 26).In case of input data being supplied to the question, it should be assumed to be a console input.\nSolutions:\nimport math #还是要注意下input多元素时的对象后为list D = input().split(\u0026#34;,\u0026#34;) def Q6(D): return math.sqrt((2*50*D)/30) #列表理解式,可以看出[]中可以使用方法 #又因为有小数,所以可以用round来取整数,或者指定小数位 result = [round(Q6(int(i))) for i in D] print(result) #要注意下传参时的数据类型 100,150,180 [18, 22, 24] Question 7 Question: _Write a program which takes 2 digits, X,Y as input and generates a 2-dimensional array. The element value in the i-th row and j-th column of the array should be i _ j.*\nNote: i=0,1.., X-1; j=0,1,¡­Y-1. Suppose the following inputs are given to the program: 3,5\nThen, the output of the program should be:\n[[0, 0, 0, 0, 0], [0, 1, 2, 3, 4], [0, 2, 4, 6, 8]]\nHints: Note: In case of input data being supplied to the question, it should be assumed to be a console input in a comma-separated form.\nSolutions:\n#map反射的一种,就是在编译的时候动态使用方法 #map(a,b) :a是一个function,b是一个可迭代的对象;就是说它会将b中的元素取出,一个个去使用a这个function x,y = map(int,input().split(\u0026#34;,\u0026#34;)) result = [] #对行遍历 #如果range(x)只传入一个参数,则会产生[0,x)区间内的int for i in range(x): tpl = [] #遍历列 for j in range(y): tpl.append(i*j) result.append(tpl) print(result) #三维五列数组 3,5 [[0, 0, 0, 0, 0], [0, 1, 2, 3, 4], [0, 2, 4, 6, 8]] OR\n#同样双循环也可以用列表推导式 #注意双循环的[[]]的位置就好 x,y = map(int,input().split(\u0026#34;,\u0026#34;)) result = [[i*j for j in range(y)]for i in range(x)] print(result) 3,5 [[0, 0, 0, 0, 0], [0, 1, 2, 3, 4], [0, 2, 4, 6, 8]] Question 8 Question: Write a program that accepts a comma separated sequence of words as input and prints the words in a comma-separated sequence after sorting them alphabetically.\nSuppose the following input is supplied to the program:\nwithout,hello,bag,world\nThen, the output should be:\nbag,hello,without,world\nHints: In case of input data being supplied to the question, it should be assumed to be a console input.\nSolutions:\na = input().split(\u0026#34;,\u0026#34;) #sort()排序的话在没有指定的情况下,会按照传入的数据类型来排序,如int按照大小,str按照ASCII码顺序 a.sort() print(a) without,hello,bag,world [\u0026#39;bag\u0026#39;, \u0026#39;hello\u0026#39;, \u0026#39;without\u0026#39;, \u0026#39;world\u0026#39;] Question 9 Question: Write a program that accepts sequence of lines as input and prints the lines after making all characters in the sentence capitalized.\nSuppose the following input is supplied to the program:\nHello world\nPractice makes perfect\nThen, the output should be:\nHELLO WORLD\nPRACTICE MAKES PERFECT\nHints: In case of input data being supplied to the question, it should be assumed to be a console input.\nSolutions:\ndef Q9(): #死循环 while True: a = input() #这里就是判断输入是否为空,空的话破环braek\u0026#39;循环 if not a: break #这里温习下return和yield的区别 #return 会破坏循环即有返回值了输出就完事了,不再进行循环了; #yield则相反,有返回值而且不会破化循环,能继续循环体的进行,返还 yield a #可以看出map可以接受函数迭代出的数据,且map的对象要用循环print出来 [print(i) for i in map(str.upper,Q9())] Hello world HELLO WORLD Practice makes perfect PRACTICE MAKES PERFECT dsa DSA Practice makes perfect PRACTICE MAKES PERFECT [None, None, None, None] OR\nQuestion 10 Question Write a program that accepts a sequence of whitespace separated words as input and prints the words after removing all duplicate words and sorting them alphanumerically.\nSuppose the following input is supplied to the program:\nhello world and practice makes perfect and hello world again\nThen, the output should be:\nagain and hello makes perfect practice world\nHints: In case of input data being supplied to the question, it should be assumed to be a console input.We use set container to remove duplicated data automatically and then use sorted() to sort the data.\nSolutions:\n#如果split没有指定sep的话,分隔符为空格 #split是将传入的数据以某种分隔符划分开,最后成一个list text = input().split() #注意理解下这里的逻辑,列表理解式不说了,count是py本身的函数计算次数的,remove是去除某个数据的 #这里表现为去除重复出现的单词,以出现次数作为度量 [text.remove(i) for i in text if text.count(i)\u0026gt;1] text.sort() print(text) # print(text.sort())这样写不行的,唔知道点解 hello world and practice makes perfect and hello world again [\u0026#39;again\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;hello\u0026#39;, \u0026#39;makes\u0026#39;, \u0026#39;perfect\u0026#39;, \u0026#39;practice\u0026#39;, \u0026#39;world\u0026#39;] OR\n#可以用set集合的特性去重;但是要注意了set()之后获得的对象不是list,而是set()元组 #可以直接用sorted简化第一种写法 # sorted(list(set(input().split())))这样不行 #sort/sorted 使用的区别：sort是这样用的a.sort；sorted是这样的 sorted(a)且可以指定key #发现set本身会去重排序了 set(input().split()) hello world and practice makes perfect and hello world again {\u0026#39;again\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;hello\u0026#39;, \u0026#39;makes\u0026#39;, \u0026#39;perfect\u0026#39;, \u0026#39;practice\u0026#39;, \u0026#39;world\u0026#39;} 0 练习coding : 叶宇浩Miles\n","date":"2021-06-06T00:00:00Z","permalink":"https://example.com/p/py_basic_q1-10/","title":"py_basic_Q1-10"},{"content":" 这是为了加强Python基础，在Github上找来的练习，地址：https://github.com/darkprinx/break-the-ice-with-python\nQuestion 11 Question Write a program which accepts a sequence of comma separated 4 digit binary numbers as its input and then check whether they are divisible by 5 or not. The numbers that are divisible by 5 are to be printed in a comma separated sequence.\nExample:\n0100,0011,1010,1001\nThen the output should be:\n1010\nNotes: Assume the data is input by console.\nHints: In case of input data being supplied to the question, it should be assumed to be a console input.\nSolutions:\n#python中有一个过滤器filter方法，使用和map一样，传入方法和数据 def Q11(a): #int(x,base)base是进制，默认为十进制 #这里的输入为二进制 return int(a,2) %5 == 0#这里就相当于一个条件判断了 data = input().split(\u0026#34;,\u0026#34;) data = filter(Q11,data) # [print(i) for i in data] #除了可以用遍历输出，还可以用join遍历 print(\u0026#34;,\u0026#34;.join(data)) 0100,0011,1010,1001 1010 OR\ndata = input().split(\u0026#34;,\u0026#34;) #直接用匿名表达式 #lambda x:function 传入x并将其用function处理,可以是条件判断等等 data = filter(lambda a:int(a,2) %5 == 0,data) print(\u0026#34;,\u0026#34;.join(data)) 0100,0011,1010,1001 1010 Question 12 Question: Write a program, which will find all such numbers between 1000 and 3000 (both included) such that each digit of the number is an even number.The numbers obtained should be printed in a comma-separated sequence on a single line.\nHints: In case of input data being supplied to the question, it should be assumed to be a console input.\nSolutions:\neven number偶数\nprint(\u0026#34;,\u0026#34;.join(filter(lambda x: int(x)%2==0, [str(x) for x in range(1000,3001)]))) #再次提醒下注意数据的类型 1000,1002,1004,1006,1008,1010,1012,1014,1016,1018,1020,1022,1024,1026,1028,1030,1032,1034,1036,1038,1040,1042,1044,1046,1048,1050,1052,1054,1056,1058,1060,1062,1064,1066,1068,1070,1072,1074,1076,1078,1080,1082,1084,1086,1088,1090,1092,1094,1096,1098,1100,1102,1104,1106,1108,1110,1112,1114,1116,1118,1120,1122,1124,1126,1128,1130,1132,1134,1136,1138,1140,1142,1144,1146,1148,1150,1152,1154,1156,1158,1160,1162,1164,1166,1168,1170,1172,1174,1176,1178,1180,1182,1184,1186,1188,1190,1192,1194,1196,1198,1200,1202,1204,1206,1208,1210,1212,1214,1216,1218,1220,1222,1224,1226,1228,1230,1232,1234,1236,1238,1240,1242,1244,1246,1248,1250,1252,1254,1256,1258,1260,1262,1264,1266,1268,1270,1272,1274,1276,1278,1280,1282,1284,1286,1288,1290,1292,1294,1296,1298,1300,1302,1304,1306,1308,1310,1312,1314,1316,1318,1320,1322,1324,1326,1328,1330,1332,1334,1336,1338,1340,1342,1344,1346,1348,1350,1352,1354,1356,1358,1360,1362,1364,1366,1368,1370,1372,1374,1376,1378,1380,1382,1384,1386,1388,1390,1392,1394,1396,1398,1400,1402,1404,1406,1408,1410,1412,1414,1416,1418,1420,1422,1424,1426,1428,1430,1432,1434,1436,1438,1440,1442,1444,1446,1448,1450,1452,1454,1456,1458,1460,1462,1464,1466,1468,1470,1472,1474,1476,1478,1480,1482,1484,1486,1488,1490,1492,1494,1496,1498,1500,1502,1504,1506,1508,1510,1512,1514,1516,1518,1520,1522,1524,1526,1528,1530,1532,1534,1536,1538,1540,1542,1544,1546,1548,1550,1552,1554,1556,1558,1560,1562,1564,1566,1568,1570,1572,1574,1576,1578,1580,1582,1584,1586,1588,1590,1592,1594,1596,1598,1600,1602,1604,1606,1608,1610,1612,1614,1616,1618,1620,1622,1624,1626,1628,1630,1632,1634,1636,1638,1640,1642,1644,1646,1648,1650,1652,1654,1656,1658,1660,1662,1664,1666,1668,1670,1672,1674,1676,1678,1680,1682,1684,1686,1688,1690,1692,1694,1696,1698,1700,1702,1704,1706,1708,1710,1712,1714,1716,1718,1720,1722,1724,1726,1728,1730,1732,1734,1736,1738,1740,1742,1744,1746,1748,1750,1752,1754,1756,1758,1760,1762,1764,1766,1768,1770,1772,1774,1776,1778,1780,1782,1784,1786,1788,1790,1792,1794,1796,1798,1800,1802,1804,1806,1808,1810,1812,1814,1816,1818,1820,1822,1824,1826,1828,1830,1832,1834,1836,1838,1840,1842,1844,1846,1848,1850,1852,1854,1856,1858,1860,1862,1864,1866,1868,1870,1872,1874,1876,1878,1880,1882,1884,1886,1888,1890,1892,1894,1896,1898,1900,1902,1904,1906,1908,1910,1912,1914,1916,1918,1920,1922,1924,1926,1928,1930,1932,1934,1936,1938,1940,1942,1944,1946,1948,1950,1952,1954,1956,1958,1960,1962,1964,1966,1968,1970,1972,1974,1976,1978,1980,1982,1984,1986,1988,1990,1992,1994,1996,1998,2000,2002,2004,2006,2008,2010,2012,2014,2016,2018,2020,2022,2024,2026,2028,2030,2032,2034,2036,2038,2040,2042,2044,2046,2048,2050,2052,2054,2056,2058,2060,2062,2064,2066,2068,2070,2072,2074,2076,2078,2080,2082,2084,2086,2088,2090,2092,2094,2096,2098,2100,2102,2104,2106,2108,2110,2112,2114,2116,2118,2120,2122,2124,2126,2128,2130,2132,2134,2136,2138,2140,2142,2144,2146,2148,2150,2152,2154,2156,2158,2160,2162,2164,2166,2168,2170,2172,2174,2176,2178,2180,2182,2184,2186,2188,2190,2192,2194,2196,2198,2200,2202,2204,2206,2208,2210,2212,2214,2216,2218,2220,2222,2224,2226,2228,2230,2232,2234,2236,2238,2240,2242,2244,2246,2248,2250,2252,2254,2256,2258,2260,2262,2264,2266,2268,2270,2272,2274,2276,2278,2280,2282,2284,2286,2288,2290,2292,2294,2296,2298,2300,2302,2304,2306,2308,2310,2312,2314,2316,2318,2320,2322,2324,2326,2328,2330,2332,2334,2336,2338,2340,2342,2344,2346,2348,2350,2352,2354,2356,2358,2360,2362,2364,2366,2368,2370,2372,2374,2376,2378,2380,2382,2384,2386,2388,2390,2392,2394,2396,2398,2400,2402,2404,2406,2408,2410,2412,2414,2416,2418,2420,2422,2424,2426,2428,2430,2432,2434,2436,2438,2440,2442,2444,2446,2448,2450,2452,2454,2456,2458,2460,2462,2464,2466,2468,2470,2472,2474,2476,2478,2480,2482,2484,2486,2488,2490,2492,2494,2496,2498,2500,2502,2504,2506,2508,2510,2512,2514,2516,2518,2520,2522,2524,2526,2528,2530,2532,2534,2536,2538,2540,2542,2544,2546,2548,2550,2552,2554,2556,2558,2560,2562,2564,2566,2568,2570,2572,2574,2576,2578,2580,2582,2584,2586,2588,2590,2592,2594,2596,2598,2600,2602,2604,2606,2608,2610,2612,2614,2616,2618,2620,2622,2624,2626,2628,2630,2632,2634,2636,2638,2640,2642,2644,2646,2648,2650,2652,2654,2656,2658,2660,2662,2664,2666,2668,2670,2672,2674,2676,2678,2680,2682,2684,2686,2688,2690,2692,2694,2696,2698,2700,2702,2704,2706,2708,2710,2712,2714,2716,2718,2720,2722,2724,2726,2728,2730,2732,2734,2736,2738,2740,2742,2744,2746,2748,2750,2752,2754,2756,2758,2760,2762,2764,2766,2768,2770,2772,2774,2776,2778,2780,2782,2784,2786,2788,2790,2792,2794,2796,2798,2800,2802,2804,2806,2808,2810,2812,2814,2816,2818,2820,2822,2824,2826,2828,2830,2832,2834,2836,2838,2840,2842,2844,2846,2848,2850,2852,2854,2856,2858,2860,2862,2864,2866,2868,2870,2872,2874,2876,2878,2880,2882,2884,2886,2888,2890,2892,2894,2896,2898,2900,2902,2904,2906,2908,2910,2912,2914,2916,2918,2920,2922,2924,2926,2928,2930,2932,2934,2936,2938,2940,2942,2944,2946,2948,2950,2952,2954,2956,2958,2960,2962,2964,2966,2968,2970,2972,2974,2976,2978,2980,2982,2984,2986,2988,2990,2992,2994,2996,2998,3000 Question 13 Question: Write a program that accepts a sentence and calculate the number of letters and digits.\nSuppose the following input is supplied to the program:\nhello world! 123\nThen, the output should be:\nLETTERS 10\nDIGITS 3\nHints: In case of input data being supplied to the question, it should be assumed to be a console input.\nSolutions:\nsentence = input() #初始化lettert和digits的值 letter,digits = 0,0 for i in sentence: #py有内置的方法isalpha判断是否为字母 if i.isalpha(): letter += 1 #同样，py有内置的方法isnumeric判断是否为数字 elif i.isnumeric(): digits += 1 print(letter,digits) hello world! 123 10 3 \u0026mdash;\nConclusion All the above problems are mostly string related problems. Major parts of the solution includes string releted functions and comprehension method to write down the code in more shorter form.\nQuestion 14 Question: Write a program that accepts a sentence and calculate the number of upper case letters and lower case letters.\nSuppose the following input is supplied to the program:\nHello world!\nThen, the output should be:\nUPPER CASE 1\nLOWER CASE 9\nHints: In case of input data being supplied to the question, it should be assumed to be a console input.\nSolutions:\n#同py有内置的isupper和islower方法判断字母是否为大小写 sentence = input() UPPER = sum([1 for i in sentence if i.isupper()]) LOWER = sum([1 for i in sentence if i.islower()]) print(UPPER,LOWER) Hello world! 1 9 Question 15 Question: Write a program that computes the value of a+aa+aaa+aaaa with a given digit as the value of a.\nSuppose the following input is supplied to the program:\n9\nThen, the output should be:\n11106\nHints: In case of input data being supplied to the question, it should be assumed to be a console input.\nSolutions:\n#这里其实想说的是：数值类型数据相加是属于数学意义上的相加乘；而string类型相加乘则是拼接，就是重叠string而已 a = input() #记得初始化变量 total = 0 tmp = str() for i in range(4):#再说一遍range如没有指定区间，则默认为[0，x)即从零开始 tmp += a #int会将tmp的数字转为int类型最为数字相加和 #9/99/999/9999 total += int(tmp) print(total) 9 11106 OR\n#这里用str得相乘看看 a = input() totla = int(a) + int(a*2) + int(a*3) + int(a*4) print(total) 9 11106 Question 16 Question: Use a list comprehension to square each odd number in a list. The list is input by a sequence of comma-separated numbers. \u0026gt;Suppose the following input is supplied to the program:\n1,2,3,4,5,6,7,8,9\nThen, the output should be:\n1,9,25,49,81\nHints: In case of input data being supplied to the question, it should be assumed to be a console input.\nSolutions:\n#不能这样for int(i) in input() #可以看出for得in后面可以直接用input的 [int(i)**2 for i in input().split(\u0026#34;,\u0026#34;) if int(i)%2 != 0 ] 1,2,3,4,5,6,7,8,9 [1, 9, 25, 49, 81] _There were a mistake in the the test case and the solution\u0026rsquo;s whice were notified and fixed with the help of @dwedigital. My warm thanks to him. _\nQuestion 17 Question: Write a program that computes the net amount of a bank account based a transaction log from console input. The transaction log format is shown as following:\nD 100 W 200 D means deposit while W means withdrawal. Suppose the following input is supplied to the program:\nD 300 D 300 W 200 D 100 Then, the output should be:\n500 Hints: In case of input data being supplied to the question, it should be assumed to be a console input.\nSolutions:\nlist = [] while True: #input要放在循环中才可以反复输入 a = input() if not a: break else: #将输入的持续性存起来 #这里也是为什么前面用到list的原因了，因为要存储之前的输入的data list.append(a) #输入的格式为D 300 sum([int(i[2:]) if i[0] == \u0026#34;D\u0026#34; else -int(i[2:]) for i in list ]) D 300 D 300 W 200 D 100 500 Question 18 Question: A website requires the users to input username and password to register. Write a program to check the validity of password input by users.\nFollowing are the criteria for checking the password:\nAt least 1 letter between [a-z] At least 1 number between [0-9] At least 1 letter between [A-Z] At least 1 character from [$#@] Minimum length of transaction password: 6 Maximum length of transaction password: 12 Your program should accept a sequence of comma separated passwords and will check them according to the above criteria. Passwords that match the criteria are to be printed, each separated by a comma.\nExample\nIf the following passwords are given as input to the program:\nABd1234@1,a F1#,2w3E*,2We3345\nThen, the output of the program should be:\nABd1234@1\nHints: In case of input data being supplied to the question, it should be assumed to be a console input.\nSolutions:\n#这里就是用到正则去判断而已 import re a = input().split(\u0026#34;,\u0026#34;) def Q18(s): x = 0 #re.search会匹配是否存在对应的规则，存在则返回一个true #这里会自动转换类型True--\u0026gt;1;False--\u0026gt;0 #这里的正则其实就是看是否存在大小写、特殊符号、数字 x += bool(re.search(\u0026#34;[a-z]\u0026#34;,s)) x += bool(re.search(\u0026#34;[0-9]\u0026#34;,s)) x += bool(re.search(\u0026#34;[A-Z]\u0026#34;,s)) x += bool(re.search(\u0026#34;[$#@]\u0026#34;,s)) #同样注意这个写法 x += len(s)\u0026lt;=12 and len(s)\u0026gt;=6 return x == 5 list = filter(Q18,a) #用join遍历出来 #这里再说下join/split;join是将传入数据的元素一个个取出处理，连接成一个整体；而split是将传入的数据以某种形式划分开，然后过程一个大list print(\u0026#34;,\u0026#34;.join(list)) ABd1234@1,a F1#,2w3E*,2We3345 ABd1234@1 Question 19 Question: You are required to write a program to sort the (name, age, score) tuples by ascending order where name is string, age and score are numbers. The tuples are input by console. The sort criteria is:\n1: Sort based on name 2: Then sort based on age 3: Then sort by score The priority is that name \u0026gt; age \u0026gt; score.\nIf the following tuples are given as input to the program:\nTom,19,80\nJohn,20,90\nJony,17,91\nJony,17,93\nJson,21,85\nThen, the output of the program should be:\n[(\u0026lsquo;John\u0026rsquo;, \u0026lsquo;20\u0026rsquo;, \u0026lsquo;90\u0026rsquo;), (\u0026lsquo;Jony\u0026rsquo;, \u0026lsquo;17\u0026rsquo;, \u0026lsquo;91\u0026rsquo;), (\u0026lsquo;Jony\u0026rsquo;, \u0026lsquo;17\u0026rsquo;, \u0026lsquo;93\u0026rsquo;), (\u0026lsquo;Json\u0026rsquo;, \u0026lsquo;21\u0026rsquo;, \u0026lsquo;85\u0026rsquo;), (\u0026lsquo;Tom\u0026rsquo;, \u0026lsquo;19\u0026rsquo;, \u0026lsquo;80\u0026rsquo;)]\nHints: In case of input data being supplied to the question, it should be assumed to be a console input.We use itemgetter to enable multiple sort keys.\nSolutions:\nlist = [] while True: a = input().split(\u0026#34;,\u0026#34;) #注意因为上面拆分了数组，所以判断为空时记得指定元素位置，否则不会中断 if not a[0]: break else: #这里将每次输入的整体作为一个元组去存在list中了 list.append(tuple(a)) #sort()中的参数key可以指定排序的关键,且默认升序(小到大) #注意这里的写法 #The priority is that name \u0026gt; age \u0026gt; score. list.sort(key=lambda x:(x[0],x[1],x[2])) print(list) Tom,19,80 John,20,90 Jony,17,91 Jony,17,93 Json,21,85 [(\u0026#39;John\u0026#39;, \u0026#39;20\u0026#39;, \u0026#39;90\u0026#39;), (\u0026#39;Jony\u0026#39;, \u0026#39;17\u0026#39;, \u0026#39;91\u0026#39;), (\u0026#39;Jony\u0026#39;, \u0026#39;17\u0026#39;, \u0026#39;93\u0026#39;), (\u0026#39;Json\u0026#39;, \u0026#39;21\u0026#39;, \u0026#39;85\u0026#39;), (\u0026#39;Tom\u0026#39;, \u0026#39;19\u0026#39;, \u0026#39;80\u0026#39;)] Question 20 Question: Define a class with a generator which can iterate the numbers, which are divisible by 7, between a given range 0 and n.\nHints: Consider use class, function and comprehension.\nSolution: Python 3\nclass Q20: def __init__(self): pass def seven(self,n): #这里不用return，因为有了return就会中断循环；而yield不会，且能正常返回每个值 for i in range(1,n+1): if i%7 == 0: yield i Q20 = Q20() [print(i) for i in Q20.seven(40)] 7 14 21 28 35 [None, None, None, None, None] 0 练习coding : 叶宇浩Miles\n","date":"2021-06-06T00:00:00Z","permalink":"https://example.com/p/py_basic_q11-20/","title":"py_basic_Q11-20"},{"content":" ","date":"2021-05-25T00:00:00Z","permalink":"https://example.com/p/%E5%88%A9%E7%94%A8python%E8%BF%9B%E8%A1%8C%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/","title":"《利用Python进行数据分析》"},{"content":"这是正文内容！！ 随便放上一张照片看看 image 这是一个标题测试 这是另一个标题测试 这是标题测试 这是另一个图片测试 image ","date":"2021-05-13T00:00:00Z","permalink":"https://example.com/p/test/","title":"test"},{"content":" ","date":"2021-04-01T00:00:00Z","permalink":"https://example.com/p/%E7%9C%8B%E6%BC%AB%E7%94%BB%E5%AD%A6python/","title":"《看漫画学PYTHON》"},{"content":" 这是看完pythond的书后，再过一遍基础知识的post，原文地址：https://github.com/fengdu78/Data-Science-Notes/tree/master/1.python-basic 本章是python语言的基础部分，也是后续内容的基础。\nPython数据类型 1.1 字符串 在Python中用引号引起来的字符集称之为字符串，比如：\u0026lsquo;hello\u0026rsquo;、\u0026ldquo;my Python\u0026rdquo;、\u0026ldquo;2+3\u0026quot;等都是字符串\nPython中字符串中使用的引号可以是单引号、双引号跟三引号\nprint (\u0026#39;hello world!\u0026#39;) hello world! c = \u0026#39;It is a \u0026#34;dog\u0026#34;!\u0026#39; print (c) It is a \u0026#34;dog\u0026#34;! c1= \u0026#34;It\u0026#39;s a dog!\u0026#34; print (c1) It\u0026#39;s a dog! c2 = \u0026#34;\u0026#34;\u0026#34;hello world !\u0026#34;\u0026#34;\u0026#34; print (c2) hello world ! 转义字符\u0026rsquo;' 转义字符\\可以转义很多字符，比如\\n表示换行，\\t表示制表符，字符\\本身也要转义，所以\\表示的字符就是\\\nprint (\u0026#39;It\\\u0026#39;s a dog!\u0026#39;) print (\u0026#34;hello world!\\nhello Python!\u0026#34;) print (\u0026#39;\\\\\\t\\\\\u0026#39;) It\u0026#39;s a dog! hello world! hello Python! \\\t\\ 原样输出引号内字符串可以使用在引号前加r\nprint (r\u0026#39;\\\\\\t\\\\\u0026#39;) \\\\\\t\\\\ 子字符串及运算 s = \u0026#39;Python\u0026#39; print( \u0026#39;Py\u0026#39; in s) print( \u0026#39;py\u0026#39; in s) True False 取子字符串有两种方法，使用[]索引或者切片运算法[:]，这两个方法使用面非常广\nprint (s[2]) t print (s[1:4]) yth 字符串连接与格式化输出 word1 = \u0026#39;\u0026#34;hello\u0026#34;\u0026#39; word2 = \u0026#39;\u0026#34;world\u0026#34;\u0026#39; sentence = word1.strip(\u0026#39;\u0026#34;\u0026#39;) + \u0026#39; \u0026#39; + word2.strip(\u0026#39;\u0026#34;\u0026#39;) + \u0026#39;!\u0026#39; print( \u0026#39;The first word is %s, and the second word is %s\u0026#39; %(word1, word2)) print (sentence) The first word is \u0026#34;hello\u0026#34;, and the second word is \u0026#34;world\u0026#34; hello world! 1.2 整数与浮点数 整数 Python可以处理任意大小的整数，当然包括负整数，在程序中的表示方法和数学上的写法一模一样\ni = 7 print (i) 7 7 + 3 10 7 - 3 4 7 * 3 21 7 ** 3 343 7 / 3#Python3之后，整数除法和浮点数除法已经没有差异 2.3333333333333335 7 % 3 1 7//3 2 浮点数 7.0 / 3 2.3333333333333335 3.14 * 10 ** 2 314.0 其它表示方法\n0b1111 15 0xff 255 1.2e-5 1.2e-05 更多运算\nimport math print (math.log(math.e)) # 更多运算可查阅文档 1.0 1.3 布尔值 True True False False True and False False True or False True not True False True + False 1 18 \u0026gt;= 6 * 3 or \u0026#39;py\u0026#39; in \u0026#39;Python\u0026#39; True 18 \u0026gt;= 6 * 3 and \u0026#39;py\u0026#39; in \u0026#39;Python\u0026#39; False 18 \u0026gt;= 6 * 3 and \u0026#39;Py\u0026#39; in \u0026#39;Python\u0026#39; True 1.4 日期时间 import time now = time.strptime(\u0026#39;2016-07-20\u0026#39;, \u0026#39;%Y-%m-%d\u0026#39;) print (now) time.struct_time(tm_year=2016, tm_mon=7, tm_mday=20, tm_hour=0, tm_min=0, tm_sec=0, tm_wday=2, tm_yday=202, tm_isdst=-1) time.strftime(\u0026#39;%Y-%m-%d\u0026#39;, now) \u0026#39;2016-07-20\u0026#39; import datetime someDay = datetime.date(1999,2,10) anotherDay = datetime.date(1999,2,15) deltaDay = anotherDay - someDay deltaDay.days 5 还有其他一些datetime格式\n查看变量类型 type(None) NoneType type(1.0) float type(True) bool s=\u0026#34;NoneType\u0026#34; type(s) str 类型转换 str(10086) \u0026#39;10086\u0026#39; ?float() float(10086) 10086.0 int(\u0026#39;10086\u0026#39;) 10086 complex(10086) (10086+0j) 2 Python数据结构 列表（list）、元组（tuple）、集合（set）、字典（dict）\n2.1 列表(list) 用来存储一连串元素的容器，列表用[]来表示，其中元素的类型可不相同。\nmylist= [0, 1, 2, 3, 4, 5] print (mylist) [0, 1, 2, 3, 4, 5] 列表索引和切片\n# 索引从0开始，含左不含右 print (\u0026#39;[4]=\u0026#39;, mylist[4]) print (\u0026#39;[-4]=\u0026#39;, mylist[-4]) print (\u0026#39;[0:4]=\u0026#39;, mylist[0:4]) print (\u0026#39;[:4]=\u0026#39;, mylist[:4])#dddd print( \u0026#39;[4:]=\u0026#39;, mylist[4:]) print (\u0026#39;[0:4:2]=\u0026#39;, mylist[0:4:2]) print (\u0026#39;[-5:-1:]=\u0026#39;, mylist[-5:-1:]) print (\u0026#39;[-2::-1]=\u0026#39;, mylist[-2::-1]) [4]= 4 [-4]= 2 [0:4]= [0, 1, 2, 3] [:4]= [0, 1, 2, 3] [4:]= [4, 5] [0:4:2]= [0, 2] [-5:-1:]= [1, 2, 3, 4] [-2::-1]= [4, 3, 2, 1, 0] 修改列表\nmylist[3] = \u0026#34;小月\u0026#34; print (mylist[3]) mylist[5]=\u0026#34;小楠\u0026#34; print (mylist[5]) mylist[5]=19978 print (mylist[5]) 小月 小楠 19978 print (mylist) [0, 1, 2, \u0026#39;小月\u0026#39;, 4, 19978] 插入元素\nmylist.append(\u0026#39;han\u0026#39;) # 添加到尾部 mylist.extend([\u0026#39;long\u0026#39;, \u0026#39;wan\u0026#39;]) print (mylist) [0, 1, 2, \u0026#39;小月\u0026#39;, 4, 19978, \u0026#39;han\u0026#39;, \u0026#39;long\u0026#39;, \u0026#39;wan\u0026#39;] scores = [90, 80, 75, 66] mylist.insert(1, scores) # 添加到指定位置 mylist [0, [90, 80, 75, 66], 1, 2, \u0026#39;小月\u0026#39;, 4, 19978, \u0026#39;han\u0026#39;, \u0026#39;long\u0026#39;, \u0026#39;wan\u0026#39;] a=[] 删除元素\nprint (mylist.pop(1)) # 该函数返回被弹出的元素，不传入参数则删除最后一个元素 print (mylist) [90, 80, 75, 66] [0, 1, 2, \u0026#39;小月\u0026#39;, 4, 19978, \u0026#39;han\u0026#39;, \u0026#39;long\u0026#39;, \u0026#39;wan\u0026#39;] 判断元素是否在列表中等\nprint( \u0026#39;wan\u0026#39; in mylist) print (\u0026#39;han\u0026#39; not in mylist) True False mylist.count(\u0026#39;wan\u0026#39;) 1 mylist.index(\u0026#39;wan\u0026#39;) 8 range函数生成整数列表\nprint (range(10)) print (range(-5, 5)) print (range(-10, 10, 2)) print (range(16, 10, -1)) range(0, 10) range(-5, 5) range(-10, 10, 2) range(16, 10, -1) 2.2 元组(tuple) 元组类似列表，元组里面的元素也是进行索引计算。列表里面的元素的值可以修改，而元组里面的元素的值不能修改，只能读取。元组的符号是()。\nstudentsTuple = (\u0026#34;ming\u0026#34;, \u0026#34;jun\u0026#34;, \u0026#34;qiang\u0026#34;, \u0026#34;wu\u0026#34;, scores) studentsTuple (\u0026#39;ming\u0026#39;, \u0026#39;jun\u0026#39;, \u0026#39;qiang\u0026#39;, \u0026#39;wu\u0026#39;, [90, 80, 75, 66]) try: studentsTuple[1] = \u0026#39;fu\u0026#39; except TypeError: print (\u0026#39;TypeError\u0026#39;) TypeError scores[1]= 100 studentsTuple (\u0026#39;ming\u0026#39;, \u0026#39;jun\u0026#39;, \u0026#39;qiang\u0026#39;, \u0026#39;wu\u0026#39;, [90, 100, 75, 66]) \u0026#39;ming\u0026#39; in studentsTuple True studentsTuple[0:4] (\u0026#39;ming\u0026#39;, \u0026#39;jun\u0026#39;, \u0026#39;qiang\u0026#39;, \u0026#39;wu\u0026#39;) studentsTuple.count(\u0026#39;ming\u0026#39;) 1 studentsTuple.index(\u0026#39;jun\u0026#39;) 1 len(studentsTuple) 5 2.3 集合(set) Python中集合主要有两个功能，一个功能是进行集合操作，另一个功能是消除重复元素。 集合的格式是：set()，其中()内可以是列表、字典或字符串，因为字符串是以列表的形式存储的\nstudentsSet = set(mylist) print (studentsSet) {0, 1, 2, \u0026#39;han\u0026#39;, 4, \u0026#39;long\u0026#39;, \u0026#39;wan\u0026#39;, 19978, \u0026#39;小月\u0026#39;} studentsSet.add(\u0026#39;xu\u0026#39;) print (studentsSet) {0, 1, 2, \u0026#39;han\u0026#39;, 4, \u0026#39;long\u0026#39;, \u0026#39;xu\u0026#39;, \u0026#39;wan\u0026#39;, 19978, \u0026#39;小月\u0026#39;} studentsSet.remove(\u0026#39;xu\u0026#39;) print (studentsSet) {0, 1, 2, \u0026#39;han\u0026#39;, 4, \u0026#39;long\u0026#39;, \u0026#39;wan\u0026#39;, 19978, \u0026#39;小月\u0026#39;} a = set(\u0026#34;abcnmaaaaggsng\u0026#34;) print (\u0026#39;a=\u0026#39;, a) a= {\u0026#39;n\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;s\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;a\u0026#39;} b = set(\u0026#34;cdfm\u0026#34;) print (\u0026#39;b=\u0026#39;, b) b= {\u0026#39;f\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;c\u0026#39;} #交集 x = a \u0026amp; b print( \u0026#39;x=\u0026#39;, x) x= {\u0026#39;c\u0026#39;, \u0026#39;m\u0026#39;} #并集 y = a | b print (\u0026#39;y=\u0026#39;, y) #差集 z = a - b print( \u0026#39;z=\u0026#39;, z) #去除重复元素 new = set(a) print( z) y= {\u0026#39;g\u0026#39;, \u0026#39;n\u0026#39;, \u0026#39;f\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;m\u0026#39;, \u0026#39;s\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;d\u0026#39;} z= {\u0026#39;g\u0026#39;, \u0026#39;n\u0026#39;, \u0026#39;s\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;a\u0026#39;} {\u0026#39;g\u0026#39;, \u0026#39;n\u0026#39;, \u0026#39;s\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;a\u0026#39;} 2.4字典(dict) Python中的字典dict也叫做关联数组，用大括号{}括起来，在其他语言中也称为map，使用键-值（key-value）存储，具有极快的查找速度，其中key不能重复。\nk = {\u0026#34;name\u0026#34;:\u0026#34;weiwei\u0026#34;, \u0026#34;home\u0026#34;:\u0026#34;guilin\u0026#34;} print (k[\u0026#34;home\u0026#34;]) guilin print( k.keys()) print( k.values()) dict_keys([\u0026#39;name\u0026#39;, \u0026#39;home\u0026#39;]) dict_values([\u0026#39;weiwei\u0026#39;, \u0026#39;guilin\u0026#39;]) 添加、修改字典里面的项目\nk[\u0026#34;like\u0026#34;] = \u0026#34;music\u0026#34; k[\u0026#39;name\u0026#39;] = \u0026#39;guangzhou\u0026#39; print (k) {\u0026#39;name\u0026#39;: \u0026#39;guangzhou\u0026#39;, \u0026#39;home\u0026#39;: \u0026#39;guilin\u0026#39;, \u0026#39;like\u0026#39;: \u0026#39;music\u0026#39;} k.get(\u0026#39;edu\u0026#39;, -1) # 通过dict提供的get方法，如果key不存在，可以返回None，或者自己指定的value -1 删除key-value元素\nk.pop(\u0026#39;like\u0026#39;) print (k) {\u0026#39;name\u0026#39;: \u0026#39;guangzhou\u0026#39;, \u0026#39;home\u0026#39;: \u0026#39;guilin\u0026#39;} 2.5 列表、元组、集合、字典的互相转换 type(mylist) list tuple(mylist) (0, 1, 2, \u0026#39;小月\u0026#39;, 4, 19978, \u0026#39;han\u0026#39;, \u0026#39;long\u0026#39;, \u0026#39;wan\u0026#39;) list(k) [\u0026#39;name\u0026#39;, \u0026#39;home\u0026#39;] zl = zip((\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;, \u0026#39;C\u0026#39;), [1, 2, 3, 4]) # zip可以将列表、元组、集合、字典‘缝合’起来 print (zl) print (dict(zl)) \u0026lt;zip object at 0x00000135BEAF2A88\u0026gt; {\u0026#39;A\u0026#39;: 1, \u0026#39;B\u0026#39;: 2, \u0026#39;C\u0026#39;: 3} 3 Python控制流 在Python中通常的情况下程序的执行是从上往下执行的，而某些时候我们为了改变程序的执行顺序，使用控制流语句控制程序执行方式。Python中有三种控制流类型：顺序结构、分支结构、循环结构。\n另外，Python可以使用分号\u0026rdquo;;\u0026ldquo;分隔语句，但一般是使用换行来分隔；语句块不用大括号\u0026rdquo;{}\u0026quot;，而使用缩进（可以使用四个空格）来表示\n3.1 顺序结构 s = \u0026#39;7\u0026#39; num = int(s) # 一般不使用这种分隔方式 num -= 1 # num = num - 1 num *= 6 # num = num * 6 print (num) 36 3.2 分支结构：Python中if语句是用来判断选择执行哪个语句块的 if \u0026lt;True or Flase表达式\u0026gt;:\n执行语句块\nelif \u0026lt;True or Flase表达式\u0026gt;：\n执行语句块\nelse： # 都不满足\n执行语句块\nelif子句可以有多条，elif和else部分可省略 salary = 1000 if salary \u0026gt; 10000: print (\u0026#34;Wow!!!!!!!\u0026#34;) elif salary \u0026gt; 5000: print (\u0026#34;That\u0026#39;s OK.\u0026#34;) elif salary \u0026gt; 3000: print (\u0026#34;5555555555\u0026#34;) else: print (\u0026#34;..........\u0026#34;) .......... 3.3 循环结构 while 循环\nwhile \u0026lt;True or Flase表达式\u0026gt;:\n循环执行语句块\nelse： # 不满足条件\n执行语句块\n#else部分可以省略\na = 1 while a \u0026lt; 10: if a \u0026lt;= 5: print (a) else: print (\u0026#34;Hello\u0026#34;) a = a + 1 else: print (\u0026#34;Done\u0026#34;) 1 2 3 4 5 Hello Hello Hello Hello Done for 循环 for (条件变量) in (集合)：\n执行语句块\n“集合”并不单指set，而是“形似”集合的列表、元组、字典、数组都可以进行循环 条件变量可以有多个 heights = {\u0026#39;Yao\u0026#39;:226, \u0026#39;Sharq\u0026#39;:216, \u0026#39;AI\u0026#39;:183} for i in heights: print (i, heights[i]) Yao 226 Sharq 216 AI 183 for key, value in heights.items(): print(key, value) Yao 226 Sharq 216 AI 183 total = 0 for i in range(1, 101): total += i#total=total+i print (total) 5050 3.4 break、continue和pass break:跳出循环\ncontinue:跳出当前循环,继续下一次循环\npass:占位符，什么也不做\nfor i in range(1, 5): if i == 3: break print (i) 1 2 for i in range(1, 5): if i == 3: continue print (i) 1 2 4 for i in range(1, 5): if i == 3: pass print (i) 1 2 3 4 3.5 列表生成式 三种形式\n[\u0026lt;表达式\u0026gt; for (条件变量) in (集合)] [\u0026lt;表达式\u0026gt; for (条件变量) in (集合) if \u0026lt;\u0026lsquo;True or False\u0026rsquo;表达式\u0026gt;] [\u0026lt;表达式\u0026gt; if \u0026lt;\u0026lsquo;True or False\u0026rsquo;表达式\u0026gt; else \u0026lt;表达式\u0026gt; for (条件变量) in (集合) ] fruits = [\u0026#39;\u0026#34;Apple\u0026#39;, \u0026#39;Watermelon\u0026#39;, \u0026#39;\u0026#34;Banana\u0026#34;\u0026#39;] [x.strip(\u0026#39;\u0026#34;\u0026#39;) for x in fruits] [\u0026#39;Apple\u0026#39;, \u0026#39;Watermelon\u0026#39;, \u0026#39;Banana\u0026#39;] # 另一种写法 test_list=[] for x in fruits: x=x.strip(\u0026#39;\u0026#34;\u0026#39;) test_list.append(x) test_list [\u0026#39;Apple\u0026#39;, \u0026#39;Watermelon\u0026#39;, \u0026#39;Banana\u0026#39;] [x ** 2 for x in range(21) if x%2] [1, 9, 25, 49, 81, 121, 169, 225, 289, 361] # 另一种写法 test_list=[] for x in range(21): if x%2: x=x**2 test_list.append(x) test_list [1, 9, 25, 49, 81, 121, 169, 225, 289, 361] [m + n for m in \u0026#39;ABC\u0026#39; for n in \u0026#39;XYZ\u0026#39;] [\u0026#39;AX\u0026#39;, \u0026#39;AY\u0026#39;, \u0026#39;AZ\u0026#39;, \u0026#39;BX\u0026#39;, \u0026#39;BY\u0026#39;, \u0026#39;BZ\u0026#39;, \u0026#39;CX\u0026#39;, \u0026#39;CY\u0026#39;, \u0026#39;CZ\u0026#39;] # 另一种写法 test_list=[] for m in \u0026#39;ABC\u0026#39;: for n in \u0026#39;XYZ\u0026#39;: x=m+n test_list.append(x) test_list [\u0026#39;AX\u0026#39;, \u0026#39;AY\u0026#39;, \u0026#39;AZ\u0026#39;, \u0026#39;BX\u0026#39;, \u0026#39;BY\u0026#39;, \u0026#39;BZ\u0026#39;, \u0026#39;CX\u0026#39;, \u0026#39;CY\u0026#39;, \u0026#39;CZ\u0026#39;] d = {\u0026#39;x\u0026#39;: \u0026#39;A\u0026#39;, \u0026#39;y\u0026#39;: \u0026#39;B\u0026#39;, \u0026#39;z\u0026#39;: \u0026#39;C\u0026#39; } [k + \u0026#39;=\u0026#39; + v for k, v in d.items()] [\u0026#39;x=A\u0026#39;, \u0026#39;y=B\u0026#39;, \u0026#39;z=C\u0026#39;] # 另一种写法 test_list=[] for k, v in d.items(): x=k + \u0026#39;=\u0026#39; + v test_list.append(x) test_list [\u0026#39;x=A\u0026#39;, \u0026#39;y=B\u0026#39;, \u0026#39;z=C\u0026#39;] 4 Python函数 函数是用来封装特定功能的实体，可对不同类型和结构的数据进行操作，达到预定目标\n4.1 调用函数 Python内置了很多有用的函数，我们可以直接调用，进行数据分析时多数情况下是通过调用定义好的函数来操作数据的 str1 = \u0026#34;as\u0026#34; int1 = -9 print (len(str1)) print (abs(int1)) 2 9 fruits = [\u0026#39;Apple\u0026#39;, \u0026#39;Banana\u0026#39;, \u0026#39;Melon\u0026#39;] fruits.append(\u0026#39;Grape\u0026#39;) print (fruits) [\u0026#39;Apple\u0026#39;, \u0026#39;Banana\u0026#39;, \u0026#39;Melon\u0026#39;, \u0026#39;Grape\u0026#39;] 4.2 定义函数 当系统自带函数不足以完成指定的功能时，需要用户自定义函数来完成。\ndef 函数名()：\n函数内容\n函数内容\n\u0026lt;return 返回值\u0026gt;\ndef my_abs(x): if x \u0026gt;= 0: return x else: return -x my_abs(-9) 9 可以没有return\ndef filter_fruit(someList, d): for i in someList: if i == d: someList.remove(i) else: pass print (filter_fruit(fruits, \u0026#39;Melon\u0026#39;)) print (fruits) None [\u0026#39;Apple\u0026#39;, \u0026#39;Banana\u0026#39;, \u0026#39;Grape\u0026#39;] 多个返回值的情况\ndef test(i, j): k = i * j return i, j, k a , b , c = test(4, 5) print (a, b , c) type(test(4, 5)) 4 5 20 tuple 4.3 高阶函数 把另一个函数作为参数传入一个函数，这样的函数称为高阶函数 函数本身也可以赋值给变量，函数与其它对象具有同等地位\nmyFunction = abs myFunction(-9) 9 参数传入函数 def add(x, y, f): return f(x) + f(y) add(7, -5, myFunction) 12 常用高阶函数 map/reduce: map将传入的函数依次作用到序列的每个元素，并把结果作为新的list返回；reduce把一个函数作用在一个序列[x1, x2, x3\u0026hellip;]上，这个函数必须接收两个参数，reduce把结果继续和序列的下一个元素做累积计算\nmyList = [-1, 2, -3, 4, -5, 6, 7] map(abs, myList) \u0026lt;map at 0x135bea80438\u0026gt; from functools import reduce def powerAdd(a, b): return pow(a, 2) + pow(b, 2) reduce(powerAdd, myList) # 是否是计算平方和？ 3560020598205630145296938 filter： filter()把传入的函数依次作用于每个元素，然后根据返回值是True还是False决定保留还是丢弃该元素\ndef is_odd(x): return x % 3 # 0被判断为False，其它被判断为True filter(is_odd, myList) \u0026lt;filter at 0x135bea93b00\u0026gt; sorted: 实现对序列排序，默认情况下对于两个元素x和y，如果认为x \u0026lt; y，则返回-1，如果认为x == y，则返回0，如果认为x \u0026gt; y，则返回1\n默认排序：数字大小或字母序（针对字符串）\nsorted(myList) [-5, -3, -1, 2, 4, 6, 7] 返回函数: 高阶函数除了可以接受函数作为参数外，还可以把函数作为结果值返回 def powAdd(x, y): def power(n): return pow(x, n) + pow(y, n) return power myF = powAdd(3, 4) myF \u0026lt;function __main__.powAdd.\u0026lt;locals\u0026gt;.power\u0026gt; myF(2) 25 匿名函数：高阶函数传入函数时，不需要显式地定义函数，直接传入匿名函数更方便 f = lambda x: x * x f(4) 16 等同于：\ndef f(x): return x * x map(lambda x: x * x, myList) \u0026lt;map at 0x135beaaa0b8\u0026gt; 匿名函数可以传入多个参数\nreduce(lambda x, y: x + y, map(lambda x: x * x, myList)) 140 返回函数可以是匿名函数\ndef powAdd1(x, y): return lambda n: pow(x, n) + pow(y, n) lamb = powAdd1(3, 4) lamb(2) 25 其它 标识符第一个字符只能是字母或下划线，第一个字符不能出现数字或其他字符；标识符除第一个字符外，其他部分可以是字母或者下划线或者数字，标识符大小写敏感，比如name跟Name是不同的标识符。\nPython规范：\n类标识符每个字符第一个字母大写；\n对象\\变量标识符的第一个字母小写，其余首字母大写，或使用下划线\u0026rsquo;_\u0026rsquo; 连接；\n函数命名同普通对象。\n关键字\n关键字是指系统中自带的具备特定含义的标识符\n# 查看一下关键字有哪些，避免关键字做自定义标识符 import keyword print (keyword.kwlist) [\u0026#39;False\u0026#39;, \u0026#39;None\u0026#39;, \u0026#39;True\u0026#39;, \u0026#39;and\u0026#39;, \u0026#39;as\u0026#39;, \u0026#39;assert\u0026#39;, \u0026#39;break\u0026#39;, \u0026#39;class\u0026#39;, \u0026#39;continue\u0026#39;, \u0026#39;def\u0026#39;, \u0026#39;del\u0026#39;, \u0026#39;elif\u0026#39;, \u0026#39;else\u0026#39;, \u0026#39;except\u0026#39;, \u0026#39;finally\u0026#39;, \u0026#39;for\u0026#39;, \u0026#39;from\u0026#39;, \u0026#39;global\u0026#39;, \u0026#39;if\u0026#39;, \u0026#39;import\u0026#39;, \u0026#39;in\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;lambda\u0026#39;, \u0026#39;nonlocal\u0026#39;, \u0026#39;not\u0026#39;, \u0026#39;or\u0026#39;, \u0026#39;pass\u0026#39;, \u0026#39;raise\u0026#39;, \u0026#39;return\u0026#39;, \u0026#39;try\u0026#39;, \u0026#39;while\u0026#39;, \u0026#39;with\u0026#39;, \u0026#39;yield\u0026#39;] 注释 Python中的注释一般用#进行注释\n帮助 可以用？或者help()\n","date":"2021-03-05T00:00:00Z","permalink":"https://example.com/p/python%E5%9F%BA%E7%A1%80%E6%95%B4%E7%90%86/","title":"python基础整理"},{"content":" 这是看完pythond的书后，再过一遍基础知识的post，原文地址：https://github.com/fengdu78/Data-Science-Notes/tree/master/1.python-basic\nprint name = input(\u0026#34;What is your name?\u0026#34;) print(\u0026#34;Hello \u0026#34;+name ) # 或者print(\u0026#34;Hello\u0026#34;,name ),print中逗号分隔直接将字符串用空格分隔，若用+号连接，并且想留空格，则在前一字符串留空格即可 What is your name?Mike Hello Mike 输入输出 username=input(\u0026#34;username:\u0026#34;) password=input(\u0026#34;password:\u0026#34;) print(username,password) username:111 password:666 111 666 格式输入输出 # 第一种方法 name=input(\u0026#34;Name:\u0026#34;) age=input(\u0026#34;age:\u0026#34;) job=input(\u0026#34;job:\u0026#34;) info=\u0026#39;\u0026#39;\u0026#39;---------info of ---------\u0026#39;\u0026#39;\u0026#39; + \u0026#39;\u0026#39;\u0026#39; Name:\u0026#39;\u0026#39;\u0026#39;+name+\u0026#39;\u0026#39;\u0026#39; Age:\u0026#39;\u0026#39;\u0026#39;+age+\u0026#39;\u0026#39;\u0026#39; Job:\u0026#39;\u0026#39;\u0026#39;+job print(info) Name:Mike age:28 job:worker ---------info of --------- Name:Mike Age:28 Job:worker # 第二种方法 name=input(\u0026#34;Name:\u0026#34;) age=int(input(\u0026#34;age:\u0026#34;)) #如果不用int()就会报错(虽然输入为数字，但是print(type(age))为str型)，因为python如果不强制类型转化，就会默认字符型 job=input(\u0026#34;job:\u0026#34;) info=\u0026#39;\u0026#39;\u0026#39;---------info of --------- Name:%s Age:%d Job:%s\u0026#39;\u0026#39;\u0026#39;%(name,age,job) print(info) Name:Mike age:28 job:worker ---------info of --------- Name:Mike Age:28 Job:worker # 第三种方法 name=input(\u0026#34;Name:\u0026#34;) age=int(input(\u0026#34;age:\u0026#34;)) #如果不用int()就会报错(虽然输入为数字，但是print(type(age))为str型)，因为python如果不强制类型转化，就会默认字符型 job=input(\u0026#34;job:\u0026#34;) info=\u0026#39;\u0026#39;\u0026#39;---------info of --------- Name:{_name} Age:{_age} Job:{_job}\u0026#39;\u0026#39;\u0026#39;.format(_name=name,_age=age,_job=job) print(info) Name:Mike age:28 job:worker ---------info of --------- Name:Mike Age:28 Job:worker # 第四种方法 name=input(\u0026#34;Name:\u0026#34;) age=int(input(\u0026#34;age:\u0026#34;)) #如果不用int()就会报错(虽然输入为数字，但是print(type(age))为str型)，因为python如果不强制类型转化，就会默认字符型 job=input(\u0026#34;job:\u0026#34;) info=\u0026#39;\u0026#39;\u0026#39;---------info of --------- Name:{0} Age:{1} Job:{2}\u0026#39;\u0026#39;\u0026#39;.format(name,age,job) print(info) Name:Mike age:28 job:worker ---------info of --------- Name:Mike Age:28 Job:worker 输入密码不可见 import getpass pwd=getpass.getpass(\u0026#34;请输入密码:\u0026#34;) print(pwd) 请输入密码:········ 3434 验证，python缩进 _username=\u0026#39;Alex Li\u0026#39; _password=\u0026#39;abc123\u0026#39; username=input(\u0026#34;username:\u0026#34;) password=input(\u0026#34;password:\u0026#34;) if _username==username and _password==password: print((\u0026#34;Welcome user {name} login...\u0026#34;).format(name=username)) else: print(\u0026#34;Invalid username or password!\u0026#34;) username:df password:i\u0026#39; Invalid username or password! 指向\u0026mdash;修改字符串 print(\u0026#34;Hello World\u0026#34;) name = \u0026#34;Alex Li\u0026#34; name2=name print(name) print(\u0026#34;My name is\u0026#34;, name,name2) # Alex Li Alex Li name = \u0026#34;PaoChe Ge\u0026#34; # name2=name指的是name2与name一样指向Alex Li的内存地址，name指向改了，但是name2不变 print(\u0026#34;My name is\u0026#34;, name,name2) # PaoChe Ge Alex Li print(\u0026#34;您好，我来了\u0026#34;) Hello World Alex Li My name is Alex Li Alex Li My name is PaoChe Ge Alex Li 您好，我来了 注释''' '''内涵 # 第一种情况就是注释 \u0026#39;\u0026#39;\u0026#39;print(\u0026#34;这是一行注释\u0026#34;)\u0026#39;\u0026#39;\u0026#39; \u0026#39;print(\u0026#34;这是一行注释\u0026#34;)\u0026#39; #第二种情况就是打印多行字符串 str=\u0026#39;\u0026#39;\u0026#39;这是第一行内容 这是第二行内容\u0026#39;\u0026#39;\u0026#39; print(str) 这是第一行内容 这是第二行内容 # 3.单套双，双套单都可以 str1=\u0026#34;i\u0026#39;am a student\u0026#34; print(str1) i\u0026#39;am a student 模块初始sys与os import sys # 打印环境变量 print(sys.path) print(sys.argv) print(sys.argv[2]) [\u0026#39;\u0026#39;, \u0026#39;C:\\\\ProgramData\\\\Anaconda3\\\\python36.zip\u0026#39;, \u0026#39;C:\\\\ProgramData\\\\Anaconda3\\\\DLLs\u0026#39;, \u0026#39;C:\\\\ProgramData\\\\Anaconda3\\\\lib\u0026#39;, \u0026#39;C:\\\\ProgramData\\\\Anaconda3\u0026#39;, \u0026#39;C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\u0026#39;, \u0026#39;C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\u0026#39;, \u0026#39;C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\win32\\\\lib\u0026#39;, \u0026#39;C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\Pythonwin\u0026#39;, \u0026#39;C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\IPython\\\\extensions\u0026#39;, \u0026#39;C:\\\\Users\\\\haigu\\\\.ipython\u0026#39;] [\u0026#39;C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py\u0026#39;, \u0026#39;-f\u0026#39;, \u0026#39;C:\\\\Users\\\\haigu\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-28ddb76e-a36b-4edc-ab46-0b848ae5812f.json\u0026#39;] C:\\Users\\haigu\\AppData\\Roaming\\jupyter\\runtime\\kernel-28ddb76e-a36b-4edc-ab46-0b848ae5812f.json # 进度条 import time for i in range(50): sys.stdout.write(\u0026#39;#\u0026#39;) sys.stdout.flush() time.sleep(0.5) ################################################## import os cmd_res = os.system(\u0026#34;dir\u0026#34;) # os.system()执行后直接输出到终端，然后结束，最后cmd_res保存的是os.system()执行后的状态码 print(\u0026#34;---\u0026gt;\u0026#34;,cmd_res) # ---\u0026gt; 0 ---\u0026gt; 0 cmd_res1=os.popen(\u0026#34;dir\u0026#34;) print(\u0026#34;---\u0026gt;\u0026#34;,cmd_res1) # 得到的是内存对象值 ---\u0026gt; \u0026lt;os._wrap_close object at 0x00000000029187B8\u0026gt; ---\u0026gt; \u0026lt;os._wrap_close object at 0x000001B85DEF9940\u0026gt; cmd_res1=os.popen(\u0026#34;dir\u0026#34;).read() print(\u0026#34;---\u0026gt;\u0026#34;,cmd_res1) # 读取数据必须再后面加个read() ---\u0026gt; 驱动器 E 中的卷是 tools 卷的序列号是 E2B0-A3DA E:\\tools\\学术\\python\\两天入门python 的目录 2018/12/15 11:10 \u0026lt;DIR\u0026gt; . 2018/12/15 11:10 \u0026lt;DIR\u0026gt; .. 2018/12/15 10:58 \u0026lt;DIR\u0026gt; .ipynb_checkpoints 2018/12/15 11:10 9,565 两天入门python.ipynb 1 个文件 9,565 字节 3 个目录 24,186,036,224 可用字节 os.mkdir(\u0026#34;new_dir3\u0026#34;) # 创建一个目录 os.removedirs(\u0026#34;new_dir3\u0026#34;) # 删除一个目录 三元运算 # 1.result = 值1 if 条件 else 值2 a=1 b=2 c=3 d=a if a\u0026gt;b else c print(d) 3 python3特性 python3中最重要的新特性大概是对文本和二进制数据作了更为清晰的区分。文本总是Unicode，由str类型表示,\n二进制数据则由bytes类型表示。Python3不会以任意隐式的方式混用str和bytes，正是这使得两者区分特别清晰。\n即：在python2中类型会自动转化，而在python3中则要么报错，要么不转化\nstr与bytes相互转化\nbytes与str转化 msg=\u0026#34;我爱北京天安门\u0026#34; print(msg) 我爱北京天安门 print(msg.encode(encoding=\u0026#34;utf-8\u0026#34;)) # str转bytes,编码 b\u0026#39;\\xe6\\x88\\x91\\xe7\\x88\\xb1\\xe5\\x8c\\x97\\xe4\\xba\\xac\\xe5\\xa4\\xa9\\xe5\\xae\\x89\\xe9\\x97\\xa8\u0026#39; print(msg.encode(encoding=\u0026#34;utf-8\u0026#34;).decode(encoding=\u0026#34;utf-8\u0026#34;)) # bytes转str,解码 我爱北京天安门 循环 print(\u0026#34;第一种循环\u0026#34;) count = 0 while True: print(\u0026#34;count:\u0026#34;,count) count+=1 if(count==10): break 第一种循环 count: 0 count: 1 count: 2 count: 3 count: 4 count: 5 count: 6 count: 7 count: 8 count: 9 print(\u0026#34;第二种循环\u0026#34;) count = 0 for count in range(0,10,2): print(\u0026#34;count:\u0026#34;, count) for i in range(0,10): if i\u0026lt;5: print(\u0026#34;loop \u0026#34;,i) else: continue print(\u0026#34;hehe....\u0026#34;) my_age=28 count = 0 while count\u0026lt;3: user_input=int(input(\u0026#34;input your guess num:\u0026#34;)) if user_input==my_age: print(\u0026#34;Congratulations,you got it!\u0026#34;) break elif user_input\u0026lt;my_age: print(\u0026#34;Oops,think bigger!\u0026#34;) else: print(\u0026#34;think smaller!\u0026#34;) count+=1 print(\u0026#34;猜这么多次都不对，你个笨蛋.\u0026#34;) 第二种循环 count: 0 count: 2 count: 4 count: 6 count: 8 loop 0 hehe.... loop 1 hehe.... loop 2 hehe.... loop 3 hehe.... loop 4 hehe.... input your guess num:2 Oops,think bigger! 猜这么多次都不对，你个笨蛋. input your guess num:28 Congratulations,you got it! 练习\u0026mdash;三级菜单 data={ \u0026#39;北京\u0026#39;:{ \u0026#34;昌平\u0026#34;:{ \u0026#34;沙河\u0026#34;:[\u0026#34;oldboys\u0026#34;,\u0026#39;test\u0026#39;], \u0026#34;天通苑\u0026#34;:[\u0026#34;链家地产\u0026#34;,\u0026#34;我爱我家\u0026#34;] }, \u0026#34;朝阳\u0026#34;:{ \u0026#34;望京\u0026#34;:[\u0026#34;oldboys\u0026#34;,\u0026#39;默陌陌\u0026#39;], \u0026#34;国贸\u0026#34;:[\u0026#34;CICC\u0026#34;,\u0026#34;HP\u0026#34;], \u0026#34;东直门\u0026#34;:[\u0026#34;Advent\u0026#34;,\u0026#34;飞信\u0026#34;] }, \u0026#34;海淀\u0026#34;:{} }, \u0026#39;山东\u0026#39;:{ \u0026#34;德州\u0026#34;:{}, \u0026#34;青岛\u0026#34;:{}, \u0026#34;济南\u0026#34;:{} }, \u0026#39;广东\u0026#39;:{ \u0026#34;德州\u0026#34;:{}, \u0026#34;青岛\u0026#34;:{}, \u0026#34;济南\u0026#34;:{} }, } exit_flag = False while not exit_flag: for i in data: print(i) choice=input(\u0026#34;选择进入1\u0026gt;\u0026gt;:\u0026#34;) if choice in data: while not exit_flag: for i2 in data[choice]: print(\u0026#34;\\t\u0026#34;,i2) choice2=input(\u0026#34;选择进入2\u0026gt;\u0026gt;:\u0026#34;) if choice2 in data[choice]: while not exit_flag: for i3 in data[choice][choice2]: print(\u0026#34;\\t\\t\u0026#34;, i3) choice3 = input(\u0026#34;选择进入3\u0026gt;\u0026gt;:\u0026#34;) if choice3 in data[choice][choice2]: for i4 in data[choice][choice2][choice3]: print(i4) choice4=input(\u0026#34;最后一层，按b返回\u0026gt;\u0026gt;:\u0026#34;) if choice4==\u0026#39;b\u0026#39;: pass # pass可以理解为占位符，表示什么都不做，返回循环起始位置，以后可以在此处添加内容 elif choice4==\u0026#39;q\u0026#39;: exit_flag=True if (choice3 == \u0026#39;b\u0026#39;): break elif choice3 == \u0026#39;q\u0026#39;: exit_flag = True if (choice2 == \u0026#39;b\u0026#39;): break elif choice2 == \u0026#39;q\u0026#39;: exit_flag = True if (choice == \u0026#39;b\u0026#39;): break 北京 山东 广东 选择进入1\u0026gt;\u0026gt;:2 北京 山东 广东 选择进入1\u0026gt;\u0026gt;:3 北京 山东 广东 选择进入1\u0026gt;\u0026gt;:q 北京 山东 广东 选择进入1\u0026gt;\u0026gt;:2 北京 山东 广东 ","date":"2021-03-04T00:00:00Z","permalink":"https://example.com/p/python%E5%9F%BA%E7%A1%80_1/","title":"python基础_1"},{"content":" 这是看完pythond的书后，再过一遍基础知识的post，原文地址：https://github.com/fengdu78/Data-Science-Notes/tree/master/1.python-basic\n编码变换 # utf-8与gbk互相转化需要通过Unicode作为中介 s=\u0026#34;我爱北京天安门\u0026#34; # 默认编码为Unicode print(s.encode(\u0026#34;gbk\u0026#34;)) # Unicode可直接转化为gbk b\u0026#39;\\xce\\xd2\\xb0\\xae\\xb1\\xb1\\xbe\\xa9\\xcc\\xec\\xb0\\xb2\\xc3\\xc5\u0026#39; print(s.encode(\u0026#34;utf-8\u0026#34;)) # Unicode可直接转化为utf-8 b\u0026#39;\\xe6\\x88\\x91\\xe7\\x88\\xb1\\xe5\\x8c\\x97\\xe4\\xba\\xac\\xe5\\xa4\\xa9\\xe5\\xae\\x89\\xe9\\x97\\xa8\u0026#39; print(s.encode(\u0026#34;utf-8\u0026#34;).decode(\u0026#34;utf-8\u0026#34;).encode(\u0026#34;gb2312\u0026#34;)) # 此时s.encode(\u0026#34;utf-8\u0026#34;)即转为utf-8了，然后转为gb2312，则需要先告诉Unicode你原先的编码是什么，即s.encode(\u0026#34;utf-8\u0026#34;).decode(\u0026#34;utf-8\u0026#34;),再对其进行编码为gb2312，即最终为s.encode(\u0026#34;utf-8\u0026#34;).decode(\u0026#34;utf-8\u0026#34;).encode(\u0026#34;gb2312\u0026#34;) b\u0026#39;\\xce\\xd2\\xb0\\xae\\xb1\\xb1\\xbe\\xa9\\xcc\\xec\\xb0\\xb2\\xc3\\xc5\u0026#39; 文件 f=open(\u0026#39;ly.txt\u0026#39;,\u0026#39;r\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;) # 文件句柄 \u0026#39;w\u0026#39;为创建文件，之前的数据就没了 data=f.read() print(data) f.close() ��������������������我爱中华 f=open(\u0026#39;test\u0026#39;,\u0026#39;a\u0026#39;,encoding=\u0026#39;utf-8\u0026#39;) # 文件句柄 \u0026#39;a\u0026#39;为追加文件 append f.write(\u0026#34;\\n阿斯达所，\\n天安门上太阳升\u0026#34;) f.close() f = open(\u0026#39;ly.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) # 文件句柄 for i in range(5): print(f.readline().strip()) # strip()去掉空格和回车 for line in f.readlines(): print(line.strip()) # 到第十行不打印 for index, line in enumerate(f.readlines()): if index == 9: print(\u0026#39;----我是分隔符-----\u0026#39;) continue print(line.strip()) # 到第十行不打印 count = 0 for line in f: if count == 9: print(\u0026#39;----我是分隔符-----\u0026#39;) count += 1 continue print(line.strip()) count += 1 f = open(\u0026#39;ly.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) # 文件句柄 print(f.tell()) print(f.readline(5)) print(f.tell()) f.seek(0) print(f.readline(5)) print(f.encoding) print(f.buffer) print(f.fileno()) print(f.flush()) # 刷新缓冲区 # 进度条 import sys, time for i in range(50): sys.stdout.write(\u0026#39;#\u0026#39;) sys.stdout.flush() time.sleep(0.5) f = open(\u0026#39;ly.txt\u0026#39;, \u0026#39;a\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) # 文件句柄 f.seek(10) f.truncate(20) # 指定10到20个字符，10个字符前面留着，后面20字符清除 f = open(\u0026#39;ly.txt\u0026#39;, \u0026#39;r+\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) # 文件句柄 print(f.readline().strip()) print(f.readline().strip()) print(f.readline().strip()) f.write(\u0026#34;我爱中华\u0026#34;) f.close() # 实现简单的shell sed替换功能 f = open(\u0026#34;ly.txt\u0026#34;, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) f_new = open(\u0026#34;ly2.txt\u0026#34;, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) for line in f: if \u0026#34;肆意的快乐\u0026#34; in line: line = line.replace(\u0026#34;肆意的快乐\u0026#34;, \u0026#34;肆意的happy\u0026#34;) f_new.write(line) f.close() f_new.close() import sys f = open(\u0026#34;ly.txt\u0026#34;, \u0026#34;r\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) f_new = open(\u0026#34;ly2.txt\u0026#34;, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) find_str = sys.argv[1] replace_str = sys.argv[2] for line in f: if find_str in line: line = line.replace(find_str, replace_str) f_new.write(line) f.close() f_new.close() # with语句---为了避免打开文件后忘记关闭，可以通过管理上下文 with open(\u0026#39;ly.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: for line in f: print(line.strip()) # python2.7后，with又支持同时对多个文件的上下文进行管理，即: with open(\u0026#39;ly.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f1, open(\u0026#39;ly2.txt\u0026#39;, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;): pass ��������������������我爱中华 0 ����� 5 ����� utf-8 \u0026lt;_io.BufferedReader name=\u0026#39;ly.txt\u0026#39;\u0026gt; 5 None ##################################################�������������������� ��������������������我爱中华 全局变量 names = [\u0026#34;Alex\u0026#34;, \u0026#34;Jack\u0026#34;, \u0026#34;Rain\u0026#34;] # 除了整数和字符串在函数内不能改，列表，字典这些可以改 def change_name(): names[0] = \u0026#34;金角大王\u0026#34; print(\u0026#34;inside func\u0026#34;, names) change_name() print(names) # 当全局变量与局部变量同名时，在定义局部变量的子程序内，局部变量起作用，在其它地方全局变量起作用。 inside func [\u0026#39;金角大王\u0026#39;, \u0026#39;Jack\u0026#39;, \u0026#39;Rain\u0026#39;] [\u0026#39;金角大王\u0026#39;, \u0026#39;Jack\u0026#39;, \u0026#39;Rain\u0026#39;] list操作 __author__=\u0026#34;Alex Li\u0026#34; names=\u0026#34;zhang Gu Xiang Xu\u0026#34; names=[\u0026#34;zhang\u0026#34;,\u0026#34;Gu\u0026#34;,\u0026#34;Xiang\u0026#34;,\u0026#34;Xu\u0026#34;] # 1.切片 print(names[0],names[1],names[2]) zhang Gu Xiang print(names[1:3]) # 顾头不顾尾，切片 [\u0026#39;Gu\u0026#39;, \u0026#39;Xiang\u0026#39;] print(names[-1]) # 在不知道多长情况下，取最后一个位置 Xu print(names[-1:-3]) # 切片是从左往右，此时不输出 [] print(names[-3:-1]) # 顾头顾尾，取最后三个 [\u0026#39;Gu\u0026#39;, \u0026#39;Xiang\u0026#39;] print(names[-2:]) # 取最后两个 [\u0026#39;Xiang\u0026#39;, \u0026#39;Xu\u0026#39;] print(names[0:3]) # 切片 等价于 print(names[:3]) [\u0026#39;zhang\u0026#39;, \u0026#39;Gu\u0026#39;, \u0026#39;Xiang\u0026#39;] # 2.追加 names.append(\u0026#34;Lei\u0026#34;) print(names) [\u0026#39;zhang\u0026#39;, \u0026#39;Gu\u0026#39;, \u0026#39;Xiang\u0026#39;, \u0026#39;Xu\u0026#39;, \u0026#39;Lei\u0026#39;] # 3.指定位置插入 names.insert(1,\u0026#34;Chen\u0026#34;) # Gu前面插入 print(names) [\u0026#39;zhang\u0026#39;, \u0026#39;Chen\u0026#39;, \u0026#39;Gu\u0026#39;, \u0026#39;Xiang\u0026#39;, \u0026#39;Xu\u0026#39;, \u0026#39;Lei\u0026#39;] # 4.修改 names[2]=\u0026#34;Xie\u0026#34; print(names) [\u0026#39;zhang\u0026#39;, \u0026#39;Chen\u0026#39;, \u0026#39;Xie\u0026#39;, \u0026#39;Xiang\u0026#39;, \u0026#39;Xu\u0026#39;, \u0026#39;Lei\u0026#39;] # 5.删除 # 第一种删除方法 names.remove(\u0026#34;Chen\u0026#34;) print(names) [\u0026#39;zhang\u0026#39;, \u0026#39;Xie\u0026#39;, \u0026#39;Xiang\u0026#39;, \u0026#39;Xu\u0026#39;, \u0026#39;Lei\u0026#39;] # 第二种删除方法 del names[1] print(names) [\u0026#39;zhang\u0026#39;, \u0026#39;Xiang\u0026#39;, \u0026#39;Xu\u0026#39;, \u0026#39;Lei\u0026#39;] # 第三种删除方法 names.pop() # 默认删除最后一个 print(names) [\u0026#39;zhang\u0026#39;, \u0026#39;Xiang\u0026#39;, \u0026#39;Xu\u0026#39;] names.pop(1) #删除第二个元素 print(names) [\u0026#39;zhang\u0026#39;, \u0026#39;Xu\u0026#39;] print(names.index(\u0026#34;Xu\u0026#34;)) # 1 1 print(names[names.index(\u0026#34;Xu\u0026#34;)]) #打印出找出的元素值3 Xu # 6.统计 names.append(\u0026#34;zhang\u0026#34;) #再加一个用于学习统计\u0026#34;zhang\u0026#34;的个数 print(names.count(\u0026#34;zhang\u0026#34;)) 2 # 7.排序 names.sort() #按照ASCII码排序 print(names) [\u0026#39;Xu\u0026#39;, \u0026#39;zhang\u0026#39;, \u0026#39;zhang\u0026#39;] names.reverse() # 逆序 print(names) [\u0026#39;zhang\u0026#39;, \u0026#39;zhang\u0026#39;, \u0026#39;Xu\u0026#39;] # 8.合并 names2=[1,2,3,4] names.extend(names2) print(names,names2) [\u0026#39;zhang\u0026#39;, \u0026#39;zhang\u0026#39;, \u0026#39;Xu\u0026#39;, 1, 2, 3, 4] [1, 2, 3, 4] # 9.删掉names2 \u0026#39;\u0026#39;\u0026#39;del names2\u0026#39;\u0026#39;\u0026#39; print(names2) # NameError: name \u0026#39;names2\u0026#39; is not defined,表示已删除 [1, 2, 3, 4] # 10.浅copy names2=names.copy() print(names,names2) # 此时names2与names指向相同 [\u0026#39;zhang\u0026#39;, \u0026#39;zhang\u0026#39;, \u0026#39;Xu\u0026#39;, 1, 2, 3, 4] [\u0026#39;zhang\u0026#39;, \u0026#39;zhang\u0026#39;, \u0026#39;Xu\u0026#39;, 1, 2, 3, 4] names[2]=\u0026#34;大张\u0026#34; print(names,names2) # 此时names改变，names2不变 [\u0026#39;zhang\u0026#39;, \u0026#39;zhang\u0026#39;, \u0026#39;大张\u0026#39;, 1, 2, 3, 4] [\u0026#39;zhang\u0026#39;, \u0026#39;zhang\u0026#39;, \u0026#39;Xu\u0026#39;, 1, 2, 3, 4] # 11.浅copy在列表嵌套应用 names=[1,2,3,4,[\u0026#34;zhang\u0026#34;,\u0026#34;Gu\u0026#34;],5] print(names) [1, 2, 3, 4, [\u0026#39;zhang\u0026#39;, \u0026#39;Gu\u0026#39;], 5] names2=names.copy() names[3]=\u0026#34;斯\u0026#34; names[4][0]=\u0026#34;张改\u0026#34; print(names,names2) # copy为浅copy,第一层copy不变，后面的嵌套全部都变,修改names2与names都一样 [1, 2, 3, \u0026#39;斯\u0026#39;, [\u0026#39;张改\u0026#39;, \u0026#39;Gu\u0026#39;], 5] [1, 2, 3, 4, [\u0026#39;张改\u0026#39;, \u0026#39;Gu\u0026#39;], 5] # 12.完整克隆 import copy # 浅copy与深copy \u0026#39;\u0026#39;\u0026#39;浅copy与深copy区别就是浅copy只copy一层，而深copy就是完全克隆\u0026#39;\u0026#39;\u0026#39; names = [1, 2, 3, 4, [\u0026#34;zhang\u0026#34;, \u0026#34;Gu\u0026#34;], 5] # names2=copy.copy(names) # 这个跟列表的浅copy一样 names2 = copy.deepcopy(names) #深copy names[3] = \u0026#34;斯\u0026#34; names[4][0] = \u0026#34;张改\u0026#34; print(names, names2) [1, 2, 3, \u0026#39;斯\u0026#39;, [\u0026#39;张改\u0026#39;, \u0026#39;Gu\u0026#39;], 5] [1, 2, 3, 4, [\u0026#39;zhang\u0026#39;, \u0026#39;Gu\u0026#39;], 5] # 13.列表循环 for i in names: print(i) print(names[0:-1:2]) # 步长为2进行切片 1 2 3 斯 [\u0026#39;张改\u0026#39;, \u0026#39;Gu\u0026#39;] 5 [1, 3, [\u0026#39;张改\u0026#39;, \u0026#39;Gu\u0026#39;]] Tuple操作 names=(\u0026#39;alex\u0026#39;,\u0026#39;jack\u0026#39;,\u0026#39;alex\u0026#39;) print(names.count(\u0026#39;alex\u0026#39;)) print(names.index(\u0026#39;jack\u0026#39;)) 2 1 # 购物篮程序 product_list = [ (\u0026#39;Iphone\u0026#39;, 5800), (\u0026#39;Mac Pro\u0026#39;, 9800), (\u0026#39;Bike\u0026#39;, 5800), (\u0026#39;Watch\u0026#39;, 10600), (\u0026#39;Coffee\u0026#39;, 31), (\u0026#39;Alex Python\u0026#39;, 120), ] shopping_list = [] salary = input(\u0026#34;Input your salary:\u0026#34;) if salary.isdigit(): salary = int(salary) while True: \u0026#39;\u0026#39;\u0026#39;for item in product_list: print(product_list.index(item),item) \u0026#39;\u0026#39;\u0026#39; for index, item in enumerate(product_list): print(index, item) user_choice = input(\u0026#34;选择要买嘛？\u0026gt;\u0026gt;:\u0026#34;) if user_choice.isdigit(): user_choice = int(user_choice) if user_choice \u0026lt; len(product_list) and user_choice \u0026gt;= 0: p_item = product_list[user_choice] if p_item[1] \u0026lt;= salary: shopping_list.append(p_item) salary -= p_item[1] print( \u0026#34;Added %s into shopping cart, your current balance is \\033[31;1m%s\\033[0m\u0026#34; % (p_item, salary)) else: print(\u0026#34;\\033[41;1m你的余额只剩[%s]啦，还买个毛线\\033[0m\u0026#34; % salary) else: print(\u0026#34;product code[%s] is not exist!\u0026#34; % user_choice) elif user_choice == \u0026#39;q\u0026#39;: print(\u0026#39;-----------shopping list----------------\u0026#39;) for p in shopping_list: print(p) print(\u0026#34;Your current balance:\u0026#34;, salary) exit() else: print(\u0026#34;invalid option\u0026#34;) Input your salary:8000 0 (\u0026#39;Iphone\u0026#39;, 5800) 1 (\u0026#39;Mac Pro\u0026#39;, 9800) 2 (\u0026#39;Bike\u0026#39;, 5800) 3 (\u0026#39;Watch\u0026#39;, 10600) 4 (\u0026#39;Coffee\u0026#39;, 31) 5 (\u0026#39;Alex Python\u0026#39;, 120) 选择要买嘛？\u0026gt;\u0026gt;: invalid option 0 (\u0026#39;Iphone\u0026#39;, 5800) 1 (\u0026#39;Mac Pro\u0026#39;, 9800) 2 (\u0026#39;Bike\u0026#39;, 5800) 3 (\u0026#39;Watch\u0026#39;, 10600) 4 (\u0026#39;Coffee\u0026#39;, 31) 5 (\u0026#39;Alex Python\u0026#39;, 120) 选择要买嘛？\u0026gt;\u0026gt;:1 \u001b[41;1m你的余额只剩[8000]啦，还买个毛线\u001b[0m 0 (\u0026#39;Iphone\u0026#39;, 5800) 1 (\u0026#39;Mac Pro\u0026#39;, 9800) 2 (\u0026#39;Bike\u0026#39;, 5800) 3 (\u0026#39;Watch\u0026#39;, 10600) 4 (\u0026#39;Coffee\u0026#39;, 31) 5 (\u0026#39;Alex Python\u0026#39;, 120) 选择要买嘛？\u0026gt;\u0026gt;:0 Added (\u0026#39;Iphone\u0026#39;, 5800) into shopping cart, your current balance is \u001b[31;1m2200\u001b[0m 0 (\u0026#39;Iphone\u0026#39;, 5800) 1 (\u0026#39;Mac Pro\u0026#39;, 9800) 2 (\u0026#39;Bike\u0026#39;, 5800) 3 (\u0026#39;Watch\u0026#39;, 10600) 4 (\u0026#39;Coffee\u0026#39;, 31) 5 (\u0026#39;Alex Python\u0026#39;, 120) 选择要买嘛？\u0026gt;\u0026gt;:6 product code[6] is not exist! 0 (\u0026#39;Iphone\u0026#39;, 5800) 1 (\u0026#39;Mac Pro\u0026#39;, 9800) 2 (\u0026#39;Bike\u0026#39;, 5800) 3 (\u0026#39;Watch\u0026#39;, 10600) 4 (\u0026#39;Coffee\u0026#39;, 31) 5 (\u0026#39;Alex Python\u0026#39;, 120) 选择要买嘛？\u0026gt;\u0026gt;:q -----------shopping list---------------- (\u0026#39;Iphone\u0026#39;, 5800) Your current balance: 2200 0 (\u0026#39;Iphone\u0026#39;, 5800) 1 (\u0026#39;Mac Pro\u0026#39;, 9800) 2 (\u0026#39;Bike\u0026#39;, 5800) 3 (\u0026#39;Watch\u0026#39;, 10600) 4 (\u0026#39;Coffee\u0026#39;, 31) 5 (\u0026#39;Alex Python\u0026#39;, 120) 选择要买嘛？\u0026gt;\u0026gt;:p invalid option 0 (\u0026#39;Iphone\u0026#39;, 5800) 1 (\u0026#39;Mac Pro\u0026#39;, 9800) 2 (\u0026#39;Bike\u0026#39;, 5800) 3 (\u0026#39;Watch\u0026#39;, 10600) 4 (\u0026#39;Coffee\u0026#39;, 31) 5 (\u0026#39;Alex Python\u0026#39;, 120) 选择要买嘛？\u0026gt;\u0026gt;:no invalid option 0 (\u0026#39;Iphone\u0026#39;, 5800) 1 (\u0026#39;Mac Pro\u0026#39;, 9800) 2 (\u0026#39;Bike\u0026#39;, 5800) 3 (\u0026#39;Watch\u0026#39;, 10600) 4 (\u0026#39;Coffee\u0026#39;, 31) 5 (\u0026#39;Alex Python\u0026#39;, 120) 选择要买嘛？\u0026gt;\u0026gt;:4 Added (\u0026#39;Coffee\u0026#39;, 31) into shopping cart, your current balance is \u001b[31;1m2169\u001b[0m 0 (\u0026#39;Iphone\u0026#39;, 5800) 1 (\u0026#39;Mac Pro\u0026#39;, 9800) 2 (\u0026#39;Bike\u0026#39;, 5800) 3 (\u0026#39;Watch\u0026#39;, 10600) 4 (\u0026#39;Coffee\u0026#39;, 31) 5 (\u0026#39;Alex Python\u0026#39;, 120) 选择要买嘛？\u0026gt;\u0026gt;:4 Added (\u0026#39;Coffee\u0026#39;, 31) into shopping cart, your current balance is \u001b[31;1m2138\u001b[0m 0 (\u0026#39;Iphone\u0026#39;, 5800) 1 (\u0026#39;Mac Pro\u0026#39;, 9800) 2 (\u0026#39;Bike\u0026#39;, 5800) 3 (\u0026#39;Watch\u0026#39;, 10600) 4 (\u0026#39;Coffee\u0026#39;, 31) 5 (\u0026#39;Alex Python\u0026#39;, 120) 选择要买嘛？\u0026gt;\u0026gt;:4 Added (\u0026#39;Coffee\u0026#39;, 31) into shopping cart, your current balance is \u001b[31;1m2107\u001b[0m 0 (\u0026#39;Iphone\u0026#39;, 5800) 1 (\u0026#39;Mac Pro\u0026#39;, 9800) 2 (\u0026#39;Bike\u0026#39;, 5800) 3 (\u0026#39;Watch\u0026#39;, 10600) 4 (\u0026#39;Coffee\u0026#39;, 31) 5 (\u0026#39;Alex Python\u0026#39;, 120) 选择要买嘛？\u0026gt;\u0026gt;:43 product code[43] is not exist! 0 (\u0026#39;Iphone\u0026#39;, 5800) 1 (\u0026#39;Mac Pro\u0026#39;, 9800) 2 (\u0026#39;Bike\u0026#39;, 5800) 3 (\u0026#39;Watch\u0026#39;, 10600) 4 (\u0026#39;Coffee\u0026#39;, 31) 5 (\u0026#39;Alex Python\u0026#39;, 120) 选择要买嘛？\u0026gt;\u0026gt;:2 \u001b[41;1m你的余额只剩[2107]啦，还买个毛线\u001b[0m 0 (\u0026#39;Iphone\u0026#39;, 5800) 1 (\u0026#39;Mac Pro\u0026#39;, 9800) 2 (\u0026#39;Bike\u0026#39;, 5800) 3 (\u0026#39;Watch\u0026#39;, 10600) 4 (\u0026#39;Coffee\u0026#39;, 31) 5 (\u0026#39;Alex Python\u0026#39;, 120) 选择要买嘛？\u0026gt;\u0026gt;:1 \u001b[41;1m你的余额只剩[2107]啦，还买个毛线\u001b[0m 0 (\u0026#39;Iphone\u0026#39;, 5800) 1 (\u0026#39;Mac Pro\u0026#39;, 9800) 2 (\u0026#39;Bike\u0026#39;, 5800) 3 (\u0026#39;Watch\u0026#39;, 10600) 4 (\u0026#39;Coffee\u0026#39;, 31) 5 (\u0026#39;Alex Python\u0026#39;, 120) 选择要买嘛？\u0026gt;\u0026gt;:2 \u001b[41;1m你的余额只剩[2107]啦，还买个毛线\u001b[0m 0 (\u0026#39;Iphone\u0026#39;, 5800) 1 (\u0026#39;Mac Pro\u0026#39;, 9800) 2 (\u0026#39;Bike\u0026#39;, 5800) 3 (\u0026#39;Watch\u0026#39;, 10600) 4 (\u0026#39;Coffee\u0026#39;, 31) 5 (\u0026#39;Alex Python\u0026#39;, 120) 选择要买嘛？\u0026gt;\u0026gt;:3 \u001b[41;1m你的余额只剩[2107]啦，还买个毛线\u001b[0m 0 (\u0026#39;Iphone\u0026#39;, 5800) 1 (\u0026#39;Mac Pro\u0026#39;, 9800) 2 (\u0026#39;Bike\u0026#39;, 5800) 3 (\u0026#39;Watch\u0026#39;, 10600) 4 (\u0026#39;Coffee\u0026#39;, 31) 5 (\u0026#39;Alex Python\u0026#39;, 120) 选择要买嘛？\u0026gt;\u0026gt;:4 Added (\u0026#39;Coffee\u0026#39;, 31) into shopping cart, your current balance is \u001b[31;1m2076\u001b[0m 0 (\u0026#39;Iphone\u0026#39;, 5800) 1 (\u0026#39;Mac Pro\u0026#39;, 9800) 2 (\u0026#39;Bike\u0026#39;, 5800) 3 (\u0026#39;Watch\u0026#39;, 10600) 4 (\u0026#39;Coffee\u0026#39;, 31) 5 (\u0026#39;Alex Python\u0026#39;, 120) 选择要买嘛？\u0026gt;\u0026gt;:5 Added (\u0026#39;Alex Python\u0026#39;, 120) into shopping cart, your current balance is \u001b[31;1m1956\u001b[0m 0 (\u0026#39;Iphone\u0026#39;, 5800) 1 (\u0026#39;Mac Pro\u0026#39;, 9800) 2 (\u0026#39;Bike\u0026#39;, 5800) 3 (\u0026#39;Watch\u0026#39;, 10600) 4 (\u0026#39;Coffee\u0026#39;, 31) 5 (\u0026#39;Alex Python\u0026#39;, 120) Set操作 # 集合set 集合关系测试 list_1=[1,4,5,7,3,6,7,9] list_1=set(list_1) print(list_1,type(list_1)) {1, 3, 4, 5, 6, 7, 9} \u0026lt;class \u0026#39;set\u0026#39;\u0026gt; list_2=set([2,6,0,6,22,8,4]) print(list_2,type(list_2)) {0, 2, 4, 6, 8, 22} \u0026lt;class \u0026#39;set\u0026#39;\u0026gt; print(\u0026#34;--------------------------------\u0026#34;) # 取交集 print(\u0026#34;方法一\u0026#34;) print(list_1.intersection(list_2)) -------------------------------- 方法一 {4, 6} print(\u0026#34;方法二\u0026#34;) print(list_1\u0026amp;list_2) print(\u0026#34;--------------------------------\u0026#34;) 方法二 {4, 6} -------------------------------- # 取并集 print(\u0026#34;方法一\u0026#34;) print(list_1.union(list_2)) 方法一 {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 22} print(\u0026#34;方法二\u0026#34;) print(list_1|list_2) print(\u0026#34;--------------------------------\u0026#34;) 方法二 {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 22} -------------------------------- # 差集 in list_1 but not in list_2 print(list_1.difference(list_2)) print(list_1-list_2) print(\u0026#34;--------------------------------\u0026#34;) {1, 3, 5, 7, 9} {1, 3, 5, 7, 9} -------------------------------- # 子集 list_3=[1,4,6] list_4=[1,4,6,7] list_3=set(list_3) list_4=set(list_4) print(list_3.issubset(list_4)) True print(list_4.issuperset(list_3)) print(\u0026#34;--------------------------------\u0026#34;) True -------------------------------- # 对称差集 把list_1与list_2互相都没有的元素放在一块，其实就是去掉重复元素 print(list_1.symmetric_difference(list_2)) print(list_1^list_2) print(\u0026#34;--------------------------------\u0026#34;) {0, 1, 2, 3, 5, 7, 8, 9, 22} {0, 1, 2, 3, 5, 7, 8, 9, 22} -------------------------------- # 是否没有交集 Return True if two sets have a null intersection. list_5=set([1,2,3,4]) list_6=set([5,6,7]) print(list_5.isdisjoint(list_6)) print(\u0026#34;--------------------------------\u0026#34;) True -------------------------------- # 基本操作 # 添加一项 list_1.add(\u0026#39;x\u0026#39;) print(list_1) {1, 3, 4, 5, 6, 7, \u0026#39;x\u0026#39;, 9} # 添加多项 list_1.update([10,37,42]) print(list_1) {1, 3, 4, 5, 6, 7, \u0026#39;x\u0026#39;, 9, 10, 37, 42} # 删除一项 list_1.remove(10) print(list_1) {1, 3, 4, 5, 6, 7, \u0026#39;x\u0026#39;, 9, 37, 42} # set长度 print(len(list_1)) 10 # 测试9是否是list_1的成员 print(9 in list_1) True # pop()删除并且返回一个任意的元素 print(list_1.pop()) 1 # 删除一个指定的值 list_1.discard(\u0026#39;x\u0026#39;) print(list_1) {3, 4, 5, 6, 7, 9, 37, 42} 字符串操作 name=\u0026#34;alex\u0026#34; print(name.capitalize()) # 首字母大写 Alex print(name.count(\u0026#34;a\u0026#34;)) # 统计字母个数 1 print(name.center(50,\u0026#34;-\u0026#34;)) #总共打印50个字符，并把nam放在中间，不够的用-补上 -----------------------alex----------------------- print(name.endswith(\u0026#34;ex\u0026#34;)) # 判断字符串以什么结尾 True name=\u0026#34;alex \\tname is alex\u0026#34; print(name.expandtabs(tabsize=30)) # 将name中\\t转为30个空格 alex name is alex print(name.find(\u0026#34;x\u0026#34;)) # 取索引 3 print(name[name.find(\u0026#34;x\u0026#34;):]) # 字符串切片 x name is alex name=\u0026#34;my \\tname is {name} and i am {year} old\u0026#34; print(name.format(name=\u0026#34;alex\u0026#34;,year=23)) my name is alex and i am 23 old print(name.format_map({\u0026#39;name\u0026#39;:\u0026#39;alex\u0026#39;,\u0026#39;year\u0026#39;:23})) my name is alex and i am 23 old print(\u0026#39;ab123\u0026#39;.isalnum()) #isalnum()包含所有字母及数字，如果不是这两个，则为False True print(\u0026#39;ab123\u0026#39;.isalpha()) # False isalpha()包含纯英文字符 False print(\u0026#39;1A\u0026#39;.isdecimal()) # 是否是十进制 False False print(\u0026#39;1A\u0026#39;.isdigit()) # 是否是整数 False False print(\u0026#39;_\u0026#39;.isidentifier()) #判断是否是合法的标识符，实质是否为合法变量名 True True print(\u0026#39;aasd\u0026#39;.islower()) # 判断是否是小写 True True print(\u0026#39;\u0026#39;.isspace()) # 是否是空格 False False print(\u0026#39;My name is\u0026#39;.istitle()) # 字符串首字母大写为title，否则不是 False print(\u0026#39;+\u0026#39;.join([\u0026#39;1\u0026#39;,\u0026#39;2\u0026#39;,\u0026#39;3\u0026#39;])) # 对一列表中所有元素进行join操作 1+2+3 print(name.ljust(50,\u0026#39;*\u0026#39;)) # 左对齐字符串，多余位用*补全 my name is {name} and i am {year} old************ print(name.rjust(50,\u0026#39;-\u0026#39;)) # 右对齐字符串，多余位用*-补全 ------------my name is {name} and i am {year} old print(\u0026#39;\\n Alex\u0026#39;.lstrip()) # 去掉左边的空格/回车 Alex print(\u0026#39;\\nAlex\\n\u0026#39;.rstrip()) # 去掉右边的空格/回车 Alex print(\u0026#39;\\nAlex\\n\u0026#39;.strip()) # 去掉左边和右边的空格/回车 Alex print(\u0026#39;Alex\u0026#39;) Alex p=str.maketrans(\u0026#34;abcdef\u0026#34;,\u0026#34;123456\u0026#34;) print(\u0026#34;alex li\u0026#34;.translate(p)) #把alex li换成上一行对应的值 1l5x li print(\u0026#34;alex li\u0026#34;.replace(\u0026#39;l\u0026#39;,\u0026#39;L\u0026#39;,1)) # 替换 1表示替换几个l,从左到右计算替换个数 aLex li print(\u0026#34;alex li\u0026#34;.rfind(\u0026#39;l\u0026#39;)) # 找到的最右边的下标返回 5 print(\u0026#34;alex li\u0026#34;.split(\u0026#39;l\u0026#39;)) # 默认将字符串按照空格分隔成列表，也可以在()中填写相应的分隔符，比如以字符l分隔，print(\u0026#34;alex li\u0026#34;.split(‘l’)),而且分隔符在列表中不会出现 [\u0026#39;a\u0026#39;, \u0026#39;ex \u0026#39;, \u0026#39;i\u0026#39;] print(\u0026#34;1+2+3+4\u0026#34;.split(\u0026#39;+\u0026#39;)) # [\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;] [\u0026#39;1\u0026#39;, \u0026#39;2\u0026#39;, \u0026#39;3\u0026#39;, \u0026#39;4\u0026#39;] print(\u0026#34;1+2\\n+3+4\u0026#34;.splitlines()) # [\u0026#39;1+2\u0026#39;, \u0026#39;+3+4\u0026#39;] [\u0026#39;1+2\u0026#39;, \u0026#39;+3+4\u0026#39;] print(\u0026#34;Alex Li\u0026#34;.swapcase()) # aLEX lI aLEX lI print(\u0026#39;lex li\u0026#39;.title()) # Lex Li Lex Li print(\u0026#39;lex li\u0026#39;.zfill(50)) #不够以0填充 print(\u0026#39;---\u0026#39;) 00000000000000000000000000000000000000000000lex li --- 字典 # 字典无序 info={ \u0026#39;stu1101\u0026#39;:\u0026#34;tengxun\u0026#34;, \u0026#39;stu1102\u0026#39;:\u0026#34;baidu\u0026#34;, \u0026#39;stu1103\u0026#39;:\u0026#34;ali\u0026#34;, } print(info) {\u0026#39;stu1101\u0026#39;: \u0026#39;tengxun\u0026#39;, \u0026#39;stu1102\u0026#39;: \u0026#39;baidu\u0026#39;, \u0026#39;stu1103\u0026#39;: \u0026#39;ali\u0026#39;} # 0.查找 # 方法一:确定存在 print(info[\u0026#34;stu1101\u0026#34;]) # 查找若不在，则报错 tengxun print(info.get(\u0026#34;stu11004\u0026#34;)) # 查找不在不会报错，直接返回None，若有直接返回 None print(\u0026#39;stu1103\u0026#39; in info) # True True # 1.修改 info[\u0026#34;stu1101\u0026#34;]=\u0026#34;腾讯\u0026#34; print(info) {\u0026#39;stu1101\u0026#39;: \u0026#39;腾讯\u0026#39;, \u0026#39;stu1102\u0026#39;: \u0026#39;baidu\u0026#39;, \u0026#39;stu1103\u0026#39;: \u0026#39;ali\u0026#39;} # 2.增加 info[\u0026#34;stu1104\u0026#34;]=\u0026#34;zhubajie\u0026#34; print(info) {\u0026#39;stu1101\u0026#39;: \u0026#39;腾讯\u0026#39;, \u0026#39;stu1102\u0026#39;: \u0026#39;baidu\u0026#39;, \u0026#39;stu1103\u0026#39;: \u0026#39;ali\u0026#39;, \u0026#39;stu1104\u0026#39;: \u0026#39;zhubajie\u0026#39;} # 3.删除 # 方法一 del info[\u0026#34;stu1101\u0026#34;] print(info) {\u0026#39;stu1102\u0026#39;: \u0026#39;baidu\u0026#39;, \u0026#39;stu1103\u0026#39;: \u0026#39;ali\u0026#39;, \u0026#39;stu1104\u0026#39;: \u0026#39;zhubajie\u0026#39;} # 方法二 info.pop(\u0026#34;stu1102\u0026#34;) print(info) \u0026#39;\u0026#39;\u0026#39; # 随机删除 info.popitem() print(info) \u0026#39;\u0026#39;\u0026#39; {\u0026#39;stu1103\u0026#39;: \u0026#39;ali\u0026#39;, \u0026#39;stu1104\u0026#39;: \u0026#39;zhubajie\u0026#39;} \u0026#39;\\n# 随机删除\\ninfo.popitem()\\nprint(info)\\n\u0026#39; # 4.多级字典嵌套及操作 av_catalog = { \u0026#34;A\u0026#34;:{ \u0026#34;www.yo333.com\u0026#34;: [\u0026#34;aaa\u0026#34;,\u0026#34;111\u0026#34;], \u0026#34;www.po333.com\u0026#34;: [\u0026#34;bbb\u0026#34;,\u0026#34;222\u0026#34;], \u0026#34;333you.com\u0026#34;: [\u0026#34;ccc\u0026#34;,\u0026#34;333\u0026#34;], \u0026#34;333art.com\u0026#34;:[\u0026#34;ddd\u0026#34;,\u0026#34;444\u0026#34;] }, \u0026#34;B\u0026#34;:{ \u0026#34;tokyo-lot\u0026#34;:[\u0026#34;eee\u0026#34;,\u0026#34;555\u0026#34;] }, \u0026#34;C\u0026#34;:{ \u0026#34;1022\u0026#34;:[\u0026#34;fff\u0026#34;,\u0026#34;666\u0026#34;] } } b={ \u0026#39;stu1101\u0026#39;:\u0026#34;Alex\u0026#34;, 1:3, 2:5 } info.update(b) #将两个字典合并，存在key,则更新value，不存在key，则合并 print(info) {\u0026#39;stu1103\u0026#39;: \u0026#39;ali\u0026#39;, \u0026#39;stu1104\u0026#39;: \u0026#39;zhubajie\u0026#39;, \u0026#39;stu1101\u0026#39;: \u0026#39;Alex\u0026#39;, 1: 3, 2: 5} print(info.items()) #把一个字典转成列表 dict_items([(\u0026#39;stu1103\u0026#39;, \u0026#39;ali\u0026#39;), (\u0026#39;stu1104\u0026#39;, \u0026#39;zhubajie\u0026#39;), (\u0026#39;stu1101\u0026#39;, \u0026#39;Alex\u0026#39;), (1, 3), (2, 5)]) c=info.fromkeys([6,7,8],\u0026#34;test\u0026#34;) print(c) {6: \u0026#39;test\u0026#39;, 7: \u0026#39;test\u0026#39;, 8: \u0026#39;test\u0026#39;} c=info.fromkeys([6,7,8],[1,{\u0026#39;name\u0026#39;:\u0026#39;alex\u0026#39;},444]) print(c) {6: [1, {\u0026#39;name\u0026#39;: \u0026#39;alex\u0026#39;}, 444], 7: [1, {\u0026#39;name\u0026#39;: \u0026#39;alex\u0026#39;}, 444], 8: [1, {\u0026#39;name\u0026#39;: \u0026#39;alex\u0026#39;}, 444]} c[7][1][\u0026#39;name\u0026#39;]=\u0026#39;Jack Chen\u0026#39; # 3个key共用一个value,修改一个则所有的都修改了 print(c) {6: [1, {\u0026#39;name\u0026#39;: \u0026#39;Jack Chen\u0026#39;}, 444], 7: [1, {\u0026#39;name\u0026#39;: \u0026#39;Jack Chen\u0026#39;}, 444], 8: [1, {\u0026#39;name\u0026#39;: \u0026#39;Jack Chen\u0026#39;}, 444]} print(\u0026#34;--------\u0026#34;) av_catalog[\u0026#34;A\u0026#34;][\u0026#34;333you.com\u0026#34;][1]=\u0026#34;可以在国内做镜像\u0026#34; # 二级字典替换 av_catalog.setdefault(\u0026#34;taiwan\u0026#34;,{\u0026#34;www.baidu.com\u0026#34;:[1,2]}) # 如果不重名，即创建一个新的值，如果重名，将找到的值返回 print(av_catalog) -------- {\u0026#39;A\u0026#39;: {\u0026#39;www.yo333.com\u0026#39;: [\u0026#39;aaa\u0026#39;, \u0026#39;111\u0026#39;], \u0026#39;www.po333.com\u0026#39;: [\u0026#39;bbb\u0026#39;, \u0026#39;222\u0026#39;], \u0026#39;333you.com\u0026#39;: [\u0026#39;ccc\u0026#39;, \u0026#39;可以在国内做镜像\u0026#39;], \u0026#39;333art.com\u0026#39;: [\u0026#39;ddd\u0026#39;, \u0026#39;444\u0026#39;]}, \u0026#39;B\u0026#39;: {\u0026#39;tokyo-lot\u0026#39;: [\u0026#39;eee\u0026#39;, \u0026#39;555\u0026#39;]}, \u0026#39;C\u0026#39;: {\u0026#39;1022\u0026#39;: [\u0026#39;fff\u0026#39;, \u0026#39;666\u0026#39;]}, \u0026#39;taiwan\u0026#39;: {\u0026#39;www.baidu.com\u0026#39;: [1, 2]}} print(info.keys()) # 打印出所有的key dict_keys([\u0026#39;stu1103\u0026#39;, \u0026#39;stu1104\u0026#39;, \u0026#39;stu1101\u0026#39;, 1, 2]) print(info.values()) # 打印出所有的value dict_values([\u0026#39;ali\u0026#39;, \u0026#39;zhubajie\u0026#39;, \u0026#39;Alex\u0026#39;, 3, 5]) print(\u0026#34;---------------\u0026#34;) for i in info: print(i,info[i]) #效率更高点 print(\u0026#34;---------------\u0026#34;) --------------- stu1103 ali stu1104 zhubajie stu1101 Alex 1 3 2 5 --------------- for k,v in info.items(): print(k,v) stu1103 ali stu1104 zhubajie stu1101 Alex 1 3 2 5 函数 # 1.无参函数 # 定义一个函数 def fun1(): \u0026#39;\u0026#39;\u0026#39;testing\u0026#39;\u0026#39;\u0026#39; print(\u0026#39;in the fun1\u0026#39;) return 1 # 定义一个过程 实质就是无返回值的函数 def fun2(): \u0026#39;\u0026#39;\u0026#39;testing2\u0026#39;\u0026#39;\u0026#39; print(\u0026#39;in the fun2\u0026#39;) x=fun1() y=fun2() print(x) print(y) # 没有返回值得情况下，python隐式地返回一个None in the fun1 in the fun2 1 None import time def logger(): time_format=\u0026#39;%Y-%m-%d %X %A %B %p %I\u0026#39; time_current=time.strftime(time_format) with open(\u0026#39;a.txt\u0026#39;,\u0026#39;a+\u0026#39;)as f: f.write(\u0026#39;time %s end action\\n\u0026#39;%time_current) def test1(): print(\u0026#39;in the test1\u0026#39;) logger() def test2(): print(\u0026#39;in the test2\u0026#39;) logger() return 0 def test3(): print(\u0026#39;in the test3\u0026#39;) logger() return 1,{5:\u0026#34;sda\u0026#34;,6:\u0026#34;zad\u0026#34;},[1,2,3] x=test1() y=test2() z=test3() in the test1 in the test2 in the test3 print(x) # None print(y) # 0 print(z) # (1, {5: \u0026#39;sda\u0026#39;, 6: \u0026#39;zad\u0026#39;}, [1, 2, 3]) \u0026#39;\u0026#39;\u0026#39; 总结： 返回值数=0:返回None 返回值数=1:返回object 返回值数\u0026gt;1:返回tuple \u0026#39;\u0026#39;\u0026#39; None 0 (1, {5: \u0026#39;sda\u0026#39;, 6: \u0026#39;zad\u0026#39;}, [1, 2, 3]) \u0026#39;\\n总结：\\n 返回值数=0:返回None\\n 返回值数=1:返回object\\n 返回值数\u0026gt;1:返回tuple\\n\u0026#39; # 2.有参函数 # 默认参数特点：调用函数的时候，默认参数非必须传递 # 用途：1.默认安装值 def test(x,y): print(x) print(y) test(1,2) # 位置参数调用 与形参意义对应 test(y=1,x=2) # 关键字调用，与形参顺序无关 test(3,y=2) # 如果既有关键字调用又有位置参数，前面一个一定是位置参数，一句话：关键参数一定不能写在位置参数前面 1 2 2 1 3 2 \u0026#39;\u0026#39;\u0026#39; 比如加入一个参数z \u0026#39;\u0026#39;\u0026#39; def test1(x,y,z): print(x) print(y) print(z) # 关键参数一定不能放在位置参数前面 test1(3,4,z=6) test1(3,z=6,y=4) 3 4 6 3 4 6 # 默认参数, def test(x,y,z=2): print(x) print(y) print(z) test(1,2) 1 2 2 # 用*args传递多个参数，转换成元组的方式 *表示一个功能代号，表示接受的参数不固定，args可以随意起名 def test(*args): print(args) test(1,3,4,5,5,6) test(*[1,3,4,5,5,6]) # args=tuple([1,2,3,4,5]) def test(x,*args): print(x) print(args) test(1,2,3,4,5,6,7) # 1 (2,3,4,5,6,7) (1, 3, 4, 5, 5, 6) (1, 3, 4, 5, 5, 6) 1 (2, 3, 4, 5, 6, 7) # 字典传值 **kwagrs:把N个关键字参数，转换成字典的方式 def test(**kwargs): print(kwargs) print(kwargs[\u0026#39;name\u0026#39;],kwargs[\u0026#39;age\u0026#39;],kwargs[\u0026#39;id\u0026#39;],kwargs[\u0026#39;sex\u0026#39;]) test(name=\u0026#39;alex\u0026#39;,age=8,id=10,sex=\u0026#39;M\u0026#39;) # {\u0026#39;name\u0026#39;: \u0026#39;alex\u0026#39;, \u0026#39;age\u0026#39;: 8, \u0026#39;id\u0026#39;: 10, \u0026#39;sex\u0026#39;: \u0026#39;M\u0026#39;} {\u0026#39;name\u0026#39;: \u0026#39;alex\u0026#39;, \u0026#39;age\u0026#39;: 8, \u0026#39;id\u0026#39;: 10, \u0026#39;sex\u0026#39;: \u0026#39;M\u0026#39;} alex 8 10 M test(**{\u0026#39;name\u0026#39;:\u0026#39;alex\u0026#39;,\u0026#39;age\u0026#39;:8,\u0026#39;id\u0026#39;:10,\u0026#39;sex\u0026#39;:\u0026#39;M\u0026#39;}) {\u0026#39;name\u0026#39;: \u0026#39;alex\u0026#39;, \u0026#39;age\u0026#39;: 8, \u0026#39;id\u0026#39;: 10, \u0026#39;sex\u0026#39;: \u0026#39;M\u0026#39;} alex 8 10 M def test(name,**kwargs): print(name) print(kwargs) test(\u0026#39;alex\u0026#39;,age=18,sex=\u0026#39;M\u0026#39;) # 字典 {\u0026#39;age\u0026#39;: 18, \u0026#39;sex\u0026#39;: \u0026#39;M\u0026#39;} alex {\u0026#39;age\u0026#39;: 18, \u0026#39;sex\u0026#39;: \u0026#39;M\u0026#39;} # 默认参数得放在参数组前面 def test(name,age=18,**kwargs): print(name) print(age) print(kwargs) test(\u0026#39;alex\u0026#39;,sex=\u0026#39;M\u0026#39;,hobby=\u0026#39;tesla\u0026#39;,age=3) alex 3 {\u0026#39;sex\u0026#39;: \u0026#39;M\u0026#39;, \u0026#39;hobby\u0026#39;: \u0026#39;tesla\u0026#39;} test(\u0026#39;alex\u0026#39;,3,sex=\u0026#39;M\u0026#39;,hobby=\u0026#39;tesla\u0026#39;) alex 3 {\u0026#39;sex\u0026#39;: \u0026#39;M\u0026#39;, \u0026#39;hobby\u0026#39;: \u0026#39;tesla\u0026#39;} test(\u0026#39;alex\u0026#39;) # 后面的**kwargs不赋值输出为空字典 alex 18 {} def test(name,age=18,*args,**kwargs): print(name) print(age) print(args) print(kwargs) test(\u0026#39;alex\u0026#39;,age=34,sex=\u0026#39;M\u0026#39;,hobby=\u0026#39;tesla\u0026#39;) # alex 34 () {\u0026#39;sex\u0026#39;: \u0026#39;M\u0026#39;, \u0026#39;hobby\u0026#39;: \u0026#39;tesla\u0026#39;} alex 34 () {\u0026#39;sex\u0026#39;: \u0026#39;M\u0026#39;, \u0026#39;hobby\u0026#39;: \u0026#39;tesla\u0026#39;} 高阶函数 # 高阶函数 变量可以指向函数，函数的参数能接受变量，那么一个函数就可以接受另一个函数作为参数，这个函数就叫做高阶函数 def f(x): return x def add(x,y,f): return f(x)+f(y) res=add(1,2,f) print(res) # 3 3 ","date":"2021-03-04T00:00:00Z","permalink":"https://example.com/p/python%E5%9F%BA%E7%A1%80_2/","title":"python基础_2"},{"content":" 第一个学的python的包就是matplotlib，特地整理了去年的学习笔记\nD图形绘制需要（x,y,z)三组值，下面通过numpy和Axes3D函数会议3D图形。\n其中Axes3D是mpl_toolkits.mplot3d中的一个绘图函数，mpl_toolkits.mplot3d\n是Matplotlib里面专门用来画三维图的工具包\n#导入 # from mpl_toolkits.mplot3d import * from mpl_toolkits.mplot3d import Axes3D import numpy as np import matplotlib.pyplot as plt #建立画布，生成数据 fig = plt.figure() ax = Axes3D(fig) x = np.arange(-8,8,0.25) y = np.arange(-8,8,0.25) #生成x、y轴数据 x,y = np.meshgrid(x,y) r = np.sqrt(x**2+y**2) #生成z值 z = np.sin(r)/r #绘图 ax.plot_surface(x,y,z,rstride=1,cstride=1) ax.contourf(x,y,z,zdir=\u0026#34;z\u0026#34;,offset=-2) plt.show() 折线图 code:\nimport matplotlib as mpl from mpl_toolkits.mplot3d import Axes3D import numpy as np import matplotlib.pyplot as plt mpl.rcParams[\u0026#34;legend.fontsize\u0026#34;] = 10 fig = plt.figure() ax = fig.gca(projection=\u0026#34;3d\u0026#34;) theta = np.linspace(-4*np.pi,4*np.pi,100) z = np.linspace(-2,2,100) r = z**2+1 x = r*np.sin(theta) y = r*np.cos(theta) ax.plot(x,y,z,label=\u0026#34;paramtric curve\u0026#34;) ax.legend() plt.show() 散点图 from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt import numpy as np def randrange(n,vmin,vmax): return (vmax - vmin) * np.random.rand(n) + vmin fig = plt.figure() ax = fig.add_subplot(111,projection=\u0026#34;3d\u0026#34;) n = 100 for c,m,zlow,zhigh in [(\u0026#34;r\u0026#34;,\u0026#34;o\u0026#34;,-50,-25),(\u0026#34;b\u0026#34;,\u0026#34;^\u0026#34;,-30,-5)]: xs = randrange(n,23,32) ys = randrange(n,0,100) zs = randrange(n,zlow,zhigh) ax.scatter(xs,ys,zs,c=c,marker=m) ax.set_xlabel(\u0026#34;X Label\u0026#34;) ax.set_ylabel(\u0026#34;Y Label\u0026#34;) ax.set_zlabel(\u0026#34;Z Label\u0026#34;) plt.show() 线框图 from mpl_toolkits.mplot3d import axes3d import matplotlib.pyplot as plt fig = plt.figure() ax = fig.add_subplot(111,projection=\u0026#34;3d\u0026#34;) X,Y,Z = axes3d.get_test_data(0.05) ax.plot_wireframe(X,Y,Z,rstride=10,cstride=10) plt.show() 表面图 from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt from matplotlib import cm from matplotlib.ticker import LinearLocator,FormatStrFormatter fig = plt.figure() ax = fig.gca(projection=\u0026#34;3d\u0026#34;) X = np.arange(-5, 5, 0.25) Y = np.arange(-5, 5, 0.25) X, Y = np.meshgrid(X, Y) R = np.sqrt(X ** 2 + Y ** 2) Z = np.sin(R) # Plot the surface. surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm, linewidth=0, antialiased=False) # Customize the z axis. ax.set_zlim(-1.01, 1.01) ax.zaxis.set_major_locator(LinearLocator(10)) ax.zaxis.set_major_formatter(FormatStrFormatter(\u0026#39;%.02f\u0026#39;)) # Add a color bar which maps values to colors. fig.colorbar(surf, shrink=0.5, aspect=5) plt.show() 柱状图 from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt import numpy as np fig = plt.figure(figsize=(10,10)) ax = fig.add_subplot(111, projection=\u0026#39;3d\u0026#39;) for c, z in zip([\u0026#39;r\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;y\u0026#39;], [30, 20, 10, 0]): xs = np.arange(20) ys = np.random.rand(20) # You can provide either a single color or an array. To demonstrate this, # the first bar of each set will be colored cyan. cs = [c] * len(xs) cs[0] = \u0026#39;c\u0026#39; ax.bar(xs, ys, zs=z, zdir=\u0026#39;y\u0026#39;, color=cs, alpha=0.8) ax.set_xlabel(\u0026#39;X\u0026#39;) ax.set_ylabel(\u0026#39;Y\u0026#39;) ax.set_zlabel(\u0026#39;Z\u0026#39;) plt.show() 箭头图 from mpl_toolkits.mplot3d import axes3d import matplotlib.pyplot as plt import numpy as np fig = plt.figure(figsize=(10,10)) ax = fig.gca(projection=\u0026#39;3d\u0026#39;) # Make the grid x, y, z = np.meshgrid(np.arange(-0.8, 1, 0.2), np.arange(-0.8, 1, 0.2), np.arange(-0.8, 1, 0.8)) # Make the direction data for the arrows u = np.sin(np.pi * x) * np.cos(np.pi * y) * np.cos(np.pi * z) v = -np.cos(np.pi * x) * np.sin(np.pi * y) * np.cos(np.pi * z) w = (np.sqrt(2.0 / 3.0) * np.cos(np.pi * x) * np.cos(np.pi * y) * np.sin(np.pi * z)) ax.quiver(x, y, z, u, v, w, length=0.1, normalize=True) plt.show() 2D转3D图 from mpl_toolkits.mplot3d import Axes3D import numpy as np import matplotlib.pyplot as plt fig = plt.figure(figsize=(10,10)) ax = fig.gca(projection=\u0026#39;3d\u0026#39;) # Plot a sin curve using the x and y axes. x = np.linspace(0, 1, 100) y = np.sin(x * 2 * np.pi) / 2 + 0.5 ax.plot(x, y, zs=0, zdir=\u0026#39;z\u0026#39;, label=\u0026#39;curve in (x,y)\u0026#39;) # Plot scatterplot data (20 2D points per colour) on the x and z axes. colors = (\u0026#39;r\u0026#39;, \u0026#39;g\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;k\u0026#39;) x = np.random.sample(20 * len(colors)) y = np.random.sample(20 * len(colors)) labels = np.random.randint(3, size=80) # By using zdir=\u0026#39;y\u0026#39;, the y value of these points is fixed to the zs value 0 # and the (x,y) points are plotted on the x and z axes. ax.scatter(x, y, zs=0, zdir=\u0026#39;y\u0026#39;, c=labels, label=\u0026#39;points in (x,z)\u0026#39;) # Make legend, set axes limits and labels ax.legend() ax.set_xlim(0, 1) ax.set_ylim(0, 1) ax.set_zlim(0, 1) ax.set_xlabel(\u0026#39;X\u0026#39;) ax.set_ylabel(\u0026#39;Y\u0026#39;) ax.set_zlabel(\u0026#39;Z\u0026#39;) # Customize the view angle so it\u0026#39;s easier to see that the scatter points lie # on the plane y=0 ax.view_init(elev=20., azim=-35) plt.show() 文本图 from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt fig = plt.figure(figsize=(10,10)) ax = fig.gca(projection=\u0026#39;3d\u0026#39;) # Demo 1: zdir zdirs = (None, \u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;, \u0026#39;z\u0026#39;, (1, 1, 0), (1, 1, 1)) xs = (1, 4, 4, 9, 4, 1) ys = (2, 5, 8, 10, 1, 2) zs = (10, 3, 8, 9, 1, 8) for zdir, x, y, z in zip(zdirs, xs, ys, zs): label = \u0026#39;(%d, %d, %d), dir=%s\u0026#39; % (x, y, z, zdir) ax.text(x, y, z, label, zdir) # Demo 2: color ax.text(9, 0, 0, \u0026#34;red\u0026#34;, color=\u0026#39;red\u0026#39;) # Demo 3: text2D # Placement 0, 0 would be the bottom left, 1, 1 would be the top right. ax.text2D(0.05, 0.95, \u0026#34;2D Text\u0026#34;, transform=ax.transAxes) # Tweaking display region and labels ax.set_xlim(0, 10) ax.set_ylim(0, 10) ax.set_zlim(0, 10) ax.set_xlabel(\u0026#39;X axis\u0026#39;) ax.set_ylabel(\u0026#39;Y axis\u0026#39;) ax.set_zlabel(\u0026#39;Z axis\u0026#39;) plt.show() 3D拼图 import matplotlib.pyplot as plt from mpl_toolkits.mplot3d.axes3d import Axes3D, get_test_data from matplotlib import cm import numpy as np # set up a figure twice as wide as it is tall fig = plt.figure(figsize=plt.figaspect(0.5)) # =============== # First subplot # =============== # set up the axes for the first plot ax = fig.add_subplot(1, 2, 1, projection=\u0026#39;3d\u0026#39;) # plot a 3D surface like in the example mplot3d/surface3d_demo X = np.arange(-5, 5, 0.25) Y = np.arange(-5, 5, 0.25) X, Y = np.meshgrid(X, Y) R = np.sqrt(X ** 2 + Y ** 2) Z = np.sin(R) surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.coolwarm, linewidth=0, antialiased=False) ax.set_zlim(-1.01, 1.01) fig.colorbar(surf, shrink=0.5, aspect=10) # =============== # Second subplot # =============== # set up the axes for the second plot ax = fig.add_subplot(1, 2, 2, projection=\u0026#39;3d\u0026#39;) # plot a 3D wireframe like in the example mplot3d/wire3d_demo X, Y, Z = get_test_data(0.05) ax.plot_wireframe(X, Y, Z, rstride=10, cstride=10) plt.show() ","date":"2021-01-10T00:00:00Z","permalink":"https://example.com/p/python%E7%BB%98%E5%88%B63d%E5%9B%BE%E5%BD%A2axes3d%E6%95%B0%E6%8D%AE%E8%AF%BB%E5%86%99/","title":"Python绘制3D图形：Axes3D数据读写"}]